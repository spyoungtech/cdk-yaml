from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_codebuild.Artifacts
class ArtifactsDef(BaseClass):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The artifact identifier. This property is required on secondary artifacts.')
    _init_params: typing.ClassVar[list[str]] = ['identifier']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['s3']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.Artifacts'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['s3']
    ...


    s3: typing.Optional[models.aws_codebuild.ArtifactsDefS3Params] = pydantic.Field(None, description='')
    resource_config: typing.Optional[models.aws_codebuild.ArtifactsDefConfig] = pydantic.Field(None)


class ArtifactsDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_codebuild.ArtifactsDefBindParams]] = pydantic.Field(None, description='Callback when an Artifacts class is used in a CodeBuild Project.')

class ArtifactsDefBindParams(pydantic.BaseModel):
    ...

class ArtifactsDefS3Params(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The name of the output bucket.')
    encryption: typing.Optional[bool] = pydantic.Field(None, description='If this is false, build output will not be encrypted. This is useful if the artifact to publish a static website or sharing content with others Default: true - output will be encrypted\n')
    include_build_id: typing.Optional[bool] = pydantic.Field(None, description='Indicates if the build ID should be included in the path. If this is set to true, then the build artifact will be stored in "//". Default: true\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the build output ZIP file or folder inside the bucket. The full S3 object key will be "//" or "/" depending on whether ``includeBuildId`` is set to true. If not set, ``overrideArtifactName`` will be set and the name from the buildspec will be used instead. Default: undefined, and use the name from the buildspec\n')
    package_zip: typing.Optional[bool] = pydantic.Field(None, description='If this is true, all build output will be packaged into a single .zip file. Otherwise, all files will be uploaded to /. Default: true - files will be archived\n')
    path: typing.Optional[str] = pydantic.Field(None, description='The path inside of the bucket for the build output .zip file or folder. If a value is not specified, then build output will be stored at the root of the bucket (or under the directory if ``includeBuildId`` is set to true). Default: the root of the bucket\n')
    identifier: typing.Optional[str] = pydantic.Field(None, description='The artifact identifier. This property is required on secondary artifacts.')
    ...


#  autogenerated from aws_cdk.aws_codebuild.BuildSpec
class BuildSpecDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_object', 'from_object_to_yaml', 'from_source_filename']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BuildSpec'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_object', 'from_object_to_yaml', 'from_source_filename']
    ...


    from_asset: typing.Optional[models.aws_codebuild.BuildSpecDefFromAssetParams] = pydantic.Field(None, description='Use the contents of a local file as the build spec string.\nUse this if you have a local .yml or .json file that you want to use as the buildspec')
    from_object: typing.Optional[models.aws_codebuild.BuildSpecDefFromObjectParams] = pydantic.Field(None, description='')
    from_object_to_yaml: typing.Optional[models.aws_codebuild.BuildSpecDefFromObjectToYamlParams] = pydantic.Field(None, description='Create a buildspec from an object that will be rendered as YAML in the resulting CloudFormation template.')
    from_source_filename: typing.Optional[models.aws_codebuild.BuildSpecDefFromSourceFilenameParams] = pydantic.Field(None, description="Use a file from the source as buildspec.\nUse this if you want to use a file different from 'buildspec.yml'`")

class BuildSpecDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='-')
    ...

class BuildSpecDefFromObjectParams(pydantic.BaseModel):
    value: typing.Mapping[str, typing.Any] = pydantic.Field(..., description='-')
    ...

class BuildSpecDefFromObjectToYamlParams(pydantic.BaseModel):
    value: typing.Mapping[str, typing.Any] = pydantic.Field(..., description='the object containing the buildspec that will be rendered as YAML.')
    ...

class BuildSpecDefFromSourceFilenameParams(pydantic.BaseModel):
    filename: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_codebuild.Cache
class CacheDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['bucket', 'local', 'none']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.Cache'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.CacheDefConfig] = pydantic.Field(None)


class CacheDefConfig(pydantic.BaseModel):
    bucket: typing.Optional[list[models.aws_codebuild.CacheDefBucketParams]] = pydantic.Field(None, description='Create an S3 caching strategy.')
    local: typing.Optional[list[models.aws_codebuild.CacheDefLocalParams]] = pydantic.Field(None, description='Create a local caching strategy.')
    none: typing.Optional[list[models.aws_codebuild.CacheDefNoneParams]] = pydantic.Field(None, description='')

class CacheDefBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='the S3 bucket to use for caching.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix to use to store the cache in the bucket.')
    return_config: typing.Optional[list[models.aws_codebuild.CacheDefConfig]] = pydantic.Field(None)
    ...

class CacheDefLocalParams(pydantic.BaseModel):
    modes: list[aws_cdk.aws_codebuild.LocalCacheMode] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_codebuild.CacheDefConfig]] = pydantic.Field(None)
    ...

class CacheDefNoneParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_codebuild.CacheDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codebuild.FileSystemLocation
class FileSystemLocationDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['efs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.FileSystemLocation'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['efs']
    ...


    efs: typing.Optional[models.aws_codebuild.FileSystemLocationDefEfsParams] = pydantic.Field(None, description='EFS file system provider.')

class FileSystemLocationDefEfsParams(pydantic.BaseModel):
    identifier: str = pydantic.Field(..., description='The name used to access a file system created by Amazon EFS.\n')
    location: str = pydantic.Field(..., description='A string that specifies the location of the file system, like Amazon EFS. This value looks like ``fs-abcd1234.efs.us-west-2.amazonaws.com:/my-efs-mount-directory``.\n')
    mount_point: str = pydantic.Field(..., description='The location in the container where you mount the file system.\n')
    mount_options: typing.Optional[str] = pydantic.Field(None, description="The mount options for a file system such as Amazon EFS. Default: 'nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2'.")
    ...


#  autogenerated from aws_cdk.aws_codebuild.FilterGroup
class FilterGroupDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['and_actor_account_is', 'and_actor_account_is_not', 'and_base_branch_is', 'and_base_branch_is_not', 'and_base_ref_is', 'and_base_ref_is_not', 'and_branch_is', 'and_branch_is_not', 'and_commit_message_is', 'and_commit_message_is_not', 'and_file_path_is', 'and_file_path_is_not', 'and_head_ref_is', 'and_head_ref_is_not', 'and_tag_is', 'and_tag_is_not']
    _classmethod_names: typing.ClassVar[list[str]] = ['in_event_of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.FilterGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.FilterGroupDefConfig] = pydantic.Field(None)


class FilterGroupDefConfig(pydantic.BaseModel):
    and_actor_account_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndActorAccountIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the account ID of the actor initiating the event must match the given pattern.')
    and_actor_account_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndActorAccountIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the account ID of the actor initiating the event must not match the given pattern.')
    and_base_branch_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndBaseBranchIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the Pull Request that is the source of the event must target the given base branch.\nNote that you cannot use this method if this Group contains the ``PUSH`` event action.')
    and_base_branch_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndBaseBranchIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the Pull Request that is the source of the event must not target the given base branch.\nNote that you cannot use this method if this Group contains the ``PUSH`` event action.')
    and_base_ref_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndBaseRefIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the Pull Request that is the source of the event must target the given Git reference.\nNote that you cannot use this method if this Group contains the ``PUSH`` event action.')
    and_base_ref_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndBaseRefIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the Pull Request that is the source of the event must not target the given Git reference.\nNote that you cannot use this method if this Group contains the ``PUSH`` event action.')
    and_branch_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndBranchIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must affect the given branch.')
    and_branch_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndBranchIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must not affect the given branch.')
    and_commit_message_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndCommitMessageIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must affect a head commit with the given message.')
    and_commit_message_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndCommitMessageIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must not affect a head commit with the given message.')
    and_file_path_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndFilePathIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the push that is the source of the event must affect a file that matches the given pattern.\nNote that you can only use this method if this Group contains only the ``PUSH`` event action,\nand only for GitHub, Bitbucket and GitHubEnterprise sources.')
    and_file_path_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndFilePathIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the push that is the source of the event must not affect a file that matches the given pattern.\nNote that you can only use this method if this Group contains only the ``PUSH`` event action,\nand only for GitHub, Bitbucket and GitHubEnterprise sources.')
    and_head_ref_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndHeadRefIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must affect a Git reference (ie., a branch or a tag) that matches the given pattern.')
    and_head_ref_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndHeadRefIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must not affect a Git reference (ie., a branch or a tag) that matches the given pattern.')
    and_tag_is: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndTagIsParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must affect the given tag.')
    and_tag_is_not: typing.Optional[list[models.aws_codebuild.FilterGroupDefAndTagIsNotParams]] = pydantic.Field(None, description='Create a new FilterGroup with an added condition: the event must not affect the given tag.')
    in_event_of: typing.Optional[list[models.aws_codebuild.FilterGroupDefInEventOfParams]] = pydantic.Field(None, description='Creates a new event FilterGroup that triggers on any of the provided actions.')

class FilterGroupDefAndActorAccountIsParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndActorAccountIsNotParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndBaseBranchIsParams(pydantic.BaseModel):
    branch_name: str = pydantic.Field(..., description='the name of the branch (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndBaseBranchIsNotParams(pydantic.BaseModel):
    branch_name: str = pydantic.Field(..., description='the name of the branch (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndBaseRefIsParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndBaseRefIsNotParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndBranchIsParams(pydantic.BaseModel):
    branch_name: str = pydantic.Field(..., description='the name of the branch (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndBranchIsNotParams(pydantic.BaseModel):
    branch_name: str = pydantic.Field(..., description='the name of the branch (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndCommitMessageIsParams(pydantic.BaseModel):
    commit_message: str = pydantic.Field(..., description='the commit message (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndCommitMessageIsNotParams(pydantic.BaseModel):
    commit_message: str = pydantic.Field(..., description='the commit message (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndFilePathIsParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndFilePathIsNotParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndHeadRefIsParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndHeadRefIsNotParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='a regular expression.')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndTagIsParams(pydantic.BaseModel):
    tag_name: str = pydantic.Field(..., description='the name of the tag (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefAndTagIsNotParams(pydantic.BaseModel):
    tag_name: str = pydantic.Field(..., description='the name of the tag (can be a regular expression).')
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...

class FilterGroupDefInEventOfParams(pydantic.BaseModel):
    actions: list[aws_cdk.aws_codebuild.EventAction] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_codebuild.FilterGroupDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codebuild.LinuxArmBuildImage
class LinuxArmBuildImageDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['run_script_buildspec', 'validate']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_code_build_image_id', 'from_docker_registry', 'from_ecr_repository']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.LinuxArmBuildImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_code_build_image_id', 'from_docker_registry', 'from_ecr_repository']
    ...


    from_code_build_image_id: typing.Optional[models.aws_codebuild.LinuxArmBuildImageDefFromCodeBuildImageIdParams] = pydantic.Field(None, description='Uses a Docker image provided by CodeBuild.')
    from_docker_registry: typing.Optional[models.aws_codebuild.LinuxArmBuildImageDefFromDockerRegistryParams] = pydantic.Field(None, description='')
    from_ecr_repository: typing.Optional[models.aws_codebuild.LinuxArmBuildImageDefFromEcrRepositoryParams] = pydantic.Field(None, description="Returns an ARM image running Linux from an ECR repository.\nNOTE: if the repository is external (i.e. imported), then we won't be able to add\na resource policy statement for it so CodeBuild can pull the image.")
    resource_config: typing.Optional[models.aws_codebuild.LinuxArmBuildImageDefConfig] = pydantic.Field(None)


class LinuxArmBuildImageDefConfig(pydantic.BaseModel):
    run_script_buildspec: typing.Optional[list[models.aws_codebuild.LinuxArmBuildImageDefRunScriptBuildspecParams]] = pydantic.Field(None, description='Make a buildspec to run the indicated script.')
    validate_: typing.Optional[list[models.aws_codebuild.LinuxArmBuildImageDefValidateParams]] = pydantic.Field(None, description='Validates by checking the BuildEnvironment computeType as aarch64 images only support ComputeType.SMALL and ComputeType.LARGE.', alias='validate')

class LinuxArmBuildImageDefFromCodeBuildImageIdParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The image identifier.\n')
    ...

class LinuxArmBuildImageDefFromDockerRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-')
    secrets_manager_credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The credentials, stored in Secrets Manager, used for accessing the repository holding the image, if the repository is private. Default: no credentials will be used (we assume the repository is public)\n')
    ...

class LinuxArmBuildImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='The ECR repository.\n')
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description='Image tag or digest (default "latest", digests must start with ``sha256:``).\n')
    ...

class LinuxArmBuildImageDefRunScriptBuildspecParams(pydantic.BaseModel):
    entrypoint: str = pydantic.Field(..., description='-')
    ...

class LinuxArmBuildImageDefValidateParams(pydantic.BaseModel):
    build_image: typing.Optional[typing.Union[models.aws_codebuild.LinuxArmBuildImageDef, models.aws_codebuild.LinuxBuildImageDef, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None, description='The image used for the builds. Default: LinuxBuildImage.STANDARD_1_0\n')
    certificate: typing.Union[models.aws_codebuild.BuildEnvironmentCertificateDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the PEM-encoded certificate for the build project. Default: - No external certificate is added to the project\n')
    compute_type: typing.Optional[aws_cdk.aws_codebuild.ComputeType] = pydantic.Field(None, description='The type of compute to use for this build. See the ``ComputeType`` enum for the possible values. Default: taken from ``#buildImage#defaultComputeType``\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables that your builds can use.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Indicates how the project builds Docker images. Specify true to enable running the Docker daemon inside a Docker container. This value must be set to true only if this build project will be used to build Docker images, and the specified build environment image is not one provided by AWS CodeBuild with Docker support. Otherwise, all associated builds that attempt to interact with the Docker daemon will fail. Default: false')
    ...


#  autogenerated from aws_cdk.aws_codebuild.LinuxBuildImage
class LinuxBuildImageDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['run_script_buildspec', 'validate']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_code_build_image_id', 'from_docker_registry', 'from_ecr_repository']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.LinuxBuildImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_code_build_image_id', 'from_docker_registry', 'from_ecr_repository']
    ...


    from_asset: typing.Optional[models.aws_codebuild.LinuxBuildImageDefFromAssetParams] = pydantic.Field(None, description='Uses an Docker image asset as a x86-64 Linux build image.')
    from_code_build_image_id: typing.Optional[models.aws_codebuild.LinuxBuildImageDefFromCodeBuildImageIdParams] = pydantic.Field(None, description='Uses a Docker image provided by CodeBuild.')
    from_docker_registry: typing.Optional[models.aws_codebuild.LinuxBuildImageDefFromDockerRegistryParams] = pydantic.Field(None, description='')
    from_ecr_repository: typing.Optional[models.aws_codebuild.LinuxBuildImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='')
    resource_config: typing.Optional[models.aws_codebuild.LinuxBuildImageDefConfig] = pydantic.Field(None)


class LinuxBuildImageDefConfig(pydantic.BaseModel):
    run_script_buildspec: typing.Optional[list[models.aws_codebuild.LinuxBuildImageDefRunScriptBuildspecParams]] = pydantic.Field(None, description='Make a buildspec to run the indicated script.')
    validate_: typing.Optional[list[models.aws_codebuild.LinuxBuildImageDefValidateParams]] = pydantic.Field(None, description='Allows the image a chance to validate whether the passed configuration is correct.', alias='validate')

class LinuxBuildImageDefFromAssetParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    directory: str = pydantic.Field(..., description='The directory where the Dockerfile is stored. Any directory inside with a name that matches the CDK output folder (cdk.out by default) will be excluded from the asset\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class LinuxBuildImageDefFromCodeBuildImageIdParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The image identifier.\n')
    ...

class LinuxBuildImageDefFromDockerRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-')
    secrets_manager_credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The credentials, stored in Secrets Manager, used for accessing the repository holding the image, if the repository is private. Default: no credentials will be used (we assume the repository is public)\n')
    ...

class LinuxBuildImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='The ECR repository.')
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description='Image tag or digest (default "latest", digests must start with ``sha256:``).\n')
    ...

class LinuxBuildImageDefRunScriptBuildspecParams(pydantic.BaseModel):
    entrypoint: str = pydantic.Field(..., description='-')
    ...

class LinuxBuildImageDefValidateParams(pydantic.BaseModel):
    build_image: typing.Optional[typing.Union[models.aws_codebuild.LinuxArmBuildImageDef, models.aws_codebuild.LinuxBuildImageDef, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None, description='The image used for the builds. Default: LinuxBuildImage.STANDARD_1_0\n')
    certificate: typing.Union[models.aws_codebuild.BuildEnvironmentCertificateDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the PEM-encoded certificate for the build project. Default: - No external certificate is added to the project\n')
    compute_type: typing.Optional[aws_cdk.aws_codebuild.ComputeType] = pydantic.Field(None, description='The type of compute to use for this build. See the ``ComputeType`` enum for the possible values. Default: taken from ``#buildImage#defaultComputeType``\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables that your builds can use.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Indicates how the project builds Docker images. Specify true to enable running the Docker daemon inside a Docker container. This value must be set to true only if this build project will be used to build Docker images, and the specified build environment image is not one provided by AWS CodeBuild with Docker support. Otherwise, all associated builds that attempt to interact with the Docker daemon will fail. Default: false')
    ...


#  autogenerated from aws_cdk.aws_codebuild.LinuxGpuBuildImage
class LinuxGpuBuildImageDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind', 'run_script_buildspec', 'validate']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_deep_learning_containers_image', 'from_ecr_repository']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.LinuxGpuBuildImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['aws_deep_learning_containers_image', 'from_ecr_repository']
    ...


    aws_deep_learning_containers_image: typing.Optional[models.aws_codebuild.LinuxGpuBuildImageDefAwsDeepLearningContainersImageParams] = pydantic.Field(None, description='Returns a Linux GPU build image from AWS Deep Learning Containers.')
    from_ecr_repository: typing.Optional[models.aws_codebuild.LinuxGpuBuildImageDefFromEcrRepositoryParams] = pydantic.Field(None, description="Returns a GPU image running Linux from an ECR repository.\nNOTE: if the repository is external (i.e. imported), then we won't be able to add\na resource policy statement for it so CodeBuild can pull the image.")
    resource_config: typing.Optional[models.aws_codebuild.LinuxGpuBuildImageDefConfig] = pydantic.Field(None)


class LinuxGpuBuildImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_codebuild.LinuxGpuBuildImageDefBindParams]] = pydantic.Field(None, description='Function that allows the build image access to the construct tree.')
    run_script_buildspec: typing.Optional[list[models.aws_codebuild.LinuxGpuBuildImageDefRunScriptBuildspecParams]] = pydantic.Field(None, description='Make a buildspec to run the indicated script.')
    validate_: typing.Optional[list[models.aws_codebuild.LinuxGpuBuildImageDefValidateParams]] = pydantic.Field(None, description='Allows the image a chance to validate whether the passed configuration is correct.', alias='validate')

class LinuxGpuBuildImageDefAwsDeepLearningContainersImageParams(pydantic.BaseModel):
    repository_name: str = pydantic.Field(..., description='the name of the repository, for example "pytorch-inference".\n')
    tag: str = pydantic.Field(..., description='the tag of the image, for example "1.5.0-gpu-py36-cu101-ubuntu16.04".\n')
    account: typing.Optional[str] = pydantic.Field(None, description='the AWS account ID where the DLC repository for this region is hosted in. In many cases, the CDK can infer that for you, but for some newer region our information might be out of date; in that case, you can specify the region explicitly using this optional parameter\n\n:see: https://aws.amazon.com/releasenotes/available-deep-learning-containers-images\n')
    ...

class LinuxGpuBuildImageDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description='-')
    ...

class LinuxGpuBuildImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='The ECR repository.\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='Image tag (default "latest").\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html\n')
    ...

class LinuxGpuBuildImageDefRunScriptBuildspecParams(pydantic.BaseModel):
    entrypoint: str = pydantic.Field(..., description='-')
    ...

class LinuxGpuBuildImageDefValidateParams(pydantic.BaseModel):
    build_image: typing.Optional[typing.Union[models.aws_codebuild.LinuxArmBuildImageDef, models.aws_codebuild.LinuxBuildImageDef, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None, description='The image used for the builds. Default: LinuxBuildImage.STANDARD_1_0\n')
    certificate: typing.Union[models.aws_codebuild.BuildEnvironmentCertificateDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the PEM-encoded certificate for the build project. Default: - No external certificate is added to the project\n')
    compute_type: typing.Optional[aws_cdk.aws_codebuild.ComputeType] = pydantic.Field(None, description='The type of compute to use for this build. See the ``ComputeType`` enum for the possible values. Default: taken from ``#buildImage#defaultComputeType``\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables that your builds can use.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Indicates how the project builds Docker images. Specify true to enable running the Docker daemon inside a Docker container. This value must be set to true only if this build project will be used to build Docker images, and the specified build environment image is not one provided by AWS CodeBuild with Docker support. Otherwise, all associated builds that attempt to interact with the Docker daemon will fail. Default: false')
    ...


#  autogenerated from aws_cdk.aws_codebuild.PhaseChangeEvent
class PhaseChangeEventDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.PhaseChangeEvent'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.Source
class SourceDef(BaseClass):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.')
    _init_params: typing.ClassVar[list[str]] = ['identifier']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['bit_bucket', 'code_commit', 'git_hub', 'git_hub_enterprise', 's3']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.Source'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['bit_bucket', 'code_commit', 'git_hub', 'git_hub_enterprise', 's3']
    ...


    bit_bucket: typing.Optional[models.aws_codebuild.SourceDefBitBucketParams] = pydantic.Field(None, description='')
    code_commit: typing.Optional[models.aws_codebuild.SourceDefCodeCommitParams] = pydantic.Field(None, description='')
    git_hub: typing.Optional[models.aws_codebuild.SourceDefGitHubParams] = pydantic.Field(None, description='')
    git_hub_enterprise: typing.Optional[models.aws_codebuild.SourceDefGitHubEnterpriseParams] = pydantic.Field(None, description='')
    s3: typing.Optional[models.aws_codebuild.SourceDefS3Params] = pydantic.Field(None, description='')
    resource_config: typing.Optional[models.aws_codebuild.SourceDefConfig] = pydantic.Field(None)


class SourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_codebuild.SourceDefBindParams]] = pydantic.Field(None, description='Called by the project when the source is added so that the source can perform binding operations on the source.\nFor example, it can grant permissions to the\ncode build project to read from the S3 bucket.')

class SourceDefBindParams(pydantic.BaseModel):
    ...

class SourceDefBitBucketParams(pydantic.BaseModel):
    owner: str = pydantic.Field(..., description='The BitBucket account/user that owns the repo.')
    repo: str = pydantic.Field(..., description='The name of the repo (without the username).\n')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    build_status_name: typing.Optional[str] = pydantic.Field(None, description='This parameter is used for the ``name`` parameter in the Bitbucket commit status. Can use built-in CodeBuild variables, like $AWS_REGION. Default: "AWS CodeBuild $AWS_REGION ($PROJECT_NAME)"\n')
    build_status_url: typing.Optional[str] = pydantic.Field(None, description='The URL that the build will report back to the source provider. Can use built-in CodeBuild variables, like $AWS_REGION. Default: - link to the AWS Console for CodeBuild to a particular build execution\n')
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n')
    report_build_status: typing.Optional[bool] = pydantic.Field(None, description="Whether to send notifications on your build's start and end. Default: true\n")
    webhook: typing.Optional[bool] = pydantic.Field(None, description='Whether to create a webhook that will trigger a build every time an event happens in the repository. Default: true if any ``webhookFilters`` were provided, false otherwise\n')
    webhook_filters: typing.Optional[typing.Sequence[models.aws_codebuild.FilterGroupDef]] = pydantic.Field(None, description='A list of webhook filters that can constraint what events in the repository will trigger a build. A build is triggered if any of the provided filter groups match. Only valid if ``webhook`` was not provided as false. Default: every push and every Pull Request (create or update) triggers a build\n')
    webhook_triggers_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build from a webhook instead of a standard one. Enabling this will enable batch builds on the CodeBuild project. Default: false\n')
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.')
    ...

class SourceDefCodeCommitParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_codecommit.RepositoryDef] = pydantic.Field(..., description='')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n')
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.')
    ...

class SourceDefGitHubParams(pydantic.BaseModel):
    owner: str = pydantic.Field(..., description='The GitHub account/user that owns the repo.')
    repo: str = pydantic.Field(..., description='The name of the repo (without the username).\n')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    build_status_context: typing.Optional[str] = pydantic.Field(None, description='This parameter is used for the ``context`` parameter in the GitHub commit status. Can use built-in CodeBuild variables, like $AWS_REGION. Default: "AWS CodeBuild $AWS_REGION ($PROJECT_NAME)"\n')
    build_status_url: typing.Optional[str] = pydantic.Field(None, description='The URL that the build will report back to the source provider. Can use built-in CodeBuild variables, like $AWS_REGION. Default: - link to the AWS Console for CodeBuild to a particular build execution\n')
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n')
    report_build_status: typing.Optional[bool] = pydantic.Field(None, description="Whether to send notifications on your build's start and end. Default: true\n")
    webhook: typing.Optional[bool] = pydantic.Field(None, description='Whether to create a webhook that will trigger a build every time an event happens in the repository. Default: true if any ``webhookFilters`` were provided, false otherwise\n')
    webhook_filters: typing.Optional[typing.Sequence[models.aws_codebuild.FilterGroupDef]] = pydantic.Field(None, description='A list of webhook filters that can constraint what events in the repository will trigger a build. A build is triggered if any of the provided filter groups match. Only valid if ``webhook`` was not provided as false. Default: every push and every Pull Request (create or update) triggers a build\n')
    webhook_triggers_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build from a webhook instead of a standard one. Enabling this will enable batch builds on the CodeBuild project. Default: false\n')
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.')
    ...

class SourceDefGitHubEnterpriseParams(pydantic.BaseModel):
    https_clone_url: str = pydantic.Field(..., description='The HTTPS URL of the repository in your GitHub Enterprise installation.')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    build_status_context: typing.Optional[str] = pydantic.Field(None, description='This parameter is used for the ``context`` parameter in the GitHub commit status. Can use built-in CodeBuild variables, like $AWS_REGION. Default: "AWS CodeBuild $AWS_REGION ($PROJECT_NAME)"\n')
    build_status_url: typing.Optional[str] = pydantic.Field(None, description='The URL that the build will report back to the source provider. Can use built-in CodeBuild variables, like $AWS_REGION. Default: - link to the AWS Console for CodeBuild to a particular build execution\n')
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n')
    ignore_ssl_errors: typing.Optional[bool] = pydantic.Field(None, description='Whether to ignore SSL errors when connecting to the repository. Default: false\n')
    report_build_status: typing.Optional[bool] = pydantic.Field(None, description="Whether to send notifications on your build's start and end. Default: true\n")
    webhook: typing.Optional[bool] = pydantic.Field(None, description='Whether to create a webhook that will trigger a build every time an event happens in the repository. Default: true if any ``webhookFilters`` were provided, false otherwise\n')
    webhook_filters: typing.Optional[typing.Sequence[models.aws_codebuild.FilterGroupDef]] = pydantic.Field(None, description='A list of webhook filters that can constraint what events in the repository will trigger a build. A build is triggered if any of the provided filter groups match. Only valid if ``webhook`` was not provided as false. Default: every push and every Pull Request (create or update) triggers a build\n')
    webhook_triggers_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build from a webhook instead of a standard one. Enabling this will enable batch builds on the CodeBuild project. Default: false\n')
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.')
    ...

class SourceDefS3Params(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    path: str = pydantic.Field(..., description='')
    version: typing.Optional[str] = pydantic.Field(None, description='The version ID of the object that represents the build input ZIP file to use. Default: latest\n')
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.')
    ...


#  autogenerated from aws_cdk.aws_codebuild.StateChangeEvent
class StateChangeEventDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.StateChangeEvent'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.WindowsBuildImage
class WindowsBuildImageDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['run_script_buildspec', 'validate']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_registry', 'from_ecr_repository']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.WindowsBuildImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_registry', 'from_ecr_repository']
    ...


    from_asset: typing.Optional[models.aws_codebuild.WindowsBuildImageDefFromAssetParams] = pydantic.Field(None, description='Uses an Docker image asset as a Windows build image.')
    from_docker_registry: typing.Optional[models.aws_codebuild.WindowsBuildImageDefFromDockerRegistryParams] = pydantic.Field(None, description='')
    from_ecr_repository: typing.Optional[models.aws_codebuild.WindowsBuildImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='')
    resource_config: typing.Optional[models.aws_codebuild.WindowsBuildImageDefConfig] = pydantic.Field(None)


class WindowsBuildImageDefConfig(pydantic.BaseModel):
    run_script_buildspec: typing.Optional[list[models.aws_codebuild.WindowsBuildImageDefRunScriptBuildspecParams]] = pydantic.Field(None, description='Make a buildspec to run the indicated script.')
    validate_: typing.Optional[list[models.aws_codebuild.WindowsBuildImageDefValidateParams]] = pydantic.Field(None, description='Allows the image a chance to validate whether the passed configuration is correct.', alias='validate')

class WindowsBuildImageDefFromAssetParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    props: typing.Union[models.aws_ecr_assets.DockerImageAssetPropsDef, dict[str, typing.Any]] = pydantic.Field(..., description='-\n')
    image_type: typing.Optional[aws_cdk.aws_codebuild.WindowsImageType] = pydantic.Field(None, description='-')
    ...

class WindowsBuildImageDefFromDockerRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-')
    options: typing.Union[models.aws_codebuild.DockerImageOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='-\n')
    image_type: typing.Optional[aws_cdk.aws_codebuild.WindowsImageType] = pydantic.Field(None, description='-\n')
    ...

class WindowsBuildImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='The ECR repository.')
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description='Image tag or digest (default "latest", digests must start with ``sha256:``).\n')
    image_type: typing.Optional[aws_cdk.aws_codebuild.WindowsImageType] = pydantic.Field(None, description='-\n')
    ...

class WindowsBuildImageDefRunScriptBuildspecParams(pydantic.BaseModel):
    entrypoint: str = pydantic.Field(..., description='-')
    ...

class WindowsBuildImageDefValidateParams(pydantic.BaseModel):
    build_image: typing.Optional[typing.Union[models.aws_codebuild.LinuxArmBuildImageDef, models.aws_codebuild.LinuxBuildImageDef, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None, description='The image used for the builds. Default: LinuxBuildImage.STANDARD_1_0\n')
    certificate: typing.Union[models.aws_codebuild.BuildEnvironmentCertificateDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the PEM-encoded certificate for the build project. Default: - No external certificate is added to the project\n')
    compute_type: typing.Optional[aws_cdk.aws_codebuild.ComputeType] = pydantic.Field(None, description='The type of compute to use for this build. See the ``ComputeType`` enum for the possible values. Default: taken from ``#buildImage#defaultComputeType``\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables that your builds can use.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Indicates how the project builds Docker images. Specify true to enable running the Docker daemon inside a Docker container. This value must be set to true only if this build project will be used to build Docker images, and the specified build environment image is not one provided by AWS CodeBuild with Docker support. Otherwise, all associated builds that attempt to interact with the Docker daemon will fail. Default: false')
    ...


#  autogenerated from aws_cdk.aws_codebuild.BitBucketSourceCredentials
class BitBucketSourceCredentialsDef(BaseConstruct):
    password: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Your BitBucket application password.\n')
    username: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Your BitBucket username.')
    _init_params: typing.ClassVar[list[str]] = ['password', 'username']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BitBucketSourceCredentials'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.BitBucketSourceCredentialsDefConfig] = pydantic.Field(None)


class BitBucketSourceCredentialsDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class BitBucketSourceCredentialsDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_codebuild.GitHubEnterpriseSourceCredentials
class GitHubEnterpriseSourceCredentialsDef(BaseConstruct):
    access_token: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The personal access token to use when contacting the instance of the GitHub Enterprise API.')
    _init_params: typing.ClassVar[list[str]] = ['access_token']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.GitHubEnterpriseSourceCredentials'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.GitHubEnterpriseSourceCredentialsDefConfig] = pydantic.Field(None)


class GitHubEnterpriseSourceCredentialsDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class GitHubEnterpriseSourceCredentialsDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_codebuild.GitHubSourceCredentials
class GitHubSourceCredentialsDef(BaseConstruct):
    access_token: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The personal access token to use when contacting the GitHub API.')
    _init_params: typing.ClassVar[list[str]] = ['access_token']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.GitHubSourceCredentials'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.GitHubSourceCredentialsDefConfig] = pydantic.Field(None)


class GitHubSourceCredentialsDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class GitHubSourceCredentialsDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_codebuild.PipelineProject
class PipelineProjectDef(BaseConstruct):
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description="Whether to allow the CodeBuild to send all network traffic. If set to false, you must individually add traffic rules to allow the CodeBuild project to connect to network targets. Only used if 'vpc' is supplied. Default: true\n")
    badge: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether AWS CodeBuild generates a publicly accessible URL for your project's build badge. For more information, see Build Badges Sample in the AWS CodeBuild User Guide. Default: false\n")
    build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description='Filename or contents of buildspec in JSON format. Default: - Empty buildspec.\n')
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: Cache.none\n')
    check_secrets_in_plain_text_env_variables: typing.Optional[bool] = pydantic.Field(None, description='Whether to check for the presence of any secrets in the environment variables of the default type, BuildEnvironmentVariableType.PLAINTEXT. Since using a secret for the value of that kind of variable would result in it being displayed in plain text in the AWS Console, the construct will throw an exception if it detects a secret was passed there. Pass this property as false if you want to skip this validation, and keep using a secret in a plain text environment variable. Default: true\n')
    concurrent_build_limit: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of concurrent builds. Minimum value is 1 and maximum is account build limit. Default: - no explicit limit is set\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the project. Use the description to identify the purpose of the project. Default: - No description.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='Encryption key to use to read and write artifacts. Default: - The AWS-managed CMK for Amazon Simple Storage Service (Amazon S3) is used.\n')
    environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Build environment to use for the build. Default: BuildEnvironment.LinuxBuildImage.STANDARD_1_0\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional environment variables to add to the build environment. Default: - No additional environment variables are specified.\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='An ProjectFileSystemLocation objects for a CodeBuild build project. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Default: - no file system locations\n')
    grant_report_group_permissions: typing.Optional[bool] = pydantic.Field(None, description="Add permissions to this project's role to create and use test report groups with name starting with the name of this project. That is the standard report group that gets created when a simple name (in contrast to an ARN) is used in the 'reports' section of the buildspec of this project. This is usually harmless, but you can turn these off if you don't plan on using test reports in this project. Default: true\n")
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for the build project. A project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    project_name: typing.Optional[str] = pydantic.Field(None, description='The physical, human-readable name of the CodeBuild Project. Default: - Name is automatically generated.\n')
    queued_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's still in queue. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: - no queue timeout is set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Service Role to assume while running the build. Default: - A role will be created.\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="What security group to associate with the codebuild project's network interfaces. If no security group is identified, one will be created automatically. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    ssm_session_permissions: typing.Optional[bool] = pydantic.Field(None, description='Add the permissions necessary for debugging builds with SSM Session Manager. If the following prerequisites have been met: - The necessary permissions have been added by setting this flag to true. - The build image has the SSM agent installed (true for default CodeBuild images). - The build is started with `debugSessionEnabled <https://docs.aws.amazon.com/codebuild/latest/APIReference/API_StartBuild.html#CodeBuild-StartBuild-request-debugSessionEnabled>`_ set to true. Then the build container can be paused and inspected using Session Manager by invoking the ``codebuild-breakpoint`` command somewhere during the build. ``codebuild-breakpoint`` commands will be ignored if the build is not started with ``debugSessionEnabled=true``. Default: false\n')
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Where to place the network interfaces within the VPC. To access AWS services, your CodeBuild project needs to be in one of the following types of subnets: 1. Subnets with access to the internet (of type PRIVATE_WITH_EGRESS). 2. Private subnets unconnected to the internet, but with `VPC endpoints <https://docs.aws.amazon.com/codebuild/latest/userguide/use-vpc-endpoints-with-codebuild.html>`_ for the necessary services. If you don't specify a subnet selection, the default behavior is to use PRIVATE_WITH_EGRESS subnets first if they exist, then PRIVATE_WITHOUT_EGRESS, and finally PUBLIC subnets. If your VPC doesn't have PRIVATE_WITH_EGRESS subnets but you need AWS service access, add VPC Endpoints to your private subnets. Default: - private subnets if available else public subnets\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place codebuild network interfaces. Specify this if the codebuild project needs to access resources in a VPC. Default: - No VPC is specified.')
    _init_params: typing.ClassVar[list[str]] = ['allow_all_outbound', 'badge', 'build_spec', 'cache', 'check_secrets_in_plain_text_env_variables', 'concurrent_build_limit', 'description', 'encryption_key', 'environment', 'environment_variables', 'file_system_locations', 'grant_report_group_permissions', 'logging', 'project_name', 'queued_timeout', 'role', 'security_groups', 'ssm_session_permissions', 'subnet_selection', 'timeout', 'vpc']
    _method_names: typing.ClassVar[list[str]] = ['add_file_system_location', 'add_secondary_artifact', 'add_secondary_source', 'add_to_role_policy', 'apply_removal_policy', 'bind_as_notification_rule_source', 'bind_to_code_pipeline', 'enable_batch_builds', 'metric', 'metric_builds', 'metric_duration', 'metric_failed_builds', 'metric_succeeded_builds', 'notify_on', 'notify_on_build_failed', 'notify_on_build_succeeded', 'on_build_failed', 'on_build_started', 'on_build_succeeded', 'on_event', 'on_phase_change', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_project_arn', 'from_project_name', 'serialize_env_variables']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.PipelineProject'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_project_arn', 'from_project_name']
    ...


    from_project_arn: typing.Optional[models.aws_codebuild.PipelineProjectDefFromProjectArnParams] = pydantic.Field(None, description='')
    from_project_name: typing.Optional[models.aws_codebuild.PipelineProjectDefFromProjectNameParams] = pydantic.Field(None, description='Import a Project defined either outside the CDK, or in a different CDK Stack (and exported using the ``export`` method).')
    resource_config: typing.Optional[models.aws_codebuild.PipelineProjectDefConfig] = pydantic.Field(None)


class PipelineProjectDefConfig(pydantic.BaseModel):
    add_file_system_location: typing.Optional[list[models.aws_codebuild.PipelineProjectDefAddFileSystemLocationParams]] = pydantic.Field(None, description='Adds a fileSystemLocation to the Project.')
    add_secondary_artifact: typing.Optional[list[models.aws_codebuild.PipelineProjectDefAddSecondaryArtifactParams]] = pydantic.Field(None, description='Adds a secondary artifact to the Project.')
    add_secondary_source: typing.Optional[list[models.aws_codebuild.PipelineProjectDefAddSecondarySourceParams]] = pydantic.Field(None, description='Adds a secondary source to the Project.')
    add_to_role_policy: typing.Optional[list[models.aws_codebuild.PipelineProjectDefAddToRolePolicyParams]] = pydantic.Field(None, description="Add a permission only if there's a policy attached.")
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    bind_as_notification_rule_source: typing.Optional[list[models.aws_codebuild.PipelineProjectDefBindAsNotificationRuleSourceParams]] = pydantic.Field(None, description='Returns a source configuration for notification rule.')
    bind_to_code_pipeline: typing.Optional[list[models.aws_codebuild.PipelineProjectDefBindToCodePipelineParams]] = pydantic.Field(None, description='A callback invoked when the given project is added to a CodePipeline.')
    enable_batch_builds: typing.Optional[bool] = pydantic.Field(None, description='Enable batch builds.\nReturns an object contining the batch service role if batch builds\ncould be enabled.')
    metric: typing.Optional[list[models.aws_codebuild.PipelineProjectDefMetricParams]] = pydantic.Field(None, description='')
    metric_builds: typing.Optional[list[models.aws_codebuild.PipelineProjectDefMetricBuildsParams]] = pydantic.Field(None, description='Measures the number of builds triggered.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    metric_duration: typing.Optional[list[models.aws_codebuild.PipelineProjectDefMetricDurationParams]] = pydantic.Field(None, description='Measures the duration of all builds over time.\nUnits: Seconds\n\nValid CloudWatch statistics: Average (recommended), Maximum, Minimum')
    metric_failed_builds: typing.Optional[list[models.aws_codebuild.PipelineProjectDefMetricFailedBuildsParams]] = pydantic.Field(None, description='Measures the number of builds that failed because of client error or because of a timeout.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    metric_succeeded_builds: typing.Optional[list[models.aws_codebuild.PipelineProjectDefMetricSucceededBuildsParams]] = pydantic.Field(None, description='Measures the number of successful builds.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    notify_on: typing.Optional[list[models.aws_codebuild.PipelineProjectDefNotifyOnParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule triggered when the project events emitted by you specified, it very similar to ``onEvent`` API.\nYou can also use the methods ``notifyOnBuildSucceeded`` and\n``notifyOnBuildFailed`` to define rules for these specific event emitted.')
    notify_on_build_failed: typing.Optional[list[models.aws_codebuild.PipelineProjectDefNotifyOnBuildFailedParams]] = pydantic.Field(None, description='Defines a CodeStar notification rule which triggers when a build fails.')
    notify_on_build_succeeded: typing.Optional[list[models.aws_codebuild.PipelineProjectDefNotifyOnBuildSucceededParams]] = pydantic.Field(None, description='Defines a CodeStar notification rule which triggers when a build completes successfully.')
    on_build_failed: typing.Optional[list[models.aws_codebuild.PipelineProjectDefOnBuildFailedParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build fails.\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    on_build_started: typing.Optional[list[models.aws_codebuild.PipelineProjectDefOnBuildStartedParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build starts.\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    on_build_succeeded: typing.Optional[list[models.aws_codebuild.PipelineProjectDefOnBuildSucceededParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build completes successfully.\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    on_event: typing.Optional[list[models.aws_codebuild.PipelineProjectDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule triggered when something happens with this project.')
    on_phase_change: typing.Optional[list[models.aws_codebuild.PipelineProjectDefOnPhaseChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule that triggers upon phase change of this build project.')
    on_state_change: typing.Optional[list[models.aws_codebuild.PipelineProjectDefOnStateChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule triggered when the build project state changes.\nYou can filter specific build status events using an event\npattern filter on the ``build-status`` detail field:\n\nconst rule = project.onStateChange(\'OnBuildStarted\', { target });\nrule.addEventPattern({\ndetail: {\n\'build-status\': [\n"IN_PROGRESS",\n"SUCCEEDED",\n"FAILED",\n"STOPPED"\n]\n}\n});\n\nYou can also use the methods ``onBuildFailed`` and ``onBuildSucceeded`` to define rules for\nthese specific state changes.\n\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    serialize_env_variables: typing.Optional[list[models.aws_codebuild.PipelineProjectDefSerializeEnvVariablesParams]] = pydantic.Field(None, description='Convert the environment variables map of string to ``BuildEnvironmentVariable``, which is the customer-facing type, to a list of ``CfnProject.EnvironmentVariableProperty``, which is the representation of environment variables in CloudFormation.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)

class PipelineProjectDefAddFileSystemLocationParams(pydantic.BaseModel):
    file_system_location: models.UnsupportedResource = pydantic.Field(..., description='the fileSystemLocation to add.')
    ...

class PipelineProjectDefAddSecondaryArtifactParams(pydantic.BaseModel):
    secondary_artifact: typing.Union[models.aws_codebuild.ArtifactsDef] = pydantic.Field(..., description='the artifact to add as a secondary artifact.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html\n')
    ...

class PipelineProjectDefAddSecondarySourceParams(pydantic.BaseModel):
    secondary_source: typing.Union[models.aws_codebuild.SourceDef] = pydantic.Field(..., description='the source to add as a secondary source.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html\n')
    ...

class PipelineProjectDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='The permissions statement to add.')
    ...

class PipelineProjectDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class PipelineProjectDefBindAsNotificationRuleSourceParams(pydantic.BaseModel):
    ...

class PipelineProjectDefBindToCodePipelineParams(pydantic.BaseModel):
    artifact_bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The artifact bucket that will be used by the action that invokes this project.')
    ...

class PipelineProjectDefFromProjectArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    id: str = pydantic.Field(..., description='-\n')
    project_arn: str = pydantic.Field(..., description='-')
    ...

class PipelineProjectDefFromProjectNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='the parent Construct for this Construct.\n')
    id: str = pydantic.Field(..., description='the logical name of this Construct.\n')
    project_name: str = pydantic.Field(..., description='the name of the project to import.\n')
    ...

class PipelineProjectDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='The name of the metric.')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefMetricBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefMetricFailedBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefMetricSucceededBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefNotifyOnParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    events: typing.Sequence[aws_cdk.aws_codebuild.ProjectNotificationEvents] = pydantic.Field(..., description='A list of event types associated with this notification rule for CodeBuild Project. For a complete list of event types and IDs, see Notification concepts in the Developer Tools Console User Guide.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefNotifyOnBuildFailedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefNotifyOnBuildSucceededParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefOnBuildFailedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefOnBuildStartedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefOnBuildSucceededParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefOnPhaseChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefOnStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class PipelineProjectDefSerializeEnvVariablesParams(pydantic.BaseModel):
    environment_variables: typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]] = pydantic.Field(..., description='the map of string to environment variables.\n')
    validate_no_plain_text_secrets: typing.Optional[bool] = pydantic.Field(None, description="whether to throw an exception if any of the plain text environment variables contain secrets, defaults to 'false'.\n")
    principal: typing.Optional[models.AnyResource] = pydantic.Field(None, description='-\n')
    ...


#  autogenerated from aws_cdk.aws_codebuild.Project
class ProjectDef(BaseConstruct):
    artifacts: typing.Optional[typing.Union[models.aws_codebuild.ArtifactsDef]] = pydantic.Field(None, description='Defines where build artifacts will be stored. Could be: PipelineBuildArtifacts, NoArtifacts and S3Artifacts. Default: NoArtifacts\n')
    secondary_artifacts: typing.Optional[typing.Sequence[typing.Union[models.aws_codebuild.ArtifactsDef]]] = pydantic.Field(None, description='The secondary artifacts for the Project. Can also be added after the Project has been created by using the ``Project#addSecondaryArtifact`` method. Default: - No secondary artifacts.\n')
    secondary_sources: typing.Optional[typing.Sequence[typing.Union[models.aws_codebuild.SourceDef]]] = pydantic.Field(None, description='The secondary sources for the Project. Can be also added after the Project has been created by using the ``Project#addSecondarySource`` method. Default: - No secondary sources.\n')
    source: typing.Optional[typing.Union[models.aws_codebuild.SourceDef]] = pydantic.Field(None, description='The source of the build. *Note*: if ``NoSource`` is given as the source, then you need to provide an explicit ``buildSpec``. Default: - NoSource\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description="Whether to allow the CodeBuild to send all network traffic. If set to false, you must individually add traffic rules to allow the CodeBuild project to connect to network targets. Only used if 'vpc' is supplied. Default: true\n")
    badge: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether AWS CodeBuild generates a publicly accessible URL for your project's build badge. For more information, see Build Badges Sample in the AWS CodeBuild User Guide. Default: false\n")
    build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description='Filename or contents of buildspec in JSON format. Default: - Empty buildspec.\n')
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: Cache.none\n')
    check_secrets_in_plain_text_env_variables: typing.Optional[bool] = pydantic.Field(None, description='Whether to check for the presence of any secrets in the environment variables of the default type, BuildEnvironmentVariableType.PLAINTEXT. Since using a secret for the value of that kind of variable would result in it being displayed in plain text in the AWS Console, the construct will throw an exception if it detects a secret was passed there. Pass this property as false if you want to skip this validation, and keep using a secret in a plain text environment variable. Default: true\n')
    concurrent_build_limit: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of concurrent builds. Minimum value is 1 and maximum is account build limit. Default: - no explicit limit is set\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the project. Use the description to identify the purpose of the project. Default: - No description.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='Encryption key to use to read and write artifacts. Default: - The AWS-managed CMK for Amazon Simple Storage Service (Amazon S3) is used.\n')
    environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Build environment to use for the build. Default: BuildEnvironment.LinuxBuildImage.STANDARD_1_0\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional environment variables to add to the build environment. Default: - No additional environment variables are specified.\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='An ProjectFileSystemLocation objects for a CodeBuild build project. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Default: - no file system locations\n')
    grant_report_group_permissions: typing.Optional[bool] = pydantic.Field(None, description="Add permissions to this project's role to create and use test report groups with name starting with the name of this project. That is the standard report group that gets created when a simple name (in contrast to an ARN) is used in the 'reports' section of the buildspec of this project. This is usually harmless, but you can turn these off if you don't plan on using test reports in this project. Default: true\n")
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for the build project. A project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    project_name: typing.Optional[str] = pydantic.Field(None, description='The physical, human-readable name of the CodeBuild Project. Default: - Name is automatically generated.\n')
    queued_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's still in queue. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: - no queue timeout is set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Service Role to assume while running the build. Default: - A role will be created.\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="What security group to associate with the codebuild project's network interfaces. If no security group is identified, one will be created automatically. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    ssm_session_permissions: typing.Optional[bool] = pydantic.Field(None, description='Add the permissions necessary for debugging builds with SSM Session Manager. If the following prerequisites have been met: - The necessary permissions have been added by setting this flag to true. - The build image has the SSM agent installed (true for default CodeBuild images). - The build is started with `debugSessionEnabled <https://docs.aws.amazon.com/codebuild/latest/APIReference/API_StartBuild.html#CodeBuild-StartBuild-request-debugSessionEnabled>`_ set to true. Then the build container can be paused and inspected using Session Manager by invoking the ``codebuild-breakpoint`` command somewhere during the build. ``codebuild-breakpoint`` commands will be ignored if the build is not started with ``debugSessionEnabled=true``. Default: false\n')
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Where to place the network interfaces within the VPC. To access AWS services, your CodeBuild project needs to be in one of the following types of subnets: 1. Subnets with access to the internet (of type PRIVATE_WITH_EGRESS). 2. Private subnets unconnected to the internet, but with `VPC endpoints <https://docs.aws.amazon.com/codebuild/latest/userguide/use-vpc-endpoints-with-codebuild.html>`_ for the necessary services. If you don't specify a subnet selection, the default behavior is to use PRIVATE_WITH_EGRESS subnets first if they exist, then PRIVATE_WITHOUT_EGRESS, and finally PUBLIC subnets. If your VPC doesn't have PRIVATE_WITH_EGRESS subnets but you need AWS service access, add VPC Endpoints to your private subnets. Default: - private subnets if available else public subnets\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place codebuild network interfaces. Specify this if the codebuild project needs to access resources in a VPC. Default: - No VPC is specified.')
    _init_params: typing.ClassVar[list[str]] = ['artifacts', 'secondary_artifacts', 'secondary_sources', 'source', 'allow_all_outbound', 'badge', 'build_spec', 'cache', 'check_secrets_in_plain_text_env_variables', 'concurrent_build_limit', 'description', 'encryption_key', 'environment', 'environment_variables', 'file_system_locations', 'grant_report_group_permissions', 'logging', 'project_name', 'queued_timeout', 'role', 'security_groups', 'ssm_session_permissions', 'subnet_selection', 'timeout', 'vpc']
    _method_names: typing.ClassVar[list[str]] = ['add_file_system_location', 'add_secondary_artifact', 'add_secondary_source', 'add_to_role_policy', 'apply_removal_policy', 'bind_as_notification_rule_source', 'bind_to_code_pipeline', 'enable_batch_builds', 'metric', 'metric_builds', 'metric_duration', 'metric_failed_builds', 'metric_succeeded_builds', 'notify_on', 'notify_on_build_failed', 'notify_on_build_succeeded', 'on_build_failed', 'on_build_started', 'on_build_succeeded', 'on_event', 'on_phase_change', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_project_arn', 'from_project_name', 'serialize_env_variables']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.Project'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_project_arn', 'from_project_name']
    ...


    from_project_arn: typing.Optional[models.aws_codebuild.ProjectDefFromProjectArnParams] = pydantic.Field(None, description='')
    from_project_name: typing.Optional[models.aws_codebuild.ProjectDefFromProjectNameParams] = pydantic.Field(None, description='Import a Project defined either outside the CDK, or in a different CDK Stack (and exported using the ``export`` method).')
    resource_config: typing.Optional[models.aws_codebuild.ProjectDefConfig] = pydantic.Field(None)


class ProjectDefConfig(pydantic.BaseModel):
    add_file_system_location: typing.Optional[list[models.aws_codebuild.ProjectDefAddFileSystemLocationParams]] = pydantic.Field(None, description='Adds a fileSystemLocation to the Project.')
    add_secondary_artifact: typing.Optional[list[models.aws_codebuild.ProjectDefAddSecondaryArtifactParams]] = pydantic.Field(None, description='Adds a secondary artifact to the Project.')
    add_secondary_source: typing.Optional[list[models.aws_codebuild.ProjectDefAddSecondarySourceParams]] = pydantic.Field(None, description='Adds a secondary source to the Project.')
    add_to_role_policy: typing.Optional[list[models.aws_codebuild.ProjectDefAddToRolePolicyParams]] = pydantic.Field(None, description="Add a permission only if there's a policy attached.")
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    bind_as_notification_rule_source: typing.Optional[list[models.aws_codebuild.ProjectDefBindAsNotificationRuleSourceParams]] = pydantic.Field(None, description='Returns a source configuration for notification rule.')
    bind_to_code_pipeline: typing.Optional[list[models.aws_codebuild.ProjectDefBindToCodePipelineParams]] = pydantic.Field(None, description='A callback invoked when the given project is added to a CodePipeline.')
    enable_batch_builds: typing.Optional[bool] = pydantic.Field(None, description='Enable batch builds.\nReturns an object contining the batch service role if batch builds\ncould be enabled.')
    metric: typing.Optional[list[models.aws_codebuild.ProjectDefMetricParams]] = pydantic.Field(None, description='')
    metric_builds: typing.Optional[list[models.aws_codebuild.ProjectDefMetricBuildsParams]] = pydantic.Field(None, description='Measures the number of builds triggered.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    metric_duration: typing.Optional[list[models.aws_codebuild.ProjectDefMetricDurationParams]] = pydantic.Field(None, description='Measures the duration of all builds over time.\nUnits: Seconds\n\nValid CloudWatch statistics: Average (recommended), Maximum, Minimum')
    metric_failed_builds: typing.Optional[list[models.aws_codebuild.ProjectDefMetricFailedBuildsParams]] = pydantic.Field(None, description='Measures the number of builds that failed because of client error or because of a timeout.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    metric_succeeded_builds: typing.Optional[list[models.aws_codebuild.ProjectDefMetricSucceededBuildsParams]] = pydantic.Field(None, description='Measures the number of successful builds.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    notify_on: typing.Optional[list[models.aws_codebuild.ProjectDefNotifyOnParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule triggered when the project events emitted by you specified, it very similar to ``onEvent`` API.\nYou can also use the methods ``notifyOnBuildSucceeded`` and\n``notifyOnBuildFailed`` to define rules for these specific event emitted.')
    notify_on_build_failed: typing.Optional[list[models.aws_codebuild.ProjectDefNotifyOnBuildFailedParams]] = pydantic.Field(None, description='Defines a CodeStar notification rule which triggers when a build fails.')
    notify_on_build_succeeded: typing.Optional[list[models.aws_codebuild.ProjectDefNotifyOnBuildSucceededParams]] = pydantic.Field(None, description='Defines a CodeStar notification rule which triggers when a build completes successfully.')
    on_build_failed: typing.Optional[list[models.aws_codebuild.ProjectDefOnBuildFailedParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build fails.\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    on_build_started: typing.Optional[list[models.aws_codebuild.ProjectDefOnBuildStartedParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build starts.\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    on_build_succeeded: typing.Optional[list[models.aws_codebuild.ProjectDefOnBuildSucceededParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build completes successfully.\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    on_event: typing.Optional[list[models.aws_codebuild.ProjectDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule triggered when something happens with this project.')
    on_phase_change: typing.Optional[list[models.aws_codebuild.ProjectDefOnPhaseChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule that triggers upon phase change of this build project.')
    on_state_change: typing.Optional[list[models.aws_codebuild.ProjectDefOnStateChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule triggered when the build project state changes.\nYou can filter specific build status events using an event\npattern filter on the ``build-status`` detail field:\n\nconst rule = project.onStateChange(\'OnBuildStarted\', { target });\nrule.addEventPattern({\ndetail: {\n\'build-status\': [\n"IN_PROGRESS",\n"SUCCEEDED",\n"FAILED",\n"STOPPED"\n]\n}\n});\n\nYou can also use the methods ``onBuildFailed`` and ``onBuildSucceeded`` to define rules for\nthese specific state changes.\n\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')
    serialize_env_variables: typing.Optional[list[models.aws_codebuild.ProjectDefSerializeEnvVariablesParams]] = pydantic.Field(None, description='Convert the environment variables map of string to ``BuildEnvironmentVariable``, which is the customer-facing type, to a list of ``CfnProject.EnvironmentVariableProperty``, which is the representation of environment variables in CloudFormation.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)

class ProjectDefAddFileSystemLocationParams(pydantic.BaseModel):
    file_system_location: models.UnsupportedResource = pydantic.Field(..., description='the fileSystemLocation to add.')
    ...

class ProjectDefAddSecondaryArtifactParams(pydantic.BaseModel):
    secondary_artifact: typing.Union[models.aws_codebuild.ArtifactsDef] = pydantic.Field(..., description='the artifact to add as a secondary artifact.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html\n')
    ...

class ProjectDefAddSecondarySourceParams(pydantic.BaseModel):
    secondary_source: typing.Union[models.aws_codebuild.SourceDef] = pydantic.Field(..., description='the source to add as a secondary source.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html\n')
    ...

class ProjectDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='The permissions statement to add.')
    ...

class ProjectDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ProjectDefBindAsNotificationRuleSourceParams(pydantic.BaseModel):
    ...

class ProjectDefBindToCodePipelineParams(pydantic.BaseModel):
    artifact_bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The artifact bucket that will be used by the action that invokes this project.')
    ...

class ProjectDefFromProjectArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    id: str = pydantic.Field(..., description='-\n')
    project_arn: str = pydantic.Field(..., description='-')
    ...

class ProjectDefFromProjectNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='the parent Construct for this Construct.\n')
    id: str = pydantic.Field(..., description='the logical name of this Construct.\n')
    project_name: str = pydantic.Field(..., description='the name of the project to import.\n')
    ...

class ProjectDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='The name of the metric.')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefMetricBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefMetricFailedBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefMetricSucceededBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefNotifyOnParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    events: typing.Sequence[aws_cdk.aws_codebuild.ProjectNotificationEvents] = pydantic.Field(..., description='A list of event types associated with this notification rule for CodeBuild Project. For a complete list of event types and IDs, see Notification concepts in the Developer Tools Console User Guide.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefNotifyOnBuildFailedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefNotifyOnBuildSucceededParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefOnBuildFailedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefOnBuildStartedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefOnBuildSucceededParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefOnPhaseChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefOnStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class ProjectDefSerializeEnvVariablesParams(pydantic.BaseModel):
    environment_variables: typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]] = pydantic.Field(..., description='the map of string to environment variables.\n')
    validate_no_plain_text_secrets: typing.Optional[bool] = pydantic.Field(None, description="whether to throw an exception if any of the plain text environment variables contain secrets, defaults to 'false'.\n")
    principal: typing.Optional[models.AnyResource] = pydantic.Field(None, description='-\n')
    ...


#  autogenerated from aws_cdk.aws_codebuild.ReportGroup
class ReportGroupDef(BaseConstruct):
    export_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='An optional S3 bucket to export the reports to. Default: - the reports will not be exported\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='What to do when this resource is deleted from a stack. As CodeBuild does not allow deleting a ResourceGroup that has reports inside of it, this is set to retain the resource by default. Default: RemovalPolicy.RETAIN\n')
    report_group_name: typing.Optional[str] = pydantic.Field(None, description='The physical name of the report group. Default: - CloudFormation-generated name\n')
    type: typing.Optional[aws_cdk.aws_codebuild.ReportGroupType] = pydantic.Field(None, description='The type of report group. This can be one of the following values:. - **TEST** - The report group contains test reports. - **CODE_COVERAGE** - The report group contains code coverage reports. Default: TEST\n')
    zip_export: typing.Optional[bool] = pydantic.Field(None, description='Whether to output the report files into the export bucket as-is, or create a ZIP from them before doing the export. Ignored if ``exportBucket`` has not been provided. Default: - false (the files will not be ZIPped)')
    _init_params: typing.ClassVar[list[str]] = ['export_bucket', 'removal_policy', 'report_group_name', 'type', 'zip_export']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy', 'grant_write']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_report_group_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.ReportGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_report_group_name']
    ...


    from_report_group_name: typing.Optional[models.aws_codebuild.ReportGroupDefFromReportGroupNameParams] = pydantic.Field(None, description='Reference an existing ReportGroup, defined outside of the CDK code, by name.')
    resource_config: typing.Optional[models.aws_codebuild.ReportGroupDefConfig] = pydantic.Field(None)


class ReportGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_write: typing.Optional[list[models.aws_codebuild.ReportGroupDefGrantWriteParams]] = pydantic.Field(None, description='Grants the given entity permissions to write (that is, upload reports to) this report group.')

class ReportGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ReportGroupDefFromReportGroupNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    report_group_name: str = pydantic.Field(..., description='-')
    ...

class ReportGroupDefGrantWriteParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codebuild.UntrustedCodeBoundaryPolicy
class UntrustedCodeBoundaryPolicyDef(BaseConstruct):
    additional_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Additional statements to add to the default set of statements. Default: - No additional statements\n')
    managed_policy_name: typing.Optional[str] = pydantic.Field(None, description='The name of the managed policy. Default: - A name is automatically generated.')
    _init_params: typing.ClassVar[list[str]] = ['additional_statements', 'managed_policy_name']
    _method_names: typing.ClassVar[list[str]] = ['add_statements', 'apply_removal_policy', 'attach_to_group', 'attach_to_role', 'attach_to_user']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_aws_managed_policy_name', 'from_managed_policy_arn', 'from_managed_policy_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.UntrustedCodeBoundaryPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_aws_managed_policy_name', 'from_managed_policy_arn', 'from_managed_policy_name']
    ...


    from_aws_managed_policy_name: typing.Optional[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefFromAwsManagedPolicyNameParams] = pydantic.Field(None, description='Import a managed policy from one of the policies that AWS manages.\nFor this managed policy, you only need to know the name to be able to use it.\n\nSome managed policy names start with "service-role/", some start with\n"job-function/", and some don\'t start with anything. Include the\nprefix when constructing this object.')
    from_managed_policy_arn: typing.Optional[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefFromManagedPolicyArnParams] = pydantic.Field(None, description='Import an external managed policy by ARN.\nFor this managed policy, you only need to know the ARN to be able to use it.\nThis can be useful if you got the ARN from a CloudFormation Export.\n\nIf the imported Managed Policy ARN is a Token (such as a\n``CfnParameter.valueAsString`` or a ``Fn.importValue()``) *and* the referenced\nmanaged policy has a ``path`` (like ``arn:...:policy/AdminPolicy/AdminAllow``), the\n``managedPolicyName`` property will not resolve to the correct value. Instead it\nwill resolve to the first path component. We unfortunately cannot express\nthe correct calculation of the full path name as a CloudFormation\nexpression. In this scenario the Managed Policy ARN should be supplied without the\n``path`` in order to resolve the correct managed policy resource.')
    from_managed_policy_name: typing.Optional[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefFromManagedPolicyNameParams] = pydantic.Field(None, description='Import a customer managed policy from the managedPolicyName.\nFor this managed policy, you only need to know the name to be able to use it.')
    resource_config: typing.Optional[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefConfig] = pydantic.Field(None)


class UntrustedCodeBoundaryPolicyDefConfig(pydantic.BaseModel):
    add_statements: typing.Optional[list[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefAddStatementsParams]] = pydantic.Field(None, description='Adds a statement to the policy document.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    attach_to_group: typing.Optional[list[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefAttachToGroupParams]] = pydantic.Field(None, description='Attaches this policy to a group.')
    attach_to_role: typing.Optional[list[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefAttachToRoleParams]] = pydantic.Field(None, description='Attaches this policy to a role.')
    attach_to_user: typing.Optional[list[models.aws_codebuild.UntrustedCodeBoundaryPolicyDefAttachToUserParams]] = pydantic.Field(None, description='Attaches this policy to a user.')
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)

class UntrustedCodeBoundaryPolicyDefAddStatementsParams(pydantic.BaseModel):
    statement: list[models.aws_iam.PolicyStatementDef] = pydantic.Field(...)
    ...

class UntrustedCodeBoundaryPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class UntrustedCodeBoundaryPolicyDefAttachToGroupParams(pydantic.BaseModel):
    group: typing.Union[models.aws_iam.GroupDef] = pydantic.Field(..., description='-')
    ...

class UntrustedCodeBoundaryPolicyDefAttachToRoleParams(pydantic.BaseModel):
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='-')
    ...

class UntrustedCodeBoundaryPolicyDefAttachToUserParams(pydantic.BaseModel):
    user: typing.Union[models.aws_iam.UserDef] = pydantic.Field(..., description='-')
    ...

class UntrustedCodeBoundaryPolicyDefFromAwsManagedPolicyNameParams(pydantic.BaseModel):
    managed_policy_name: str = pydantic.Field(..., description='-')
    ...

class UntrustedCodeBoundaryPolicyDefFromManagedPolicyArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='construct scope.\n')
    id: str = pydantic.Field(..., description='construct id.\n')
    managed_policy_arn: str = pydantic.Field(..., description='the ARN of the managed policy to import.')
    ...

class UntrustedCodeBoundaryPolicyDefFromManagedPolicyNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    managed_policy_name: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_codebuild.ArtifactsConfig
class ArtifactsConfigDef(BaseStruct):
    artifacts_property: typing.Union[_REQUIRED_INIT_PARAM, models.aws_codebuild.CfnProject_ArtifactsPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The low-level CloudFormation artifacts property.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    artifacts_config = codebuild.ArtifactsConfig(\n        artifacts_property=codebuild.CfnProject.ArtifactsProperty(\n            type="type",\n\n            # the properties below are optional\n            artifact_identifier="artifactIdentifier",\n            encryption_disabled=False,\n            location="location",\n            name="name",\n            namespace_type="namespaceType",\n            override_artifact_name=False,\n            packaging="packaging",\n            path="path"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['artifacts_property']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.ArtifactsConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.ArtifactsProps
class ArtifactsPropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The artifact identifier. This property is required on secondary artifacts.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    artifacts_props = codebuild.ArtifactsProps(\n        identifier="identifier"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.ArtifactsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.BatchBuildConfig
class BatchBuildConfigDef(BaseStruct):
    role: typing.Union[_REQUIRED_INIT_PARAM, models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The IAM batch service Role of this Project.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    batch_build_config = codebuild.BatchBuildConfig(\n        role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BatchBuildConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.BatchBuildConfigDefConfig] = pydantic.Field(None)


class BatchBuildConfigDefConfig(pydantic.BaseModel):
    role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.BindToCodePipelineOptions
class BindToCodePipelineOptionsDef(BaseStruct):
    artifact_bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The artifact bucket that will be used by the action that invokes this project.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n    from aws_cdk import aws_s3 as s3\n\n    # bucket: s3.Bucket\n\n    bind_to_code_pipeline_options = codebuild.BindToCodePipelineOptions(\n        artifact_bucket=bucket\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['artifact_bucket']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BindToCodePipelineOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.BindToCodePipelineOptionsDefConfig] = pydantic.Field(None)


class BindToCodePipelineOptionsDefConfig(pydantic.BaseModel):
    artifact_bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.BitBucketSourceCredentialsProps
class BitBucketSourceCredentialsPropsDef(BaseStruct):
    password: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Your BitBucket application password.\n')
    username: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Your BitBucket username.\n\n:exampleMetadata: infused\n\nExample::\n\n    codebuild.BitBucketSourceCredentials(self, "CodeBuildBitBucketCreds",\n        username=SecretValue.secrets_manager("my-bitbucket-creds", json_field="username"),\n        password=SecretValue.secrets_manager("my-bitbucket-creds", json_field="password")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['password', 'username']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BitBucketSourceCredentialsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.BitBucketSourceCredentialsPropsDefConfig] = pydantic.Field(None)


class BitBucketSourceCredentialsPropsDefConfig(pydantic.BaseModel):
    password_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)
    username_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.BitBucketSourceProps
class BitBucketSourcePropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.\n')
    owner: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The BitBucket account/user that owns the repo.\n')
    repo: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the repo (without the username).\n')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    build_status_name: typing.Optional[str] = pydantic.Field(None, description='This parameter is used for the ``name`` parameter in the Bitbucket commit status. Can use built-in CodeBuild variables, like $AWS_REGION. Default: "AWS CodeBuild $AWS_REGION ($PROJECT_NAME)"\n')
    build_status_url: typing.Optional[str] = pydantic.Field(None, description='The URL that the build will report back to the source provider. Can use built-in CodeBuild variables, like $AWS_REGION. Default: - link to the AWS Console for CodeBuild to a particular build execution\n')
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n')
    report_build_status: typing.Optional[bool] = pydantic.Field(None, description="Whether to send notifications on your build's start and end. Default: true\n")
    webhook: typing.Optional[bool] = pydantic.Field(None, description='Whether to create a webhook that will trigger a build every time an event happens in the repository. Default: true if any ``webhookFilters`` were provided, false otherwise\n')
    webhook_filters: typing.Optional[typing.Sequence[models.aws_codebuild.FilterGroupDef]] = pydantic.Field(None, description='A list of webhook filters that can constraint what events in the repository will trigger a build. A build is triggered if any of the provided filter groups match. Only valid if ``webhook`` was not provided as false. Default: every push and every Pull Request (create or update) triggers a build\n')
    webhook_triggers_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build from a webhook instead of a standard one. Enabling this will enable batch builds on the CodeBuild project. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    bb_source = codebuild.Source.bit_bucket(\n        owner="owner",\n        repo="repo"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'owner', 'repo', 'branch_or_ref', 'build_status_name', 'build_status_url', 'clone_depth', 'fetch_submodules', 'report_build_status', 'webhook', 'webhook_filters', 'webhook_triggers_batch_build']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BitBucketSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.BucketCacheOptions
class BucketCacheOptionsDef(BaseStruct):
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix to use to store the cache in the bucket.')
    _init_params: typing.ClassVar[list[str]] = ['prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BucketCacheOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.BuildEnvironment
class BuildEnvironmentDef(BaseStruct):
    build_image: typing.Optional[typing.Union[models.aws_codebuild.LinuxArmBuildImageDef, models.aws_codebuild.LinuxBuildImageDef, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None, description='The image used for the builds. Default: LinuxBuildImage.STANDARD_1_0')
    certificate: typing.Union[models.aws_codebuild.BuildEnvironmentCertificateDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the PEM-encoded certificate for the build project. Default: - No external certificate is added to the project\n')
    compute_type: typing.Optional[aws_cdk.aws_codebuild.ComputeType] = pydantic.Field(None, description='The type of compute to use for this build. See the ``ComputeType`` enum for the possible values. Default: taken from ``#buildImage#defaultComputeType``\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables that your builds can use.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Indicates how the project builds Docker images. Specify true to enable running the Docker daemon inside a Docker container. This value must be set to true only if this build project will be used to build Docker images, and the specified build environment image is not one provided by AWS CodeBuild with Docker support. Otherwise, all associated builds that attempt to interact with the Docker daemon will fail. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n    # my_security_group: ec2.SecurityGroup\n\n    pipelines.CodeBuildStep("Synth",\n        # ...standard ShellStep props...\n        commands=[],\n        env={},\n\n        # If you are using a CodeBuildStep explicitly, set the \'cdk.out\' directory\n        # to be the synth step\'s output.\n        primary_output_directory="cdk.out",\n\n        # Control the name of the project\n        project_name="MyProject",\n\n        # Control parts of the BuildSpec other than the regular \'build\' and \'install\' commands\n        partial_build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2"\n        }),\n\n        # Control the build environment\n        build_environment=codebuild.BuildEnvironment(\n            compute_type=codebuild.ComputeType.LARGE,\n            privileged=True\n        ),\n        timeout=Duration.minutes(90),\n        file_system_locations=[codebuild.FileSystemLocation.efs(\n            identifier="myidentifier2",\n            location="myclodation.mydnsroot.com:/loc",\n            mount_point="/media",\n            mount_options="opts"\n        )],\n\n        # Control Elastic Network Interface creation\n        vpc=vpc,\n        subnet_selection=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS),\n        security_groups=[my_security_group],\n\n        # Control caching\n        cache=codebuild.Cache.bucket(s3.Bucket(self, "Cache")),\n\n        # Additional policy statements for the execution role\n        role_policy_statements=[\n            iam.PolicyStatement()\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['build_image', 'certificate', 'compute_type', 'environment_variables', 'privileged']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BuildEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.BuildEnvironmentCertificate
class BuildEnvironmentCertificateDef(BaseStruct):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The bucket where the certificate is.\n')
    object_key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The full path and name of the key file.\n\n:exampleMetadata: infused\n\nExample::\n\n    # ecr_repository: ecr.Repository\n\n\n    codebuild.Project(self, "Project",\n        environment=codebuild.BuildEnvironment(\n            build_image=codebuild.WindowsBuildImage.from_ecr_repository(ecr_repository, "v1.0", codebuild.WindowsImageType.SERVER_2019),\n            # optional certificate to include in the build image\n            certificate=codebuild.BuildEnvironmentCertificate(\n                bucket=s3.Bucket.from_bucket_name(self, "Bucket", "my-bucket"),\n                object_key="path/to/cert.pem"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'object_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BuildEnvironmentCertificate'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.BuildEnvironmentCertificateDefConfig] = pydantic.Field(None)


class BuildEnvironmentCertificateDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.BuildEnvironmentVariable
class BuildEnvironmentVariableDef(BaseStruct):
    value: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The value of the environment variable. For plain-text variables (the default), this is the literal value of variable. For SSM parameter variables, pass the name of the parameter here (``parameterName`` property of ``IParameter``). For SecretsManager variables secrets, pass either the secret name (``secretName`` property of ``ISecret``) or the secret ARN (``secretArn`` property of ``ISecret``) here, along with optional SecretsManager qualifiers separated by ':', like the JSON key, or the version or stage (see https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.secrets-manager for details).")
    type: typing.Optional[aws_cdk.aws_codebuild.BuildEnvironmentVariableType] = pydantic.Field(None, description='The type of environment variable. Default: PlainText\n\n:exampleMetadata: infused\n\nExample::\n\n    # later:\n    # project: codebuild.PipelineProject\n    source_output = codepipeline.Artifact()\n    build_action = codepipeline_actions.CodeBuildAction(\n        action_name="Build1",\n        input=source_output,\n        project=codebuild.PipelineProject(self, "Project",\n            build_spec=codebuild.BuildSpec.from_object({\n                "version": "0.2",\n                "env": {\n                    "exported-variables": ["MY_VAR"\n                    ]\n                },\n                "phases": {\n                    "build": {\n                        "commands": "export MY_VAR="some value""\n                    }\n                }\n            })\n        ),\n        variables_namespace="MyNamespace"\n    )\n    codepipeline_actions.CodeBuildAction(\n        action_name="CodeBuild",\n        project=project,\n        input=source_output,\n        environment_variables={\n            "MyVar": codebuild.BuildEnvironmentVariable(\n                value=build_action.variable("MY_VAR")\n            )\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['value', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BuildEnvironmentVariable'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.BuildImageBindOptions
class BuildImageBindOptionsDef(BaseStruct):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BuildImageBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.BuildImageConfig
class BuildImageConfigDef(BaseStruct):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.BuildImageConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.ArtifactsProperty
class CfnProject_ArtifactsPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of build output artifact. Valid values include:. - ``CODEPIPELINE`` : The build project has build output generated through CodePipeline. .. epigraph:: The ``CODEPIPELINE`` type is not supported for ``secondaryArtifacts`` . - ``NO_ARTIFACTS`` : The build project does not produce any build output. - ``S3`` : The build project stores build output in Amazon S3.\n')
    artifact_identifier: typing.Optional[str] = pydantic.Field(None, description='An identifier for this artifact definition.\n')
    encryption_disabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set to true if you do not want your output artifacts encrypted. This option is valid only if your artifacts type is Amazon Simple Storage Service (Amazon S3). If this is set with another artifacts type, an ``invalidInputException`` is thrown.\n')
    location: typing.Optional[str] = pydantic.Field(None, description="Information about the build output artifact location:. - If ``type`` is set to ``CODEPIPELINE`` , AWS CodePipeline ignores this value if specified. This is because CodePipeline manages its build output locations instead of CodeBuild . - If ``type`` is set to ``NO_ARTIFACTS`` , this value is ignored if specified, because no build output is produced. - If ``type`` is set to ``S3`` , this is the name of the output bucket. If you specify ``CODEPIPELINE`` or ``NO_ARTIFACTS`` for the ``Type`` property, don't specify this property. For all of the other types, you must specify this property.\n")
    name: typing.Optional[str] = pydantic.Field(None, description='Along with ``path`` and ``namespaceType`` , the pattern that AWS CodeBuild uses to name and store the output artifact:. - If ``type`` is set to ``CODEPIPELINE`` , AWS CodePipeline ignores this value if specified. This is because CodePipeline manages its build output names instead of AWS CodeBuild . - If ``type`` is set to ``NO_ARTIFACTS`` , this value is ignored if specified, because no build output is produced. - If ``type`` is set to ``S3`` , this is the name of the output artifact object. If you set the name to be a forward slash ("/"), the artifact is stored in the root of the output bucket. For example: - If ``path`` is set to ``MyArtifacts`` , ``namespaceType`` is set to ``BUILD_ID`` , and ``name`` is set to ``MyArtifact.zip`` , then the output artifact is stored in ``MyArtifacts/ *build-ID* /MyArtifact.zip`` . - If ``path`` is empty, ``namespaceType`` is set to ``NONE`` , and ``name`` is set to " ``/`` ", the output artifact is stored in the root of the output bucket. - If ``path`` is set to ``MyArtifacts`` , ``namespaceType`` is set to ``BUILD_ID`` , and ``name`` is set to " ``/`` ", the output artifact is stored in ``MyArtifacts/ *build-ID*`` . If you specify ``CODEPIPELINE`` or ``NO_ARTIFACTS`` for the ``Type`` property, don\'t specify this property. For all of the other types, you must specify this property.\n')
    namespace_type: typing.Optional[str] = pydantic.Field(None, description='Along with ``path`` and ``name`` , the pattern that AWS CodeBuild uses to determine the name and location to store the output artifact: - If ``type`` is set to ``CODEPIPELINE`` , CodePipeline ignores this value if specified. This is because CodePipeline manages its build output names instead of AWS CodeBuild . - If ``type`` is set to ``NO_ARTIFACTS`` , this value is ignored if specified, because no build output is produced. - If ``type`` is set to ``S3`` , valid values include: - ``BUILD_ID`` : Include the build ID in the location of the build output artifact. - ``NONE`` : Do not include the build ID. This is the default if ``namespaceType`` is not specified. For example, if ``path`` is set to ``MyArtifacts`` , ``namespaceType`` is set to ``BUILD_ID`` , and ``name`` is set to ``MyArtifact.zip`` , the output artifact is stored in ``MyArtifacts/<build-ID>/MyArtifact.zip`` .\n')
    override_artifact_name: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If set to true a name specified in the buildspec file overrides the artifact name. The name specified in a buildspec file is calculated at build time and uses the Shell command language. For example, you can append a date and time to your artifact name so that it is always unique.\n')
    packaging: typing.Optional[str] = pydantic.Field(None, description='The type of build output artifact to create:. - If ``type`` is set to ``CODEPIPELINE`` , CodePipeline ignores this value if specified. This is because CodePipeline manages its build output artifacts instead of AWS CodeBuild . - If ``type`` is set to ``NO_ARTIFACTS`` , this value is ignored if specified, because no build output is produced. - If ``type`` is set to ``S3`` , valid values include: - ``NONE`` : AWS CodeBuild creates in the output bucket a folder that contains the build output. This is the default if ``packaging`` is not specified. - ``ZIP`` : AWS CodeBuild creates in the output bucket a ZIP file that contains the build output.\n')
    path: typing.Optional[str] = pydantic.Field(None, description='Along with ``namespaceType`` and ``name`` , the pattern that AWS CodeBuild uses to name and store the output artifact:. - If ``type`` is set to ``CODEPIPELINE`` , CodePipeline ignores this value if specified. This is because CodePipeline manages its build output names instead of AWS CodeBuild . - If ``type`` is set to ``NO_ARTIFACTS`` , this value is ignored if specified, because no build output is produced. - If ``type`` is set to ``S3`` , this is the path to the output artifact. If ``path`` is not specified, ``path`` is not used. For example, if ``path`` is set to ``MyArtifacts`` , ``namespaceType`` is set to ``NONE`` , and ``name`` is set to ``MyArtifact.zip`` , the output artifact is stored in the output bucket at ``MyArtifacts/MyArtifact.zip`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-artifacts.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    artifacts_property = codebuild.CfnProject.ArtifactsProperty(\n        type="type",\n\n        # the properties below are optional\n        artifact_identifier="artifactIdentifier",\n        encryption_disabled=False,\n        location="location",\n        name="name",\n        namespace_type="namespaceType",\n        override_artifact_name=False,\n        packaging="packaging",\n        path="path"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'artifact_identifier', 'encryption_disabled', 'location', 'name', 'namespace_type', 'override_artifact_name', 'packaging', 'path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.ArtifactsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.BatchRestrictionsProperty
class CfnProject_BatchRestrictionsPropertyDef(BaseStruct):
    compute_types_allowed: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of strings that specify the compute types that are allowed for the batch build. See `Build environment compute types <https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html>`_ in the *AWS CodeBuild User Guide* for these values.\n')
    maximum_builds_allowed: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum number of builds allowed.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-batchrestrictions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    batch_restrictions_property = codebuild.CfnProject.BatchRestrictionsProperty(\n        compute_types_allowed=["computeTypesAllowed"],\n        maximum_builds_allowed=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_types_allowed', 'maximum_builds_allowed']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.BatchRestrictionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.BuildStatusConfigProperty
class CfnProject_BuildStatusConfigPropertyDef(BaseStruct):
    context: typing.Optional[str] = pydantic.Field(None, description='Specifies the context of the build status CodeBuild sends to the source provider. The usage of this parameter depends on the source provider. - **Bitbucket** - This parameter is used for the ``name`` parameter in the Bitbucket commit status. For more information, see `build <https://docs.aws.amazon.com/https://developer.atlassian.com/bitbucket/api/2/reference/resource/repositories/%7Bworkspace%7D/%7Brepo_slug%7D/commit/%7Bnode%7D/statuses/build>`_ in the Bitbucket API documentation. - **GitHub/GitHub Enterprise Server** - This parameter is used for the ``context`` parameter in the GitHub commit status. For more information, see `Create a commit status <https://docs.aws.amazon.com/https://developer.github.com/v3/repos/statuses/#create-a-commit-status>`_ in the GitHub developer guide.\n')
    target_url: typing.Optional[str] = pydantic.Field(None, description='Specifies the target url of the build status CodeBuild sends to the source provider. The usage of this parameter depends on the source provider. - **Bitbucket** - This parameter is used for the ``url`` parameter in the Bitbucket commit status. For more information, see `build <https://docs.aws.amazon.com/https://developer.atlassian.com/bitbucket/api/2/reference/resource/repositories/%7Bworkspace%7D/%7Brepo_slug%7D/commit/%7Bnode%7D/statuses/build>`_ in the Bitbucket API documentation. - **GitHub/GitHub Enterprise Server** - This parameter is used for the ``target_url`` parameter in the GitHub commit status. For more information, see `Create a commit status <https://docs.aws.amazon.com/https://developer.github.com/v3/repos/statuses/#create-a-commit-status>`_ in the GitHub developer guide.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-buildstatusconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    build_status_config_property = codebuild.CfnProject.BuildStatusConfigProperty(\n        context="context",\n        target_url="targetUrl"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['context', 'target_url']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.BuildStatusConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.CloudWatchLogsConfigProperty
class CfnProject_CloudWatchLogsConfigPropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The current status of the logs in CloudWatch Logs for a build project. Valid values are:. - ``ENABLED`` : CloudWatch Logs are enabled for this build project. - ``DISABLED`` : CloudWatch Logs are not enabled for this build project.\n')
    group_name: typing.Optional[str] = pydantic.Field(None, description='The group name of the logs in CloudWatch Logs. For more information, see `Working with Log Groups and Log Streams <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html>`_ .\n')
    stream_name: typing.Optional[str] = pydantic.Field(None, description='The prefix of the stream name of the CloudWatch Logs. For more information, see `Working with Log Groups and Log Streams <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-cloudwatchlogsconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    cloud_watch_logs_config_property = codebuild.CfnProject.CloudWatchLogsConfigProperty(\n        status="status",\n\n        # the properties below are optional\n        group_name="groupName",\n        stream_name="streamName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status', 'group_name', 'stream_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.CloudWatchLogsConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.EnvironmentProperty
class CfnProject_EnvironmentPropertyDef(BaseStruct):
    compute_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of compute environment. This determines the number of CPU cores and memory the build environment uses. Available values include: - ``BUILD_GENERAL1_SMALL`` : Use up to 3 GB memory and 2 vCPUs for builds. - ``BUILD_GENERAL1_MEDIUM`` : Use up to 7 GB memory and 4 vCPUs for builds. - ``BUILD_GENERAL1_LARGE`` : Use up to 15 GB memory and 8 vCPUs for builds. For more information, see `Build Environment Compute Types <https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html>`_ in the *AWS CodeBuild User Guide.*\n')
    image: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image tag or image digest that identifies the Docker image to use for this build project. Use the following formats: - For an image tag: ``<registry>/<repository>:<tag>`` . For example, in the Docker repository that CodeBuild uses to manage its Docker images, this would be ``aws/codebuild/standard:4.0`` . - For an image digest: ``<registry>/<repository>@<digest>`` . For example, to specify an image with the digest "sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf," use ``<registry>/<repository>@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf`` . For more information, see `Docker images provided by CodeBuild <https://docs.aws.amazon.com//codebuild/latest/userguide/build-env-ref-available.html>`_ in the *AWS CodeBuild user guide* .\n')
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of build environment to use for related builds. - The environment type ``ARM_CONTAINER`` is available only in regions US East (Ohio), US East (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific (Hong Kong), Asia Pacific (Jakarta), Asia Pacific (Hyderabad), Asia Pacific (Melbourne), Asia Pacific (Mumbai), Asia Pacific (Osaka), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), China (Beijing), China (Ningxia), Europe (Frankfurt), Europe (Ireland), Europe (London), Europe (Milan), Europe (Paris), Europe (Spain), Europe (Stockholm), Europe (Zurich), Israel (Tel Aviv), Middle East (Bahrain), Middle East (UAE), and South America (São Paulo). - The environment type ``LINUX_CONTAINER`` with compute type ``build.general1.2xlarge`` is available only in regions US East (Ohio), US East (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific (Hyderabad), Asia Pacific (Hong Kong), Asia Pacific (Jakarta), Asia Pacific (Melbourne), Asia Pacific (Mumbai), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), China (Beijing), China (Ningxia), Europe (Frankfurt), Europe (Ireland), Europe (London), Europe (Paris), Europe (Spain), Europe (Stockholm), Europe (Zurich), Israel (Tel Aviv), Middle East (Bahrain), Middle East (UAE), and South America (São Paulo). - The environment type ``LINUX_GPU_CONTAINER`` is available only in regions US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), China (Beijing), China (Ningxia), Europe (Frankfurt), Europe (Ireland), and Europe (London). - The environment types ``WINDOWS_SERVER_2019_CONTAINER`` are available only in regions US East (N. Virginia), US East (Ohio), US West (Oregon), and Europe (Ireland). For more information, see `Build environment compute types <https://docs.aws.amazon.com//codebuild/latest/userguide/build-env-ref-compute-types.html>`_ in the *AWS CodeBuild user guide* .\n')
    certificate: typing.Optional[str] = pydantic.Field(None, description='The ARN of the Amazon S3 bucket, path prefix, and object key that contains the PEM-encoded certificate for the build project. For more information, see `certificate <https://docs.aws.amazon.com/codebuild/latest/userguide/create-project-cli.html#cli.environment.certificate>`_ in the *AWS CodeBuild User Guide* .\n')
    environment_variables: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_EnvironmentVariablePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A set of environment variables to make available to builds for this build project.\n')
    image_pull_credentials_type: typing.Optional[str] = pydantic.Field(None, description="The type of credentials AWS CodeBuild uses to pull images in your build. There are two valid values:. - ``CODEBUILD`` specifies that AWS CodeBuild uses its own credentials. This requires that you modify your ECR repository policy to trust AWS CodeBuild service principal. - ``SERVICE_ROLE`` specifies that AWS CodeBuild uses your build project's service role. When you use a cross-account or private registry image, you must use SERVICE_ROLE credentials. When you use an AWS CodeBuild curated image, you must use CODEBUILD credentials.\n")
    privileged_mode: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Enables running the Docker daemon inside a Docker container. Set to true only if the build project is used to build Docker images. Otherwise, a build that attempts to interact with the Docker daemon fails. The default setting is ``false`` . You can initialize the Docker daemon during the install phase of your build by adding one of the following sets of commands to the install phase of your buildspec file: If the operating system\'s base image is Ubuntu Linux: ``- nohup /usr/local/bin/dockerd --host=unix:///var/run/docker.sock --host=tcp://0.0.0.0:2375 --storage-driver=overlay&`` ``- timeout 15 sh -c "until docker info; do echo .; sleep 1; done"`` If the operating system\'s base image is Alpine Linux and the previous command does not work, add the ``-t`` argument to ``timeout`` : ``- nohup /usr/local/bin/dockerd --host=unix:///var/run/docker.sock --host=tcp://0.0.0.0:2375 --storage-driver=overlay&`` ``- timeout -t 15 sh -c "until docker info; do echo .; sleep 1; done"``\n')
    registry_credential: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_RegistryCredentialPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``RegistryCredential`` is a property of the `AWS::CodeBuild::Project Environment <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-codebuild-project.html#cfn-codebuild-project-environment>`_ property that specifies information about credentials that provide access to a private Docker registry. When this is set:. - ``imagePullCredentialsType`` must be set to ``SERVICE_ROLE`` . - images cannot be curated or an Amazon ECR image.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-environment.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    environment_property = codebuild.CfnProject.EnvironmentProperty(\n        compute_type="computeType",\n        image="image",\n        type="type",\n\n        # the properties below are optional\n        certificate="certificate",\n        environment_variables=[codebuild.CfnProject.EnvironmentVariableProperty(\n            name="name",\n            value="value",\n\n            # the properties below are optional\n            type="type"\n        )],\n        image_pull_credentials_type="imagePullCredentialsType",\n        privileged_mode=False,\n        registry_credential=codebuild.CfnProject.RegistryCredentialProperty(\n            credential="credential",\n            credential_provider="credentialProvider"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_type', 'image', 'type', 'certificate', 'environment_variables', 'image_pull_credentials_type', 'privileged_mode', 'registry_credential']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.EnvironmentProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.EnvironmentVariableProperty
class CfnProject_EnvironmentVariablePropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or key of the environment variable.\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The value of the environment variable. .. epigraph:: We strongly discourage the use of ``PLAINTEXT`` environment variables to store sensitive values, especially AWS secret key IDs. ``PLAINTEXT`` environment variables can be displayed in plain text using the AWS CodeBuild console and the AWS CLI . For sensitive values, we recommend you use an environment variable of type ``PARAMETER_STORE`` or ``SECRETS_MANAGER`` .\n')
    type: typing.Optional[str] = pydantic.Field(None, description='The type of environment variable. Valid values include:. - ``PARAMETER_STORE`` : An environment variable stored in Systems Manager Parameter Store. For environment variables of this type, specify the name of the parameter as the ``value`` of the EnvironmentVariable. The parameter value will be substituted for the name at runtime. You can also define Parameter Store environment variables in the buildspec. To learn how to do so, see `env/parameter-store <https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store>`_ in the *AWS CodeBuild User Guide* . - ``PLAINTEXT`` : An environment variable in plain text format. This is the default value. - ``SECRETS_MANAGER`` : An environment variable stored in AWS Secrets Manager . For environment variables of this type, specify the name of the secret as the ``value`` of the EnvironmentVariable. The secret value will be substituted for the name at runtime. You can also define AWS Secrets Manager environment variables in the buildspec. To learn how to do so, see `env/secrets-manager <https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.secrets-manager>`_ in the *AWS CodeBuild User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-environmentvariable.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    environment_variable_property = codebuild.CfnProject.EnvironmentVariableProperty(\n        name="name",\n        value="value",\n\n        # the properties below are optional\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.EnvironmentVariableProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.GitSubmodulesConfigProperty
class CfnProject_GitSubmodulesConfigPropertyDef(BaseStruct):
    fetch_submodules: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Set to true to fetch Git submodules for your AWS CodeBuild build project.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-gitsubmodulesconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    git_submodules_config_property = codebuild.CfnProject.GitSubmodulesConfigProperty(\n        fetch_submodules=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['fetch_submodules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.GitSubmodulesConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.LogsConfigProperty
class CfnProject_LogsConfigPropertyDef(BaseStruct):
    cloud_watch_logs: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_CloudWatchLogsConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about CloudWatch Logs for a build project. CloudWatch Logs are enabled by default.\n')
    s3_logs: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_S3LogsConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs built to an S3 bucket for a build project. S3 logs are not enabled by default.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-logsconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    logs_config_property = codebuild.CfnProject.LogsConfigProperty(\n        cloud_watch_logs=codebuild.CfnProject.CloudWatchLogsConfigProperty(\n            status="status",\n\n            # the properties below are optional\n            group_name="groupName",\n            stream_name="streamName"\n        ),\n        s3_logs=codebuild.CfnProject.S3LogsConfigProperty(\n            status="status",\n\n            # the properties below are optional\n            encryption_disabled=False,\n            location="location"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_watch_logs', 's3_logs']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.LogsConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.ProjectBuildBatchConfigProperty
class CfnProject_ProjectBuildBatchConfigPropertyDef(BaseStruct):
    batch_report_mode: typing.Optional[str] = pydantic.Field(None, description='Specifies how build status reports are sent to the source provider for the batch build. This property is only used when the source provider for your project is Bitbucket, GitHub, or GitHub Enterprise, and your project is configured to report build statuses to the source provider. - **REPORT_AGGREGATED_BATCH** - (Default) Aggregate all of the build statuses into a single status report. - **REPORT_INDIVIDUAL_BUILDS** - Send a separate status report for each individual build.\n')
    combine_artifacts: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies if the build artifacts for the batch build should be combined into a single artifact location.\n')
    restrictions: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_BatchRestrictionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A ``BatchRestrictions`` object that specifies the restrictions for the batch build.\n')
    service_role: typing.Optional[str] = pydantic.Field(None, description='Specifies the service role ARN for the batch build project.\n')
    timeout_in_mins: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum amount of time, in minutes, that the batch build must be completed in.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-projectbuildbatchconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    project_build_batch_config_property = codebuild.CfnProject.ProjectBuildBatchConfigProperty(\n        batch_report_mode="batchReportMode",\n        combine_artifacts=False,\n        restrictions=codebuild.CfnProject.BatchRestrictionsProperty(\n            compute_types_allowed=["computeTypesAllowed"],\n            maximum_builds_allowed=123\n        ),\n        service_role="serviceRole",\n        timeout_in_mins=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['batch_report_mode', 'combine_artifacts', 'restrictions', 'service_role', 'timeout_in_mins']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.ProjectBuildBatchConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.ProjectCacheProperty
class CfnProject_ProjectCachePropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of cache used by the build project. Valid values include:. - ``NO_CACHE`` : The build project does not use any cache. - ``S3`` : The build project reads and writes from and to S3. - ``LOCAL`` : The build project stores a cache locally on a build host that is only available to that build host.\n')
    location: typing.Optional[str] = pydantic.Field(None, description='Information about the cache location:. - ``NO_CACHE`` or ``LOCAL`` : This value is ignored. - ``S3`` : This is the S3 bucket name/prefix.\n')
    modes: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of strings that specify the local cache modes. You can use one or more local cache modes at the same time. This is only used for ``LOCAL`` cache types. Possible values are: - **LOCAL_SOURCE_CACHE** - Caches Git metadata for primary and secondary sources. After the cache is created, subsequent builds pull only the change between commits. This mode is a good choice for projects with a clean working directory and a source that is a large Git repository. If you choose this option and your project does not use a Git repository (GitHub, GitHub Enterprise, or Bitbucket), the option is ignored. - **LOCAL_DOCKER_LAYER_CACHE** - Caches existing Docker layers. This mode is a good choice for projects that build or pull large Docker images. It can prevent the performance issues caused by pulling large Docker images down from the network. .. epigraph:: - You can use a Docker layer cache in the Linux environment only. - The ``privileged`` flag must be set so that your project has the required Docker permissions. - You should consider the security implications before you use a Docker layer cache. - **LOCAL_CUSTOM_CACHE** - Caches directories you specify in the buildspec file. This mode is a good choice if your build scenario is not suited to one of the other three local cache modes. If you use a custom cache: - Only directories can be specified for caching. You cannot specify individual files. - Symlinks are used to reference cached directories. - Cached directories are linked to your build before it downloads its project sources. Cached items are overridden if a source item has the same name. Directories are specified using cache paths in the buildspec file.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-projectcache.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    project_cache_property = codebuild.CfnProject.ProjectCacheProperty(\n        type="type",\n\n        # the properties below are optional\n        location="location",\n        modes=["modes"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'location', 'modes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.ProjectCacheProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.ProjectFileSystemLocationProperty
class CfnProject_ProjectFileSystemLocationPropertyDef(BaseStruct):
    identifier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name used to access a file system created by Amazon EFS. CodeBuild creates an environment variable by appending the ``identifier`` in all capital letters to ``CODEBUILD_`` . For example, if you specify ``my_efs`` for ``identifier`` , a new environment variable is create named ``CODEBUILD_MY_EFS`` . The ``identifier`` is used to mount your file system.\n')
    location: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A string that specifies the location of the file system created by Amazon EFS. Its format is ``efs-dns-name:/directory-path`` . You can find the DNS name of file system when you view it in the Amazon EFS console. The directory path is a path to a directory in the file system that CodeBuild mounts. For example, if the DNS name of a file system is ``fs-abcd1234.efs.us-west-2.amazonaws.com`` , and its mount directory is ``my-efs-mount-directory`` , then the ``location`` is ``fs-abcd1234.efs.us-west-2.amazonaws.com:/my-efs-mount-directory`` . The directory path in the format ``efs-dns-name:/directory-path`` is optional. If you do not specify a directory path, the location is only the DNS name and CodeBuild mounts the entire file system.\n')
    mount_point: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The location in the container where you mount the file system.\n')
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of the file system. The one supported type is ``EFS`` .\n')
    mount_options: typing.Optional[str] = pydantic.Field(None, description='The mount options for a file system created by Amazon EFS. The default mount options used by CodeBuild are ``nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2`` . For more information, see `Recommended NFS Mount Options <https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-nfs-mount-settings.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-projectfilesystemlocation.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    project_file_system_location_property = codebuild.CfnProject.ProjectFileSystemLocationProperty(\n        identifier="identifier",\n        location="location",\n        mount_point="mountPoint",\n        type="type",\n\n        # the properties below are optional\n        mount_options="mountOptions"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'location', 'mount_point', 'type', 'mount_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.ProjectFileSystemLocationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.ProjectSourceVersionProperty
class CfnProject_ProjectSourceVersionPropertyDef(BaseStruct):
    source_identifier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An identifier for a source in the build project. The identifier can only contain alphanumeric characters and underscores, and must be less than 128 characters in length.\n')
    source_version: typing.Optional[str] = pydantic.Field(None, description='The source version for the corresponding source identifier. If specified, must be one of:. - For CodeCommit: the commit ID, branch, or Git tag to use. - For GitHub: the commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. If a pull request ID is specified, it must use the format ``pr/pull-request-ID`` (for example, ``pr/25`` ). If a branch name is specified, the branch\'s HEAD commit ID is used. If not specified, the default branch\'s HEAD commit ID is used. - For Bitbucket: the commit ID, branch name, or tag name that corresponds to the version of the source code you want to build. If a branch name is specified, the branch\'s HEAD commit ID is used. If not specified, the default branch\'s HEAD commit ID is used. - For Amazon S3: the version ID of the object that represents the build input ZIP file to use. For more information, see `Source Version Sample with CodeBuild <https://docs.aws.amazon.com/codebuild/latest/userguide/sample-source-version.html>`_ in the *AWS CodeBuild User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-projectsourceversion.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    project_source_version_property = codebuild.CfnProject.ProjectSourceVersionProperty(\n        source_identifier="sourceIdentifier",\n\n        # the properties below are optional\n        source_version="sourceVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['source_identifier', 'source_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.ProjectSourceVersionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.ProjectTriggersProperty
class CfnProject_ProjectTriggersPropertyDef(BaseStruct):
    build_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the type of build this webhook will trigger. Allowed values are:. - **BUILD** - A single build - **BUILD_BATCH** - A batch build\n')
    filter_groups: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_WebhookFilterPropertyDef, dict[str, typing.Any]]]]], None] = pydantic.Field(None, description='A list of lists of ``WebhookFilter`` objects used to determine which webhook events are triggered. At least one ``WebhookFilter`` in the array must specify ``EVENT`` as its type.\n')
    webhook: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether or not to begin automatically rebuilding the source code every time a code change is pushed to the repository.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-projecttriggers.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    project_triggers_property = codebuild.CfnProject.ProjectTriggersProperty(\n        build_type="buildType",\n        filter_groups=[[codebuild.CfnProject.WebhookFilterProperty(\n            pattern="pattern",\n            type="type",\n\n            # the properties below are optional\n            exclude_matched_pattern=False\n        )]],\n        webhook=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['build_type', 'filter_groups', 'webhook']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.ProjectTriggersProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.RegistryCredentialProperty
class CfnProject_RegistryCredentialPropertyDef(BaseStruct):
    credential: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) or name of credentials created using AWS Secrets Manager . .. epigraph:: The ``credential`` can use the name of the credentials only if they exist in your current AWS Region .\n')
    credential_provider: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The service that created the credentials to access a private Docker registry. The valid value, SECRETS_MANAGER, is for AWS Secrets Manager .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-registrycredential.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    registry_credential_property = codebuild.CfnProject.RegistryCredentialProperty(\n        credential="credential",\n        credential_provider="credentialProvider"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['credential', 'credential_provider']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.RegistryCredentialProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.S3LogsConfigProperty
class CfnProject_S3LogsConfigPropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The current status of the S3 build logs. Valid values are:. - ``ENABLED`` : S3 build logs are enabled for this build project. - ``DISABLED`` : S3 build logs are not enabled for this build project.\n')
    encryption_disabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set to true if you do not want your S3 build log output encrypted. By default S3 build logs are encrypted.\n')
    location: typing.Optional[str] = pydantic.Field(None, description='The ARN of an S3 bucket and the path prefix for S3 logs. If your Amazon S3 bucket name is ``my-bucket`` , and your path prefix is ``build-log`` , then acceptable formats are ``my-bucket/build-log`` or ``arn:aws:s3:::my-bucket/build-log`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-s3logsconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    s3_logs_config_property = codebuild.CfnProject.S3LogsConfigProperty(\n        status="status",\n\n        # the properties below are optional\n        encryption_disabled=False,\n        location="location"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status', 'encryption_disabled', 'location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.S3LogsConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.SourceAuthProperty
class CfnProject_SourceAuthPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The authorization type to use. The only valid value is ``OAUTH`` , which represents the OAuth authorization type. .. epigraph:: This data type is used by the AWS CodeBuild console only.\n')
    resource: typing.Optional[str] = pydantic.Field(None, description='The resource value that applies to the specified authorization type. .. epigraph:: This data type is used by the AWS CodeBuild console only.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-sourceauth.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    source_auth_property = codebuild.CfnProject.SourceAuthProperty(\n        type="type",\n\n        # the properties below are optional\n        resource="resource"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'resource']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.SourceAuthProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.SourceProperty
class CfnProject_SourcePropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of repository that contains the source code to be built. Valid values include:. - ``BITBUCKET`` : The source code is in a Bitbucket repository. - ``CODECOMMIT`` : The source code is in an CodeCommit repository. - ``CODEPIPELINE`` : The source code settings are specified in the source action of a pipeline in CodePipeline. - ``GITHUB`` : The source code is in a GitHub or GitHub Enterprise Cloud repository. - ``GITHUB_ENTERPRISE`` : The source code is in a GitHub Enterprise Server repository. - ``NO_SOURCE`` : The project does not have input source code. - ``S3`` : The source code is in an Amazon S3 bucket.\n')
    auth: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_SourceAuthPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Information about the authorization settings for AWS CodeBuild to access the source code to be built. This information is for the AWS CodeBuild console's use only. Your code should not get or set ``Auth`` directly.\n")
    build_spec: typing.Optional[str] = pydantic.Field(None, description='The build specification for the project. If this value is not provided, then the source code must contain a buildspec file named ``buildspec.yml`` at the root level. If this value is provided, it can be either a single string containing the entire build specification, or the path to an alternate buildspec file relative to the value of the built-in environment variable ``CODEBUILD_SRC_DIR`` . The alternate buildspec file can have a name other than ``buildspec.yml`` , for example ``myspec.yml`` or ``build_spec_qa.yml`` or similar. For more information, see the `Build Spec Reference <https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec-ref-example>`_ in the *AWS CodeBuild User Guide* .\n')
    build_status_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_BuildStatusConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Contains information that defines how the build project reports the build status to the source provider. This option is only used when the source provider is ``GITHUB`` , ``GITHUB_ENTERPRISE`` , or ``BITBUCKET`` .\n')
    git_clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build project. If your source type is Amazon S3, this value is not supported.\n')
    git_submodules_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_GitSubmodulesConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about the Git submodules configuration for the build project.\n')
    insecure_ssl: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='This is used with GitHub Enterprise only. Set to true to ignore SSL warnings while connecting to your GitHub Enterprise project repository. The default value is ``false`` . ``InsecureSsl`` should be used for testing purposes only. It should not be used in a production environment.\n')
    location: typing.Optional[str] = pydantic.Field(None, description="Information about the location of the source code to be built. Valid values include:. - For source code settings that are specified in the source action of a pipeline in CodePipeline, ``location`` should not be specified. If it is specified, CodePipeline ignores it. This is because CodePipeline uses the settings in a pipeline's source action instead of this value. - For source code in an CodeCommit repository, the HTTPS clone URL to the repository that contains the source code and the buildspec file (for example, ``https://git-codecommit.<region-ID>.amazonaws.com/v1/repos/<repo-name>`` ). - For source code in an Amazon S3 input bucket, one of the following. - The path to the ZIP file that contains the source code (for example, ``<bucket-name>/<path>/<object-name>.zip`` ). - The path to the folder that contains the source code (for example, ``<bucket-name>/<path-to-source-code>/<folder>/`` ). - For source code in a GitHub repository, the HTTPS clone URL to the repository that contains the source and the buildspec file. You must connect your AWS account to your GitHub account. Use the AWS CodeBuild console to start creating a build project. When you use the console to connect (or reconnect) with GitHub, on the GitHub *Authorize application* page, for *Organization access* , choose *Request access* next to each repository you want to allow AWS CodeBuild to have access to, and then choose *Authorize application* . (After you have connected to your GitHub account, you do not need to finish creating the build project. You can leave the AWS CodeBuild console.) To instruct AWS CodeBuild to use this connection, in the ``source`` object, set the ``auth`` object's ``type`` value to ``OAUTH`` . - For source code in a Bitbucket repository, the HTTPS clone URL to the repository that contains the source and the buildspec file. You must connect your AWS account to your Bitbucket account. Use the AWS CodeBuild console to start creating a build project. When you use the console to connect (or reconnect) with Bitbucket, on the Bitbucket *Confirm access to your account* page, choose *Grant access* . (After you have connected to your Bitbucket account, you do not need to finish creating the build project. You can leave the AWS CodeBuild console.) To instruct AWS CodeBuild to use this connection, in the ``source`` object, set the ``auth`` object's ``type`` value to ``OAUTH`` . If you specify ``CODEPIPELINE`` for the ``Type`` property, don't specify this property. For all of the other types, you must specify ``Location`` .\n")
    report_build_status: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Set to true to report the status of a build's start and finish to your source provider. This option is valid only when your source provider is GitHub, GitHub Enterprise, or Bitbucket. If this is set and you use a different source provider, an ``invalidInputException`` is thrown.\n")
    source_identifier: typing.Optional[str] = pydantic.Field(None, description='An identifier for this project source. The identifier can only contain alphanumeric characters and underscores, and must be less than 128 characters in length.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-source.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    source_property = codebuild.CfnProject.SourceProperty(\n        type="type",\n\n        # the properties below are optional\n        auth=codebuild.CfnProject.SourceAuthProperty(\n            type="type",\n\n            # the properties below are optional\n            resource="resource"\n        ),\n        build_spec="buildSpec",\n        build_status_config=codebuild.CfnProject.BuildStatusConfigProperty(\n            context="context",\n            target_url="targetUrl"\n        ),\n        git_clone_depth=123,\n        git_submodules_config=codebuild.CfnProject.GitSubmodulesConfigProperty(\n            fetch_submodules=False\n        ),\n        insecure_ssl=False,\n        location="location",\n        report_build_status=False,\n        source_identifier="sourceIdentifier"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'auth', 'build_spec', 'build_status_config', 'git_clone_depth', 'git_submodules_config', 'insecure_ssl', 'location', 'report_build_status', 'source_identifier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.SourceProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.VpcConfigProperty
class CfnProject_VpcConfigPropertyDef(BaseStruct):
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of one or more security groups IDs in your Amazon VPC. The maximum count is 5.\n')
    subnets: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of one or more subnet IDs in your Amazon VPC. The maximum count is 16.\n')
    vpc_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the Amazon VPC.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-vpcconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    vpc_config_property = codebuild.CfnProject.VpcConfigProperty(\n        security_group_ids=["securityGroupIds"],\n        subnets=["subnets"],\n        vpc_id="vpcId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['security_group_ids', 'subnets', 'vpc_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.VpcConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnProject.WebhookFilterProperty
class CfnProject_WebhookFilterPropertyDef(BaseStruct):
    pattern: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='For a ``WebHookFilter`` that uses ``EVENT`` type, a comma-separated string that specifies one or more events. For example, the webhook filter ``PUSH, PULL_REQUEST_CREATED, PULL_REQUEST_UPDATED`` allows all push, pull request created, and pull request updated events to trigger a build. For a ``WebHookFilter`` that uses any of the other filter types, a regular expression pattern. For example, a ``WebHookFilter`` that uses ``HEAD_REF`` for its ``type`` and the pattern ``^refs/heads/`` triggers a build when the head reference is a branch with a reference name ``refs/heads/branch-name`` .\n')
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of webhook filter. There are six webhook filter types: ``EVENT`` , ``ACTOR_ACCOUNT_ID`` , ``HEAD_REF`` , ``BASE_REF`` , ``FILE_PATH`` , and ``COMMIT_MESSAGE`` . - **EVENT** - A webhook event triggers a build when the provided ``pattern`` matches one of five event types: ``PUSH`` , ``PULL_REQUEST_CREATED`` , ``PULL_REQUEST_UPDATED`` , ``PULL_REQUEST_REOPENED`` , and ``PULL_REQUEST_MERGED`` . The ``EVENT`` patterns are specified as a comma-separated string. For example, ``PUSH, PULL_REQUEST_CREATED, PULL_REQUEST_UPDATED`` filters all push, pull request created, and pull request updated events. .. epigraph:: The ``PULL_REQUEST_REOPENED`` works with GitHub and GitHub Enterprise only. - **ACTOR_ACCOUNT_ID** - A webhook event triggers a build when a GitHub, GitHub Enterprise, or Bitbucket account ID matches the regular expression ``pattern`` . - **HEAD_REF** - A webhook event triggers a build when the head reference matches the regular expression ``pattern`` . For example, ``refs/heads/branch-name`` and ``refs/tags/tag-name`` . Works with GitHub and GitHub Enterprise push, GitHub and GitHub Enterprise pull request, Bitbucket push, and Bitbucket pull request events. - **BASE_REF** - A webhook event triggers a build when the base reference matches the regular expression ``pattern`` . For example, ``refs/heads/branch-name`` . .. epigraph:: Works with pull request events only. - **FILE_PATH** - A webhook triggers a build when the path of a changed file matches the regular expression ``pattern`` . .. epigraph:: Works with GitHub and Bitbucket events push and pull requests events. Also works with GitHub Enterprise push events, but does not work with GitHub Enterprise pull request events. - **COMMIT_MESSAGE** - A webhook triggers a build when the head commit message matches the regular expression ``pattern`` . .. epigraph:: Works with GitHub and Bitbucket events push and pull requests events. Also works with GitHub Enterprise push events, but does not work with GitHub Enterprise pull request events.\n')
    exclude_matched_pattern: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Used to indicate that the ``pattern`` determines which webhook events do not trigger a build. If true, then a webhook event that does not match the ``pattern`` triggers a build. If false, then a webhook event that matches the ``pattern`` triggers a build.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-webhookfilter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    webhook_filter_property = codebuild.CfnProject.WebhookFilterProperty(\n        pattern="pattern",\n        type="type",\n\n        # the properties below are optional\n        exclude_matched_pattern=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['pattern', 'type', 'exclude_matched_pattern']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject.WebhookFilterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnReportGroup.ReportExportConfigProperty
class CfnReportGroup_ReportExportConfigPropertyDef(BaseStruct):
    export_config_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The export configuration type. Valid values are:. - ``S3`` : The report results are exported to an S3 bucket. - ``NO_EXPORT`` : The report results are not exported.\n')
    s3_destination: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnReportGroup_S3ReportExportConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A ``S3ReportExportConfig`` object that contains information about the S3 bucket where the run of a report is exported.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-reportgroup-reportexportconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    report_export_config_property = codebuild.CfnReportGroup.ReportExportConfigProperty(\n        export_config_type="exportConfigType",\n\n        # the properties below are optional\n        s3_destination=codebuild.CfnReportGroup.S3ReportExportConfigProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            bucket_owner="bucketOwner",\n            encryption_disabled=False,\n            encryption_key="encryptionKey",\n            packaging="packaging",\n            path="path"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['export_config_type', 's3_destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnReportGroup.ReportExportConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnReportGroup.S3ReportExportConfigProperty
class CfnReportGroup_S3ReportExportConfigPropertyDef(BaseStruct):
    bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the S3 bucket where the raw data of a report are exported.\n')
    bucket_owner: typing.Optional[str] = pydantic.Field(None, description='The AWS account identifier of the owner of the Amazon S3 bucket. This allows report data to be exported to an Amazon S3 bucket that is owned by an account other than the account running the build.\n')
    encryption_disabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A boolean value that specifies if the results of a report are encrypted.\n')
    encryption_key: typing.Optional[str] = pydantic.Field(None, description="The encryption key for the report's encrypted raw data.\n")
    packaging: typing.Optional[str] = pydantic.Field(None, description='The type of build output artifact to create. Valid values include:. - ``NONE`` : CodeBuild creates the raw data in the output bucket. This is the default if packaging is not specified. - ``ZIP`` : CodeBuild creates a ZIP file with the raw data in the output bucket.\n')
    path: typing.Optional[str] = pydantic.Field(None, description='The path to the exported report\'s raw data results.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-reportgroup-s3reportexportconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    s3_report_export_config_property = codebuild.CfnReportGroup.S3ReportExportConfigProperty(\n        bucket="bucket",\n\n        # the properties below are optional\n        bucket_owner="bucketOwner",\n        encryption_disabled=False,\n        encryption_key="encryptionKey",\n        packaging="packaging",\n        path="path"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_owner', 'encryption_disabled', 'encryption_key', 'packaging', 'path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnReportGroup.S3ReportExportConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CloudWatchLoggingOptions
class CloudWatchLoggingOptionsDef(BaseStruct):
    enabled: typing.Optional[bool] = pydantic.Field(None, description='The current status of the logs in Amazon CloudWatch Logs for a build project. Default: true\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The Log Group to send logs to. Default: - no log group specified\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix of the stream name of the Amazon CloudWatch Logs. Default: - no prefix\n\n:exampleMetadata: infused\n\nExample::\n\n    codebuild.Project(self, "Project",\n        logging=codebuild.LoggingOptions(\n            cloud_watch=codebuild.CloudWatchLoggingOptions(\n                log_group=logs.LogGroup(self, "MyLogGroup")\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['enabled', 'log_group', 'prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CloudWatchLoggingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CodeCommitSourceProps
class CodeCommitSourcePropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.\n')
    repository: typing.Union[_REQUIRED_INIT_PARAM, models.aws_codecommit.RepositoryDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_codecommit as codecommit\n    # repo: codecommit.Repository\n    # bucket: s3.Bucket\n\n\n    project = codebuild.Project(self, "MyProject",\n        secondary_sources=[\n            codebuild.Source.code_commit(\n                identifier="source2",\n                repository=repo\n            )\n        ],\n        secondary_artifacts=[\n            codebuild.Artifacts.s3(\n                identifier="artifact2",\n                bucket=bucket,\n                path="some/path",\n                name="file.zip"\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'repository', 'branch_or_ref', 'clone_depth', 'fetch_submodules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CodeCommitSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.CodeCommitSourcePropsDefConfig] = pydantic.Field(None)


class CodeCommitSourcePropsDefConfig(pydantic.BaseModel):
    repository_config: typing.Optional[models._interface_methods.AwsCodecommitIRepositoryDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.CommonProjectProps
class CommonProjectPropsDef(BaseStruct):
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description="Whether to allow the CodeBuild to send all network traffic. If set to false, you must individually add traffic rules to allow the CodeBuild project to connect to network targets. Only used if 'vpc' is supplied. Default: true")
    badge: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether AWS CodeBuild generates a publicly accessible URL for your project's build badge. For more information, see Build Badges Sample in the AWS CodeBuild User Guide. Default: false\n")
    build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description='Filename or contents of buildspec in JSON format. Default: - Empty buildspec.\n')
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: Cache.none\n')
    check_secrets_in_plain_text_env_variables: typing.Optional[bool] = pydantic.Field(None, description='Whether to check for the presence of any secrets in the environment variables of the default type, BuildEnvironmentVariableType.PLAINTEXT. Since using a secret for the value of that kind of variable would result in it being displayed in plain text in the AWS Console, the construct will throw an exception if it detects a secret was passed there. Pass this property as false if you want to skip this validation, and keep using a secret in a plain text environment variable. Default: true\n')
    concurrent_build_limit: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of concurrent builds. Minimum value is 1 and maximum is account build limit. Default: - no explicit limit is set\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the project. Use the description to identify the purpose of the project. Default: - No description.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='Encryption key to use to read and write artifacts. Default: - The AWS-managed CMK for Amazon Simple Storage Service (Amazon S3) is used.\n')
    environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Build environment to use for the build. Default: BuildEnvironment.LinuxBuildImage.STANDARD_1_0\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional environment variables to add to the build environment. Default: - No additional environment variables are specified.\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='An ProjectFileSystemLocation objects for a CodeBuild build project. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Default: - no file system locations\n')
    grant_report_group_permissions: typing.Optional[bool] = pydantic.Field(None, description="Add permissions to this project's role to create and use test report groups with name starting with the name of this project. That is the standard report group that gets created when a simple name (in contrast to an ARN) is used in the 'reports' section of the buildspec of this project. This is usually harmless, but you can turn these off if you don't plan on using test reports in this project. Default: true\n")
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for the build project. A project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    project_name: typing.Optional[str] = pydantic.Field(None, description='The physical, human-readable name of the CodeBuild Project. Default: - Name is automatically generated.\n')
    queued_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's still in queue. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: - no queue timeout is set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Service Role to assume while running the build. Default: - A role will be created.\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="What security group to associate with the codebuild project's network interfaces. If no security group is identified, one will be created automatically. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    ssm_session_permissions: typing.Optional[bool] = pydantic.Field(None, description='Add the permissions necessary for debugging builds with SSM Session Manager. If the following prerequisites have been met: - The necessary permissions have been added by setting this flag to true. - The build image has the SSM agent installed (true for default CodeBuild images). - The build is started with `debugSessionEnabled <https://docs.aws.amazon.com/codebuild/latest/APIReference/API_StartBuild.html#CodeBuild-StartBuild-request-debugSessionEnabled>`_ set to true. Then the build container can be paused and inspected using Session Manager by invoking the ``codebuild-breakpoint`` command somewhere during the build. ``codebuild-breakpoint`` commands will be ignored if the build is not started with ``debugSessionEnabled=true``. Default: false\n')
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Where to place the network interfaces within the VPC. To access AWS services, your CodeBuild project needs to be in one of the following types of subnets: 1. Subnets with access to the internet (of type PRIVATE_WITH_EGRESS). 2. Private subnets unconnected to the internet, but with `VPC endpoints <https://docs.aws.amazon.com/codebuild/latest/userguide/use-vpc-endpoints-with-codebuild.html>`_ for the necessary services. If you don't specify a subnet selection, the default behavior is to use PRIVATE_WITH_EGRESS subnets first if they exist, then PRIVATE_WITHOUT_EGRESS, and finally PUBLIC subnets. If your VPC doesn't have PRIVATE_WITH_EGRESS subnets but you need AWS service access, add VPC Endpoints to your private subnets. Default: - private subnets if available else public subnets\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place codebuild network interfaces. Specify this if the codebuild project needs to access resources in a VPC. Default: - No VPC is specified.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_codebuild as codebuild\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_kms as kms\n    from aws_cdk import aws_logs as logs\n    from aws_cdk import aws_s3 as s3\n\n    # bucket: s3.Bucket\n    # build_image: codebuild.IBuildImage\n    # build_spec: codebuild.BuildSpec\n    # cache: codebuild.Cache\n    # file_system_location: codebuild.IFileSystemLocation\n    # key: kms.Key\n    # log_group: logs.LogGroup\n    # role: iam.Role\n    # security_group: ec2.SecurityGroup\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # value: Any\n    # vpc: ec2.Vpc\n\n    common_project_props = codebuild.CommonProjectProps(\n        allow_all_outbound=False,\n        badge=False,\n        build_spec=build_spec,\n        cache=cache,\n        check_secrets_in_plain_text_env_variables=False,\n        concurrent_build_limit=123,\n        description="description",\n        encryption_key=key,\n        environment=codebuild.BuildEnvironment(\n            build_image=build_image,\n            certificate=codebuild.BuildEnvironmentCertificate(\n                bucket=bucket,\n                object_key="objectKey"\n            ),\n            compute_type=codebuild.ComputeType.SMALL,\n            environment_variables={\n                "environment_variables_key": codebuild.BuildEnvironmentVariable(\n                    value=value,\n\n                    # the properties below are optional\n                    type=codebuild.BuildEnvironmentVariableType.PLAINTEXT\n                )\n            },\n            privileged=False\n        ),\n        environment_variables={\n            "environment_variables_key": codebuild.BuildEnvironmentVariable(\n                value=value,\n\n                # the properties below are optional\n                type=codebuild.BuildEnvironmentVariableType.PLAINTEXT\n            )\n        },\n        file_system_locations=[file_system_location],\n        grant_report_group_permissions=False,\n        logging=codebuild.LoggingOptions(\n            cloud_watch=codebuild.CloudWatchLoggingOptions(\n                enabled=False,\n                log_group=log_group,\n                prefix="prefix"\n            ),\n            s3=codebuild.S3LoggingOptions(\n                bucket=bucket,\n\n                # the properties below are optional\n                enabled=False,\n                encrypted=False,\n                prefix="prefix"\n            )\n        ),\n        project_name="projectName",\n        queued_timeout=cdk.Duration.minutes(30),\n        role=role,\n        security_groups=[security_group],\n        ssm_session_permissions=False,\n        subnet_selection=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        ),\n        timeout=cdk.Duration.minutes(30),\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_all_outbound', 'badge', 'build_spec', 'cache', 'check_secrets_in_plain_text_env_variables', 'concurrent_build_limit', 'description', 'encryption_key', 'environment', 'environment_variables', 'file_system_locations', 'grant_report_group_permissions', 'logging', 'project_name', 'queued_timeout', 'role', 'security_groups', 'ssm_session_permissions', 'subnet_selection', 'timeout', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CommonProjectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.DockerImageOptions
class DockerImageOptionsDef(BaseStruct):
    secrets_manager_credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The credentials, stored in Secrets Manager, used for accessing the repository holding the image, if the repository is private. Default: no credentials will be used (we assume the repository is public)\n\n:exampleMetadata: lit=aws-codebuild/test/integ.docker-registry.lit.ts infused\n\nExample::\n\n    environment=cdk.aws_codebuild.BuildEnvironment(\n        build_image=codebuild.LinuxBuildImage.from_docker_registry("my-registry/my-repo",\n            secrets_manager_credentials=secrets\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['secrets_manager_credentials']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.DockerImageOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.EfsFileSystemLocationProps
class EfsFileSystemLocationPropsDef(BaseStruct):
    identifier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name used to access a file system created by Amazon EFS.\n')
    location: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A string that specifies the location of the file system, like Amazon EFS. This value looks like ``fs-abcd1234.efs.us-west-2.amazonaws.com:/my-efs-mount-directory``.\n')
    mount_point: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The location in the container where you mount the file system.\n')
    mount_options: typing.Optional[str] = pydantic.Field(None, description='The mount options for a file system such as Amazon EFS. Default: \'nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2\'.\n\n:exampleMetadata: infused\n\nExample::\n\n    codebuild.Project(self, "MyProject",\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2"\n        }),\n        file_system_locations=[\n            codebuild.FileSystemLocation.efs(\n                identifier="myidentifier2",\n                location="myclodation.mydnsroot.com:/loc",\n                mount_point="/media",\n                mount_options="opts"\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'location', 'mount_point', 'mount_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.EfsFileSystemLocationProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.FileSystemConfig
class FileSystemConfigDef(BaseStruct):
    location: typing.Union[_REQUIRED_INIT_PARAM, models.aws_codebuild.CfnProject_ProjectFileSystemLocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='File system location wrapper property.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    file_system_config = codebuild.FileSystemConfig(\n        location=codebuild.CfnProject.ProjectFileSystemLocationProperty(\n            identifier="identifier",\n            location="location",\n            mount_point="mountPoint",\n            type="type",\n\n            # the properties below are optional\n            mount_options="mountOptions"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.FileSystemConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.GitHubEnterpriseSourceCredentialsProps
class GitHubEnterpriseSourceCredentialsPropsDef(BaseStruct):
    access_token: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The personal access token to use when contacting the instance of the GitHub Enterprise API.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_codebuild as codebuild\n\n    # secret_value: cdk.SecretValue\n\n    git_hub_enterprise_source_credentials_props = codebuild.GitHubEnterpriseSourceCredentialsProps(\n        access_token=secret_value\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_token']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.GitHubEnterpriseSourceCredentialsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.GitHubEnterpriseSourceCredentialsPropsDefConfig] = pydantic.Field(None)


class GitHubEnterpriseSourceCredentialsPropsDefConfig(pydantic.BaseModel):
    access_token_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.GitHubEnterpriseSourceProps
class GitHubEnterpriseSourcePropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.\n')
    https_clone_url: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The HTTPS URL of the repository in your GitHub Enterprise installation.\n')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    build_status_context: typing.Optional[str] = pydantic.Field(None, description='This parameter is used for the ``context`` parameter in the GitHub commit status. Can use built-in CodeBuild variables, like $AWS_REGION. Default: "AWS CodeBuild $AWS_REGION ($PROJECT_NAME)"\n')
    build_status_url: typing.Optional[str] = pydantic.Field(None, description='The URL that the build will report back to the source provider. Can use built-in CodeBuild variables, like $AWS_REGION. Default: - link to the AWS Console for CodeBuild to a particular build execution\n')
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n')
    ignore_ssl_errors: typing.Optional[bool] = pydantic.Field(None, description='Whether to ignore SSL errors when connecting to the repository. Default: false\n')
    report_build_status: typing.Optional[bool] = pydantic.Field(None, description="Whether to send notifications on your build's start and end. Default: true\n")
    webhook: typing.Optional[bool] = pydantic.Field(None, description='Whether to create a webhook that will trigger a build every time an event happens in the repository. Default: true if any ``webhookFilters`` were provided, false otherwise\n')
    webhook_filters: typing.Optional[typing.Sequence[models.aws_codebuild.FilterGroupDef]] = pydantic.Field(None, description='A list of webhook filters that can constraint what events in the repository will trigger a build. A build is triggered if any of the provided filter groups match. Only valid if ``webhook`` was not provided as false. Default: every push and every Pull Request (create or update) triggers a build\n')
    webhook_triggers_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build from a webhook instead of a standard one. Enabling this will enable batch builds on the CodeBuild project. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    codebuild.Project(self, "Project",\n        source=codebuild.Source.git_hub_enterprise(\n            https_clone_url="https://my-github-enterprise.com/owner/repo"\n        ),\n\n        # Enable Docker AND custom caching\n        cache=codebuild.Cache.local(codebuild.LocalCacheMode.DOCKER_LAYER, codebuild.LocalCacheMode.CUSTOM),\n\n        # BuildSpec with a \'cache\' section necessary for \'CUSTOM\' caching. This can\n        # also come from \'buildspec.yml\' in your source.\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2",\n            "phases": {\n                "build": {\n                    "commands": ["..."]\n                }\n            },\n            "cache": {\n                "paths": ["/root/cachedir/**/*"\n                ]\n            }\n        })\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'https_clone_url', 'branch_or_ref', 'build_status_context', 'build_status_url', 'clone_depth', 'fetch_submodules', 'ignore_ssl_errors', 'report_build_status', 'webhook', 'webhook_filters', 'webhook_triggers_batch_build']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.GitHubEnterpriseSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.GitHubSourceCredentialsProps
class GitHubSourceCredentialsPropsDef(BaseStruct):
    access_token: typing.Union[models.SecretValueDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The personal access token to use when contacting the GitHub API.\n\n:exampleMetadata: infused\n\nExample::\n\n    codebuild.GitHubSourceCredentials(self, "CodeBuildGitHubCreds",\n        access_token=SecretValue.secrets_manager("my-token")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_token']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.GitHubSourceCredentialsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.GitHubSourceCredentialsPropsDefConfig] = pydantic.Field(None)


class GitHubSourceCredentialsPropsDefConfig(pydantic.BaseModel):
    access_token_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.GitHubSourceProps
class GitHubSourcePropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.\n')
    owner: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The GitHub account/user that owns the repo.\n')
    repo: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the repo (without the username).\n')
    branch_or_ref: typing.Optional[str] = pydantic.Field(None, description="The commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. Default: the default branch's HEAD commit ID is used\n")
    build_status_context: typing.Optional[str] = pydantic.Field(None, description='This parameter is used for the ``context`` parameter in the GitHub commit status. Can use built-in CodeBuild variables, like $AWS_REGION. Default: "AWS CodeBuild $AWS_REGION ($PROJECT_NAME)"\n')
    build_status_url: typing.Optional[str] = pydantic.Field(None, description='The URL that the build will report back to the source provider. Can use built-in CodeBuild variables, like $AWS_REGION. Default: - link to the AWS Console for CodeBuild to a particular build execution\n')
    clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='The depth of history to download. Minimum value is 0. If this value is 0, greater than 25, or not provided, then the full history is downloaded with each build of the project.\n')
    fetch_submodules: typing.Optional[bool] = pydantic.Field(None, description='Whether to fetch submodules while cloning git repo. Default: false\n')
    report_build_status: typing.Optional[bool] = pydantic.Field(None, description="Whether to send notifications on your build's start and end. Default: true\n")
    webhook: typing.Optional[bool] = pydantic.Field(None, description='Whether to create a webhook that will trigger a build every time an event happens in the repository. Default: true if any ``webhookFilters`` were provided, false otherwise\n')
    webhook_filters: typing.Optional[typing.Sequence[models.aws_codebuild.FilterGroupDef]] = pydantic.Field(None, description='A list of webhook filters that can constraint what events in the repository will trigger a build. A build is triggered if any of the provided filter groups match. Only valid if ``webhook`` was not provided as false. Default: every push and every Pull Request (create or update) triggers a build\n')
    webhook_triggers_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build from a webhook instead of a standard one. Enabling this will enable batch builds on the CodeBuild project. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    project = codebuild.Project(self, "MyProject",\n        build_spec=codebuild.BuildSpec.from_source_filename("my-buildspec.yml"),\n        source=codebuild.Source.git_hub(\n            owner="awslabs",\n            repo="aws-cdk"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'owner', 'repo', 'branch_or_ref', 'build_status_context', 'build_status_url', 'clone_depth', 'fetch_submodules', 'report_build_status', 'webhook', 'webhook_filters', 'webhook_triggers_batch_build']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.GitHubSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.LoggingOptions
class LoggingOptionsDef(BaseStruct):
    cloud_watch: typing.Union[models.aws_codebuild.CloudWatchLoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about Amazon CloudWatch Logs for a build project. Default: - enabled\n')
    s3: typing.Union[models.aws_codebuild.S3LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs built to an S3 bucket for a build project. Default: - disabled\n\n:exampleMetadata: infused\n\nExample::\n\n    codebuild.Project(self, "Project",\n        logging=codebuild.LoggingOptions(\n            cloud_watch=codebuild.CloudWatchLoggingOptions(\n                log_group=logs.LogGroup(self, "MyLogGroup")\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_watch', 's3']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.LoggingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.PipelineProjectProps
class PipelineProjectPropsDef(BaseStruct):
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description="Whether to allow the CodeBuild to send all network traffic. If set to false, you must individually add traffic rules to allow the CodeBuild project to connect to network targets. Only used if 'vpc' is supplied. Default: true")
    badge: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether AWS CodeBuild generates a publicly accessible URL for your project's build badge. For more information, see Build Badges Sample in the AWS CodeBuild User Guide. Default: false\n")
    build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description='Filename or contents of buildspec in JSON format. Default: - Empty buildspec.\n')
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: Cache.none\n')
    check_secrets_in_plain_text_env_variables: typing.Optional[bool] = pydantic.Field(None, description='Whether to check for the presence of any secrets in the environment variables of the default type, BuildEnvironmentVariableType.PLAINTEXT. Since using a secret for the value of that kind of variable would result in it being displayed in plain text in the AWS Console, the construct will throw an exception if it detects a secret was passed there. Pass this property as false if you want to skip this validation, and keep using a secret in a plain text environment variable. Default: true\n')
    concurrent_build_limit: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of concurrent builds. Minimum value is 1 and maximum is account build limit. Default: - no explicit limit is set\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the project. Use the description to identify the purpose of the project. Default: - No description.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='Encryption key to use to read and write artifacts. Default: - The AWS-managed CMK for Amazon Simple Storage Service (Amazon S3) is used.\n')
    environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Build environment to use for the build. Default: BuildEnvironment.LinuxBuildImage.STANDARD_1_0\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional environment variables to add to the build environment. Default: - No additional environment variables are specified.\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='An ProjectFileSystemLocation objects for a CodeBuild build project. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Default: - no file system locations\n')
    grant_report_group_permissions: typing.Optional[bool] = pydantic.Field(None, description="Add permissions to this project's role to create and use test report groups with name starting with the name of this project. That is the standard report group that gets created when a simple name (in contrast to an ARN) is used in the 'reports' section of the buildspec of this project. This is usually harmless, but you can turn these off if you don't plan on using test reports in this project. Default: true\n")
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for the build project. A project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    project_name: typing.Optional[str] = pydantic.Field(None, description='The physical, human-readable name of the CodeBuild Project. Default: - Name is automatically generated.\n')
    queued_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's still in queue. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: - no queue timeout is set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Service Role to assume while running the build. Default: - A role will be created.\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="What security group to associate with the codebuild project's network interfaces. If no security group is identified, one will be created automatically. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    ssm_session_permissions: typing.Optional[bool] = pydantic.Field(None, description='Add the permissions necessary for debugging builds with SSM Session Manager. If the following prerequisites have been met: - The necessary permissions have been added by setting this flag to true. - The build image has the SSM agent installed (true for default CodeBuild images). - The build is started with `debugSessionEnabled <https://docs.aws.amazon.com/codebuild/latest/APIReference/API_StartBuild.html#CodeBuild-StartBuild-request-debugSessionEnabled>`_ set to true. Then the build container can be paused and inspected using Session Manager by invoking the ``codebuild-breakpoint`` command somewhere during the build. ``codebuild-breakpoint`` commands will be ignored if the build is not started with ``debugSessionEnabled=true``. Default: false\n')
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Where to place the network interfaces within the VPC. To access AWS services, your CodeBuild project needs to be in one of the following types of subnets: 1. Subnets with access to the internet (of type PRIVATE_WITH_EGRESS). 2. Private subnets unconnected to the internet, but with `VPC endpoints <https://docs.aws.amazon.com/codebuild/latest/userguide/use-vpc-endpoints-with-codebuild.html>`_ for the necessary services. If you don't specify a subnet selection, the default behavior is to use PRIVATE_WITH_EGRESS subnets first if they exist, then PRIVATE_WITHOUT_EGRESS, and finally PUBLIC subnets. If your VPC doesn't have PRIVATE_WITH_EGRESS subnets but you need AWS service access, add VPC Endpoints to your private subnets. Default: - private subnets if available else public subnets\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place codebuild network interfaces. Specify this if the codebuild project needs to access resources in a VPC. Default: - No VPC is specified.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Create a Cloudfront Web Distribution\n    import aws_cdk.aws_cloudfront as cloudfront\n    # distribution: cloudfront.Distribution\n\n\n    # Create the build project that will invalidate the cache\n    invalidate_build_project = codebuild.PipelineProject(self, "InvalidateProject",\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2",\n            "phases": {\n                "build": {\n                    "commands": ["aws cloudfront create-invalidation --distribution-id ${CLOUDFRONT_ID} --paths "/*""\n                    ]\n                }\n            }\n        }),\n        environment_variables={\n            "CLOUDFRONT_ID": codebuild.BuildEnvironmentVariable(value=distribution.distribution_id)\n        }\n    )\n\n    # Add Cloudfront invalidation permissions to the project\n    distribution_arn = f"arn:aws:cloudfront::{this.account}:distribution/{distribution.distributionId}"\n    invalidate_build_project.add_to_role_policy(iam.PolicyStatement(\n        resources=[distribution_arn],\n        actions=["cloudfront:CreateInvalidation"\n        ]\n    ))\n\n    # Create the pipeline (here only the S3 deploy and Invalidate cache build)\n    deploy_bucket = s3.Bucket(self, "DeployBucket")\n    deploy_input = codepipeline.Artifact()\n    codepipeline.Pipeline(self, "Pipeline",\n        stages=[codepipeline.StageProps(\n            stage_name="Deploy",\n            actions=[\n                codepipeline_actions.S3DeployAction(\n                    action_name="S3Deploy",\n                    bucket=deploy_bucket,\n                    input=deploy_input,\n                    run_order=1\n                ),\n                codepipeline_actions.CodeBuildAction(\n                    action_name="InvalidateCache",\n                    project=invalidate_build_project,\n                    input=deploy_input,\n                    run_order=2\n                )\n            ]\n        )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_all_outbound', 'badge', 'build_spec', 'cache', 'check_secrets_in_plain_text_env_variables', 'concurrent_build_limit', 'description', 'encryption_key', 'environment', 'environment_variables', 'file_system_locations', 'grant_report_group_permissions', 'logging', 'project_name', 'queued_timeout', 'role', 'security_groups', 'ssm_session_permissions', 'subnet_selection', 'timeout', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.PipelineProjectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.ProjectNotifyOnOptions
class ProjectNotifyOnOptionsDef(BaseStruct):
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n')
    events: typing.Union[typing.Sequence[aws_cdk.aws_codebuild.ProjectNotificationEvents], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A list of event types associated with this notification rule for CodeBuild Project. For a complete list of event types and IDs, see Notification concepts in the Developer Tools Console User Guide.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n    from aws_cdk import aws_codestarnotifications as codestarnotifications\n\n    project_notify_on_options = codebuild.ProjectNotifyOnOptions(\n        events=[codebuild.ProjectNotificationEvents.BUILD_FAILED],\n\n        # the properties below are optional\n        detail_type=codestarnotifications.DetailType.BASIC,\n        enabled=False,\n        notification_rule_name="notificationRuleName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['detail_type', 'enabled', 'notification_rule_name', 'events']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.ProjectNotifyOnOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.ProjectProps
class ProjectPropsDef(BaseStruct):
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description="Whether to allow the CodeBuild to send all network traffic. If set to false, you must individually add traffic rules to allow the CodeBuild project to connect to network targets. Only used if 'vpc' is supplied. Default: true")
    badge: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether AWS CodeBuild generates a publicly accessible URL for your project's build badge. For more information, see Build Badges Sample in the AWS CodeBuild User Guide. Default: false\n")
    build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description='Filename or contents of buildspec in JSON format. Default: - Empty buildspec.\n')
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: Cache.none\n')
    check_secrets_in_plain_text_env_variables: typing.Optional[bool] = pydantic.Field(None, description='Whether to check for the presence of any secrets in the environment variables of the default type, BuildEnvironmentVariableType.PLAINTEXT. Since using a secret for the value of that kind of variable would result in it being displayed in plain text in the AWS Console, the construct will throw an exception if it detects a secret was passed there. Pass this property as false if you want to skip this validation, and keep using a secret in a plain text environment variable. Default: true\n')
    concurrent_build_limit: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of concurrent builds. Minimum value is 1 and maximum is account build limit. Default: - no explicit limit is set\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the project. Use the description to identify the purpose of the project. Default: - No description.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='Encryption key to use to read and write artifacts. Default: - The AWS-managed CMK for Amazon Simple Storage Service (Amazon S3) is used.\n')
    environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Build environment to use for the build. Default: BuildEnvironment.LinuxBuildImage.STANDARD_1_0\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional environment variables to add to the build environment. Default: - No additional environment variables are specified.\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='An ProjectFileSystemLocation objects for a CodeBuild build project. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Default: - no file system locations\n')
    grant_report_group_permissions: typing.Optional[bool] = pydantic.Field(None, description="Add permissions to this project's role to create and use test report groups with name starting with the name of this project. That is the standard report group that gets created when a simple name (in contrast to an ARN) is used in the 'reports' section of the buildspec of this project. This is usually harmless, but you can turn these off if you don't plan on using test reports in this project. Default: true\n")
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for the build project. A project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    project_name: typing.Optional[str] = pydantic.Field(None, description='The physical, human-readable name of the CodeBuild Project. Default: - Name is automatically generated.\n')
    queued_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's still in queue. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: - no queue timeout is set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Service Role to assume while running the build. Default: - A role will be created.\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="What security group to associate with the codebuild project's network interfaces. If no security group is identified, one will be created automatically. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    ssm_session_permissions: typing.Optional[bool] = pydantic.Field(None, description='Add the permissions necessary for debugging builds with SSM Session Manager. If the following prerequisites have been met: - The necessary permissions have been added by setting this flag to true. - The build image has the SSM agent installed (true for default CodeBuild images). - The build is started with `debugSessionEnabled <https://docs.aws.amazon.com/codebuild/latest/APIReference/API_StartBuild.html#CodeBuild-StartBuild-request-debugSessionEnabled>`_ set to true. Then the build container can be paused and inspected using Session Manager by invoking the ``codebuild-breakpoint`` command somewhere during the build. ``codebuild-breakpoint`` commands will be ignored if the build is not started with ``debugSessionEnabled=true``. Default: false\n')
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Where to place the network interfaces within the VPC. To access AWS services, your CodeBuild project needs to be in one of the following types of subnets: 1. Subnets with access to the internet (of type PRIVATE_WITH_EGRESS). 2. Private subnets unconnected to the internet, but with `VPC endpoints <https://docs.aws.amazon.com/codebuild/latest/userguide/use-vpc-endpoints-with-codebuild.html>`_ for the necessary services. If you don't specify a subnet selection, the default behavior is to use PRIVATE_WITH_EGRESS subnets first if they exist, then PRIVATE_WITHOUT_EGRESS, and finally PUBLIC subnets. If your VPC doesn't have PRIVATE_WITH_EGRESS subnets but you need AWS service access, add VPC Endpoints to your private subnets. Default: - private subnets if available else public subnets\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place codebuild network interfaces. Specify this if the codebuild project needs to access resources in a VPC. Default: - No VPC is specified.\n')
    artifacts: typing.Optional[typing.Union[models.aws_codebuild.ArtifactsDef]] = pydantic.Field(None, description='Defines where build artifacts will be stored. Could be: PipelineBuildArtifacts, NoArtifacts and S3Artifacts. Default: NoArtifacts\n')
    secondary_artifacts: typing.Optional[typing.Sequence[typing.Union[models.aws_codebuild.ArtifactsDef]]] = pydantic.Field(None, description='The secondary artifacts for the Project. Can also be added after the Project has been created by using the ``Project#addSecondaryArtifact`` method. Default: - No secondary artifacts.\n')
    secondary_sources: typing.Optional[typing.Sequence[typing.Union[models.aws_codebuild.SourceDef]]] = pydantic.Field(None, description='The secondary sources for the Project. Can be also added after the Project has been created by using the ``Project#addSecondarySource`` method. Default: - No secondary sources.\n')
    source: typing.Optional[typing.Union[models.aws_codebuild.SourceDef]] = pydantic.Field(None, description='The source of the build. *Note*: if ``NoSource`` is given as the source, then you need to provide an explicit ``buildSpec``. Default: - NoSource\n\n:exampleMetadata: infused\n\nExample::\n\n    # ecr_repository: ecr.Repository\n\n\n    codebuild.Project(self, "Project",\n        environment=codebuild.BuildEnvironment(\n            build_image=codebuild.WindowsBuildImage.from_ecr_repository(ecr_repository, "v1.0", codebuild.WindowsImageType.SERVER_2019),\n            # optional certificate to include in the build image\n            certificate=codebuild.BuildEnvironmentCertificate(\n                bucket=s3.Bucket.from_bucket_name(self, "Bucket", "my-bucket"),\n                object_key="path/to/cert.pem"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_all_outbound', 'badge', 'build_spec', 'cache', 'check_secrets_in_plain_text_env_variables', 'concurrent_build_limit', 'description', 'encryption_key', 'environment', 'environment_variables', 'file_system_locations', 'grant_report_group_permissions', 'logging', 'project_name', 'queued_timeout', 'role', 'security_groups', 'ssm_session_permissions', 'subnet_selection', 'timeout', 'vpc', 'artifacts', 'secondary_artifacts', 'secondary_sources', 'source']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.ProjectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.ReportGroupProps
class ReportGroupPropsDef(BaseStruct):
    export_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='An optional S3 bucket to export the reports to. Default: - the reports will not be exported\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='What to do when this resource is deleted from a stack. As CodeBuild does not allow deleting a ResourceGroup that has reports inside of it, this is set to retain the resource by default. Default: RemovalPolicy.RETAIN\n')
    report_group_name: typing.Optional[str] = pydantic.Field(None, description='The physical name of the report group. Default: - CloudFormation-generated name\n')
    type: typing.Optional[aws_cdk.aws_codebuild.ReportGroupType] = pydantic.Field(None, description='The type of report group. This can be one of the following values:. - **TEST** - The report group contains test reports. - **CODE_COVERAGE** - The report group contains code coverage reports. Default: TEST\n')
    zip_export: typing.Optional[bool] = pydantic.Field(None, description='Whether to output the report files into the export bucket as-is, or create a ZIP from them before doing the export. Ignored if ``exportBucket`` has not been provided. Default: - false (the files will not be ZIPped)\n\n:exampleMetadata: infused\n\nExample::\n\n    # source: codebuild.Source\n\n\n    # create a new ReportGroup\n    report_group = codebuild.ReportGroup(self, "ReportGroup",\n        type=codebuild.ReportGroupType.CODE_COVERAGE\n    )\n\n    project = codebuild.Project(self, "Project",\n        source=source,\n        build_spec=codebuild.BuildSpec.from_object({\n            # ...\n            "reports": {\n                "report_group.report_group_arn": {\n                    "files": "**/*",\n                    "base-directory": "build/coverage-report.xml",\n                    "file-format": "JACOCOXML"\n                }\n            }\n        })\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['export_bucket', 'removal_policy', 'report_group_name', 'type', 'zip_export']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.ReportGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.S3ArtifactsProps
class S3ArtifactsPropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The artifact identifier. This property is required on secondary artifacts.\n')
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the output bucket.\n')
    encryption: typing.Optional[bool] = pydantic.Field(None, description='If this is false, build output will not be encrypted. This is useful if the artifact to publish a static website or sharing content with others Default: true - output will be encrypted\n')
    include_build_id: typing.Optional[bool] = pydantic.Field(None, description='Indicates if the build ID should be included in the path. If this is set to true, then the build artifact will be stored in "//". Default: true\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the build output ZIP file or folder inside the bucket. The full S3 object key will be "//" or "/" depending on whether ``includeBuildId`` is set to true. If not set, ``overrideArtifactName`` will be set and the name from the buildspec will be used instead. Default: undefined, and use the name from the buildspec\n')
    package_zip: typing.Optional[bool] = pydantic.Field(None, description='If this is true, all build output will be packaged into a single .zip file. Otherwise, all files will be uploaded to /. Default: true - files will be archived\n')
    path: typing.Optional[str] = pydantic.Field(None, description='The path inside of the bucket for the build output .zip file or folder. If a value is not specified, then build output will be stored at the root of the bucket (or under the directory if ``includeBuildId`` is set to true). Default: the root of the bucket\n\n:exampleMetadata: infused\n\nExample::\n\n    # bucket: s3.Bucket\n\n\n    project = codebuild.Project(self, "MyProject",\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2"\n        }),\n        artifacts=codebuild.Artifacts.s3(\n            bucket=bucket,\n            include_build_id=False,\n            package_zip=True,\n            path="another/path",\n            identifier="AddArtifact1"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'bucket', 'encryption', 'include_build_id', 'name', 'package_zip', 'path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.S3ArtifactsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.S3ArtifactsPropsDefConfig] = pydantic.Field(None)


class S3ArtifactsPropsDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.S3LoggingOptions
class S3LoggingOptionsDef(BaseStruct):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The S3 Bucket to send logs to.\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='The current status of the logs in Amazon CloudWatch Logs for a build project. Default: true\n')
    encrypted: typing.Optional[bool] = pydantic.Field(None, description='Encrypt the S3 build log output. Default: true\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='The path prefix for S3 logs. Default: - no prefix\n\n:exampleMetadata: infused\n\nExample::\n\n    codebuild.Project(self, "Project",\n        logging=codebuild.LoggingOptions(\n            s3=codebuild.S3LoggingOptions(\n                bucket=s3.Bucket(self, "LogBucket")\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'enabled', 'encrypted', 'prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.S3LoggingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.S3LoggingOptionsDefConfig] = pydantic.Field(None)


class S3LoggingOptionsDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.S3SourceProps
class S3SourcePropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.\n')
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    version: typing.Optional[str] = pydantic.Field(None, description='The version ID of the object that represents the build input ZIP file to use. Default: latest\n\n:exampleMetadata: infused\n\nExample::\n\n    bucket = s3.Bucket(self, "MyBucket")\n\n    codebuild.Project(self, "MyProject",\n        source=codebuild.Source.s3(\n            bucket=bucket,\n            path="path/to/file.zip"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier', 'bucket', 'path', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.S3SourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.S3SourcePropsDefConfig] = pydantic.Field(None)


class S3SourcePropsDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codebuild.SourceConfig
class SourceConfigDef(BaseStruct):
    source_property: typing.Union[_REQUIRED_INIT_PARAM, models.aws_codebuild.CfnProject_SourcePropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    build_triggers: typing.Union[models.aws_codebuild.CfnProject_ProjectTriggersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    source_version: typing.Optional[str] = pydantic.Field(None, description='``AWS::CodeBuild::Project.SourceVersion``. Default: the latest version\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    source_config = codebuild.SourceConfig(\n        source_property=codebuild.CfnProject.SourceProperty(\n            type="type",\n\n            # the properties below are optional\n            auth=codebuild.CfnProject.SourceAuthProperty(\n                type="type",\n\n                # the properties below are optional\n                resource="resource"\n            ),\n            build_spec="buildSpec",\n            build_status_config=codebuild.CfnProject.BuildStatusConfigProperty(\n                context="context",\n                target_url="targetUrl"\n            ),\n            git_clone_depth=123,\n            git_submodules_config=codebuild.CfnProject.GitSubmodulesConfigProperty(\n                fetch_submodules=False\n            ),\n            insecure_ssl=False,\n            location="location",\n            report_build_status=False,\n            source_identifier="sourceIdentifier"\n        ),\n\n        # the properties below are optional\n        build_triggers=codebuild.CfnProject.ProjectTriggersProperty(\n            build_type="buildType",\n            filter_groups=[[codebuild.CfnProject.WebhookFilterProperty(\n                pattern="pattern",\n                type="type",\n\n                # the properties below are optional\n                exclude_matched_pattern=False\n            )]],\n            webhook=False\n        ),\n        source_version="sourceVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['source_property', 'build_triggers', 'source_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.SourceConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.SourceProps
class SourcePropsDef(BaseStruct):
    identifier: typing.Optional[str] = pydantic.Field(None, description='The source identifier. This property is required on secondary sources.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    source_props = codebuild.SourceProps(\n        identifier="identifier"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.SourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.UntrustedCodeBoundaryPolicyProps
class UntrustedCodeBoundaryPolicyPropsDef(BaseStruct):
    additional_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Additional statements to add to the default set of statements. Default: - No additional statements\n')
    managed_policy_name: typing.Optional[str] = pydantic.Field(None, description='The name of the managed policy. Default: - A name is automatically generated.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n    from aws_cdk import aws_iam as iam\n\n    # policy_statement: iam.PolicyStatement\n\n    untrusted_code_boundary_policy_props = codebuild.UntrustedCodeBoundaryPolicyProps(\n        additional_statements=[policy_statement],\n        managed_policy_name="managedPolicyName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['additional_statements', 'managed_policy_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.UntrustedCodeBoundaryPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.BuildEnvironmentVariableType
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.ComputeType
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.EventAction
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.ImagePullPrincipalType
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.LocalCacheMode
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.ProjectNotificationEvents
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.ReportGroupType
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.WindowsImageType
# skipping emum

#  autogenerated from aws_cdk.aws_codebuild.IArtifacts
#  skipping Interface

#  autogenerated from aws_cdk.aws_codebuild.IBindableBuildImage
#  skipping Interface

#  autogenerated from aws_cdk.aws_codebuild.IBuildImage
#  skipping Interface

#  autogenerated from aws_cdk.aws_codebuild.IFileSystemLocation
#  skipping Interface

#  autogenerated from aws_cdk.aws_codebuild.IProject
#  skipping Interface

#  autogenerated from aws_cdk.aws_codebuild.IReportGroup
#  skipping Interface

#  autogenerated from aws_cdk.aws_codebuild.ISource
#  skipping Interface

#  autogenerated from aws_cdk.aws_codebuild.CfnProject
class CfnProjectDef(BaseCfnResource):
    artifacts: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnProject_ArtifactsPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='``Artifacts`` is a property of the `AWS::CodeBuild::Project <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-codebuild-project.html>`_ resource that specifies output settings for artifacts generated by an AWS CodeBuild build.\n')
    environment: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnProject_EnvironmentPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The build environment settings for the project, such as the environment type or the environment variables to use for the build environment.\n')
    service_role: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of the IAM role that enables AWS CodeBuild to interact with dependent AWS services on behalf of the AWS account.\n')
    source: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnProject_SourcePropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description="The source code settings for the project, such as the source code's repository type and location.\n")
    badge_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Indicates whether AWS CodeBuild generates a publicly accessible URL for your project's build badge. For more information, see `Build Badges Sample <https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-badges.html>`_ in the *AWS CodeBuild User Guide* . .. epigraph:: Including build badges with your project is currently not supported if the source type is CodePipeline. If you specify ``CODEPIPELINE`` for the ``Source`` property, do not specify the ``BadgeEnabled`` property.\n")
    build_batch_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectBuildBatchConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A ``ProjectBuildBatchConfig`` object that defines the batch build options for the project.\n')
    cache: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectCachePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings that AWS CodeBuild uses to store and reuse build dependencies.\n')
    concurrent_build_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of concurrent builds that are allowed for this project. New builds are only started if the current number of builds is less than or equal to this limit. If the current build count meets this limit, new builds are throttled and are not run.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description that makes the build project easy to identify.\n')
    encryption_key: typing.Optional[str] = pydantic.Field(None, description="The AWS Key Management Service customer master key (CMK) to be used for encrypting the build output artifacts. .. epigraph:: You can use a cross-account KMS key to encrypt the build output artifacts if your service role has permission to that key. You can specify either the Amazon Resource Name (ARN) of the CMK or, if available, the CMK's alias (using the format ``alias/<alias-name>`` ). If you don't specify a value, CodeBuild uses the managed CMK for Amazon Simple Storage Service (Amazon S3).\n")
    file_system_locations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectFileSystemLocationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``ProjectFileSystemLocation`` objects for a CodeBuild build project. A ``ProjectFileSystemLocation`` object specifies the ``identifier`` , ``location`` , ``mountOptions`` , ``mountPoint`` , and ``type`` of a file system created using Amazon Elastic File System.\n')
    logs_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_LogsConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for the build project. A project can create logs in CloudWatch Logs, an S3 bucket, or both.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the build project. The name must be unique across all of the projects in your AWS account .\n')
    queued_timeout_in_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='The number of minutes a build is allowed to be queued before it times out.\n')
    resource_access_role: typing.Optional[str] = pydantic.Field(None, description="The ARN of the IAM role that enables CodeBuild to access the CloudWatch Logs and Amazon S3 artifacts for the project's builds.\n")
    secondary_artifacts: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ArtifactsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of ``Artifacts`` objects. Each artifacts object specifies output settings that the project generates during a build.\n')
    secondary_sources: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_SourcePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``ProjectSource`` objects.\n')
    secondary_source_versions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectSourceVersionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``ProjectSourceVersion`` objects. If ``secondarySourceVersions`` is specified at the build level, then they take over these ``secondarySourceVersions`` (at the project level).\n')
    source_version: typing.Optional[str] = pydantic.Field(None, description="A version of the build input to be built for this project. If not specified, the latest version is used. If specified, it must be one of: - For CodeCommit: the commit ID, branch, or Git tag to use. - For GitHub: the commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. If a pull request ID is specified, it must use the format ``pr/pull-request-ID`` (for example ``pr/25`` ). If a branch name is specified, the branch's HEAD commit ID is used. If not specified, the default branch's HEAD commit ID is used. - For Bitbucket: the commit ID, branch name, or tag name that corresponds to the version of the source code you want to build. If a branch name is specified, the branch's HEAD commit ID is used. If not specified, the default branch's HEAD commit ID is used. - For Amazon S3: the version ID of the object that represents the build input ZIP file to use. If ``sourceVersion`` is specified at the build level, then that version takes precedence over this ``sourceVersion`` (at the project level). For more information, see `Source Version Sample with CodeBuild <https://docs.aws.amazon.com/codebuild/latest/userguide/sample-source-version.html>`_ in the *AWS CodeBuild User Guide* .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An arbitrary set of tags (key-value pairs) for the AWS CodeBuild project. These tags are available for use by AWS services that support AWS CodeBuild build project tags.\n')
    timeout_in_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='How long, in minutes, from 5 to 480 (8 hours), for AWS CodeBuild to wait before timing out any related build that did not get marked as completed. The default is 60 minutes.\n')
    triggers: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectTriggersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='For an existing AWS CodeBuild build project that has its source code stored in a GitHub repository, enables AWS CodeBuild to begin automatically rebuilding the source code every time a code change is pushed to the repository.\n')
    visibility: typing.Optional[str] = pydantic.Field(None, description="Specifies the visibility of the project's builds. Possible values are:. - **PUBLIC_READ** - The project builds are visible to the public. - **PRIVATE** - The project builds are not visible to the public.\n")
    vpc_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_VpcConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``VpcConfig`` specifies settings that enable AWS CodeBuild to access resources in an Amazon VPC. For more information, see `Use AWS CodeBuild with Amazon Virtual Private Cloud <https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html>`_ in the *AWS CodeBuild User Guide* .')
    _init_params: typing.ClassVar[list[str]] = ['artifacts', 'environment', 'service_role', 'source', 'badge_enabled', 'build_batch_config', 'cache', 'concurrent_build_limit', 'description', 'encryption_key', 'file_system_locations', 'logs_config', 'name', 'queued_timeout_in_minutes', 'resource_access_role', 'secondary_artifacts', 'secondary_sources', 'secondary_source_versions', 'source_version', 'tags', 'timeout_in_minutes', 'triggers', 'visibility', 'vpc_config']
    _method_names: typing.ClassVar[list[str]] = ['ArtifactsProperty', 'BatchRestrictionsProperty', 'BuildStatusConfigProperty', 'CloudWatchLogsConfigProperty', 'EnvironmentProperty', 'EnvironmentVariableProperty', 'GitSubmodulesConfigProperty', 'LogsConfigProperty', 'ProjectBuildBatchConfigProperty', 'ProjectCacheProperty', 'ProjectFileSystemLocationProperty', 'ProjectSourceVersionProperty', 'ProjectTriggersProperty', 'RegistryCredentialProperty', 'S3LogsConfigProperty', 'SourceAuthProperty', 'SourceProperty', 'VpcConfigProperty', 'WebhookFilterProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProject'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.CfnProjectDefConfig] = pydantic.Field(None)


class CfnProjectDefConfig(pydantic.BaseModel):
    ArtifactsProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefArtifactspropertyParams]] = pydantic.Field(None, description='')
    BatchRestrictionsProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefBatchrestrictionspropertyParams]] = pydantic.Field(None, description='')
    BuildStatusConfigProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefBuildstatusconfigpropertyParams]] = pydantic.Field(None, description='')
    CloudWatchLogsConfigProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefCloudwatchlogsconfigpropertyParams]] = pydantic.Field(None, description='')
    EnvironmentProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefEnvironmentpropertyParams]] = pydantic.Field(None, description='')
    EnvironmentVariableProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefEnvironmentvariablepropertyParams]] = pydantic.Field(None, description='')
    GitSubmodulesConfigProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefGitsubmodulesconfigpropertyParams]] = pydantic.Field(None, description='')
    LogsConfigProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefLogsconfigpropertyParams]] = pydantic.Field(None, description='')
    ProjectBuildBatchConfigProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefProjectbuildbatchconfigpropertyParams]] = pydantic.Field(None, description='')
    ProjectCacheProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefProjectcachepropertyParams]] = pydantic.Field(None, description='')
    ProjectFileSystemLocationProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefProjectfilesystemlocationpropertyParams]] = pydantic.Field(None, description='')
    ProjectSourceVersionProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefProjectsourceversionpropertyParams]] = pydantic.Field(None, description='')
    ProjectTriggersProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefProjecttriggerspropertyParams]] = pydantic.Field(None, description='')
    RegistryCredentialProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefRegistrycredentialpropertyParams]] = pydantic.Field(None, description='')
    S3LogsConfigProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefS3LogsconfigpropertyParams]] = pydantic.Field(None, description='')
    SourceAuthProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefSourceauthpropertyParams]] = pydantic.Field(None, description='')
    SourceProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefSourcepropertyParams]] = pydantic.Field(None, description='')
    VpcConfigProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefVpcconfigpropertyParams]] = pydantic.Field(None, description='')
    WebhookFilterProperty: typing.Optional[list[models.aws_codebuild.CfnProjectDefWebhookfilterpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_codebuild.CfnProjectDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_codebuild.CfnProjectDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_codebuild.CfnProjectDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_codebuild.CfnProjectDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_codebuild.CfnProjectDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_codebuild.CfnProjectDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_codebuild.CfnProjectDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_codebuild.CfnProjectDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_codebuild.CfnProjectDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_codebuild.CfnProjectDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_codebuild.CfnProjectDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_codebuild.CfnProjectDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_codebuild.CfnProjectDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnProjectDefArtifactspropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    artifact_identifier: typing.Optional[str] = pydantic.Field(None, description='')
    encryption_disabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    location: typing.Optional[str] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    namespace_type: typing.Optional[str] = pydantic.Field(None, description='')
    override_artifact_name: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    packaging: typing.Optional[str] = pydantic.Field(None, description='')
    path: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefBatchrestrictionspropertyParams(pydantic.BaseModel):
    compute_types_allowed: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    maximum_builds_allowed: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnProjectDefBuildstatusconfigpropertyParams(pydantic.BaseModel):
    context: typing.Optional[str] = pydantic.Field(None, description='')
    target_url: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefCloudwatchlogsconfigpropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    group_name: typing.Optional[str] = pydantic.Field(None, description='')
    stream_name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefEnvironmentpropertyParams(pydantic.BaseModel):
    compute_type: str = pydantic.Field(..., description='')
    image: str = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    certificate: typing.Optional[str] = pydantic.Field(None, description='')
    environment_variables: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_EnvironmentVariablePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    image_pull_credentials_type: typing.Optional[str] = pydantic.Field(None, description='')
    privileged_mode: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    registry_credential: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_RegistryCredentialPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnProjectDefEnvironmentvariablepropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value: str = pydantic.Field(..., description='')
    type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefGitsubmodulesconfigpropertyParams(pydantic.BaseModel):
    fetch_submodules: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    ...

class CfnProjectDefLogsconfigpropertyParams(pydantic.BaseModel):
    cloud_watch_logs: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_CloudWatchLogsConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    s3_logs: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_S3LogsConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnProjectDefProjectbuildbatchconfigpropertyParams(pydantic.BaseModel):
    batch_report_mode: typing.Optional[str] = pydantic.Field(None, description='')
    combine_artifacts: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    restrictions: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_BatchRestrictionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    service_role: typing.Optional[str] = pydantic.Field(None, description='')
    timeout_in_mins: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnProjectDefProjectcachepropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    location: typing.Optional[str] = pydantic.Field(None, description='')
    modes: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnProjectDefProjectfilesystemlocationpropertyParams(pydantic.BaseModel):
    identifier: str = pydantic.Field(..., description='')
    location: str = pydantic.Field(..., description='')
    mount_point: str = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    mount_options: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefProjectsourceversionpropertyParams(pydantic.BaseModel):
    source_identifier: str = pydantic.Field(..., description='')
    source_version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefProjecttriggerspropertyParams(pydantic.BaseModel):
    build_type: typing.Optional[str] = pydantic.Field(None, description='')
    filter_groups: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_WebhookFilterPropertyDef, dict[str, typing.Any]]]]], None] = pydantic.Field(None, description='')
    webhook: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnProjectDefRegistrycredentialpropertyParams(pydantic.BaseModel):
    credential: str = pydantic.Field(..., description='')
    credential_provider: str = pydantic.Field(..., description='')
    ...

class CfnProjectDefS3LogsconfigpropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    encryption_disabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    location: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefSourceauthpropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    resource: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefSourcepropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    auth: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_SourceAuthPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    build_spec: typing.Optional[str] = pydantic.Field(None, description='')
    build_status_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_BuildStatusConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    git_clone_depth: typing.Union[int, float, None] = pydantic.Field(None, description='')
    git_submodules_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_GitSubmodulesConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    insecure_ssl: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    location: typing.Optional[str] = pydantic.Field(None, description='')
    report_build_status: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    source_identifier: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefVpcconfigpropertyParams(pydantic.BaseModel):
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    subnets: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    vpc_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnProjectDefWebhookfilterpropertyParams(pydantic.BaseModel):
    pattern: str = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    exclude_matched_pattern: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnProjectDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnProjectDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnProjectDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnProjectDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnProjectDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnProjectDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnProjectDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnProjectDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnProjectDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnProjectDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnProjectDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnProjectDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnProjectDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnProjectDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_codebuild.CfnReportGroup
class CfnReportGroupDef(BaseCfnResource):
    export_config: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnReportGroup_ReportExportConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Information about the destination where the raw data of this ``ReportGroup`` is exported.\n')
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of the ``ReportGroup`` . This can be one of the following values:. - **CODE_COVERAGE** - The report group contains code coverage reports. - **TEST** - The report group contains test reports.\n')
    delete_reports: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When deleting a report group, specifies if reports within the report group should be deleted. - **true** - Deletes any reports that belong to the report group before deleting the report group. - **false** - You must delete any reports in the report group. This is the default value. If you delete a report group that contains one or more reports, an exception is thrown.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the ``ReportGroup`` .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of tag key and value pairs associated with this report group. These tags are available for use by AWS services that support AWS CodeBuild report group tags.')
    _init_params: typing.ClassVar[list[str]] = ['export_config', 'type', 'delete_reports', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['ReportExportConfigProperty', 'S3ReportExportConfigProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnReportGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.CfnReportGroupDefConfig] = pydantic.Field(None)


class CfnReportGroupDefConfig(pydantic.BaseModel):
    ReportExportConfigProperty: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefReportexportconfigpropertyParams]] = pydantic.Field(None, description='')
    S3ReportExportConfigProperty: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefS3ReportexportconfigpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_codebuild.CfnReportGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnReportGroupDefReportexportconfigpropertyParams(pydantic.BaseModel):
    export_config_type: str = pydantic.Field(..., description='')
    s3_destination: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnReportGroup_S3ReportExportConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnReportGroupDefS3ReportexportconfigpropertyParams(pydantic.BaseModel):
    bucket: str = pydantic.Field(..., description='')
    bucket_owner: typing.Optional[str] = pydantic.Field(None, description='')
    encryption_disabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    encryption_key: typing.Optional[str] = pydantic.Field(None, description='')
    packaging: typing.Optional[str] = pydantic.Field(None, description='')
    path: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnReportGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnReportGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReportGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnReportGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReportGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnReportGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnReportGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnReportGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnReportGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnReportGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReportGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnReportGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnReportGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReportGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_codebuild.CfnSourceCredential
class CfnSourceCredentialDef(BaseCfnResource):
    auth_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of authentication used by the credentials. Valid options are OAUTH, BASIC_AUTH, or PERSONAL_ACCESS_TOKEN.\n')
    server_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of source provider. The valid options are GITHUB, GITHUB_ENTERPRISE, or BITBUCKET.\n')
    token: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='For GitHub or GitHub Enterprise, this is the personal access token. For Bitbucket, this is the app password.\n')
    username: typing.Optional[str] = pydantic.Field(None, description='The Bitbucket username when the ``authType`` is BASIC_AUTH. This parameter is not valid for other types of source providers or connections.')
    _init_params: typing.ClassVar[list[str]] = ['auth_type', 'server_type', 'token', 'username']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnSourceCredential'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_codebuild.CfnSourceCredentialDefConfig] = pydantic.Field(None)


class CfnSourceCredentialDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_codebuild.CfnSourceCredentialDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnSourceCredentialDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnSourceCredentialDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSourceCredentialDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnSourceCredentialDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSourceCredentialDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnSourceCredentialDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnSourceCredentialDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnSourceCredentialDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnSourceCredentialDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnSourceCredentialDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSourceCredentialDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnSourceCredentialDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnSourceCredentialDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSourceCredentialDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_codebuild.CfnProjectProps
class CfnProjectPropsDef(BaseCfnProperty):
    artifacts: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnProject_ArtifactsPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='``Artifacts`` is a property of the `AWS::CodeBuild::Project <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-codebuild-project.html>`_ resource that specifies output settings for artifacts generated by an AWS CodeBuild build.\n')
    environment: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnProject_EnvironmentPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The build environment settings for the project, such as the environment type or the environment variables to use for the build environment.\n')
    service_role: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of the IAM role that enables AWS CodeBuild to interact with dependent AWS services on behalf of the AWS account.\n')
    source: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnProject_SourcePropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description="The source code settings for the project, such as the source code's repository type and location.\n")
    badge_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Indicates whether AWS CodeBuild generates a publicly accessible URL for your project's build badge. For more information, see `Build Badges Sample <https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-badges.html>`_ in the *AWS CodeBuild User Guide* . .. epigraph:: Including build badges with your project is currently not supported if the source type is CodePipeline. If you specify ``CODEPIPELINE`` for the ``Source`` property, do not specify the ``BadgeEnabled`` property.\n")
    build_batch_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectBuildBatchConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A ``ProjectBuildBatchConfig`` object that defines the batch build options for the project.\n')
    cache: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectCachePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings that AWS CodeBuild uses to store and reuse build dependencies.\n')
    concurrent_build_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of concurrent builds that are allowed for this project. New builds are only started if the current number of builds is less than or equal to this limit. If the current build count meets this limit, new builds are throttled and are not run.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description that makes the build project easy to identify.\n')
    encryption_key: typing.Optional[str] = pydantic.Field(None, description="The AWS Key Management Service customer master key (CMK) to be used for encrypting the build output artifacts. .. epigraph:: You can use a cross-account KMS key to encrypt the build output artifacts if your service role has permission to that key. You can specify either the Amazon Resource Name (ARN) of the CMK or, if available, the CMK's alias (using the format ``alias/<alias-name>`` ). If you don't specify a value, CodeBuild uses the managed CMK for Amazon Simple Storage Service (Amazon S3).\n")
    file_system_locations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectFileSystemLocationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``ProjectFileSystemLocation`` objects for a CodeBuild build project. A ``ProjectFileSystemLocation`` object specifies the ``identifier`` , ``location`` , ``mountOptions`` , ``mountPoint`` , and ``type`` of a file system created using Amazon Elastic File System.\n')
    logs_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_LogsConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for the build project. A project can create logs in CloudWatch Logs, an S3 bucket, or both.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the build project. The name must be unique across all of the projects in your AWS account .\n')
    queued_timeout_in_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='The number of minutes a build is allowed to be queued before it times out.\n')
    resource_access_role: typing.Optional[str] = pydantic.Field(None, description="The ARN of the IAM role that enables CodeBuild to access the CloudWatch Logs and Amazon S3 artifacts for the project's builds.\n")
    secondary_artifacts: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ArtifactsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of ``Artifacts`` objects. Each artifacts object specifies output settings that the project generates during a build.\n')
    secondary_sources: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_SourcePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``ProjectSource`` objects.\n')
    secondary_source_versions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectSourceVersionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``ProjectSourceVersion`` objects. If ``secondarySourceVersions`` is specified at the build level, then they take over these ``secondarySourceVersions`` (at the project level).\n')
    source_version: typing.Optional[str] = pydantic.Field(None, description="A version of the build input to be built for this project. If not specified, the latest version is used. If specified, it must be one of: - For CodeCommit: the commit ID, branch, or Git tag to use. - For GitHub: the commit ID, pull request ID, branch name, or tag name that corresponds to the version of the source code you want to build. If a pull request ID is specified, it must use the format ``pr/pull-request-ID`` (for example ``pr/25`` ). If a branch name is specified, the branch's HEAD commit ID is used. If not specified, the default branch's HEAD commit ID is used. - For Bitbucket: the commit ID, branch name, or tag name that corresponds to the version of the source code you want to build. If a branch name is specified, the branch's HEAD commit ID is used. If not specified, the default branch's HEAD commit ID is used. - For Amazon S3: the version ID of the object that represents the build input ZIP file to use. If ``sourceVersion`` is specified at the build level, then that version takes precedence over this ``sourceVersion`` (at the project level). For more information, see `Source Version Sample with CodeBuild <https://docs.aws.amazon.com/codebuild/latest/userguide/sample-source-version.html>`_ in the *AWS CodeBuild User Guide* .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An arbitrary set of tags (key-value pairs) for the AWS CodeBuild project. These tags are available for use by AWS services that support AWS CodeBuild build project tags.\n')
    timeout_in_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='How long, in minutes, from 5 to 480 (8 hours), for AWS CodeBuild to wait before timing out any related build that did not get marked as completed. The default is 60 minutes.\n')
    triggers: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_ProjectTriggersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='For an existing AWS CodeBuild build project that has its source code stored in a GitHub repository, enables AWS CodeBuild to begin automatically rebuilding the source code every time a code change is pushed to the repository.\n')
    visibility: typing.Optional[str] = pydantic.Field(None, description="Specifies the visibility of the project's builds. Possible values are:. - **PUBLIC_READ** - The project builds are visible to the public. - **PRIVATE** - The project builds are not visible to the public.\n")
    vpc_config: typing.Union[models.UnsupportedResource, models.aws_codebuild.CfnProject_VpcConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``VpcConfig`` specifies settings that enable AWS CodeBuild to access resources in an Amazon VPC. For more information, see `Use AWS CodeBuild with Amazon Virtual Private Cloud <https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html>`_ in the *AWS CodeBuild User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-codebuild-project.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    cfn_project_props = codebuild.CfnProjectProps(\n        artifacts=codebuild.CfnProject.ArtifactsProperty(\n            type="type",\n\n            # the properties below are optional\n            artifact_identifier="artifactIdentifier",\n            encryption_disabled=False,\n            location="location",\n            name="name",\n            namespace_type="namespaceType",\n            override_artifact_name=False,\n            packaging="packaging",\n            path="path"\n        ),\n        environment=codebuild.CfnProject.EnvironmentProperty(\n            compute_type="computeType",\n            image="image",\n            type="type",\n\n            # the properties below are optional\n            certificate="certificate",\n            environment_variables=[codebuild.CfnProject.EnvironmentVariableProperty(\n                name="name",\n                value="value",\n\n                # the properties below are optional\n                type="type"\n            )],\n            image_pull_credentials_type="imagePullCredentialsType",\n            privileged_mode=False,\n            registry_credential=codebuild.CfnProject.RegistryCredentialProperty(\n                credential="credential",\n                credential_provider="credentialProvider"\n            )\n        ),\n        service_role="serviceRole",\n        source=codebuild.CfnProject.SourceProperty(\n            type="type",\n\n            # the properties below are optional\n            auth=codebuild.CfnProject.SourceAuthProperty(\n                type="type",\n\n                # the properties below are optional\n                resource="resource"\n            ),\n            build_spec="buildSpec",\n            build_status_config=codebuild.CfnProject.BuildStatusConfigProperty(\n                context="context",\n                target_url="targetUrl"\n            ),\n            git_clone_depth=123,\n            git_submodules_config=codebuild.CfnProject.GitSubmodulesConfigProperty(\n                fetch_submodules=False\n            ),\n            insecure_ssl=False,\n            location="location",\n            report_build_status=False,\n            source_identifier="sourceIdentifier"\n        ),\n\n        # the properties below are optional\n        badge_enabled=False,\n        build_batch_config=codebuild.CfnProject.ProjectBuildBatchConfigProperty(\n            batch_report_mode="batchReportMode",\n            combine_artifacts=False,\n            restrictions=codebuild.CfnProject.BatchRestrictionsProperty(\n                compute_types_allowed=["computeTypesAllowed"],\n                maximum_builds_allowed=123\n            ),\n            service_role="serviceRole",\n            timeout_in_mins=123\n        ),\n        cache=codebuild.CfnProject.ProjectCacheProperty(\n            type="type",\n\n            # the properties below are optional\n            location="location",\n            modes=["modes"]\n        ),\n        concurrent_build_limit=123,\n        description="description",\n        encryption_key="encryptionKey",\n        file_system_locations=[codebuild.CfnProject.ProjectFileSystemLocationProperty(\n            identifier="identifier",\n            location="location",\n            mount_point="mountPoint",\n            type="type",\n\n            # the properties below are optional\n            mount_options="mountOptions"\n        )],\n        logs_config=codebuild.CfnProject.LogsConfigProperty(\n            cloud_watch_logs=codebuild.CfnProject.CloudWatchLogsConfigProperty(\n                status="status",\n\n                # the properties below are optional\n                group_name="groupName",\n                stream_name="streamName"\n            ),\n            s3_logs=codebuild.CfnProject.S3LogsConfigProperty(\n                status="status",\n\n                # the properties below are optional\n                encryption_disabled=False,\n                location="location"\n            )\n        ),\n        name="name",\n        queued_timeout_in_minutes=123,\n        resource_access_role="resourceAccessRole",\n        secondary_artifacts=[codebuild.CfnProject.ArtifactsProperty(\n            type="type",\n\n            # the properties below are optional\n            artifact_identifier="artifactIdentifier",\n            encryption_disabled=False,\n            location="location",\n            name="name",\n            namespace_type="namespaceType",\n            override_artifact_name=False,\n            packaging="packaging",\n            path="path"\n        )],\n        secondary_sources=[codebuild.CfnProject.SourceProperty(\n            type="type",\n\n            # the properties below are optional\n            auth=codebuild.CfnProject.SourceAuthProperty(\n                type="type",\n\n                # the properties below are optional\n                resource="resource"\n            ),\n            build_spec="buildSpec",\n            build_status_config=codebuild.CfnProject.BuildStatusConfigProperty(\n                context="context",\n                target_url="targetUrl"\n            ),\n            git_clone_depth=123,\n            git_submodules_config=codebuild.CfnProject.GitSubmodulesConfigProperty(\n                fetch_submodules=False\n            ),\n            insecure_ssl=False,\n            location="location",\n            report_build_status=False,\n            source_identifier="sourceIdentifier"\n        )],\n        secondary_source_versions=[codebuild.CfnProject.ProjectSourceVersionProperty(\n            source_identifier="sourceIdentifier",\n\n            # the properties below are optional\n            source_version="sourceVersion"\n        )],\n        source_version="sourceVersion",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        timeout_in_minutes=123,\n        triggers=codebuild.CfnProject.ProjectTriggersProperty(\n            build_type="buildType",\n            filter_groups=[[codebuild.CfnProject.WebhookFilterProperty(\n                pattern="pattern",\n                type="type",\n\n                # the properties below are optional\n                exclude_matched_pattern=False\n            )]],\n            webhook=False\n        ),\n        visibility="visibility",\n        vpc_config=codebuild.CfnProject.VpcConfigProperty(\n            security_group_ids=["securityGroupIds"],\n            subnets=["subnets"],\n            vpc_id="vpcId"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['artifacts', 'environment', 'service_role', 'source', 'badge_enabled', 'build_batch_config', 'cache', 'concurrent_build_limit', 'description', 'encryption_key', 'file_system_locations', 'logs_config', 'name', 'queued_timeout_in_minutes', 'resource_access_role', 'secondary_artifacts', 'secondary_sources', 'secondary_source_versions', 'source_version', 'tags', 'timeout_in_minutes', 'triggers', 'visibility', 'vpc_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnProjectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnReportGroupProps
class CfnReportGroupPropsDef(BaseCfnProperty):
    export_config: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_codebuild.CfnReportGroup_ReportExportConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Information about the destination where the raw data of this ``ReportGroup`` is exported.\n')
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of the ``ReportGroup`` . This can be one of the following values:. - **CODE_COVERAGE** - The report group contains code coverage reports. - **TEST** - The report group contains test reports.\n')
    delete_reports: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When deleting a report group, specifies if reports within the report group should be deleted. - **true** - Deletes any reports that belong to the report group before deleting the report group. - **false** - You must delete any reports in the report group. This is the default value. If you delete a report group that contains one or more reports, an exception is thrown.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the ``ReportGroup`` .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of tag key and value pairs associated with this report group. These tags are available for use by AWS services that support AWS CodeBuild report group tags.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-codebuild-reportgroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    cfn_report_group_props = codebuild.CfnReportGroupProps(\n        export_config=codebuild.CfnReportGroup.ReportExportConfigProperty(\n            export_config_type="exportConfigType",\n\n            # the properties below are optional\n            s3_destination=codebuild.CfnReportGroup.S3ReportExportConfigProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                bucket_owner="bucketOwner",\n                encryption_disabled=False,\n                encryption_key="encryptionKey",\n                packaging="packaging",\n                path="path"\n            )\n        ),\n        type="type",\n\n        # the properties below are optional\n        delete_reports=False,\n        name="name",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['export_config', 'type', 'delete_reports', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnReportGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codebuild.CfnSourceCredentialProps
class CfnSourceCredentialPropsDef(BaseCfnProperty):
    auth_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of authentication used by the credentials. Valid options are OAUTH, BASIC_AUTH, or PERSONAL_ACCESS_TOKEN.\n')
    server_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of source provider. The valid options are GITHUB, GITHUB_ENTERPRISE, or BITBUCKET.\n')
    token: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='For GitHub or GitHub Enterprise, this is the personal access token. For Bitbucket, this is the app password.\n')
    username: typing.Optional[str] = pydantic.Field(None, description='The Bitbucket username when the ``authType`` is BASIC_AUTH. This parameter is not valid for other types of source providers or connections.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-codebuild-sourcecredential.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n\n    cfn_source_credential_props = codebuild.CfnSourceCredentialProps(\n        auth_type="authType",\n        server_type="serverType",\n        token="token",\n\n        # the properties below are optional\n        username="username"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth_type', 'server_type', 'token', 'username']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codebuild.CfnSourceCredentialProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    Artifacts: typing.Optional[dict[str, models.aws_codebuild.ArtifactsDef]] = pydantic.Field(None)
    BuildSpec: typing.Optional[dict[str, models.aws_codebuild.BuildSpecDef]] = pydantic.Field(None)
    Cache: typing.Optional[dict[str, models.aws_codebuild.CacheDef]] = pydantic.Field(None)
    FileSystemLocation: typing.Optional[dict[str, models.aws_codebuild.FileSystemLocationDef]] = pydantic.Field(None)
    FilterGroup: typing.Optional[dict[str, models.aws_codebuild.FilterGroupDef]] = pydantic.Field(None)
    LinuxArmBuildImage: typing.Optional[dict[str, models.aws_codebuild.LinuxArmBuildImageDef]] = pydantic.Field(None)
    LinuxBuildImage: typing.Optional[dict[str, models.aws_codebuild.LinuxBuildImageDef]] = pydantic.Field(None)
    LinuxGpuBuildImage: typing.Optional[dict[str, models.aws_codebuild.LinuxGpuBuildImageDef]] = pydantic.Field(None)
    PhaseChangeEvent: typing.Optional[dict[str, models.aws_codebuild.PhaseChangeEventDef]] = pydantic.Field(None)
    Source: typing.Optional[dict[str, models.aws_codebuild.SourceDef]] = pydantic.Field(None)
    StateChangeEvent: typing.Optional[dict[str, models.aws_codebuild.StateChangeEventDef]] = pydantic.Field(None)
    WindowsBuildImage: typing.Optional[dict[str, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None)
    BitBucketSourceCredentials: typing.Optional[dict[str, models.aws_codebuild.BitBucketSourceCredentialsDef]] = pydantic.Field(None)
    GitHubEnterpriseSourceCredentials: typing.Optional[dict[str, models.aws_codebuild.GitHubEnterpriseSourceCredentialsDef]] = pydantic.Field(None)
    GitHubSourceCredentials: typing.Optional[dict[str, models.aws_codebuild.GitHubSourceCredentialsDef]] = pydantic.Field(None)
    PipelineProject: typing.Optional[dict[str, models.aws_codebuild.PipelineProjectDef]] = pydantic.Field(None)
    Project: typing.Optional[dict[str, models.aws_codebuild.ProjectDef]] = pydantic.Field(None)
    ReportGroup: typing.Optional[dict[str, models.aws_codebuild.ReportGroupDef]] = pydantic.Field(None)
    UntrustedCodeBoundaryPolicy: typing.Optional[dict[str, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef]] = pydantic.Field(None)
    ArtifactsConfig: typing.Optional[dict[str, models.aws_codebuild.ArtifactsConfigDef]] = pydantic.Field(None)
    ArtifactsProps: typing.Optional[dict[str, models.aws_codebuild.ArtifactsPropsDef]] = pydantic.Field(None)
    BatchBuildConfig: typing.Optional[dict[str, models.aws_codebuild.BatchBuildConfigDef]] = pydantic.Field(None)
    BindToCodePipelineOptions: typing.Optional[dict[str, models.aws_codebuild.BindToCodePipelineOptionsDef]] = pydantic.Field(None)
    BitBucketSourceCredentialsProps: typing.Optional[dict[str, models.aws_codebuild.BitBucketSourceCredentialsPropsDef]] = pydantic.Field(None)
    BitBucketSourceProps: typing.Optional[dict[str, models.aws_codebuild.BitBucketSourcePropsDef]] = pydantic.Field(None)
    BucketCacheOptions: typing.Optional[dict[str, models.aws_codebuild.BucketCacheOptionsDef]] = pydantic.Field(None)
    BuildEnvironment: typing.Optional[dict[str, models.aws_codebuild.BuildEnvironmentDef]] = pydantic.Field(None)
    BuildEnvironmentCertificate: typing.Optional[dict[str, models.aws_codebuild.BuildEnvironmentCertificateDef]] = pydantic.Field(None)
    BuildEnvironmentVariable: typing.Optional[dict[str, models.aws_codebuild.BuildEnvironmentVariableDef]] = pydantic.Field(None)
    BuildImageBindOptions: typing.Optional[dict[str, models.aws_codebuild.BuildImageBindOptionsDef]] = pydantic.Field(None)
    BuildImageConfig: typing.Optional[dict[str, models.aws_codebuild.BuildImageConfigDef]] = pydantic.Field(None)
    CfnProject_ArtifactsProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_ArtifactsPropertyDef]] = pydantic.Field(None)
    CfnProject_BatchRestrictionsProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_BatchRestrictionsPropertyDef]] = pydantic.Field(None)
    CfnProject_BuildStatusConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_BuildStatusConfigPropertyDef]] = pydantic.Field(None)
    CfnProject_CloudWatchLogsConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_CloudWatchLogsConfigPropertyDef]] = pydantic.Field(None)
    CfnProject_EnvironmentProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_EnvironmentPropertyDef]] = pydantic.Field(None)
    CfnProject_EnvironmentVariableProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_EnvironmentVariablePropertyDef]] = pydantic.Field(None)
    CfnProject_GitSubmodulesConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_GitSubmodulesConfigPropertyDef]] = pydantic.Field(None)
    CfnProject_LogsConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_LogsConfigPropertyDef]] = pydantic.Field(None)
    CfnProject_ProjectBuildBatchConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_ProjectBuildBatchConfigPropertyDef]] = pydantic.Field(None)
    CfnProject_ProjectCacheProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_ProjectCachePropertyDef]] = pydantic.Field(None)
    CfnProject_ProjectFileSystemLocationProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_ProjectFileSystemLocationPropertyDef]] = pydantic.Field(None)
    CfnProject_ProjectSourceVersionProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_ProjectSourceVersionPropertyDef]] = pydantic.Field(None)
    CfnProject_ProjectTriggersProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_ProjectTriggersPropertyDef]] = pydantic.Field(None)
    CfnProject_RegistryCredentialProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_RegistryCredentialPropertyDef]] = pydantic.Field(None)
    CfnProject_S3LogsConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_S3LogsConfigPropertyDef]] = pydantic.Field(None)
    CfnProject_SourceAuthProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_SourceAuthPropertyDef]] = pydantic.Field(None)
    CfnProject_SourceProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_SourcePropertyDef]] = pydantic.Field(None)
    CfnProject_VpcConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_VpcConfigPropertyDef]] = pydantic.Field(None)
    CfnProject_WebhookFilterProperty: typing.Optional[dict[str, models.aws_codebuild.CfnProject_WebhookFilterPropertyDef]] = pydantic.Field(None)
    CfnReportGroup_ReportExportConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnReportGroup_ReportExportConfigPropertyDef]] = pydantic.Field(None)
    CfnReportGroup_S3ReportExportConfigProperty: typing.Optional[dict[str, models.aws_codebuild.CfnReportGroup_S3ReportExportConfigPropertyDef]] = pydantic.Field(None)
    CloudWatchLoggingOptions: typing.Optional[dict[str, models.aws_codebuild.CloudWatchLoggingOptionsDef]] = pydantic.Field(None)
    CodeCommitSourceProps: typing.Optional[dict[str, models.aws_codebuild.CodeCommitSourcePropsDef]] = pydantic.Field(None)
    CommonProjectProps: typing.Optional[dict[str, models.aws_codebuild.CommonProjectPropsDef]] = pydantic.Field(None)
    DockerImageOptions: typing.Optional[dict[str, models.aws_codebuild.DockerImageOptionsDef]] = pydantic.Field(None)
    EfsFileSystemLocationProps: typing.Optional[dict[str, models.aws_codebuild.EfsFileSystemLocationPropsDef]] = pydantic.Field(None)
    FileSystemConfig: typing.Optional[dict[str, models.aws_codebuild.FileSystemConfigDef]] = pydantic.Field(None)
    GitHubEnterpriseSourceCredentialsProps: typing.Optional[dict[str, models.aws_codebuild.GitHubEnterpriseSourceCredentialsPropsDef]] = pydantic.Field(None)
    GitHubEnterpriseSourceProps: typing.Optional[dict[str, models.aws_codebuild.GitHubEnterpriseSourcePropsDef]] = pydantic.Field(None)
    GitHubSourceCredentialsProps: typing.Optional[dict[str, models.aws_codebuild.GitHubSourceCredentialsPropsDef]] = pydantic.Field(None)
    GitHubSourceProps: typing.Optional[dict[str, models.aws_codebuild.GitHubSourcePropsDef]] = pydantic.Field(None)
    LoggingOptions: typing.Optional[dict[str, models.aws_codebuild.LoggingOptionsDef]] = pydantic.Field(None)
    PipelineProjectProps: typing.Optional[dict[str, models.aws_codebuild.PipelineProjectPropsDef]] = pydantic.Field(None)
    ProjectNotifyOnOptions: typing.Optional[dict[str, models.aws_codebuild.ProjectNotifyOnOptionsDef]] = pydantic.Field(None)
    ProjectProps: typing.Optional[dict[str, models.aws_codebuild.ProjectPropsDef]] = pydantic.Field(None)
    ReportGroupProps: typing.Optional[dict[str, models.aws_codebuild.ReportGroupPropsDef]] = pydantic.Field(None)
    S3ArtifactsProps: typing.Optional[dict[str, models.aws_codebuild.S3ArtifactsPropsDef]] = pydantic.Field(None)
    S3LoggingOptions: typing.Optional[dict[str, models.aws_codebuild.S3LoggingOptionsDef]] = pydantic.Field(None)
    S3SourceProps: typing.Optional[dict[str, models.aws_codebuild.S3SourcePropsDef]] = pydantic.Field(None)
    SourceConfig: typing.Optional[dict[str, models.aws_codebuild.SourceConfigDef]] = pydantic.Field(None)
    SourceProps: typing.Optional[dict[str, models.aws_codebuild.SourcePropsDef]] = pydantic.Field(None)
    UntrustedCodeBoundaryPolicyProps: typing.Optional[dict[str, models.aws_codebuild.UntrustedCodeBoundaryPolicyPropsDef]] = pydantic.Field(None)
    CfnProject: typing.Optional[dict[str, models.aws_codebuild.CfnProjectDef]] = pydantic.Field(None)
    CfnReportGroup: typing.Optional[dict[str, models.aws_codebuild.CfnReportGroupDef]] = pydantic.Field(None)
    CfnSourceCredential: typing.Optional[dict[str, models.aws_codebuild.CfnSourceCredentialDef]] = pydantic.Field(None)
    CfnProjectProps: typing.Optional[dict[str, models.aws_codebuild.CfnProjectPropsDef]] = pydantic.Field(None)
    CfnReportGroupProps: typing.Optional[dict[str, models.aws_codebuild.CfnReportGroupPropsDef]] = pydantic.Field(None)
    CfnSourceCredentialProps: typing.Optional[dict[str, models.aws_codebuild.CfnSourceCredentialPropsDef]] = pydantic.Field(None)
    ...

import models
