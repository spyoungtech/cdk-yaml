from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_cloudfront.experimental.EdgeFunction
class EdgeFunctionDef(BaseConstruct):
    stack_id: typing.Optional[str] = pydantic.Field(None, description='The stack ID of Lambda@Edge function. Default: - ``edge-lambda-stack-${region}``\n')
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functionâ€™s /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['stack_id', 'code', 'handler', 'runtime', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_alias', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudfront.experimental.EdgeFunction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_cloudfront.experimental.EdgeFunctionDefConfig] = pydantic.Field(None)


class EdgeFunctionDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefAddAliasParams]] = pydantic.Field(None, description='Defines an alias for this version.')
    add_event_source: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefAddEventSourceParams]] = pydantic.Field(None, description='Adds an event source to this function.')
    add_event_source_mapping: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    configure_async_invoke: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    grant_invoke: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    metric: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Lambda Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefMetricDurationParams]] = pydantic.Field(None, description='Metric for the Duration of this Lambda How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefMetricInvocationsParams]] = pydantic.Field(None, description='Metric for the number of invocations of this Lambda How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_cloudfront.experimental.EdgeFunctionDefMetricThrottlesParams]] = pydantic.Field(None, description='Metric for the number of throttled invocations of this Lambda How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    current_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    lambda__config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)

class EdgeFunctionDefAddAliasParams(pydantic.BaseModel):
    alias_name: str = pydantic.Field(..., description='-\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    return_config: typing.Optional[list[models.aws_lambda.AliasDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class EdgeFunctionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.')
    ...

class EdgeFunctionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class EdgeFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class EdgeFunctionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class EdgeFunctionDefGrantInvokeParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class EdgeFunctionDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefGrantInvokeUrlParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefGrantInvokeVersionParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EdgeFunctionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_cloudfront.experimental.EdgeFunctionProps
class EdgeFunctionPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functionâ€™s /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n')
    stack_id: typing.Optional[str] = pydantic.Field(None, description='The stack ID of Lambda@Edge function. Default: - ``edge-lambda-stack-${region}``\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_bucket: s3.Bucket\n    # A Lambda@Edge function added to default behavior of a Distribution\n    # and triggered on every request\n    my_func = cloudfront.experimental.EdgeFunction(self, "MyFunction",\n        runtime=lambda_.Runtime.NODEJS_LATEST,\n        handler="index.handler",\n        code=lambda_.Code.from_asset(path.join(__dirname, "lambda-handler"))\n    )\n    cloudfront.Distribution(self, "myDist",\n        default_behavior=cloudfront.BehaviorOptions(\n            origin=origins.S3Origin(my_bucket),\n            edge_lambdas=[cloudfront.EdgeLambda(\n                function_version=my_func.current_version,\n                event_type=cloudfront.LambdaEdgeEventType.VIEWER_REQUEST\n            )\n            ]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'code', 'handler', 'runtime', 'stack_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudfront.experimental.EdgeFunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_cloudfront.experimental.EdgeFunctionPropsDefConfig] = pydantic.Field(None)


class EdgeFunctionPropsDefConfig(pydantic.BaseModel):
    code_config: typing.Optional[models.aws_lambda.CodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)


class ModuleModel(pydantic.BaseModel):
    EdgeFunction: typing.Optional[dict[str, models.aws_cloudfront.experimental.EdgeFunctionDef]] = pydantic.Field(None)
    EdgeFunctionProps: typing.Optional[dict[str, models.aws_cloudfront.experimental.EdgeFunctionPropsDef]] = pydantic.Field(None)
    ...

import models
