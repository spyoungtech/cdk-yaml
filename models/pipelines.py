from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams

#  autogenerated from aws_cdk.pipelines.ArtifactMap
class ArtifactMapDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ArtifactMap'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.CodeBuildStep
class CodeBuildStepDef(BaseClass):
    action_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Custom execution role to be used for the Code Build Action. Default: - A role is automatically created\n')
    build_environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Changes to environment. This environment will be combined with the pipeline's default environment. Default: - Use the pipeline's default build environment\n")
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: - No cache\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='ProjectFileSystemLocation objects for CodeBuild build projects. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Default: - no file system locations\n')
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for CodeBuild projects. A CodeBuild project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    partial_build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description="Additional configuration that can only be configured via BuildSpec. You should not use this to specify output artifacts; those should be supplied via the other properties of this class, otherwise CDK Pipelines won't be able to inspect the artifacts. Set the ``commands`` to an empty array if you want to fully specify the BuildSpec using this field. The BuildSpec must be available inline--it cannot reference a file on disk. Default: - BuildSpec completely derived from other properties\n")
    project_name: typing.Optional[str] = pydantic.Field(None, description='Name for the generated CodeBuild project. Default: - Automatically generated\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Custom execution role to be used for the CodeBuild project. Default: - A role is automatically created\n')
    role_policy_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Policy statements to add to role used during the synth. Can be used to add acces to a CodeArtifact repository etc. Default: - No policy statements added to CodeBuild Project Role\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="Which security group to associate with the script's project network interfaces. If no security group is identified, one will be created automatically. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Which subnets to use. Only used if 'vpc' is supplied. Default: - All private subnets.\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where to execute the SimpleSynth. Default: - No VPC\n')
    commands: typing.Sequence[str] = pydantic.Field(..., description='Commands to run.\n')
    additional_inputs: typing.Optional[typing.Mapping[str, typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]]] = pydantic.Field(None, description="Additional FileSets to put in other directories. Specifies a mapping from directory name to FileSets. During the script execution, the FileSets will be available in the directories indicated. The directory names may be relative. For example, you can put the main input and an additional input side-by-side with the following configuration:: const script = new pipelines.ShellStep('MainScript', { commands: ['npm ci','npm run build','npx cdk synth'], input: pipelines.CodePipelineSource.gitHub('org/source1', 'main'), additionalInputs: { '../siblingdir': pipelines.CodePipelineSource.gitHub('org/source2', 'main'), } }); Default: - No additional inputs\n")
    env: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set. Default: - No environment variables\n')
    env_from_cfn_outputs: typing.Optional[typing.Mapping[str, models.CfnOutputDef]] = pydantic.Field(None, description='Set environment variables based on Stack Outputs. ``ShellStep``s following stack or stage deployments may access the ``CfnOutput``s of those stacks to get access to --for example--automatically generated resource names or endpoint URLs. Default: - No environment variables created from stack outputs\n')
    input: typing.Optional[typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]] = pydantic.Field(None, description='FileSet to run these scripts on. The files in the FileSet will be placed in the working directory when the script is executed. Use ``additionalInputs`` to download file sets to other directories as well. Default: - No input specified\n')
    install_commands: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Installation commands to run before the regular commands. For deployment engines that support it, install commands will be classified differently in the job history from the regular ``commands``. Default: - No installation commands\n')
    primary_output_directory: typing.Optional[str] = pydantic.Field(None, description='The directory that will contain the primary output fileset. After running the script, the contents of the given directory will be treated as the primary output of this Step. Default: - No primary output')
    _init_params: typing.ClassVar[list[str]] = ['action_role', 'build_environment', 'cache', 'file_system_locations', 'logging', 'partial_build_spec', 'project_name', 'role', 'role_policy_statements', 'security_groups', 'subnet_selection', 'timeout', 'vpc', 'commands', 'additional_inputs', 'env', 'env_from_cfn_outputs', 'input', 'install_commands', 'primary_output_directory']
    _method_names: typing.ClassVar[list[str]] = ['add_output_directory', 'add_step_dependency', 'exported_variable', 'primary_output_directory']
    _classmethod_names: typing.ClassVar[list[str]] = ['sequence']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodeBuildStep'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeBuildStepDefConfig] = pydantic.Field(None)


class CodeBuildStepDefConfig(pydantic.BaseModel):
    add_output_directory: typing.Optional[list[CodeBuildStepDefAddOutputDirectoryParams]] = pydantic.Field(None, description='Add an additional output FileSet based on a directory.\nAfter running the script, the contents of the given directory\nwill be exported as a ``FileSet``. Use the ``FileSet`` as the\ninput to another step.\n\nMultiple calls with the exact same directory name string (not normalized)\nwill return the same FileSet.')
    add_step_dependency: typing.Optional[list[CodeBuildStepDefAddStepDependencyParams]] = pydantic.Field(None, description='Add a dependency on another step.')
    exported_variable: typing.Optional[list[CodeBuildStepDefExportedVariableParams]] = pydantic.Field(None, description='Reference a CodePipeline variable defined by the CodeBuildStep.\nThe variable must be set in the shell of the CodeBuild step when\nit finishes its ``post_build`` phase.')
    primary_output_directory: typing.Optional[list[CodeBuildStepDefPrimaryOutputDirectoryParams]] = pydantic.Field(None, description='Configure the given output directory as primary output.\nIf no primary output has been configured yet, this directory\nwill become the primary output of this ShellStep, otherwise this\nmethod will throw if the given directory is different than the\ncurrently configured primary output directory.')
    sequence: typing.Optional[list[CodeBuildStepDefSequenceParams]] = pydantic.Field(None, description='Define a sequence of steps to be executed in order.\nIf you need more fine-grained step ordering, use the ``addStepDependency()``\nAPI. For example, if you want ``secondStep`` to occur after ``firstStep``, call\n``secondStep.addStepDependency(firstStep)``.')
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    project_config: typing.Optional[models._interface_methods.AwsCodebuildIProjectDefConfig] = pydantic.Field(None)

class CodeBuildStepDefAddOutputDirectoryParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.pipelines.FileSetDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStepDefAddStepDependencyParams(pydantic.BaseModel):
    step: models.pipelines.StepDef = pydantic.Field(..., description='-')
    ...

class CodeBuildStepDefExportedVariableParams(pydantic.BaseModel):
    variable_name: str = pydantic.Field(..., description='the name of the variable for reference.\n\nExample::\n\n    # Access the output of one CodeBuildStep in another CodeBuildStep\n    # pipeline: pipelines.CodePipeline\n\n\n    step1 = pipelines.CodeBuildStep("Step1",\n        commands=["export MY_VAR=hello"]\n    )\n\n    step2 = pipelines.CodeBuildStep("Step2",\n        env={\n            "IMPORTED_VAR": step1.exported_variable("MY_VAR")\n        },\n        commands=["echo $IMPORTED_VAR"]\n    )\n')
    ...

class CodeBuildStepDefPrimaryOutputDirectoryParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.pipelines.FileSetDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStepDefSequenceParams(pydantic.BaseModel):
    steps: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.pipelines.CodePipelineFileSet
class CodePipelineFileSetDef(BaseClass):
    producer: typing.Optional[models.pipelines.StepDef] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['producer']
    _method_names: typing.ClassVar[list[str]] = ['produced_by']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_artifact']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodePipelineFileSet'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodePipelineFileSetDefConfig] = pydantic.Field(None)


class CodePipelineFileSetDefConfig(pydantic.BaseModel):
    from_artifact: typing.Optional[list[CodePipelineFileSetDefFromArtifactParams]] = pydantic.Field(None, description='Turn a CodePipeline Artifact into a FileSet.')
    produced_by: typing.Optional[list[CodePipelineFileSetDefProducedByParams]] = pydantic.Field(None, description='Mark the given Step as the producer for this FileSet.\nThis method can only be called once.')
    producer_config: typing.Optional[models.pipelines.StepDefConfig] = pydantic.Field(None)

class CodePipelineFileSetDefFromArtifactParams(pydantic.BaseModel):
    artifact: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.pipelines.CodePipelineFileSetDefConfig]] = pydantic.Field(None)
    ...

class CodePipelineFileSetDefProducedByParams(pydantic.BaseModel):
    producer: typing.Optional[models.pipelines.StepDef] = pydantic.Field(None, description='-')
    ...


#  autogenerated from aws_cdk.pipelines.CodePipelineSource
class CodePipelineSourceDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['add_step_dependency', 'produce_action', 'source_attribute']
    _classmethod_names: typing.ClassVar[list[str]] = ['code_commit', 'connection', 'ecr', 'git_hub', 's3', 'sequence']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodePipelineSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodePipelineSourceDefConfig] = pydantic.Field(None)


class CodePipelineSourceDefConfig(pydantic.BaseModel):
    add_step_dependency: typing.Optional[list[CodePipelineSourceDefAddStepDependencyParams]] = pydantic.Field(None, description='Add a dependency on another step.')
    code_commit: typing.Optional[list[CodePipelineSourceDefCodeCommitParams]] = pydantic.Field(None, description='Returns a CodeCommit source.\nIf you need access to symlinks or the repository history, be sure to set\n``codeBuildCloneOutput``.')
    connection: typing.Optional[list[CodePipelineSourceDefConnectionParams]] = pydantic.Field(None, description='Returns a CodeStar connection source.\nA CodeStar connection allows AWS CodePipeline to\naccess external resources, such as repositories in GitHub, GitHub Enterprise or\nBitBucket.\n\nTo use this method, you first need to create a CodeStar connection\nusing the AWS console. In the process, you may have to sign in to the external provider\n-- GitHub, for example -- to authorize AWS to read and modify your repository.\nOnce you have done this, copy the connection ARN and use it to create the source.\n\nExample::\n\n   pipelines.CodePipelineSource.connection("owner/repo", "main",\n       connection_arn="arn:aws:codestar-connections:us-east-1:222222222222:connection/7d2469ff-514a-4e4f-9003-5ca4a43cdc41"\n   )\n\nIf you need access to symlinks or the repository history, be sure to set\n``codeBuildCloneOutput``.')
    ecr: typing.Optional[list[CodePipelineSourceDefEcrParams]] = pydantic.Field(None, description='Returns an ECR source.')
    git_hub: typing.Optional[list[CodePipelineSourceDefGitHubParams]] = pydantic.Field(None, description='Returns a GitHub source, using OAuth tokens to authenticate with GitHub and a separate webhook to detect changes.\nThis is no longer\nthe recommended method. Please consider using ``connection()``\ninstead.\n\nPass in the owner and repository in a single string, like this::\n\n   pipelines.CodePipelineSource.git_hub("owner/repo", "main")\n\nAuthentication will be done by a secret called ``github-token`` in AWS\nSecrets Manager (unless specified otherwise).\n\nIf you rotate the value in the Secret, you must also change at least one property\non the Pipeline, to force CloudFormation to re-read the secret.\n\nThe token should have these permissions:\n\n- **repo** - to read the repository\n- **admin:repo_hook** - if you plan to use webhooks (true by default)\n\nIf you need access to symlinks or the repository history, use a source of type\n``connection`` instead.')
    produce_action: typing.Optional[list[CodePipelineSourceDefProduceActionParams]] = pydantic.Field(None, description='Create the desired Action and add it to the pipeline.')
    s3: typing.Optional[list[CodePipelineSourceDefS3Params]] = pydantic.Field(None, description='Returns an S3 source.')
    sequence: typing.Optional[list[CodePipelineSourceDefSequenceParams]] = pydantic.Field(None, description='Define a sequence of steps to be executed in order.\nIf you need more fine-grained step ordering, use the ``addStepDependency()``\nAPI. For example, if you want ``secondStep`` to occur after ``firstStep``, call\n``secondStep.addStepDependency(firstStep)``.')
    source_attribute: typing.Optional[list[CodePipelineSourceDefSourceAttributeParams]] = pydantic.Field(None, description="Return an attribute of the current source revision.\nThese values can be passed into the environment variables of pipeline steps,\nso your steps can access information about the source revision.\n\nPipeline synth step has some source attributes predefined in the environment.\nIf these suffice, you don't need to use this method for the synth step.")

class CodePipelineSourceDefAddStepDependencyParams(pydantic.BaseModel):
    step: models.pipelines.StepDef = pydantic.Field(..., description='-')
    ...

class CodePipelineSourceDefCodeCommitParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_codecommit.RepositoryDef] = pydantic.Field(..., description='The CodeCommit repository.\n')
    branch: str = pydantic.Field(..., description='The branch to use.\n')
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository name\n')
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='If this is set, the next CodeBuild job clones the repository (instead of CodePipeline downloading the files). This provides access to repository history, and retains symlinks (symlinks would otherwise be removed by CodePipeline). **Note**: if this option is true, only CodeBuild jobs can use the output artifact. Default: false\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role to be used by on commit event rule. Used only when trigger value is CodeCommitTrigger.EVENTS. Default: a new role will be created.\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.CodeCommitTrigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Default: CodeCommitTrigger.EVENTS\n\nExample::\n\n    # repository: codecommit.IRepository\n\n    pipelines.CodePipelineSource.code_commit(repository, "main")\n')
    return_config: typing.Optional[list[models.pipelines.CodePipelineSourceDefConfig]] = pydantic.Field(None)
    ...

class CodePipelineSourceDefConnectionParams(pydantic.BaseModel):
    repo_string: str = pydantic.Field(..., description="A string that encodes owner and repository separated by a slash (e.g. 'owner/repo').\n")
    branch: str = pydantic.Field(..., description='The branch to use.\n')
    connection_arn: str = pydantic.Field(..., description='The ARN of the CodeStar Connection created in the AWS console that has permissions to access this GitHub or BitBucket repository.\n')
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository string\n')
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='If this is set, the next CodeBuild job clones the repository (instead of CodePipeline downloading the files). This provides access to repository history, and retains symlinks (symlinks would otherwise be removed by CodePipeline). **Note**: if this option is true, only CodeBuild jobs can use the output artifact. Default: false\n')
    trigger_on_push: typing.Optional[bool] = pydantic.Field(None, description='Controls automatically starting your pipeline when a new commit is made on the configured repository and branch. If unspecified, the default value is true, and the field does not display by default. Default: true\n\n:see: https://docs.aws.amazon.com/dtconsole/latest/userguide/welcome-connections.html\n')
    return_config: typing.Optional[list[models.pipelines.CodePipelineSourceDefConfig]] = pydantic.Field(None)
    ...

class CodePipelineSourceDefEcrParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='The repository that will be watched for changes.\n')
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository name\n')
    image_tag: typing.Optional[str] = pydantic.Field(None, description='The image tag that will be checked for changes. Default: latest\n\nExample::\n\n    # repository: ecr.IRepository\n\n    pipelines.CodePipelineSource.ecr(repository,\n        image_tag="latest"\n    )\n')
    return_config: typing.Optional[list[models.pipelines.CodePipelineSourceDefConfig]] = pydantic.Field(None)
    ...

class CodePipelineSourceDefGitHubParams(pydantic.BaseModel):
    repo_string: str = pydantic.Field(..., description='-\n')
    branch: str = pydantic.Field(..., description='-\n')
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository string\n')
    authentication: typing.Optional[models.SecretValueDef] = pydantic.Field(None, description="A GitHub OAuth token to use for authentication. It is recommended to use a Secrets Manager ``Secret`` to obtain the token:: const oauth = cdk.SecretValue.secretsManager('my-github-token'); The GitHub Personal Access Token should have these scopes: - **repo** - to read the repository - **admin:repo_hook** - if you plan to use webhooks (true by default) Default: - SecretValue.secretsManager('github-token')\n")
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.GitHubTrigger] = pydantic.Field(None, description='How AWS CodePipeline should be triggered. With the default value "WEBHOOK", a webhook is created in GitHub that triggers the action. With "POLL", CodePipeline periodically checks the source for changes. With "None", the action is not triggered through changes in the source. To use ``WEBHOOK``, your GitHub Personal Access Token should have **admin:repo_hook** scope (in addition to the regular **repo** scope). Default: GitHubTrigger.WEBHOOK')
    return_config: typing.Optional[list[models.pipelines.CodePipelineSourceDefConfig]] = pydantic.Field(None)
    ...

class CodePipelineSourceDefProduceActionParams(pydantic.BaseModel):
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    action_name: str = pydantic.Field(..., description='Name the action should get.\n')
    artifacts: models.pipelines.ArtifactMapDef = pydantic.Field(..., description='Helper object to translate FileSets to CodePipeline Artifacts.\n')
    pipeline: models.pipelines.CodePipelineDef = pydantic.Field(..., description='The pipeline the action is being generated for.\n')
    run_order: typing.Union[int, float] = pydantic.Field(..., description='RunOrder the action should get.\n')
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='Scope in which to create constructs.\n')
    stack_outputs_map: models.pipelines.StackOutputsMapDef = pydantic.Field(..., description='Helper object to produce variables exported from stack deployments. If your step references outputs from a stack deployment, use this to map the output references to Codepipeline variable names. Note - Codepipeline variables can only be referenced in action configurations.\n')
    before_self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Whether or not this action is inserted before self mutation. If it is, the action should take care to reflect some part of its own definition in the pipeline action definition, to trigger a restart after self-mutation (if necessary). Default: false\n')
    code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If this action factory creates a CodeBuild step, default options to inherit. Default: - No CodeBuild project defaults\n')
    fallback_artifact: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="An input artifact that CodeBuild projects that don't actually need an input artifact can use. CodeBuild Projects MUST have an input artifact in order to be added to the Pipeline. If the Project doesn't actually care about its input (it can be anything), it can use the Artifact passed here. Default: - A fallback artifact does not exist\n")
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="If this step is producing outputs, the variables namespace assigned to it. Pass this on to the Action you are creating. Default: - Step doesn't produce any outputs")
    ...

class CodePipelineSourceDefS3Params(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The bucket where the source code is located.\n')
    object_key: str = pydantic.Field(..., description='-\n')
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The bucket name\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that will be assumed by the pipeline prior to executing the ``S3Source`` action. Default: - a new role will be generated\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.S3Trigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Note that if this is S3Trigger.EVENTS, you need to make sure to include the source Bucket in a CloudTrail Trail, as otherwise the CloudWatch Events will not be emitted. Default: S3Trigger.POLL\n\nExample::\n\n    # bucket: s3.Bucket\n\n    pipelines.CodePipelineSource.s3(bucket, "path/to/file.zip")\n')
    return_config: typing.Optional[list[models.pipelines.CodePipelineSourceDefConfig]] = pydantic.Field(None)
    ...

class CodePipelineSourceDefSequenceParams(pydantic.BaseModel):
    steps: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='-')
    ...

class CodePipelineSourceDefSourceAttributeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n\n:see: https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-variables.html#reference-variables-list\n\nExample::\n\n    # Access the CommitId of a GitHub source in the synth\n    source = pipelines.CodePipelineSource.git_hub("owner/repo", "main")\n\n    pipeline = pipelines.CodePipeline(scope, "MyPipeline",\n        synth=pipelines.ShellStep("Synth",\n            input=source,\n            commands=[],\n            env={\n                "COMMIT_ID": source.source_attribute("CommitId")\n            }\n        )\n    )\n')
    ...


#  autogenerated from aws_cdk.pipelines.ConfirmPermissionsBroadening
class ConfirmPermissionsBroadeningDef(BaseClass):
    stage: models.StageDef = pydantic.Field(..., description='The CDK Stage object to check the stacks of. This should be the same Stage object you are passing to ``addStage()``.\n')
    notification_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='Topic to send notifications when a human needs to give manual confirmation. Default: - no notification')
    _init_params: typing.ClassVar[list[str]] = ['stage', 'notification_topic']
    _method_names: typing.ClassVar[list[str]] = ['add_step_dependency', 'produce_action']
    _classmethod_names: typing.ClassVar[list[str]] = ['sequence']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ConfirmPermissionsBroadening'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ConfirmPermissionsBroadeningDefConfig] = pydantic.Field(None)


class ConfirmPermissionsBroadeningDefConfig(pydantic.BaseModel):
    add_step_dependency: typing.Optional[list[ConfirmPermissionsBroadeningDefAddStepDependencyParams]] = pydantic.Field(None, description='Add a dependency on another step.')
    produce_action: typing.Optional[list[ConfirmPermissionsBroadeningDefProduceActionParams]] = pydantic.Field(None, description='Create the desired Action and add it to the pipeline.')
    sequence: typing.Optional[list[ConfirmPermissionsBroadeningDefSequenceParams]] = pydantic.Field(None, description='Define a sequence of steps to be executed in order.\nIf you need more fine-grained step ordering, use the ``addStepDependency()``\nAPI. For example, if you want ``secondStep`` to occur after ``firstStep``, call\n``secondStep.addStepDependency(firstStep)``.')

class ConfirmPermissionsBroadeningDefAddStepDependencyParams(pydantic.BaseModel):
    step: models.pipelines.StepDef = pydantic.Field(..., description='-')
    ...

class ConfirmPermissionsBroadeningDefProduceActionParams(pydantic.BaseModel):
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    action_name: str = pydantic.Field(..., description='Name the action should get.\n')
    artifacts: models.pipelines.ArtifactMapDef = pydantic.Field(..., description='Helper object to translate FileSets to CodePipeline Artifacts.\n')
    pipeline: models.pipelines.CodePipelineDef = pydantic.Field(..., description='The pipeline the action is being generated for.\n')
    run_order: typing.Union[int, float] = pydantic.Field(..., description='RunOrder the action should get.\n')
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='Scope in which to create constructs.\n')
    stack_outputs_map: models.pipelines.StackOutputsMapDef = pydantic.Field(..., description='Helper object to produce variables exported from stack deployments. If your step references outputs from a stack deployment, use this to map the output references to Codepipeline variable names. Note - Codepipeline variables can only be referenced in action configurations.\n')
    before_self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Whether or not this action is inserted before self mutation. If it is, the action should take care to reflect some part of its own definition in the pipeline action definition, to trigger a restart after self-mutation (if necessary). Default: false\n')
    code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If this action factory creates a CodeBuild step, default options to inherit. Default: - No CodeBuild project defaults\n')
    fallback_artifact: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="An input artifact that CodeBuild projects that don't actually need an input artifact can use. CodeBuild Projects MUST have an input artifact in order to be added to the Pipeline. If the Project doesn't actually care about its input (it can be anything), it can use the Artifact passed here. Default: - A fallback artifact does not exist\n")
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="If this step is producing outputs, the variables namespace assigned to it. Pass this on to the Action you are creating. Default: - Step doesn't produce any outputs")
    ...

class ConfirmPermissionsBroadeningDefSequenceParams(pydantic.BaseModel):
    steps: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.pipelines.DockerCredential
class DockerCredentialDef(BaseClass):
    usages: typing.Optional[typing.Sequence[aws_cdk.pipelines.DockerCredentialUsage]] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['usages']
    _method_names: typing.ClassVar[list[str]] = ['grant_read']
    _classmethod_names: typing.ClassVar[list[str]] = ['custom_registry', 'docker_hub', 'ecr']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.DockerCredential'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DockerCredentialDefConfig] = pydantic.Field(None)


class DockerCredentialDefConfig(pydantic.BaseModel):
    custom_registry: typing.Optional[list[DockerCredentialDefCustomRegistryParams]] = pydantic.Field(None, description="Creates a DockerCredential for a registry, based on its domain name (e.g., 'www.example.com').")
    docker_hub: typing.Optional[list[DockerCredentialDefDockerHubParams]] = pydantic.Field(None, description="Creates a DockerCredential for DockerHub.\nConvenience method for ``customRegistry('https://index.docker.io/v1/', opts)``.")
    ecr: typing.Optional[list[DockerCredentialDefEcrParams]] = pydantic.Field(None, description='Creates a DockerCredential for one or more ECR repositories.\nNOTE - All ECR repositories in the same account and region share a domain name\n(e.g., 0123456789012.dkr.ecr.eu-west-1.amazonaws.com), and can only have one associated\nset of credentials (and DockerCredential). Attempting to associate one set of credentials\nwith one ECR repo and another with another ECR repo in the same account and region will\nresult in failures when using these credentials in the pipeline.')
    grant_read: typing.Optional[list[DockerCredentialDefGrantReadParams]] = pydantic.Field(None, description='Grant read-only access to the registry credentials.\nThis grants read access to any secrets, and pull access to any repositories.')

class DockerCredentialDefCustomRegistryParams(pydantic.BaseModel):
    registry_domain: str = pydantic.Field(..., description='-\n')
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='-\n')
    assume_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role to assume prior to accessing the secret. Default: - none. The current execution role will be used.\n')
    secret_password_field: typing.Optional[str] = pydantic.Field(None, description="The name of the JSON field of the secret which contains the secret/password. Default: 'secret'\n")
    secret_username_field: typing.Optional[str] = pydantic.Field(None, description="The name of the JSON field of the secret which contains the user/login name. Default: 'username'\n")
    usages: typing.Optional[typing.Sequence[aws_cdk.pipelines.DockerCredentialUsage]] = pydantic.Field(None, description='Defines which stages of the pipeline should be granted access to these credentials. Default: - all relevant stages (synth, self-update, asset publishing) are granted access.')
    return_config: typing.Optional[list[models.pipelines.DockerCredentialDefConfig]] = pydantic.Field(None)
    ...

class DockerCredentialDefDockerHubParams(pydantic.BaseModel):
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='-\n')
    assume_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role to assume prior to accessing the secret. Default: - none. The current execution role will be used.\n')
    secret_password_field: typing.Optional[str] = pydantic.Field(None, description="The name of the JSON field of the secret which contains the secret/password. Default: 'secret'\n")
    secret_username_field: typing.Optional[str] = pydantic.Field(None, description="The name of the JSON field of the secret which contains the user/login name. Default: 'username'\n")
    usages: typing.Optional[typing.Sequence[aws_cdk.pipelines.DockerCredentialUsage]] = pydantic.Field(None, description='Defines which stages of the pipeline should be granted access to these credentials. Default: - all relevant stages (synth, self-update, asset publishing) are granted access.')
    return_config: typing.Optional[list[models.pipelines.DockerCredentialDefConfig]] = pydantic.Field(None)
    ...

class DockerCredentialDefEcrParams(pydantic.BaseModel):
    repositories: typing.Sequence[typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef]] = pydantic.Field(..., description='-\n')
    assume_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role to assume prior to accessing the secret. Default: - none. The current execution role will be used.\n')
    usages: typing.Optional[typing.Sequence[aws_cdk.pipelines.DockerCredentialUsage]] = pydantic.Field(None, description='Defines which stages of the pipeline should be granted access to these credentials. Default: - all relevant stages (synth, self-update, asset publishing) are granted access.')
    return_config: typing.Optional[list[models.pipelines.DockerCredentialDefConfig]] = pydantic.Field(None)
    ...

class DockerCredentialDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    usage: aws_cdk.pipelines.DockerCredentialUsage = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.pipelines.FileSet
class FileSetDef(BaseClass):
    producer: typing.Optional[models.pipelines.StepDef] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['producer']
    _method_names: typing.ClassVar[list[str]] = ['produced_by']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.FileSet'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[FileSetDefConfig] = pydantic.Field(None)


class FileSetDefConfig(pydantic.BaseModel):
    produced_by: typing.Optional[list[FileSetDefProducedByParams]] = pydantic.Field(None, description='Mark the given Step as the producer for this FileSet.\nThis method can only be called once.')
    producer_config: typing.Optional[models.pipelines.StepDefConfig] = pydantic.Field(None)

class FileSetDefProducedByParams(pydantic.BaseModel):
    producer: typing.Optional[models.pipelines.StepDef] = pydantic.Field(None, description='-')
    ...


#  autogenerated from aws_cdk.pipelines.ManualApprovalStep
class ManualApprovalStepDef(BaseClass):
    comment: typing.Optional[str] = pydantic.Field(None, description='The comment to display with this manual approval. Default: - No comment')
    _init_params: typing.ClassVar[list[str]] = ['comment']
    _method_names: typing.ClassVar[list[str]] = ['add_step_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = ['sequence']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ManualApprovalStep'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ManualApprovalStepDefConfig] = pydantic.Field(None)


class ManualApprovalStepDefConfig(pydantic.BaseModel):
    add_step_dependency: typing.Optional[list[ManualApprovalStepDefAddStepDependencyParams]] = pydantic.Field(None, description='Add a dependency on another step.')
    sequence: typing.Optional[list[ManualApprovalStepDefSequenceParams]] = pydantic.Field(None, description='Define a sequence of steps to be executed in order.\nIf you need more fine-grained step ordering, use the ``addStepDependency()``\nAPI. For example, if you want ``secondStep`` to occur after ``firstStep``, call\n``secondStep.addStepDependency(firstStep)``.')

class ManualApprovalStepDefAddStepDependencyParams(pydantic.BaseModel):
    step: models.pipelines.StepDef = pydantic.Field(..., description='-')
    ...

class ManualApprovalStepDefSequenceParams(pydantic.BaseModel):
    steps: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.pipelines.PipelineBase
class PipelineBaseDef(BaseClass):
    synth: typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef] = pydantic.Field(..., description="The build step that produces the CDK Cloud Assembly. The primary output of this step needs to be the ``cdk.out`` directory generated by the ``cdk synth`` command. If you use a ``ShellStep`` here and you don't configure an output directory, the output directory will automatically be assumed to be ``cdk.out``.")
    _init_params: typing.ClassVar[list[str]] = ['synth']
    _method_names: typing.ClassVar[list[str]] = ['add_stage', 'add_wave', 'build_pipeline']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.PipelineBase'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[PipelineBaseDefConfig] = pydantic.Field(None)


class PipelineBaseDefConfig(pydantic.BaseModel):
    add_stage: typing.Optional[list[PipelineBaseDefAddStageParams]] = pydantic.Field(None, description='Deploy a single Stage by itself.\nAdd a Stage to the pipeline, to be deployed in sequence with other\nStages added to the pipeline. All Stacks in the stage will be deployed\nin an order automatically determined by their relative dependencies.')
    add_wave: typing.Optional[list[PipelineBaseDefAddWaveParams]] = pydantic.Field(None, description='Add a Wave to the pipeline, for deploying multiple Stages in parallel.\nUse the return object of this method to deploy multiple stages in parallel.\n\nExample::\n\n   # pipeline: pipelines.CodePipeline\n\n\n   wave = pipeline.add_wave("MyWave")\n   wave.add_stage(MyApplicationStage(self, "Stage1"))\n   wave.add_stage(MyApplicationStage(self, "Stage2"))')
    build_pipeline: typing.Optional[bool] = pydantic.Field(None, description='Send the current pipeline definition to the engine, and construct the pipeline.\nIt is not possible to modify the pipeline after calling this method.')

class PipelineBaseDefAddStageParams(pydantic.BaseModel):
    stage: models.StageDef = pydantic.Field(..., description='-\n')
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stacks in the stage. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stacks in the stage. Default: - No additional steps\n')
    stack_steps: typing.Optional[typing.Sequence[typing.Union[models.pipelines.StackStepsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Instructions for stack level steps. Default: - No additional instructions')
    return_config: typing.Optional[list[models.pipelines.StageDeploymentDefConfig]] = pydantic.Field(None)
    ...

class PipelineBaseDefAddWaveParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stages in the wave. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stages in the wave. Default: - No additional steps')
    return_config: typing.Optional[list[models.pipelines.WaveDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.pipelines.ShellStep
class ShellStepDef(BaseClass):
    commands: typing.Sequence[str] = pydantic.Field(..., description='Commands to run.\n')
    additional_inputs: typing.Optional[typing.Mapping[str, typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]]] = pydantic.Field(None, description="Additional FileSets to put in other directories. Specifies a mapping from directory name to FileSets. During the script execution, the FileSets will be available in the directories indicated. The directory names may be relative. For example, you can put the main input and an additional input side-by-side with the following configuration:: const script = new pipelines.ShellStep('MainScript', { commands: ['npm ci','npm run build','npx cdk synth'], input: pipelines.CodePipelineSource.gitHub('org/source1', 'main'), additionalInputs: { '../siblingdir': pipelines.CodePipelineSource.gitHub('org/source2', 'main'), } }); Default: - No additional inputs\n")
    env: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set. Default: - No environment variables\n')
    env_from_cfn_outputs: typing.Optional[typing.Mapping[str, models.CfnOutputDef]] = pydantic.Field(None, description='Set environment variables based on Stack Outputs. ``ShellStep``s following stack or stage deployments may access the ``CfnOutput``s of those stacks to get access to --for example--automatically generated resource names or endpoint URLs. Default: - No environment variables created from stack outputs\n')
    input: typing.Optional[typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]] = pydantic.Field(None, description='FileSet to run these scripts on. The files in the FileSet will be placed in the working directory when the script is executed. Use ``additionalInputs`` to download file sets to other directories as well. Default: - No input specified\n')
    install_commands: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Installation commands to run before the regular commands. For deployment engines that support it, install commands will be classified differently in the job history from the regular ``commands``. Default: - No installation commands\n')
    primary_output_directory: typing.Optional[str] = pydantic.Field(None, description='The directory that will contain the primary output fileset. After running the script, the contents of the given directory will be treated as the primary output of this Step. Default: - No primary output')
    _init_params: typing.ClassVar[list[str]] = ['commands', 'additional_inputs', 'env', 'env_from_cfn_outputs', 'input', 'install_commands', 'primary_output_directory']
    _method_names: typing.ClassVar[list[str]] = ['add_output_directory', 'add_step_dependency', 'primary_output_directory']
    _classmethod_names: typing.ClassVar[list[str]] = ['sequence']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ShellStep'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ShellStepDefConfig] = pydantic.Field(None)


class ShellStepDefConfig(pydantic.BaseModel):
    add_output_directory: typing.Optional[list[ShellStepDefAddOutputDirectoryParams]] = pydantic.Field(None, description='Add an additional output FileSet based on a directory.\nAfter running the script, the contents of the given directory\nwill be exported as a ``FileSet``. Use the ``FileSet`` as the\ninput to another step.\n\nMultiple calls with the exact same directory name string (not normalized)\nwill return the same FileSet.')
    add_step_dependency: typing.Optional[list[ShellStepDefAddStepDependencyParams]] = pydantic.Field(None, description='Add a dependency on another step.')
    primary_output_directory: typing.Optional[list[ShellStepDefPrimaryOutputDirectoryParams]] = pydantic.Field(None, description='Configure the given output directory as primary output.\nIf no primary output has been configured yet, this directory\nwill become the primary output of this ShellStep, otherwise this\nmethod will throw if the given directory is different than the\ncurrently configured primary output directory.')
    sequence: typing.Optional[list[ShellStepDefSequenceParams]] = pydantic.Field(None, description='Define a sequence of steps to be executed in order.\nIf you need more fine-grained step ordering, use the ``addStepDependency()``\nAPI. For example, if you want ``secondStep`` to occur after ``firstStep``, call\n``secondStep.addStepDependency(firstStep)``.')

class ShellStepDefAddOutputDirectoryParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.pipelines.FileSetDefConfig]] = pydantic.Field(None)
    ...

class ShellStepDefAddStepDependencyParams(pydantic.BaseModel):
    step: models.pipelines.StepDef = pydantic.Field(..., description='-')
    ...

class ShellStepDefPrimaryOutputDirectoryParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.pipelines.FileSetDefConfig]] = pydantic.Field(None)
    ...

class ShellStepDefSequenceParams(pydantic.BaseModel):
    steps: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.pipelines.StackDeployment
class StackDeploymentDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['add_stack_dependency', 'add_stack_steps']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_artifact']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StackDeployment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StackDeploymentDefConfig] = pydantic.Field(None)


class StackDeploymentDefConfig(pydantic.BaseModel):
    add_stack_dependency: typing.Optional[list[StackDeploymentDefAddStackDependencyParams]] = pydantic.Field(None, description='Add a dependency on another stack.')
    add_stack_steps: typing.Optional[list[StackDeploymentDefAddStackStepsParams]] = pydantic.Field(None, description='Adds steps to each phase of the stack.')
    from_artifact: typing.Optional[list[StackDeploymentDefFromArtifactParams]] = pydantic.Field(None, description='Build a ``StackDeployment`` from a Stack Artifact in a Cloud Assembly.')

class StackDeploymentDefAddStackDependencyParams(pydantic.BaseModel):
    stack_deployment: models.pipelines.StackDeploymentDef = pydantic.Field(..., description='-')
    ...

class StackDeploymentDefAddStackStepsParams(pydantic.BaseModel):
    pre: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='steps executed before stack.prepare.\n')
    change_set: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='steps executed after stack.prepare and before stack.deploy.\n')
    post: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='steps executed after stack.deploy.')
    ...

class StackDeploymentDefFromArtifactParams(pydantic.BaseModel):
    stack_artifact: models.cx_api.CloudFormationStackArtifactDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.pipelines.StackDeploymentDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.pipelines.StackOutputReference
class StackOutputReferenceDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_cfn_output']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StackOutputReference'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StackOutputReferenceDefConfig] = pydantic.Field(None)


class StackOutputReferenceDefConfig(pydantic.BaseModel):
    from_cfn_output: typing.Optional[list[StackOutputReferenceDefFromCfnOutputParams]] = pydantic.Field(None, description='Create a StackOutputReference that references the given CfnOutput.')

class StackOutputReferenceDefFromCfnOutputParams(pydantic.BaseModel):
    output: models.CfnOutputDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.pipelines.StackOutputReferenceDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.pipelines.StackOutputsMap
class StackOutputsMapDef(BaseClass):
    pipeline: models.pipelines.PipelineBaseDef = pydantic.Field(..., description='-')
    _init_params: typing.ClassVar[list[str]] = ['pipeline']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StackOutputsMap'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.StageDeployment
class StageDeploymentDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['add_post', 'add_pre']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_stage']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StageDeployment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StageDeploymentDefConfig] = pydantic.Field(None)


class StageDeploymentDefConfig(pydantic.BaseModel):
    add_post: typing.Optional[list[StageDeploymentDefAddPostParams]] = pydantic.Field(None, description='Add an additional step to run after all of the stacks in this stage.')
    add_pre: typing.Optional[list[StageDeploymentDefAddPreParams]] = pydantic.Field(None, description='Add an additional step to run before any of the stacks in this stage.')
    from_stage: typing.Optional[list[StageDeploymentDefFromStageParams]] = pydantic.Field(None, description='Create a new ``StageDeployment`` from a ``Stage``.\nSynthesizes the target stage, and deployes the stacks found inside\nin dependency order.')

class StageDeploymentDefAddPostParams(pydantic.BaseModel):
    steps: list[models.pipelines.StepDef] = pydantic.Field(...)
    ...

class StageDeploymentDefAddPreParams(pydantic.BaseModel):
    steps: list[models.pipelines.StepDef] = pydantic.Field(...)
    ...

class StageDeploymentDefFromStageParams(pydantic.BaseModel):
    stage: models.StageDef = pydantic.Field(..., description='-\n')
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stacks in the stage. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stacks in the stage. Default: - No additional steps\n')
    stack_steps: typing.Optional[typing.Sequence[typing.Union[models.pipelines.StackStepsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Instructions for additional steps that are run at the stack level. Default: - No additional instructions\n')
    stage_name: typing.Optional[str] = pydantic.Field(None, description="Stage name to use in the pipeline. Default: - Use Stage's construct ID")
    return_config: typing.Optional[list[models.pipelines.StageDeploymentDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.pipelines.Step
class StepDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['add_step_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = ['sequence']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.Step'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StepDefConfig] = pydantic.Field(None)


class StepDefConfig(pydantic.BaseModel):
    add_step_dependency: typing.Optional[list[StepDefAddStepDependencyParams]] = pydantic.Field(None, description='Add a dependency on another step.')
    sequence: typing.Optional[list[StepDefSequenceParams]] = pydantic.Field(None, description='Define a sequence of steps to be executed in order.\nIf you need more fine-grained step ordering, use the ``addStepDependency()``\nAPI. For example, if you want ``secondStep`` to occur after ``firstStep``, call\n``secondStep.addStepDependency(firstStep)``.')

class StepDefAddStepDependencyParams(pydantic.BaseModel):
    step: models.pipelines.StepDef = pydantic.Field(..., description='-')
    ...

class StepDefSequenceParams(pydantic.BaseModel):
    steps: typing.Sequence[models.pipelines.StepDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.pipelines.Wave
class WaveDef(BaseClass):
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stages in the wave. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stages in the wave. Default: - No additional steps')
    _init_params: typing.ClassVar[list[str]] = ['post', 'pre']
    _method_names: typing.ClassVar[list[str]] = ['add_post', 'add_pre', 'add_stage']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.Wave'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[WaveDefConfig] = pydantic.Field(None)


class WaveDefConfig(pydantic.BaseModel):
    add_post: typing.Optional[list[WaveDefAddPostParams]] = pydantic.Field(None, description='Add an additional step to run after all of the stages in this wave.')
    add_pre: typing.Optional[list[WaveDefAddPreParams]] = pydantic.Field(None, description='Add an additional step to run before any of the stages in this wave.')
    add_stage: typing.Optional[list[WaveDefAddStageParams]] = pydantic.Field(None, description='Add a Stage to this wave.\nIt will be deployed in parallel with all other stages in this\nwave.')

class WaveDefAddPostParams(pydantic.BaseModel):
    steps: list[models.pipelines.StepDef] = pydantic.Field(...)
    ...

class WaveDefAddPreParams(pydantic.BaseModel):
    steps: list[models.pipelines.StepDef] = pydantic.Field(...)
    ...

class WaveDefAddStageParams(pydantic.BaseModel):
    stage: models.StageDef = pydantic.Field(..., description='-\n')
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stacks in the stage. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stacks in the stage. Default: - No additional steps\n')
    stack_steps: typing.Optional[typing.Sequence[typing.Union[models.pipelines.StackStepsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Instructions for stack level steps. Default: - No additional instructions')
    return_config: typing.Optional[list[models.pipelines.StageDeploymentDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.pipelines.CodePipeline
class CodePipelineDef(BaseConstruct):
    synth: typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef] = pydantic.Field(..., description="The build step that produces the CDK Cloud Assembly. The primary output of this step needs to be the ``cdk.out`` directory generated by the ``cdk synth`` command. If you use a ``ShellStep`` here and you don't configure an output directory, the output directory will automatically be assumed to be ``cdk.out``.\n")
    artifact_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description="An existing S3 Bucket to use for storing the pipeline's artifact. Default: - A new S3 bucket will be created.\n")
    asset_publishing_code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional customizations to apply to the asset publishing CodeBuild projects. Default: - Only ``codeBuildDefaults`` are applied\n')
    cli_version: typing.Optional[str] = pydantic.Field(None, description='CDK CLI version to use in self-mutation and asset publishing steps. If you want to lock the CDK CLI version used in the pipeline, by steps that are automatically generated for you, specify the version here. We recommend you do not specify this value, as not specifying it always uses the latest CLI version which is backwards compatible with old versions. If you do specify it, be aware that this version should always be equal to or higher than the version of the CDK framework used by the CDK app, when the CDK commands are run during your pipeline execution. When you change this version, the *next time* the ``SelfMutate`` step runs it will still be using the CLI of the the *previous* version that was in this property: it will only start using the new version after ``SelfMutate`` completes successfully. That means that if you want to update both framework and CLI version, you should update the CLI version first, commit, push and deploy, and only then update the framework version. Default: - Latest version\n')
    code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Customize the CodeBuild projects created for this pipeline. Default: - All projects run non-privileged build, SMALL instance, LinuxBuildImage.STANDARD_6_0\n')
    code_pipeline: typing.Optional[models.aws_codepipeline.PipelineDef] = pydantic.Field(None, description='An existing Pipeline to be reused and built upon. [disable-awslint:ref-via-interface] Default: - a new underlying pipeline is created.\n')
    cross_account_keys: typing.Optional[bool] = pydantic.Field(None, description='Create KMS keys for the artifact buckets, allowing cross-account deployments. The artifact buckets have to be encrypted to support deploying CDK apps to another account, so if you want to do that or want to have your artifact buckets encrypted, be sure to set this value to ``true``. Be aware there is a cost associated with maintaining the KMS keys. Default: false\n')
    docker_credentials: typing.Optional[typing.Sequence[models.pipelines.DockerCredentialDef]] = pydantic.Field(None, description='A list of credentials used to authenticate to Docker registries. Specify any credentials necessary within the pipeline to build, synth, update, or publish assets. Default: []\n')
    docker_enabled_for_self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Enable Docker for the self-mutate step. Set this to true if the pipeline itself uses Docker container assets (for example, if you use ``LinuxBuildImage.fromAsset()`` as the build image of a CodeBuild step in the pipeline). You do not need to set it if you build Docker image assets in the application Stages and Stacks that are *deployed* by this pipeline. Configures privileged mode for the self-mutation CodeBuild action. If you are about to turn this on in an already-deployed Pipeline, set the value to ``true`` first, commit and allow the pipeline to self-update, and only then use the Docker asset in the pipeline. Default: false\n')
    docker_enabled_for_synth: typing.Optional[bool] = pydantic.Field(None, description='Enable Docker for the \'synth\' step. Set this to true if you are using file assets that require "bundling" anywhere in your application (meaning an asset compilation step will be run with the tools provided by a Docker image), both for the Pipeline stack as well as the application stacks. A common way to use bundling assets in your application is by using the ``@aws-cdk/aws-lambda-nodejs`` library. Configures privileged mode for the synth CodeBuild action. If you are about to turn this on in an already-deployed Pipeline, set the value to ``true`` first, commit and allow the pipeline to self-update, and only then use the bundled asset. Default: false\n')
    enable_key_rotation: typing.Optional[bool] = pydantic.Field(None, description='Enable KMS key rotation for the generated KMS keys. By default KMS key rotation is disabled, but will add additional costs when enabled. Default: - false (key rotation is disabled)\n')
    pipeline_name: typing.Optional[str] = pydantic.Field(None, description='The name of the CodePipeline pipeline. Default: - Automatically generated\n')
    publish_assets_in_parallel: typing.Optional[bool] = pydantic.Field(None, description='Publish assets in multiple CodeBuild projects. If set to false, use one Project per type to publish all assets. Publishing in parallel improves concurrency and may reduce publishing latency, but may also increase overall provisioning time of the CodeBuild projects. Experiment and see what value works best for you. Default: true\n')
    reuse_cross_region_support_stacks: typing.Optional[bool] = pydantic.Field(None, description='Reuse the same cross region support stack for all pipelines in the App. Default: - true (Use the same support stack for all pipelines in App)\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be assumed by this Pipeline. Default: - A new role is created\n')
    self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Whether the pipeline will update itself. This needs to be set to ``true`` to allow the pipeline to reconfigure itself when assets or stages are being added to it, and ``true`` is the recommended setting. You can temporarily set this to ``false`` while you are iterating on the pipeline itself and prefer to deploy changes using ``cdk deploy``. Default: true\n')
    self_mutation_code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional customizations to apply to the self mutation CodeBuild projects. Default: - Only ``codeBuildDefaults`` are applied\n')
    synth_code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional customizations to apply to the synthesize CodeBuild projects. Default: - Only ``codeBuildDefaults`` are applied\n')
    use_change_sets: typing.Optional[bool] = pydantic.Field(None, description='Deploy every stack by creating a change set and executing it. When enabled, creates a "Prepare" and "Execute" action for each stack. Disable to deploy the stack in one pipeline action. Default: true')
    _init_params: typing.ClassVar[list[str]] = ['synth', 'artifact_bucket', 'asset_publishing_code_build_defaults', 'cli_version', 'code_build_defaults', 'code_pipeline', 'cross_account_keys', 'docker_credentials', 'docker_enabled_for_self_mutation', 'docker_enabled_for_synth', 'enable_key_rotation', 'pipeline_name', 'publish_assets_in_parallel', 'reuse_cross_region_support_stacks', 'role', 'self_mutation', 'self_mutation_code_build_defaults', 'synth_code_build_defaults', 'use_change_sets']
    _method_names: typing.ClassVar[list[str]] = ['add_stage', 'add_wave', 'build_pipeline']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodePipeline'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodePipelineDefConfig] = pydantic.Field(None)


class CodePipelineDefConfig(pydantic.BaseModel):
    add_stage: typing.Optional[list[CodePipelineDefAddStageParams]] = pydantic.Field(None, description='Deploy a single Stage by itself.\nAdd a Stage to the pipeline, to be deployed in sequence with other\nStages added to the pipeline. All Stacks in the stage will be deployed\nin an order automatically determined by their relative dependencies.')
    add_wave: typing.Optional[list[CodePipelineDefAddWaveParams]] = pydantic.Field(None, description='Add a Wave to the pipeline, for deploying multiple Stages in parallel.\nUse the return object of this method to deploy multiple stages in parallel.\n\nExample::\n\n   # pipeline: pipelines.CodePipeline\n\n\n   wave = pipeline.add_wave("MyWave")\n   wave.add_stage(MyApplicationStage(self, "Stage1"))\n   wave.add_stage(MyApplicationStage(self, "Stage2"))')
    build_pipeline: typing.Optional[bool] = pydantic.Field(None, description='Send the current pipeline definition to the engine, and construct the pipeline.\nIt is not possible to modify the pipeline after calling this method.')
    pipeline_config: typing.Optional[models.aws_codepipeline.PipelineDefConfig] = pydantic.Field(None)
    self_mutation_project_config: typing.Optional[models._interface_methods.AwsCodebuildIProjectDefConfig] = pydantic.Field(None)
    synth_project_config: typing.Optional[models._interface_methods.AwsCodebuildIProjectDefConfig] = pydantic.Field(None)

class CodePipelineDefAddStageParams(pydantic.BaseModel):
    stage: models.StageDef = pydantic.Field(..., description='-\n')
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stacks in the stage. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stacks in the stage. Default: - No additional steps\n')
    stack_steps: typing.Optional[typing.Sequence[typing.Union[models.pipelines.StackStepsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Instructions for stack level steps. Default: - No additional instructions')
    return_config: typing.Optional[list[models.pipelines.StageDeploymentDefConfig]] = pydantic.Field(None)
    ...

class CodePipelineDefAddWaveParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stages in the wave. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stages in the wave. Default: - No additional steps')
    return_config: typing.Optional[list[models.pipelines.WaveDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.pipelines.AddStageOpts
class AddStageOptsDef(BaseStruct):
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stacks in the stage. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stacks in the stage. Default: - No additional steps\n')
    stack_steps: typing.Optional[typing.Sequence[typing.Union[models.pipelines.StackStepsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Instructions for stack level steps. Default: - No additional instructions\n\n:exampleMetadata: infused\n\nExample::\n\n    # pipeline: pipelines.CodePipeline\n\n    preprod = MyApplicationStage(self, "PreProd")\n    prod = MyApplicationStage(self, "Prod")\n\n    pipeline.add_stage(preprod,\n        post=[\n            pipelines.ShellStep("Validate Endpoint",\n                commands=["curl -Ssf https://my.webservice.com/"]\n            )\n        ]\n    )\n    pipeline.add_stage(prod,\n        pre=[\n            pipelines.ManualApprovalStep("PromoteToProd")\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['post', 'pre', 'stack_steps']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.AddStageOpts'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.CodeBuildOptions
class CodeBuildOptionsDef(BaseStruct):
    build_environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Partial build environment, will be combined with other build environments that apply. Default: - Non-privileged build, SMALL instance, LinuxBuildImage.STANDARD_6_0\n')
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: - No cache\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='ProjectFileSystemLocation objects for CodeBuild build projects. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Requires a vpc to be set and privileged to be set to true. Default: - no file system locations\n')
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for CodeBuild projects. A CodeBuild project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    partial_build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description='Partial buildspec, will be combined with other buildspecs that apply. The BuildSpec must be available inline--it cannot reference a file on disk. Default: - No initial BuildSpec\n')
    role_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Policy statements to add to role. Default: - No policy statements added to CodeBuild Project Role\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="Which security group(s) to associate with the project network interfaces. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Which subnets to use. Only used if 'vpc' is supplied. Default: - All private subnets.\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where to create the CodeBuild network interfaces in. Default: - No VPC\n\n:exampleMetadata: infused\n\nExample::\n\n    # source: pipelines.IFileSetProducer # the repository source\n    # synth_commands: List[str] # Commands to synthesize your app\n    # install_commands: List[str]\n    # Commands to install your toolchain\n\n    pipeline = pipelines.CodePipeline(self, "Pipeline",\n        # Standard CodePipeline properties...\n        synth=pipelines.ShellStep("Synth",\n            input=source,\n            commands=synth_commands\n        ),\n\n        # Configure CodeBuild to use a drop-in Docker replacement.\n        code_build_defaults=pipelines.CodeBuildOptions(\n            partial_build_spec=codebuild.BuildSpec.from_object({\n                "phases": {\n                    "install": {\n                        # Add the shell commands to install your drop-in Docker\n                        # replacement to the CodeBuild enviromment.\n                        "commands": install_commands\n                    }\n                }\n            }),\n            build_environment=codebuild.BuildEnvironment(\n                environment_variables={\n                    # Instruct the AWS CDK to use `drop-in-replacement` instead of\n                    # `docker` when building / publishing docker images.\n                    # e.g., `drop-in-replacement build . -f path/to/Dockerfile`\n                    "CDK_DOCKER": codebuild.BuildEnvironmentVariable(value="drop-in-replacement")\n                }\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['build_environment', 'cache', 'file_system_locations', 'logging', 'partial_build_spec', 'role_policy', 'security_groups', 'subnet_selection', 'timeout', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodeBuildOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.CodeBuildStepProps
class CodeBuildStepPropsDef(BaseStruct):
    commands: typing.Sequence[str] = pydantic.Field(..., description='Commands to run.\n')
    additional_inputs: typing.Optional[typing.Mapping[str, typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]]] = pydantic.Field(None, description="Additional FileSets to put in other directories. Specifies a mapping from directory name to FileSets. During the script execution, the FileSets will be available in the directories indicated. The directory names may be relative. For example, you can put the main input and an additional input side-by-side with the following configuration:: const script = new pipelines.ShellStep('MainScript', { commands: ['npm ci','npm run build','npx cdk synth'], input: pipelines.CodePipelineSource.gitHub('org/source1', 'main'), additionalInputs: { '../siblingdir': pipelines.CodePipelineSource.gitHub('org/source2', 'main'), } }); Default: - No additional inputs\n")
    env: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set. Default: - No environment variables\n')
    env_from_cfn_outputs: typing.Optional[typing.Mapping[str, models.CfnOutputDef]] = pydantic.Field(None, description='Set environment variables based on Stack Outputs. ``ShellStep``s following stack or stage deployments may access the ``CfnOutput``s of those stacks to get access to --for example--automatically generated resource names or endpoint URLs. Default: - No environment variables created from stack outputs\n')
    input: typing.Optional[typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]] = pydantic.Field(None, description='FileSet to run these scripts on. The files in the FileSet will be placed in the working directory when the script is executed. Use ``additionalInputs`` to download file sets to other directories as well. Default: - No input specified\n')
    install_commands: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Installation commands to run before the regular commands. For deployment engines that support it, install commands will be classified differently in the job history from the regular ``commands``. Default: - No installation commands\n')
    primary_output_directory: typing.Optional[str] = pydantic.Field(None, description='The directory that will contain the primary output fileset. After running the script, the contents of the given directory will be treated as the primary output of this Step. Default: - No primary output\n')
    action_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Custom execution role to be used for the Code Build Action. Default: - A role is automatically created\n')
    build_environment: typing.Union[models.aws_codebuild.BuildEnvironmentDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Changes to environment. This environment will be combined with the pipeline's default environment. Default: - Use the pipeline's default build environment\n")
    cache: typing.Optional[models.aws_codebuild.CacheDef] = pydantic.Field(None, description='Caching strategy to use. Default: - No cache\n')
    file_system_locations: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='ProjectFileSystemLocation objects for CodeBuild build projects. A ProjectFileSystemLocation object specifies the identifier, location, mountOptions, mountPoint, and type of a file system created using Amazon Elastic File System. Default: - no file system locations\n')
    logging: typing.Union[models.aws_codebuild.LoggingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about logs for CodeBuild projects. A CodeBuild project can create logs in Amazon CloudWatch Logs, an S3 bucket, or both. Default: - no log configuration is set\n')
    partial_build_spec: typing.Optional[models.aws_codebuild.BuildSpecDef] = pydantic.Field(None, description="Additional configuration that can only be configured via BuildSpec. You should not use this to specify output artifacts; those should be supplied via the other properties of this class, otherwise CDK Pipelines won't be able to inspect the artifacts. Set the ``commands`` to an empty array if you want to fully specify the BuildSpec using this field. The BuildSpec must be available inline--it cannot reference a file on disk. Default: - BuildSpec completely derived from other properties\n")
    project_name: typing.Optional[str] = pydantic.Field(None, description='Name for the generated CodeBuild project. Default: - Automatically generated\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Custom execution role to be used for the CodeBuild project. Default: - A role is automatically created\n')
    role_policy_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Policy statements to add to role used during the synth. Can be used to add acces to a CodeArtifact repository etc. Default: - No policy statements added to CodeBuild Project Role\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="Which security group to associate with the script's project network interfaces. If no security group is identified, one will be created automatically. Only used if 'vpc' is supplied. Default: - Security group will be automatically created.\n")
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Which subnets to use. Only used if 'vpc' is supplied. Default: - All private subnets.\n")
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of minutes after which AWS CodeBuild stops the build if it's not complete. For valid values, see the timeoutInMinutes field in the AWS CodeBuild User Guide. Default: Duration.hours(1)\n")
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where to execute the SimpleSynth. Default: - No VPC\n\n:exampleMetadata: infused\n\nExample::\n\n    pipeline = pipelines.CodePipeline(self, "Pipeline",\n        synth=pipelines.ShellStep("Synth",\n            input=pipelines.CodePipelineSource.connection("my-org/my-app", "main",\n                connection_arn="arn:aws:codestar-connections:us-east-1:222222222222:connection/7d2469ff-514a-4e4f-9003-5ca4a43cdc41"\n            ),\n            commands=["npm ci", "npm run build", "npx cdk synth"]\n        ),\n\n        # Turn this on because the pipeline uses Docker image assets\n        docker_enabled_for_self_mutation=True\n    )\n\n    pipeline.add_wave("MyWave",\n        post=[\n            pipelines.CodeBuildStep("RunApproval",\n                commands=["command-from-image"],\n                build_environment=codebuild.BuildEnvironment(\n                    # The user of a Docker image asset in the pipeline requires turning on\n                    # \'dockerEnabledForSelfMutation\'.\n                    build_image=codebuild.LinuxBuildImage.from_asset(self, "Image",\n                        directory="./docker-image"\n                    )\n                )\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['commands', 'additional_inputs', 'env', 'env_from_cfn_outputs', 'input', 'install_commands', 'primary_output_directory', 'action_role', 'build_environment', 'cache', 'file_system_locations', 'logging', 'partial_build_spec', 'project_name', 'role', 'role_policy_statements', 'security_groups', 'subnet_selection', 'timeout', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodeBuildStepProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.CodeCommitSourceOptions
class CodeCommitSourceOptionsDef(BaseStruct):
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository name\n')
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='If this is set, the next CodeBuild job clones the repository (instead of CodePipeline downloading the files). This provides access to repository history, and retains symlinks (symlinks would otherwise be removed by CodePipeline). **Note**: if this option is true, only CodeBuild jobs can use the output artifact. Default: false\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role to be used by on commit event rule. Used only when trigger value is CodeCommitTrigger.EVENTS. Default: a new role will be created.\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.CodeCommitTrigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Default: CodeCommitTrigger.EVENTS\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import pipelines\n\n    # role: iam.Role\n\n    code_commit_source_options = pipelines.CodeCommitSourceOptions(\n        action_name="actionName",\n        code_build_clone_output=False,\n        event_role=role,\n        trigger=codepipeline_actions.CodeCommitTrigger.NONE\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'code_build_clone_output', 'event_role', 'trigger']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodeCommitSourceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.CodePipelineActionFactoryResult
class CodePipelineActionFactoryResultDef(BaseStruct):
    run_orders_consumed: typing.Union[int, float] = pydantic.Field(..., description='How many RunOrders were consumed. If you add 1 action, return the value 1 here.\n')
    project: typing.Optional[typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef]] = pydantic.Field(None, description='If a CodeBuild project got created, the project. Default: - This factory did not create a CodeBuild project\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codebuild as codebuild\n    from aws_cdk import pipelines\n\n    # project: codebuild.Project\n\n    code_pipeline_action_factory_result = pipelines.CodePipelineActionFactoryResult(\n        run_orders_consumed=123,\n\n        # the properties below are optional\n        project=project\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['run_orders_consumed', 'project']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodePipelineActionFactoryResult'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.CodePipelineProps
class CodePipelinePropsDef(BaseStruct):
    synth: typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef] = pydantic.Field(..., description="The build step that produces the CDK Cloud Assembly. The primary output of this step needs to be the ``cdk.out`` directory generated by the ``cdk synth`` command. If you use a ``ShellStep`` here and you don't configure an output directory, the output directory will automatically be assumed to be ``cdk.out``.\n")
    artifact_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description="An existing S3 Bucket to use for storing the pipeline's artifact. Default: - A new S3 bucket will be created.\n")
    asset_publishing_code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional customizations to apply to the asset publishing CodeBuild projects. Default: - Only ``codeBuildDefaults`` are applied\n')
    cli_version: typing.Optional[str] = pydantic.Field(None, description='CDK CLI version to use in self-mutation and asset publishing steps. If you want to lock the CDK CLI version used in the pipeline, by steps that are automatically generated for you, specify the version here. We recommend you do not specify this value, as not specifying it always uses the latest CLI version which is backwards compatible with old versions. If you do specify it, be aware that this version should always be equal to or higher than the version of the CDK framework used by the CDK app, when the CDK commands are run during your pipeline execution. When you change this version, the *next time* the ``SelfMutate`` step runs it will still be using the CLI of the the *previous* version that was in this property: it will only start using the new version after ``SelfMutate`` completes successfully. That means that if you want to update both framework and CLI version, you should update the CLI version first, commit, push and deploy, and only then update the framework version. Default: - Latest version\n')
    code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Customize the CodeBuild projects created for this pipeline. Default: - All projects run non-privileged build, SMALL instance, LinuxBuildImage.STANDARD_6_0\n')
    code_pipeline: typing.Optional[models.aws_codepipeline.PipelineDef] = pydantic.Field(None, description='An existing Pipeline to be reused and built upon. [disable-awslint:ref-via-interface] Default: - a new underlying pipeline is created.\n')
    cross_account_keys: typing.Optional[bool] = pydantic.Field(None, description='Create KMS keys for the artifact buckets, allowing cross-account deployments. The artifact buckets have to be encrypted to support deploying CDK apps to another account, so if you want to do that or want to have your artifact buckets encrypted, be sure to set this value to ``true``. Be aware there is a cost associated with maintaining the KMS keys. Default: false\n')
    docker_credentials: typing.Optional[typing.Sequence[models.pipelines.DockerCredentialDef]] = pydantic.Field(None, description='A list of credentials used to authenticate to Docker registries. Specify any credentials necessary within the pipeline to build, synth, update, or publish assets. Default: []\n')
    docker_enabled_for_self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Enable Docker for the self-mutate step. Set this to true if the pipeline itself uses Docker container assets (for example, if you use ``LinuxBuildImage.fromAsset()`` as the build image of a CodeBuild step in the pipeline). You do not need to set it if you build Docker image assets in the application Stages and Stacks that are *deployed* by this pipeline. Configures privileged mode for the self-mutation CodeBuild action. If you are about to turn this on in an already-deployed Pipeline, set the value to ``true`` first, commit and allow the pipeline to self-update, and only then use the Docker asset in the pipeline. Default: false\n')
    docker_enabled_for_synth: typing.Optional[bool] = pydantic.Field(None, description='Enable Docker for the \'synth\' step. Set this to true if you are using file assets that require "bundling" anywhere in your application (meaning an asset compilation step will be run with the tools provided by a Docker image), both for the Pipeline stack as well as the application stacks. A common way to use bundling assets in your application is by using the ``@aws-cdk/aws-lambda-nodejs`` library. Configures privileged mode for the synth CodeBuild action. If you are about to turn this on in an already-deployed Pipeline, set the value to ``true`` first, commit and allow the pipeline to self-update, and only then use the bundled asset. Default: false\n')
    enable_key_rotation: typing.Optional[bool] = pydantic.Field(None, description='Enable KMS key rotation for the generated KMS keys. By default KMS key rotation is disabled, but will add additional costs when enabled. Default: - false (key rotation is disabled)\n')
    pipeline_name: typing.Optional[str] = pydantic.Field(None, description='The name of the CodePipeline pipeline. Default: - Automatically generated\n')
    publish_assets_in_parallel: typing.Optional[bool] = pydantic.Field(None, description='Publish assets in multiple CodeBuild projects. If set to false, use one Project per type to publish all assets. Publishing in parallel improves concurrency and may reduce publishing latency, but may also increase overall provisioning time of the CodeBuild projects. Experiment and see what value works best for you. Default: true\n')
    reuse_cross_region_support_stacks: typing.Optional[bool] = pydantic.Field(None, description='Reuse the same cross region support stack for all pipelines in the App. Default: - true (Use the same support stack for all pipelines in App)\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be assumed by this Pipeline. Default: - A new role is created\n')
    self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Whether the pipeline will update itself. This needs to be set to ``true`` to allow the pipeline to reconfigure itself when assets or stages are being added to it, and ``true`` is the recommended setting. You can temporarily set this to ``false`` while you are iterating on the pipeline itself and prefer to deploy changes using ``cdk deploy``. Default: true\n')
    self_mutation_code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional customizations to apply to the self mutation CodeBuild projects. Default: - Only ``codeBuildDefaults`` are applied\n')
    synth_code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional customizations to apply to the synthesize CodeBuild projects. Default: - Only ``codeBuildDefaults`` are applied\n')
    use_change_sets: typing.Optional[bool] = pydantic.Field(None, description='Deploy every stack by creating a change set and executing it. When enabled, creates a "Prepare" and "Execute" action for each stack. Disable to deploy the stack in one pipeline action. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    # code_pipeline: codepipeline.Pipeline\n\n\n    source_artifact = codepipeline.Artifact("MySourceArtifact")\n\n    pipeline = pipelines.CodePipeline(self, "Pipeline",\n        code_pipeline=code_pipeline,\n        synth=pipelines.ShellStep("Synth",\n            input=pipelines.CodePipelineFileSet.from_artifact(source_artifact),\n            commands=["npm ci", "npm run build", "npx cdk synth"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['synth', 'artifact_bucket', 'asset_publishing_code_build_defaults', 'cli_version', 'code_build_defaults', 'code_pipeline', 'cross_account_keys', 'docker_credentials', 'docker_enabled_for_self_mutation', 'docker_enabled_for_synth', 'enable_key_rotation', 'pipeline_name', 'publish_assets_in_parallel', 'reuse_cross_region_support_stacks', 'role', 'self_mutation', 'self_mutation_code_build_defaults', 'synth_code_build_defaults', 'use_change_sets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.CodePipelineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.ConnectionSourceOptions
class ConnectionSourceOptionsDef(BaseStruct):
    connection_arn: str = pydantic.Field(..., description='The ARN of the CodeStar Connection created in the AWS console that has permissions to access this GitHub or BitBucket repository.\n')
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository string\n')
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='If this is set, the next CodeBuild job clones the repository (instead of CodePipeline downloading the files). This provides access to repository history, and retains symlinks (symlinks would otherwise be removed by CodePipeline). **Note**: if this option is true, only CodeBuild jobs can use the output artifact. Default: false\n')
    trigger_on_push: typing.Optional[bool] = pydantic.Field(None, description='Controls automatically starting your pipeline when a new commit is made on the configured repository and branch. If unspecified, the default value is true, and the field does not display by default. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    pipeline = pipelines.CodePipeline(self, "Pipeline",\n        synth=pipelines.ShellStep("Synth",\n            input=pipelines.CodePipelineSource.connection("my-org/my-app", "main",\n                connection_arn="arn:aws:codestar-connections:us-east-1:222222222222:connection/7d2469ff-514a-4e4f-9003-5ca4a43cdc41"\n            ),\n            commands=["npm ci", "npm run build", "npx cdk synth"]\n        ),\n\n        # Turn this on because the pipeline uses Docker image assets\n        docker_enabled_for_self_mutation=True\n    )\n\n    pipeline.add_wave("MyWave",\n        post=[\n            pipelines.CodeBuildStep("RunApproval",\n                commands=["command-from-image"],\n                build_environment=codebuild.BuildEnvironment(\n                    # The user of a Docker image asset in the pipeline requires turning on\n                    # \'dockerEnabledForSelfMutation\'.\n                    build_image=codebuild.LinuxBuildImage.from_asset(self, "Image",\n                        directory="./docker-image"\n                    )\n                )\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['connection_arn', 'action_name', 'code_build_clone_output', 'trigger_on_push']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ConnectionSourceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.EcrDockerCredentialOptions
class EcrDockerCredentialOptionsDef(BaseStruct):
    assume_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role to assume prior to accessing the secret. Default: - none. The current execution role will be used.\n')
    usages: typing.Optional[typing.Sequence[aws_cdk.pipelines.DockerCredentialUsage]] = pydantic.Field(None, description='Defines which stages of the pipeline should be granted access to these credentials. Default: - all relevant stages (synth, self-update, asset publishing) are granted access.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import pipelines\n\n    # role: iam.Role\n\n    ecr_docker_credential_options = pipelines.EcrDockerCredentialOptions(\n        assume_role=role,\n        usages=[pipelines.DockerCredentialUsage.SYNTH]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['assume_role', 'usages']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.EcrDockerCredentialOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.ECRSourceOptions
class ECRSourceOptionsDef(BaseStruct):
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository name\n')
    image_tag: typing.Optional[str] = pydantic.Field(None, description='The image tag that will be checked for changes. Default: latest\n\n:exampleMetadata: infused\n\nExample::\n\n    # repository: ecr.IRepository\n\n    pipelines.CodePipelineSource.ecr(repository,\n        image_tag="latest"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'image_tag']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ECRSourceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.ExternalDockerCredentialOptions
class ExternalDockerCredentialOptionsDef(BaseStruct):
    assume_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role to assume prior to accessing the secret. Default: - none. The current execution role will be used.\n')
    secret_password_field: typing.Optional[str] = pydantic.Field(None, description="The name of the JSON field of the secret which contains the secret/password. Default: 'secret'\n")
    secret_username_field: typing.Optional[str] = pydantic.Field(None, description="The name of the JSON field of the secret which contains the user/login name. Default: 'username'\n")
    usages: typing.Optional[typing.Sequence[aws_cdk.pipelines.DockerCredentialUsage]] = pydantic.Field(None, description='Defines which stages of the pipeline should be granted access to these credentials. Default: - all relevant stages (synth, self-update, asset publishing) are granted access.\n\n:exampleMetadata: infused\n\nExample::\n\n    docker_hub_secret = secretsmanager.Secret.from_secret_complete_arn(self, "DHSecret", "arn:aws:...")\n    # Only the image asset publishing actions will be granted read access to the secret.\n    creds = pipelines.DockerCredential.docker_hub(docker_hub_secret, usages=[pipelines.DockerCredentialUsage.ASSET_PUBLISHING])\n')
    _init_params: typing.ClassVar[list[str]] = ['assume_role', 'secret_password_field', 'secret_username_field', 'usages']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ExternalDockerCredentialOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.FileSetLocation
class FileSetLocationDef(BaseStruct):
    directory: str = pydantic.Field(..., description='The (relative) directory where the FileSet is found.\n')
    file_set: models.pipelines.FileSetDef = pydantic.Field(..., description='The FileSet object.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import pipelines\n\n    # file_set: pipelines.FileSet\n\n    file_set_location = pipelines.FileSetLocation(\n        directory="directory",\n        file_set=file_set\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['directory', 'file_set']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.FileSetLocation'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.GitHubSourceOptions
class GitHubSourceOptionsDef(BaseStruct):
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The repository string\n')
    authentication: typing.Optional[models.SecretValueDef] = pydantic.Field(None, description="A GitHub OAuth token to use for authentication. It is recommended to use a Secrets Manager ``Secret`` to obtain the token:: const oauth = cdk.SecretValue.secretsManager('my-github-token'); The GitHub Personal Access Token should have these scopes: - **repo** - to read the repository - **admin:repo_hook** - if you plan to use webhooks (true by default) Default: - SecretValue.secretsManager('github-token')\n")
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.GitHubTrigger] = pydantic.Field(None, description='How AWS CodePipeline should be triggered. With the default value "WEBHOOK", a webhook is created in GitHub that triggers the action. With "POLL", CodePipeline periodically checks the source for changes. With "None", the action is not triggered through changes in the source. To use ``WEBHOOK``, your GitHub Personal Access Token should have **admin:repo_hook** scope (in addition to the regular **repo** scope). Default: GitHubTrigger.WEBHOOK\n\n:exampleMetadata: infused\n\nExample::\n\n    pipelines.CodePipelineSource.git_hub("org/repo", "branch",\n        # This is optional\n        authentication=cdk.SecretValue.secrets_manager("my-token")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'authentication', 'trigger']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.GitHubSourceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.ManualApprovalStepProps
class ManualApprovalStepPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='The comment to display with this manual approval. Default: - No comment\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import pipelines\n\n    manual_approval_step_props = pipelines.ManualApprovalStepProps(\n        comment="comment"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ManualApprovalStepProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.PermissionsBroadeningCheckProps
class PermissionsBroadeningCheckPropsDef(BaseStruct):
    stage: models.StageDef = pydantic.Field(..., description='The CDK Stage object to check the stacks of. This should be the same Stage object you are passing to ``addStage()``.\n')
    notification_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='Topic to send notifications when a human needs to give manual confirmation. Default: - no notification\n\n:exampleMetadata: infused\n\nExample::\n\n    # pipeline: pipelines.CodePipeline\n\n    stage = MyApplicationStage(self, "MyApplication")\n    pipeline.add_stage(stage,\n        pre=[\n            pipelines.ConfirmPermissionsBroadening("Check", stage=stage)\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['stage', 'notification_topic']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.PermissionsBroadeningCheckProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[PermissionsBroadeningCheckPropsDefConfig] = pydantic.Field(None)


class PermissionsBroadeningCheckPropsDefConfig(pydantic.BaseModel):
    stage_config: typing.Optional[models.core.StageDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.pipelines.PipelineBaseProps
class PipelineBasePropsDef(BaseStruct):
    synth: typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef] = pydantic.Field(..., description="The build step that produces the CDK Cloud Assembly. The primary output of this step needs to be the ``cdk.out`` directory generated by the ``cdk synth`` command. If you use a ``ShellStep`` here and you don't configure an output directory, the output directory will automatically be assumed to be ``cdk.out``.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import pipelines\n\n    # file_set_producer: pipelines.IFileSetProducer\n\n    pipeline_base_props = pipelines.PipelineBaseProps(\n        synth=file_set_producer\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['synth']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.PipelineBaseProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.ProduceActionOptions
class ProduceActionOptionsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='Name the action should get.\n')
    artifacts: models.pipelines.ArtifactMapDef = pydantic.Field(..., description='Helper object to translate FileSets to CodePipeline Artifacts.\n')
    pipeline: models.pipelines.CodePipelineDef = pydantic.Field(..., description='The pipeline the action is being generated for.\n')
    run_order: typing.Union[int, float] = pydantic.Field(..., description='RunOrder the action should get.\n')
    stack_outputs_map: models.pipelines.StackOutputsMapDef = pydantic.Field(..., description='Helper object to produce variables exported from stack deployments. If your step references outputs from a stack deployment, use this to map the output references to Codepipeline variable names. Note - Codepipeline variables can only be referenced in action configurations.\n')
    before_self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Whether or not this action is inserted before self mutation. If it is, the action should take care to reflect some part of its own definition in the pipeline action definition, to trigger a restart after self-mutation (if necessary). Default: false\n')
    code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If this action factory creates a CodeBuild step, default options to inherit. Default: - No CodeBuild project defaults\n')
    fallback_artifact: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="An input artifact that CodeBuild projects that don't actually need an input artifact can use. CodeBuild Projects MUST have an input artifact in order to be added to the Pipeline. If the Project doesn't actually care about its input (it can be anything), it can use the Artifact passed here. Default: - A fallback artifact does not exist\n")
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description='If this step is producing outputs, the variables namespace assigned to it. Pass this on to the Action you are creating. Default: - Step doesn\'t produce any outputs\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_codebuild as codebuild\n    from aws_cdk import aws_codepipeline as codepipeline\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_logs as logs\n    from aws_cdk import aws_s3 as s3\n    from aws_cdk import pipelines\n    import constructs as constructs\n\n    # artifact: codepipeline.Artifact\n    # artifact_map: pipelines.ArtifactMap\n    # bucket: s3.Bucket\n    # build_image: codebuild.IBuildImage\n    # build_spec: codebuild.BuildSpec\n    # cache: codebuild.Cache\n    # code_pipeline: pipelines.CodePipeline\n    # construct: constructs.Construct\n    # file_system_location: codebuild.IFileSystemLocation\n    # log_group: logs.LogGroup\n    # policy_statement: iam.PolicyStatement\n    # security_group: ec2.SecurityGroup\n    # stack_outputs_map: pipelines.StackOutputsMap\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # value: Any\n    # vpc: ec2.Vpc\n\n    produce_action_options = pipelines.ProduceActionOptions(\n        action_name="actionName",\n        artifacts=artifact_map,\n        pipeline=code_pipeline,\n        run_order=123,\n        scope=construct,\n        stack_outputs_map=stack_outputs_map,\n\n        # the properties below are optional\n        before_self_mutation=False,\n        code_build_defaults=pipelines.CodeBuildOptions(\n            build_environment=codebuild.BuildEnvironment(\n                build_image=build_image,\n                certificate=codebuild.BuildEnvironmentCertificate(\n                    bucket=bucket,\n                    object_key="objectKey"\n                ),\n                compute_type=codebuild.ComputeType.SMALL,\n                environment_variables={\n                    "environment_variables_key": codebuild.BuildEnvironmentVariable(\n                        value=value,\n\n                        # the properties below are optional\n                        type=codebuild.BuildEnvironmentVariableType.PLAINTEXT\n                    )\n                },\n                privileged=False\n            ),\n            cache=cache,\n            file_system_locations=[file_system_location],\n            logging=codebuild.LoggingOptions(\n                cloud_watch=codebuild.CloudWatchLoggingOptions(\n                    enabled=False,\n                    log_group=log_group,\n                    prefix="prefix"\n                ),\n                s3=codebuild.S3LoggingOptions(\n                    bucket=bucket,\n\n                    # the properties below are optional\n                    enabled=False,\n                    encrypted=False,\n                    prefix="prefix"\n                )\n            ),\n            partial_build_spec=build_spec,\n            role_policy=[policy_statement],\n            security_groups=[security_group],\n            subnet_selection=ec2.SubnetSelection(\n                availability_zones=["availabilityZones"],\n                one_per_az=False,\n                subnet_filters=[subnet_filter],\n                subnet_group_name="subnetGroupName",\n                subnets=[subnet],\n                subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n            ),\n            timeout=cdk.Duration.minutes(30),\n            vpc=vpc\n        ),\n        fallback_artifact=artifact,\n        variables_namespace="variablesNamespace"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'artifacts', 'pipeline', 'run_order', 'stack_outputs_map', 'before_self_mutation', 'code_build_defaults', 'fallback_artifact', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ProduceActionOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.S3SourceOptions
class S3SourceOptionsDef(BaseStruct):
    action_name: typing.Optional[str] = pydantic.Field(None, description='The action name used for this source in the CodePipeline. Default: - The bucket name\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that will be assumed by the pipeline prior to executing the ``S3Source`` action. Default: - a new role will be generated\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.S3Trigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Note that if this is S3Trigger.EVENTS, you need to make sure to include the source Bucket in a CloudTrail Trail, as otherwise the CloudWatch Events will not be emitted. Default: S3Trigger.POLL\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import pipelines\n\n    # role: iam.Role\n\n    s3_source_options = pipelines.S3SourceOptions(\n        action_name="actionName",\n        role=role,\n        trigger=codepipeline_actions.S3Trigger.NONE\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'role', 'trigger']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.S3SourceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.ShellStepProps
class ShellStepPropsDef(BaseStruct):
    commands: typing.Sequence[str] = pydantic.Field(..., description='Commands to run.\n')
    additional_inputs: typing.Optional[typing.Mapping[str, typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]]] = pydantic.Field(None, description="Additional FileSets to put in other directories. Specifies a mapping from directory name to FileSets. During the script execution, the FileSets will be available in the directories indicated. The directory names may be relative. For example, you can put the main input and an additional input side-by-side with the following configuration:: const script = new pipelines.ShellStep('MainScript', { commands: ['npm ci','npm run build','npx cdk synth'], input: pipelines.CodePipelineSource.gitHub('org/source1', 'main'), additionalInputs: { '../siblingdir': pipelines.CodePipelineSource.gitHub('org/source2', 'main'), } }); Default: - No additional inputs\n")
    env: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set. Default: - No environment variables\n')
    env_from_cfn_outputs: typing.Optional[typing.Mapping[str, models.CfnOutputDef]] = pydantic.Field(None, description='Set environment variables based on Stack Outputs. ``ShellStep``s following stack or stage deployments may access the ``CfnOutput``s of those stacks to get access to --for example--automatically generated resource names or endpoint URLs. Default: - No environment variables created from stack outputs\n')
    input: typing.Optional[typing.Union[models.pipelines.CodeBuildStepDef, models.pipelines.CodePipelineFileSetDef, models.pipelines.CodePipelineSourceDef, models.pipelines.ConfirmPermissionsBroadeningDef, models.pipelines.FileSetDef, models.pipelines.ManualApprovalStepDef, models.pipelines.ShellStepDef, models.pipelines.StepDef]] = pydantic.Field(None, description='FileSet to run these scripts on. The files in the FileSet will be placed in the working directory when the script is executed. Use ``additionalInputs`` to download file sets to other directories as well. Default: - No input specified\n')
    install_commands: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Installation commands to run before the regular commands. For deployment engines that support it, install commands will be classified differently in the job history from the regular ``commands``. Default: - No installation commands\n')
    primary_output_directory: typing.Optional[str] = pydantic.Field(None, description='The directory that will contain the primary output fileset. After running the script, the contents of the given directory will be treated as the primary output of this Step. Default: - No primary output\n\n:exampleMetadata: infused\n\nExample::\n\n    # code_pipeline: codepipeline.Pipeline\n\n\n    source_artifact = codepipeline.Artifact("MySourceArtifact")\n\n    pipeline = pipelines.CodePipeline(self, "Pipeline",\n        code_pipeline=code_pipeline,\n        synth=pipelines.ShellStep("Synth",\n            input=pipelines.CodePipelineFileSet.from_artifact(source_artifact),\n            commands=["npm ci", "npm run build", "npx cdk synth"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['commands', 'additional_inputs', 'env', 'env_from_cfn_outputs', 'input', 'install_commands', 'primary_output_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.ShellStepProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.StackAsset
class StackAssetDef(BaseStruct):
    asset_id: str = pydantic.Field(..., description='Asset identifier.\n')
    asset_manifest_path: str = pydantic.Field(..., description="Absolute asset manifest path. This needs to be made relative at a later point in time, but when this information is parsed we don't know about the root cloud assembly yet.\n")
    asset_selector: str = pydantic.Field(..., description='Asset selector to pass to ``cdk-assets``.\n')
    asset_type: aws_cdk.pipelines.AssetType = pydantic.Field(..., description='Type of asset to publish.\n')
    is_template: bool = pydantic.Field(..., description='Does this asset represent the CloudFormation template for the stack. Default: false\n')
    asset_publishing_role_arn: typing.Optional[str] = pydantic.Field(None, description='Role ARN to assume to publish. Default: - No need to assume any role\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import pipelines\n\n    stack_asset = pipelines.StackAsset(\n        asset_id="assetId",\n        asset_manifest_path="assetManifestPath",\n        asset_selector="assetSelector",\n        asset_type=pipelines.AssetType.FILE,\n        is_template=False,\n\n        # the properties below are optional\n        asset_publishing_role_arn="assetPublishingRoleArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['asset_id', 'asset_manifest_path', 'asset_selector', 'asset_type', 'is_template', 'asset_publishing_role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StackAsset'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.StackDeploymentProps
class StackDeploymentPropsDef(BaseStruct):
    absolute_template_path: str = pydantic.Field(..., description='Template path on disk to cloud assembly (cdk.out).\n')
    construct_path: str = pydantic.Field(..., description='Construct path for this stack.\n')
    stack_artifact_id: str = pydantic.Field(..., description='Artifact ID for this stack.\n')
    stack_name: str = pydantic.Field(..., description='Name for this stack.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account where the stack should be deployed. Default: - Pipeline account\n')
    assets: typing.Optional[typing.Sequence[typing.Union[models.pipelines.StackAssetDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Assets referenced by this stack. Default: - No assets\n')
    assume_role_arn: typing.Optional[str] = pydantic.Field(None, description="Role to assume before deploying this stack. Default: - Don't assume any role\n")
    execution_role_arn: typing.Optional[str] = pydantic.Field(None, description='Execution role to pass to CloudFormation. Default: - No execution role\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region where the stack should be deployed. Default: - Pipeline region\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Tags to apply to the stack. Default: - No tags\n')
    template_s3_uri: typing.Optional[str] = pydantic.Field(None, description='The S3 URL which points to the template asset location in the publishing bucket. Default: - Stack template is not published\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import pipelines\n\n    stack_deployment_props = pipelines.StackDeploymentProps(\n        absolute_template_path="absoluteTemplatePath",\n        construct_path="constructPath",\n        stack_artifact_id="stackArtifactId",\n        stack_name="stackName",\n\n        # the properties below are optional\n        account="account",\n        assets=[pipelines.StackAsset(\n            asset_id="assetId",\n            asset_manifest_path="assetManifestPath",\n            asset_selector="assetSelector",\n            asset_type=pipelines.AssetType.FILE,\n            is_template=False,\n\n            # the properties below are optional\n            asset_publishing_role_arn="assetPublishingRoleArn"\n        )],\n        assume_role_arn="assumeRoleArn",\n        execution_role_arn="executionRoleArn",\n        region="region",\n        tags={\n            "tags_key": "tags"\n        },\n        template_s3_uri="templateS3Uri"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['absolute_template_path', 'construct_path', 'stack_artifact_id', 'stack_name', 'account', 'assets', 'assume_role_arn', 'execution_role_arn', 'region', 'tags', 'template_s3_uri']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StackDeploymentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.StackSteps
class StackStepsDef(BaseStruct):
    stack: models.StackDef = pydantic.Field(..., description='The stack you want the steps to run in.\n')
    change_set: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Steps that execute after stack is prepared but before stack is deployed. Default: - no additional steps\n')
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Steps that execute after stack is deployed. Default: - no additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Steps that execute before stack is prepared. Default: - no additional steps\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import pipelines\n\n    # stack: cdk.Stack\n    # step: pipelines.Step\n\n    stack_steps = pipelines.StackSteps(\n        stack=stack,\n\n        # the properties below are optional\n        change_set=[step],\n        post=[step],\n        pre=[step]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['stack', 'change_set', 'post', 'pre']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StackSteps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StackStepsDefConfig] = pydantic.Field(None)


class StackStepsDefConfig(pydantic.BaseModel):
    stack_config: typing.Optional[models.core.StackDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.pipelines.StageDeploymentProps
class StageDeploymentPropsDef(BaseStruct):
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stacks in the stage. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stacks in the stage. Default: - No additional steps\n')
    stack_steps: typing.Optional[typing.Sequence[typing.Union[models.pipelines.StackStepsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Instructions for additional steps that are run at the stack level. Default: - No additional instructions\n')
    stage_name: typing.Optional[str] = pydantic.Field(None, description='Stage name to use in the pipeline. Default: - Use Stage\'s construct ID\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import pipelines\n\n    # stack: cdk.Stack\n    # step: pipelines.Step\n\n    stage_deployment_props = pipelines.StageDeploymentProps(\n        post=[step],\n        pre=[step],\n        stack_steps=[pipelines.StackSteps(\n            stack=stack,\n\n            # the properties below are optional\n            change_set=[step],\n            post=[step],\n            pre=[step]\n        )],\n        stage_name="stageName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['post', 'pre', 'stack_steps', 'stage_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.StageDeploymentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.WaveOptions
class WaveOptionsDef(BaseStruct):
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stages in the wave. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stages in the wave. Default: - No additional steps\n\n:exampleMetadata: infused\n\nExample::\n\n    pipeline = pipelines.CodePipeline(self, "Pipeline",\n        synth=pipelines.ShellStep("Synth",\n            input=pipelines.CodePipelineSource.connection("my-org/my-app", "main",\n                connection_arn="arn:aws:codestar-connections:us-east-1:222222222222:connection/7d2469ff-514a-4e4f-9003-5ca4a43cdc41"\n            ),\n            commands=["npm ci", "npm run build", "npx cdk synth"]\n        ),\n\n        # Turn this on because the pipeline uses Docker image assets\n        docker_enabled_for_self_mutation=True\n    )\n\n    pipeline.add_wave("MyWave",\n        post=[\n            pipelines.CodeBuildStep("RunApproval",\n                commands=["command-from-image"],\n                build_environment=codebuild.BuildEnvironment(\n                    # The user of a Docker image asset in the pipeline requires turning on\n                    # \'dockerEnabledForSelfMutation\'.\n                    build_image=codebuild.LinuxBuildImage.from_asset(self, "Image",\n                        directory="./docker-image"\n                    )\n                )\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['post', 'pre']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.WaveOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.WaveProps
class WavePropsDef(BaseStruct):
    post: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run after all of the stages in the wave. Default: - No additional steps\n')
    pre: typing.Optional[typing.Sequence[models.pipelines.StepDef]] = pydantic.Field(None, description='Additional steps to run before any of the stages in the wave. Default: - No additional steps\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import pipelines\n\n    # step: pipelines.Step\n\n    wave_props = pipelines.WaveProps(\n        post=[step],\n        pre=[step]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['post', 'pre']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.pipelines.WaveProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.pipelines.AssetType
# skipping emum

#  autogenerated from aws_cdk.pipelines.DockerCredentialUsage
# skipping emum

#  autogenerated from aws_cdk.pipelines.ICodePipelineActionFactory
#  skipping Interface

#  autogenerated from aws_cdk.pipelines.IFileSetProducer
#  skipping Interface

import models

class ModuleModel(pydantic.BaseModel):
    ArtifactMap: typing.Optional[dict[str, ArtifactMapDef]] = pydantic.Field(None)
    CodeBuildStep: typing.Optional[dict[str, CodeBuildStepDef]] = pydantic.Field(None)
    CodePipelineFileSet: typing.Optional[dict[str, CodePipelineFileSetDef]] = pydantic.Field(None)
    CodePipelineSource: typing.Optional[dict[str, CodePipelineSourceDef]] = pydantic.Field(None)
    ConfirmPermissionsBroadening: typing.Optional[dict[str, ConfirmPermissionsBroadeningDef]] = pydantic.Field(None)
    DockerCredential: typing.Optional[dict[str, DockerCredentialDef]] = pydantic.Field(None)
    FileSet: typing.Optional[dict[str, FileSetDef]] = pydantic.Field(None)
    ManualApprovalStep: typing.Optional[dict[str, ManualApprovalStepDef]] = pydantic.Field(None)
    PipelineBase: typing.Optional[dict[str, PipelineBaseDef]] = pydantic.Field(None)
    ShellStep: typing.Optional[dict[str, ShellStepDef]] = pydantic.Field(None)
    StackDeployment: typing.Optional[dict[str, StackDeploymentDef]] = pydantic.Field(None)
    StackOutputReference: typing.Optional[dict[str, StackOutputReferenceDef]] = pydantic.Field(None)
    StackOutputsMap: typing.Optional[dict[str, StackOutputsMapDef]] = pydantic.Field(None)
    StageDeployment: typing.Optional[dict[str, StageDeploymentDef]] = pydantic.Field(None)
    Step: typing.Optional[dict[str, StepDef]] = pydantic.Field(None)
    Wave: typing.Optional[dict[str, WaveDef]] = pydantic.Field(None)
    CodePipeline: typing.Optional[dict[str, CodePipelineDef]] = pydantic.Field(None)
    AddStageOpts: typing.Optional[dict[str, AddStageOptsDef]] = pydantic.Field(None)
    CodeBuildOptions: typing.Optional[dict[str, CodeBuildOptionsDef]] = pydantic.Field(None)
    CodeBuildStepProps: typing.Optional[dict[str, CodeBuildStepPropsDef]] = pydantic.Field(None)
    CodeCommitSourceOptions: typing.Optional[dict[str, CodeCommitSourceOptionsDef]] = pydantic.Field(None)
    CodePipelineActionFactoryResult: typing.Optional[dict[str, CodePipelineActionFactoryResultDef]] = pydantic.Field(None)
    CodePipelineProps: typing.Optional[dict[str, CodePipelinePropsDef]] = pydantic.Field(None)
    ConnectionSourceOptions: typing.Optional[dict[str, ConnectionSourceOptionsDef]] = pydantic.Field(None)
    EcrDockerCredentialOptions: typing.Optional[dict[str, EcrDockerCredentialOptionsDef]] = pydantic.Field(None)
    ECRSourceOptions: typing.Optional[dict[str, ECRSourceOptionsDef]] = pydantic.Field(None)
    ExternalDockerCredentialOptions: typing.Optional[dict[str, ExternalDockerCredentialOptionsDef]] = pydantic.Field(None)
    FileSetLocation: typing.Optional[dict[str, FileSetLocationDef]] = pydantic.Field(None)
    GitHubSourceOptions: typing.Optional[dict[str, GitHubSourceOptionsDef]] = pydantic.Field(None)
    ManualApprovalStepProps: typing.Optional[dict[str, ManualApprovalStepPropsDef]] = pydantic.Field(None)
    PermissionsBroadeningCheckProps: typing.Optional[dict[str, PermissionsBroadeningCheckPropsDef]] = pydantic.Field(None)
    PipelineBaseProps: typing.Optional[dict[str, PipelineBasePropsDef]] = pydantic.Field(None)
    ProduceActionOptions: typing.Optional[dict[str, ProduceActionOptionsDef]] = pydantic.Field(None)
    S3SourceOptions: typing.Optional[dict[str, S3SourceOptionsDef]] = pydantic.Field(None)
    ShellStepProps: typing.Optional[dict[str, ShellStepPropsDef]] = pydantic.Field(None)
    StackAsset: typing.Optional[dict[str, StackAssetDef]] = pydantic.Field(None)
    StackDeploymentProps: typing.Optional[dict[str, StackDeploymentPropsDef]] = pydantic.Field(None)
    StackSteps: typing.Optional[dict[str, StackStepsDef]] = pydantic.Field(None)
    StageDeploymentProps: typing.Optional[dict[str, StageDeploymentPropsDef]] = pydantic.Field(None)
    WaveOptions: typing.Optional[dict[str, WaveOptionsDef]] = pydantic.Field(None)
    WaveProps: typing.Optional[dict[str, WavePropsDef]] = pydantic.Field(None)
    ...
