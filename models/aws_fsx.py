from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_fsx.FileSystemBase
class FileSystemBaseDef(BaseClass):
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID this resource belongs to. Default: - the resource is in the same account as the stack it belongs to\n')
    environment_from_arn: typing.Optional[str] = pydantic.Field(None, description='ARN to deduce region and account from. The ARN is parsed and the account and region are taken from the ARN. This should be used for imported resources. Cannot be supplied together with either ``account`` or ``region``. Default: - take environment from ``account``, ``region`` parameters, or use Stack environment.\n')
    physical_name: typing.Optional[str] = pydantic.Field(None, description='The value passed in by users to the physical name prop of the resource. - ``undefined`` implies that a physical name will be allocated by CloudFormation during deployment. - a concrete value implies a specific physical name - ``PhysicalName.GENERATE_IF_NEEDED`` is a marker that indicates that a physical will only be generated by the CDK if it is needed for cross-environment references. Otherwise, it will be allocated by CloudFormation. Default: - The physical name will be allocated by CloudFormation at deployment time\n')
    region: typing.Optional[str] = pydantic.Field(None, description='The AWS region this resource belongs to. Default: - the resource is in the same region as the stack it belongs to')
    _init_params: typing.ClassVar[list[str]] = ['account', 'environment_from_arn', 'physical_name', 'region']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.FileSystemBase'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.FileSystemBaseDefConfig] = pydantic.Field(None)


class FileSystemBaseDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class FileSystemBaseDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_fsx.LustreMaintenanceTime
class LustreMaintenanceTimeDef(BaseClass):
    day: typing.Union[aws_cdk.aws_fsx.Weekday, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The day of the week for maintenance to be performed.')
    hour: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The hour of the day (from 0-23) for maintenance to be performed.\n')
    minute: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The minute of the hour (from 0-59) for maintenance to be performed.')
    _init_params: typing.ClassVar[list[str]] = ['day', 'hour', 'minute']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.LustreMaintenanceTime'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.LustreFileSystem
class LustreFileSystemDef(BaseConstruct):
    lustre_configuration: typing.Union[_REQUIRED_INIT_PARAM, models.aws_fsx.LustreConfigurationDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Additional configuration for FSx specific to Lustre.\n')
    vpc_subnet: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.PrivateSubnetDef, models.aws_ec2.PublicSubnetDef, models.aws_ec2.SubnetDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The subnet that the file system will be accessible from.\n')
    storage_capacity_gib: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The storage capacity of the file system being created. For Windows file systems, valid values are 32 GiB to 65,536 GiB. For SCRATCH_1 deployment types, valid values are 1,200, 2,400, 3,600, then continuing in increments of 3,600 GiB. For SCRATCH_2 and PERSISTENT_1 types, valid values are 1,200, 2,400, then continuing in increments of 2,400 GiB.\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The VPC to launch the file system in.\n')
    backup_id: typing.Optional[str] = pydantic.Field(None, description="The ID of the backup. Specifies the backup to use if you're creating a file system from an existing backup. Default: - no backup will be used.\n")
    kms_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key used for encryption to protect your data at rest. Default: - the aws/fsx default KMS key for the AWS account being deployed into.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Policy to apply when the file system is removed from the stack. Default: RemovalPolicy.RETAIN\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='Security Group to assign to this file system. Default: - creates new security group which allows all outbound traffic.')
    _init_params: typing.ClassVar[list[str]] = ['lustre_configuration', 'vpc_subnet', 'storage_capacity_gib', 'vpc', 'backup_id', 'kms_key', 'removal_policy', 'security_group']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_lustre_file_system_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.LustreFileSystem'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_lustre_file_system_attributes']
    ...


    from_lustre_file_system_attributes: typing.Optional[models.aws_fsx.LustreFileSystemDefFromLustreFileSystemAttributesParams] = pydantic.Field(None, description='Import an existing FSx for Lustre file system from the given properties.')
    resource_config: typing.Optional[models.aws_fsx.LustreFileSystemDefConfig] = pydantic.Field(None)


class LustreFileSystemDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class LustreFileSystemDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class LustreFileSystemDefFromLustreFileSystemAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    dns_name: str = pydantic.Field(..., description='The DNS name assigned to this file system.\n')
    file_system_id: str = pydantic.Field(..., description='The ID of the file system, assigned by Amazon FSx.\n')
    security_group: typing.Union[models.aws_ec2.SecurityGroupDef] = pydantic.Field(..., description='The security group of the file system.')
    ...


#  autogenerated from aws_cdk.aws_fsx.CfnDataRepositoryAssociation.AutoExportPolicyProperty
class CfnDataRepositoryAssociation_AutoExportPolicyPropertyDef(BaseStruct):
    events: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ``AutoExportPolicy`` can have the following event values:. - ``NEW`` - New files and directories are automatically exported to the data repository as they are added to the file system. - ``CHANGED`` - Changes to files and directories on the file system are automatically exported to the data repository. - ``DELETED`` - Files and directories are automatically deleted on the data repository when they are deleted on the file system. You can define any combination of event types for your ``AutoExportPolicy`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-datarepositoryassociation-autoexportpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    auto_export_policy_property = fsx.CfnDataRepositoryAssociation.AutoExportPolicyProperty(\n        events=["events"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['events']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnDataRepositoryAssociation.AutoExportPolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnDataRepositoryAssociation.AutoImportPolicyProperty
class CfnDataRepositoryAssociation_AutoImportPolicyPropertyDef(BaseStruct):
    events: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ``AutoImportPolicy`` can have the following event values:. - ``NEW`` - Amazon FSx automatically imports metadata of files added to the linked S3 bucket that do not currently exist in the FSx file system. - ``CHANGED`` - Amazon FSx automatically updates file metadata and invalidates existing file content on the file system as files change in the data repository. - ``DELETED`` - Amazon FSx automatically deletes files on the file system as corresponding files are deleted in the data repository. You can define any combination of event types for your ``AutoImportPolicy`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-datarepositoryassociation-autoimportpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    auto_import_policy_property = fsx.CfnDataRepositoryAssociation.AutoImportPolicyProperty(\n        events=["events"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['events']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnDataRepositoryAssociation.AutoImportPolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnDataRepositoryAssociation.S3Property
class CfnDataRepositoryAssociation_S3PropertyDef(BaseStruct):
    auto_export_policy: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnDataRepositoryAssociation_AutoExportPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Describes a data repository association's automatic export policy. The ``AutoExportPolicy`` defines the types of updated objects on the file system that will be automatically exported to the data repository. As you create, modify, or delete files, Amazon FSx for Lustre automatically exports the defined changes asynchronously once your application finishes modifying the file. The ``AutoExportPolicy`` is only supported on Amazon FSx for Lustre file systems with a data repository association.\n")
    auto_import_policy: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnDataRepositoryAssociation_AutoImportPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Describes the data repository association\'s automatic import policy. The AutoImportPolicy defines how Amazon FSx keeps your file metadata and directory listings up to date by importing changes to your Amazon FSx for Lustre file system as you modify objects in a linked S3 bucket. The ``AutoImportPolicy`` is only supported on Amazon FSx for Lustre file systems with a data repository association.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-datarepositoryassociation-s3.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    s3_property = fsx.CfnDataRepositoryAssociation.S3Property(\n        auto_export_policy=fsx.CfnDataRepositoryAssociation.AutoExportPolicyProperty(\n            events=["events"]\n        ),\n        auto_import_policy=fsx.CfnDataRepositoryAssociation.AutoImportPolicyProperty(\n            events=["events"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_export_policy', 'auto_import_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnDataRepositoryAssociation.S3Property'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.AuditLogConfigurationProperty
class CfnFileSystem_AuditLogConfigurationPropertyDef(BaseStruct):
    file_access_audit_log_level: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Sets which attempt type is logged by Amazon FSx for file and folder accesses. - ``SUCCESS_ONLY`` - only successful attempts to access files or folders are logged. - ``FAILURE_ONLY`` - only failed attempts to access files or folders are logged. - ``SUCCESS_AND_FAILURE`` - both successful attempts and failed attempts to access files or folders are logged. - ``DISABLED`` - access auditing of files and folders is turned off.\n')
    file_share_access_audit_log_level: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Sets which attempt type is logged by Amazon FSx for file share accesses. - ``SUCCESS_ONLY`` - only successful attempts to access file shares are logged. - ``FAILURE_ONLY`` - only failed attempts to access file shares are logged. - ``SUCCESS_AND_FAILURE`` - both successful attempts and failed attempts to access file shares are logged. - ``DISABLED`` - access auditing of file shares is turned off.\n')
    audit_log_destination: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the destination of the audit logs. The destination can be any Amazon CloudWatch Logs log group ARN or Amazon Kinesis Data Firehose delivery stream ARN. The name of the Amazon CloudWatch Logs log group must begin with the ``/aws/fsx`` prefix. The name of the Amazon Kinesis Data Firehose delivery stream must begin with the ``aws-fsx`` prefix. The destination ARN (either CloudWatch Logs log group or Kinesis Data Firehose delivery stream) must be in the same AWS partition, AWS Region , and AWS account as your Amazon FSx file system.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-auditlogconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    audit_log_configuration_property = fsx.CfnFileSystem.AuditLogConfigurationProperty(\n        file_access_audit_log_level="fileAccessAuditLogLevel",\n        file_share_access_audit_log_level="fileShareAccessAuditLogLevel",\n\n        # the properties below are optional\n        audit_log_destination="auditLogDestination"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_access_audit_log_level', 'file_share_access_audit_log_level', 'audit_log_destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.AuditLogConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.ClientConfigurationsProperty
class CfnFileSystem_ClientConfigurationsPropertyDef(BaseStruct):
    clients: typing.Optional[str] = pydantic.Field(None, description='A value that specifies who can mount the file system. You can provide a wildcard character ( ``*`` ), an IP address ( ``0.0.0.0`` ), or a CIDR address ( ``192.0.2.0/24`` ). By default, Amazon FSx uses the wildcard character when specifying the client.\n')
    options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The options to use when mounting the file system. For a list of options that you can use with Network File System (NFS), see the `exports(5) - Linux man page <https://docs.aws.amazon.com/https://linux.die.net/man/5/exports>`_ . When choosing your options, consider the following: - ``crossmnt`` is used by default. If you don\'t specify ``crossmnt`` when changing the client configuration, you won\'t be able to see or access snapshots in your file system\'s snapshot directory. - ``sync`` is used by default. If you instead specify ``async`` , the system acknowledges writes before writing to disk. If the system crashes before the writes are finished, you lose the unwritten data.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-clientconfigurations.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    client_configurations_property = fsx.CfnFileSystem.ClientConfigurationsProperty(\n        clients="clients",\n        options=["options"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['clients', 'options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.ClientConfigurationsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.DiskIopsConfigurationProperty
class CfnFileSystem_DiskIopsConfigurationPropertyDef(BaseStruct):
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The total number of SSD IOPS provisioned for the file system. The minimum and maximum values for this property depend on the value of ``HAPairs`` and ``StorageCapacity`` . The minimum value is calculated as ``StorageCapacity`` * 3 * ``HAPairs`` (3 IOPS per GB of ``StorageCapacity`` ). The maximum value is calculated as 200,000 * ``HAPairs`` . Amazon FSx responds with an HTTP status code 400 (Bad Request) if the value of ``Iops`` is outside of the minimum or maximum values.\n')
    mode: typing.Optional[str] = pydantic.Field(None, description='Specifies whether the file system is using the ``AUTOMATIC`` setting of SSD IOPS of 3 IOPS per GB of storage capacity, or if it using a ``USER_PROVISIONED`` value.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-diskiopsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    disk_iops_configuration_property = fsx.CfnFileSystem.DiskIopsConfigurationProperty(\n        iops=123,\n        mode="mode"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['iops', 'mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.DiskIopsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.LustreConfigurationProperty
class CfnFileSystem_LustreConfigurationPropertyDef(BaseStruct):
    auto_import_policy: typing.Optional[str] = pydantic.Field(None, description='(Optional) When you create your file system, your existing S3 objects appear as file and directory listings. Use this property to choose how Amazon FSx keeps your file and directory listings up to date as you add or modify objects in your linked S3 bucket. ``AutoImportPolicy`` can have the following values: - ``NONE`` - (Default) AutoImport is off. Amazon FSx only updates file and directory listings from the linked S3 bucket when the file system is created. FSx does not update file and directory listings for any new or changed objects after choosing this option. - ``NEW`` - AutoImport is on. Amazon FSx automatically imports directory listings of any new objects added to the linked S3 bucket that do not currently exist in the FSx file system. - ``NEW_CHANGED`` - AutoImport is on. Amazon FSx automatically imports file and directory listings of any new objects added to the S3 bucket and any existing objects that are changed in the S3 bucket after you choose this option. - ``NEW_CHANGED_DELETED`` - AutoImport is on. Amazon FSx automatically imports file and directory listings of any new objects added to the S3 bucket, any existing objects that are changed in the S3 bucket, and any objects that were deleted in the S3 bucket. For more information, see `Automatically import updates from your S3 bucket <https://docs.aws.amazon.com/fsx/latest/LustreGuide/autoimport-data-repo.html>`_ . .. epigraph:: This parameter is not supported for Lustre file systems with a data repository association.\n')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days to retain automatic backups. Setting this property to ``0`` disables automatic backups. You can retain automatic backups for a maximum of 90 days. The default is ``0`` .\n')
    copy_tags_to_backups: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="(Optional) Not available for use with file systems that are linked to a data repository. A boolean flag indicating whether tags for the file system should be copied to backups. The default value is false. If ``CopyTagsToBackups`` is set to true, all file system tags are copied to all automatic and user-initiated backups when the user doesn't specify any backup-specific tags. If ``CopyTagsToBackups`` is set to true and you specify one or more backup tags, only the specified tags are copied to backups. If you specify one or more tags when creating a user-initiated backup, no tags are copied from the file system, regardless of this value. (Default = ``false`` ) For more information, see `Working with backups <https://docs.aws.amazon.com/fsx/latest/LustreGuide/using-backups-fsx.html>`_ in the *Amazon FSx for Lustre User Guide* .\n")
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring daily time, in the format ``HH:MM`` . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``05:00`` specifies 5 AM daily.\n')
    data_compression_type: typing.Optional[str] = pydantic.Field(None, description='Sets the data compression configuration for the file system. ``DataCompressionType`` can have the following values:. - ``NONE`` - (Default) Data compression is turned off when the file system is created. - ``LZ4`` - Data compression is turned on with the LZ4 algorithm. For more information, see `Lustre data compression <https://docs.aws.amazon.com/fsx/latest/LustreGuide/data-compression.html>`_ in the *Amazon FSx for Lustre User Guide* .\n')
    deployment_type: typing.Optional[str] = pydantic.Field(None, description='(Optional) Choose ``SCRATCH_1`` and ``SCRATCH_2`` deployment types when you need temporary storage and shorter-term processing of data. The ``SCRATCH_2`` deployment type provides in-transit encryption of data and higher burst throughput capacity than ``SCRATCH_1`` . Choose ``PERSISTENT_1`` for longer-term storage and for throughput-focused workloads that aren’t latency-sensitive. ``PERSISTENT_1`` supports encryption of data in transit, and is available in all AWS Regions in which FSx for Lustre is available. Choose ``PERSISTENT_2`` for longer-term storage and for latency-sensitive workloads that require the highest levels of IOPS/throughput. ``PERSISTENT_2`` supports SSD storage, and offers higher ``PerUnitStorageThroughput`` (up to 1000 MB/s/TiB). You can optionally specify a metadata configuration mode for ``PERSISTENT_2`` which supports increasing metadata performance. ``PERSISTENT_2`` is available in a limited number of AWS Regions . For more information, and an up-to-date list of AWS Regions in which ``PERSISTENT_2`` is available, see `File system deployment options for FSx for Lustre <https://docs.aws.amazon.com/fsx/latest/LustreGuide/using-fsx-lustre.html#lustre-deployment-types>`_ in the *Amazon FSx for Lustre User Guide* . .. epigraph:: If you choose ``PERSISTENT_2`` , and you set ``FileSystemTypeVersion`` to ``2.10`` , the ``CreateFileSystem`` operation fails. Encryption of data in transit is automatically turned on when you access ``SCRATCH_2`` , ``PERSISTENT_1`` , and ``PERSISTENT_2`` file systems from Amazon EC2 instances that support automatic encryption in the AWS Regions where they are available. For more information about encryption in transit for FSx for Lustre file systems, see `Encrypting data in transit <https://docs.aws.amazon.com/fsx/latest/LustreGuide/encryption-in-transit-fsxl.html>`_ in the *Amazon FSx for Lustre User Guide* . (Default = ``SCRATCH_1`` )\n')
    drive_cache_type: typing.Optional[str] = pydantic.Field(None, description='The type of drive cache used by ``PERSISTENT_1`` file systems that are provisioned with HDD storage devices. This parameter is required when storage type is HDD. Set this property to ``READ`` to improve the performance for frequently accessed files by caching up to 20% of the total storage capacity of the file system. This parameter is required when ``StorageType`` is set to ``HDD`` and ``DeploymentType`` is ``PERSISTENT_1`` .\n')
    export_path: typing.Optional[str] = pydantic.Field(None, description='(Optional) Specifies the path in the Amazon S3 bucket where the root of your Amazon FSx file system is exported. The path must use the same Amazon S3 bucket as specified in ImportPath. You can provide an optional prefix to which new and changed data is to be exported from your Amazon FSx for Lustre file system. If an ``ExportPath`` value is not provided, Amazon FSx sets a default export path, ``s3://import-bucket/FSxLustre[creation-timestamp]`` . The timestamp is in UTC format, for example ``s3://import-bucket/FSxLustre20181105T222312Z`` . The Amazon S3 export bucket must be the same as the import bucket specified by ``ImportPath`` . If you specify only a bucket name, such as ``s3://import-bucket`` , you get a 1:1 mapping of file system objects to S3 bucket objects. This mapping means that the input data in S3 is overwritten on export. If you provide a custom prefix in the export path, such as ``s3://import-bucket/[custom-optional-prefix]`` , Amazon FSx exports the contents of your file system to that export prefix in the Amazon S3 bucket. .. epigraph:: This parameter is not supported for file systems with a data repository association.\n')
    imported_file_chunk_size: typing.Union[int, float, None] = pydantic.Field(None, description='(Optional) For files imported from a data repository, this value determines the stripe count and maximum amount of data per file (in MiB) stored on a single physical disk. The maximum number of disks that a single file can be striped across is limited by the total number of disks that make up the file system. The default chunk size is 1,024 MiB (1 GiB) and can go as high as 512,000 MiB (500 GiB). Amazon S3 objects have a maximum size of 5 TB. .. epigraph:: This parameter is not supported for Lustre file systems with a data repository association.\n')
    import_path: typing.Optional[str] = pydantic.Field(None, description="(Optional) The path to the Amazon S3 bucket (including the optional prefix) that you're using as the data repository for your Amazon FSx for Lustre file system. The root of your FSx for Lustre file system will be mapped to the root of the Amazon S3 bucket you select. An example is ``s3://import-bucket/optional-prefix`` . If you specify a prefix after the Amazon S3 bucket name, only object keys with that prefix are loaded into the file system. .. epigraph:: This parameter is not supported for Lustre file systems with a data repository association.\n")
    metadata_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_MetadataConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    per_unit_storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='Required with ``PERSISTENT_1`` and ``PERSISTENT_2`` deployment types, provisions the amount of read and write throughput for each 1 tebibyte (TiB) of file system storage capacity, in MB/s/TiB. File system throughput capacity is calculated by multiplying ﬁle system storage capacity (TiB) by the ``PerUnitStorageThroughput`` (MB/s/TiB). For a 2.4-TiB ﬁle system, provisioning 50 MB/s/TiB of ``PerUnitStorageThroughput`` yields 120 MB/s of ﬁle system throughput. You pay for the amount of throughput that you provision. Valid values: - For ``PERSISTENT_1`` SSD storage: 50, 100, 200 MB/s/TiB. - For ``PERSISTENT_1`` HDD storage: 12, 40 MB/s/TiB. - For ``PERSISTENT_2`` SSD storage: 125, 250, 500, 1000 MB/s/TiB.\n')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring weekly time, in the format ``D:HH:MM`` . ``D`` is the day of the week, for which 1 represents Monday and 7 represents Sunday. For further details, see `the ISO-8601 spec as described on Wikipedia <https://docs.aws.amazon.com/https://en.wikipedia.org/wiki/ISO_week_date>`_ . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``1:05:00`` specifies maintenance at 5 AM Monday.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-lustreconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    lustre_configuration_property = fsx.CfnFileSystem.LustreConfigurationProperty(\n        auto_import_policy="autoImportPolicy",\n        automatic_backup_retention_days=123,\n        copy_tags_to_backups=False,\n        daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n        data_compression_type="dataCompressionType",\n        deployment_type="deploymentType",\n        drive_cache_type="driveCacheType",\n        export_path="exportPath",\n        imported_file_chunk_size=123,\n        import_path="importPath",\n        metadata_configuration=fsx.CfnFileSystem.MetadataConfigurationProperty(\n            iops=123,\n            mode="mode"\n        ),\n        per_unit_storage_throughput=123,\n        weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_import_policy', 'automatic_backup_retention_days', 'copy_tags_to_backups', 'daily_automatic_backup_start_time', 'data_compression_type', 'deployment_type', 'drive_cache_type', 'export_path', 'imported_file_chunk_size', 'import_path', 'metadata_configuration', 'per_unit_storage_throughput', 'weekly_maintenance_start_time']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.LustreConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.MetadataConfigurationProperty
class CfnFileSystem_MetadataConfigurationPropertyDef(BaseStruct):
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='')
    mode: typing.Optional[str] = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['iops', 'mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.MetadataConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.NfsExportsProperty
class CfnFileSystem_NfsExportsPropertyDef(BaseStruct):
    client_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_ClientConfigurationsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of configuration objects that contain the client and options for mounting the OpenZFS file system.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-nfsexports.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    nfs_exports_property = fsx.CfnFileSystem.NfsExportsProperty(\n        client_configurations=[fsx.CfnFileSystem.ClientConfigurationsProperty(\n            clients="clients",\n            options=["options"]\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['client_configurations']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.NfsExportsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.OntapConfigurationProperty
class CfnFileSystem_OntapConfigurationPropertyDef(BaseStruct):
    deployment_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the FSx for ONTAP file system deployment type to use in creating the file system. - ``MULTI_AZ_1`` - (Default) A high availability file system configured for Multi-AZ redundancy to tolerate temporary Availability Zone (AZ) unavailability. - ``SINGLE_AZ_1`` - A file system configured for Single-AZ redundancy. - ``SINGLE_AZ_2`` - A file system configured with multiple high-availability (HA) pairs for Single-AZ redundancy. For information about the use cases for Multi-AZ and Single-AZ deployments, refer to `Choosing a file system deployment type <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/high-availability-AZ.html>`_ .\n')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days to retain automatic backups. Setting this property to ``0`` disables automatic backups. You can retain automatic backups for a maximum of 90 days. The default is ``30`` .\n')
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring daily time, in the format ``HH:MM`` . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``05:00`` specifies 5 AM daily.\n')
    disk_iops_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_DiskIopsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The SSD IOPS configuration for the FSx for ONTAP file system.\n')
    endpoint_ip_address_range: typing.Optional[str] = pydantic.Field(None, description="(Multi-AZ only) Specifies the IP address range in which the endpoints to access your file system will be created. By default in the Amazon FSx API, Amazon FSx selects an unused IP address range for you from the 198.19.* range. By default in the Amazon FSx console, Amazon FSx chooses the last 64 IP addresses from the VPC’s primary CIDR range to use as the endpoint IP address range for the file system. You can have overlapping endpoint IP addresses for file systems deployed in the same VPC/route tables, as long as they don't overlap with any subnet.\n")
    fsx_admin_password: typing.Optional[str] = pydantic.Field(None, description='The ONTAP administrative password for the ``fsxadmin`` user with which you administer your file system using the NetApp ONTAP CLI and REST API.\n')
    ha_pairs: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies how many high-availability (HA) pairs of file servers will power your file system. Scale-up file systems are powered by 1 HA pair. The default value is 1. FSx for ONTAP scale-out file systems are powered by up to 12 HA pairs. The value of this property affects the values of ``StorageCapacity`` , ``Iops`` , and ``ThroughputCapacity`` . For more information, see `High-availability (HA) pairs <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/administering-file-systems.html#HA-pairs>`_ in the FSx for ONTAP user guide. Amazon FSx responds with an HTTP status code 400 (Bad Request) for the following conditions: - The value of ``HAPairs`` is less than 1 or greater than 12. - The value of ``HAPairs`` is greater than 1 and the value of ``DeploymentType`` is ``SINGLE_AZ_1`` or ``MULTI_AZ_1`` .\n')
    preferred_subnet_id: typing.Optional[str] = pydantic.Field(None, description='Required when ``DeploymentType`` is set to ``MULTI_AZ_1`` . This specifies the subnet in which you want the preferred file server to be located.\n')
    route_table_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="(Multi-AZ only) Specifies the route tables in which Amazon FSx creates the rules for routing traffic to the correct file server. You should specify all virtual private cloud (VPC) route tables associated with the subnets in which your clients are located. By default, Amazon FSx selects your VPC's default route table. .. epigraph:: Amazon FSx manages these route tables for Multi-AZ file systems using tag-based authentication. These route tables are tagged with ``Key: AmazonFSx; Value: ManagedByAmazonFSx`` . When creating FSx for ONTAP Multi-AZ file systems using AWS CloudFormation we recommend that you add the ``Key: AmazonFSx; Value: ManagedByAmazonFSx`` tag manually.\n")
    throughput_capacity: typing.Union[int, float, None] = pydantic.Field(None, description="Sets the throughput capacity for the file system that you're creating in megabytes per second (MBps). For more information, see `Managing throughput capacity <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/managing-throughput-capacity.html>`_ in the FSx for ONTAP User Guide. Amazon FSx responds with an HTTP status code 400 (Bad Request) for the following conditions: - The value of ``ThroughputCapacity`` and ``ThroughputCapacityPerHAPair`` are not the same value. - The value of ``ThroughputCapacity`` when divided by the value of ``HAPairs`` is outside of the valid range for ``ThroughputCapacity`` .\n")
    throughput_capacity_per_ha_pair: typing.Union[int, float, None] = pydantic.Field(None, description='Use to choose the throughput capacity per HA pair, rather than the total throughput for the file system. You can define either the ``ThroughputCapacityPerHAPair`` or the ``ThroughputCapacity`` when creating a file system, but not both. This field and ``ThroughputCapacity`` are the same for scale-up file systems powered by one HA pair. - For ``SINGLE_AZ_1`` and ``MULTI_AZ_1`` file systems, valid values are 128, 256, 512, 1024, 2048, or 4096 MBps. - For ``SINGLE_AZ_2`` file systems, valid values are 3072 or 6144 MBps. Amazon FSx responds with an HTTP status code 400 (Bad Request) for the following conditions: - The value of ``ThroughputCapacity`` and ``ThroughputCapacityPerHAPair`` are not the same value for file systems with one HA pair. - The value of deployment type is ``SINGLE_AZ_2`` and ``ThroughputCapacity`` / ``ThroughputCapacityPerHAPair`` is a valid HA pair (a value between 2 and 12). - The value of ``ThroughputCapacityPerHAPair`` is not a valid value.\n')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring weekly time, in the format ``D:HH:MM`` . ``D`` is the day of the week, for which 1 represents Monday and 7 represents Sunday. For further details, see `the ISO-8601 spec as described on Wikipedia <https://docs.aws.amazon.com/https://en.wikipedia.org/wiki/ISO_week_date>`_ . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``1:05:00`` specifies maintenance at 5 AM Monday.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-ontapconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    ontap_configuration_property = fsx.CfnFileSystem.OntapConfigurationProperty(\n        deployment_type="deploymentType",\n\n        # the properties below are optional\n        automatic_backup_retention_days=123,\n        daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n        disk_iops_configuration=fsx.CfnFileSystem.DiskIopsConfigurationProperty(\n            iops=123,\n            mode="mode"\n        ),\n        endpoint_ip_address_range="endpointIpAddressRange",\n        fsx_admin_password="fsxAdminPassword",\n        ha_pairs=123,\n        preferred_subnet_id="preferredSubnetId",\n        route_table_ids=["routeTableIds"],\n        throughput_capacity=123,\n        throughput_capacity_per_ha_pair=123,\n        weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['deployment_type', 'automatic_backup_retention_days', 'daily_automatic_backup_start_time', 'disk_iops_configuration', 'endpoint_ip_address_range', 'fsx_admin_password', 'ha_pairs', 'preferred_subnet_id', 'route_table_ids', 'throughput_capacity', 'throughput_capacity_per_ha_pair', 'weekly_maintenance_start_time']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.OntapConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.OpenZFSConfigurationProperty
class CfnFileSystem_OpenZFSConfigurationPropertyDef(BaseStruct):
    deployment_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the file system deployment type. Single AZ deployment types are configured for redundancy within a single Availability Zone in an AWS Region . Valid values are the following: - ``MULTI_AZ_1`` - Creates file systems with high availability that are configured for Multi-AZ redundancy to tolerate temporary unavailability in Availability Zones (AZs). ``Multi_AZ_1`` is available only in the US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Tokyo), and Europe (Ireland) AWS Regions . - ``SINGLE_AZ_1`` - Creates file systems with throughput capacities of 64 - 4,096 MB/s. ``Single_AZ_1`` is available in all AWS Regions where Amazon FSx for OpenZFS is available. - ``SINGLE_AZ_2`` - Creates file systems with throughput capacities of 160 - 10,240 MB/s using an NVMe L2ARC cache. ``Single_AZ_2`` is available only in the US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Tokyo), and Europe (Ireland) AWS Regions . For more information, see `Deployment type availability <https://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/availability-durability.html#available-aws-regions>`_ and `File system performance <https://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/performance.html#zfs-fs-performance>`_ in the *Amazon FSx for OpenZFS User Guide* .\n')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days to retain automatic backups. Setting this property to ``0`` disables automatic backups. You can retain automatic backups for a maximum of 90 days. The default is ``30`` .\n')
    copy_tags_to_backups: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A Boolean value indicating whether tags for the file system should be copied to backups. This value defaults to ``false`` . If it's set to ``true`` , all tags for the file system are copied to all automatic and user-initiated backups where the user doesn't specify tags. If this value is ``true`` , and you specify one or more tags, only the specified tags are copied to backups. If you specify one or more tags when creating a user-initiated backup, no tags are copied from the file system, regardless of this value.\n")
    copy_tags_to_volumes: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A Boolean value indicating whether tags for the file system should be copied to volumes. This value defaults to ``false`` . If it's set to ``true`` , all tags for the file system are copied to volumes where the user doesn't specify tags. If this value is ``true`` , and you specify one or more tags, only the specified tags are copied to volumes. If you specify one or more tags when creating the volume, no tags are copied from the file system, regardless of this value.\n")
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring daily time, in the format ``HH:MM`` . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``05:00`` specifies 5 AM daily.\n')
    disk_iops_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_DiskIopsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The SSD IOPS (input/output operations per second) configuration for an Amazon FSx for NetApp ONTAP, Amazon FSx for Windows File Server, or FSx for OpenZFS file system. By default, Amazon FSx automatically provisions 3 IOPS per GB of storage capacity. You can provision additional IOPS per GB of storage. The configuration consists of the total number of provisioned SSD IOPS and how it is was provisioned, or the mode (by the customer or by Amazon FSx).\n')
    endpoint_ip_address_range: typing.Optional[str] = pydantic.Field(None, description="(Multi-AZ only) Specifies the IP address range in which the endpoints to access your file system will be created. By default in the Amazon FSx API and Amazon FSx console, Amazon FSx selects an available /28 IP address range for you from one of the VPC's CIDR ranges. You can have overlapping endpoint IP addresses for file systems deployed in the same VPC/route tables.\n")
    options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="To delete a file system if there are child volumes present below the root volume, use the string ``DELETE_CHILD_VOLUMES_AND_SNAPSHOTS`` . If your file system has child volumes and you don't use this option, the delete request will fail.\n")
    preferred_subnet_id: typing.Optional[str] = pydantic.Field(None, description='Required when ``DeploymentType`` is set to ``MULTI_AZ_1`` . This specifies the subnet in which you want the preferred file server to be located.\n')
    root_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_RootVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration Amazon FSx uses when creating the root value of the Amazon FSx for OpenZFS file system. All volumes are children of the root volume.\n')
    route_table_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="(Multi-AZ only) Specifies the route tables in which Amazon FSx creates the rules for routing traffic to the correct file server. You should specify all virtual private cloud (VPC) route tables associated with the subnets in which your clients are located. By default, Amazon FSx selects your VPC's default route table.\n")
    throughput_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the throughput of an Amazon FSx for OpenZFS file system, measured in megabytes per second (MBps). Valid values depend on the DeploymentType you choose, as follows: - For ``MULTI_AZ_1`` and ``SINGLE_AZ_2`` , valid values are 160, 320, 640, 1280, 2560, 3840, 5120, 7680, or 10240 MBps. - For ``SINGLE_AZ_1`` , valid values are 64, 128, 256, 512, 1024, 2048, 3072, or 4096 MBps. You pay for additional throughput capacity that you provision.\n')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring weekly time, in the format ``D:HH:MM`` . ``D`` is the day of the week, for which 1 represents Monday and 7 represents Sunday. For further details, see `the ISO-8601 spec as described on Wikipedia <https://docs.aws.amazon.com/https://en.wikipedia.org/wiki/ISO_week_date>`_ . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``1:05:00`` specifies maintenance at 5 AM Monday.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-openzfsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    open_zFSConfiguration_property = fsx.CfnFileSystem.OpenZFSConfigurationProperty(\n        deployment_type="deploymentType",\n\n        # the properties below are optional\n        automatic_backup_retention_days=123,\n        copy_tags_to_backups=False,\n        copy_tags_to_volumes=False,\n        daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n        disk_iops_configuration=fsx.CfnFileSystem.DiskIopsConfigurationProperty(\n            iops=123,\n            mode="mode"\n        ),\n        endpoint_ip_address_range="endpointIpAddressRange",\n        options=["options"],\n        preferred_subnet_id="preferredSubnetId",\n        root_volume_configuration=fsx.CfnFileSystem.RootVolumeConfigurationProperty(\n            copy_tags_to_snapshots=False,\n            data_compression_type="dataCompressionType",\n            nfs_exports=[fsx.CfnFileSystem.NfsExportsProperty(\n                client_configurations=[fsx.CfnFileSystem.ClientConfigurationsProperty(\n                    clients="clients",\n                    options=["options"]\n                )]\n            )],\n            read_only=False,\n            record_size_ki_b=123,\n            user_and_group_quotas=[fsx.CfnFileSystem.UserAndGroupQuotasProperty(\n                id=123,\n                storage_capacity_quota_gi_b=123,\n                type="type"\n            )]\n        ),\n        route_table_ids=["routeTableIds"],\n        throughput_capacity=123,\n        weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['deployment_type', 'automatic_backup_retention_days', 'copy_tags_to_backups', 'copy_tags_to_volumes', 'daily_automatic_backup_start_time', 'disk_iops_configuration', 'endpoint_ip_address_range', 'options', 'preferred_subnet_id', 'root_volume_configuration', 'route_table_ids', 'throughput_capacity', 'weekly_maintenance_start_time']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.OpenZFSConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.RootVolumeConfigurationProperty
class CfnFileSystem_RootVolumeConfigurationPropertyDef(BaseStruct):
    copy_tags_to_snapshots: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A Boolean value indicating whether tags for the volume should be copied to snapshots of the volume. This value defaults to ``false`` . If it's set to ``true`` , all tags for the volume are copied to snapshots where the user doesn't specify tags. If this value is ``true`` and you specify one or more tags, only the specified tags are copied to snapshots. If you specify one or more tags when creating the snapshot, no tags are copied from the volume, regardless of this value.\n")
    data_compression_type: typing.Optional[str] = pydantic.Field(None, description="Specifies the method used to compress the data on the volume. The compression type is ``NONE`` by default. - ``NONE`` - Doesn't compress the data on the volume. ``NONE`` is the default. - ``ZSTD`` - Compresses the data in the volume using the Zstandard (ZSTD) compression algorithm. Compared to LZ4, Z-Standard provides a better compression ratio to minimize on-disk storage utilization. - ``LZ4`` - Compresses the data in the volume using the LZ4 compression algorithm. Compared to Z-Standard, LZ4 is less compute-intensive and delivers higher write throughput speeds.\n")
    nfs_exports: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_NfsExportsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The configuration object for mounting a file system.\n')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A Boolean value indicating whether the volume is read-only. Setting this value to ``true`` can be useful after you have completed changes to a volume and no longer want changes to occur.\n')
    record_size_kib: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the record size of an OpenZFS root volume, in kibibytes (KiB). Valid values are 4, 8, 16, 32, 64, 128, 256, 512, or 1024 KiB. The default is 128 KiB. Most workloads should use the default record size. Database workflows can benefit from a smaller record size, while streaming workflows can benefit from a larger record size. For additional guidance on setting a custom record size, see `Tips for maximizing performance <https://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/performance.html#performance-tips-zfs>`_ in the *Amazon FSx for OpenZFS User Guide* .\n')
    user_and_group_quotas: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_UserAndGroupQuotasPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An object specifying how much storage users or groups can use on the volume.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-rootvolumeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    root_volume_configuration_property = fsx.CfnFileSystem.RootVolumeConfigurationProperty(\n        copy_tags_to_snapshots=False,\n        data_compression_type="dataCompressionType",\n        nfs_exports=[fsx.CfnFileSystem.NfsExportsProperty(\n            client_configurations=[fsx.CfnFileSystem.ClientConfigurationsProperty(\n                clients="clients",\n                options=["options"]\n            )]\n        )],\n        read_only=False,\n        record_size_ki_b=123,\n        user_and_group_quotas=[fsx.CfnFileSystem.UserAndGroupQuotasProperty(\n            id=123,\n            storage_capacity_quota_gi_b=123,\n            type="type"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['copy_tags_to_snapshots', 'data_compression_type', 'nfs_exports', 'read_only', 'record_size_kib', 'user_and_group_quotas']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.RootVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.SelfManagedActiveDirectoryConfigurationProperty
class CfnFileSystem_SelfManagedActiveDirectoryConfigurationPropertyDef(BaseStruct):
    dns_ips: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of up to three IP addresses of DNS servers or domain controllers in the self-managed AD directory.\n')
    domain_name: typing.Optional[str] = pydantic.Field(None, description='The fully qualified domain name of the self-managed AD directory, such as ``corp.example.com`` .\n')
    file_system_administrators_group: typing.Optional[str] = pydantic.Field(None, description="(Optional) The name of the domain group whose members are granted administrative privileges for the file system. Administrative privileges include taking ownership of files and folders, setting audit controls (audit ACLs) on files and folders, and administering the file system remotely by using the FSx Remote PowerShell. The group that you specify must already exist in your domain. If you don't provide one, your AD domain's Domain Admins group is used.\n")
    organizational_unit_distinguished_name: typing.Optional[str] = pydantic.Field(None, description="(Optional) The fully qualified distinguished name of the organizational unit within your self-managed AD directory. Amazon FSx only accepts OU as the direct parent of the file system. An example is ``OU=FSx,DC=yourdomain,DC=corp,DC=com`` . To learn more, see `RFC 2253 <https://docs.aws.amazon.com/https://tools.ietf.org/html/rfc2253>`_ . If none is provided, the FSx file system is created in the default location of your self-managed AD directory. .. epigraph:: Only Organizational Unit (OU) objects can be the direct parent of the file system that you're creating.\n")
    password: typing.Optional[str] = pydantic.Field(None, description='The password for the service account on your self-managed AD domain that Amazon FSx will use to join to your AD domain.\n')
    user_name: typing.Optional[str] = pydantic.Field(None, description='The user name for the service account on your self-managed AD domain that Amazon FSx will use to join to your AD domain. This account must have the permission to join computers to the domain in the organizational unit provided in ``OrganizationalUnitDistinguishedName`` , or in the default location of your AD domain.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-selfmanagedactivedirectoryconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    self_managed_active_directory_configuration_property = fsx.CfnFileSystem.SelfManagedActiveDirectoryConfigurationProperty(\n        dns_ips=["dnsIps"],\n        domain_name="domainName",\n        file_system_administrators_group="fileSystemAdministratorsGroup",\n        organizational_unit_distinguished_name="organizationalUnitDistinguishedName",\n        password="password",\n        user_name="userName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dns_ips', 'domain_name', 'file_system_administrators_group', 'organizational_unit_distinguished_name', 'password', 'user_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.SelfManagedActiveDirectoryConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.UserAndGroupQuotasProperty
class CfnFileSystem_UserAndGroupQuotasPropertyDef(BaseStruct):
    storage_capacity_quota_gib: typing.Union[int, float, None] = pydantic.Field(None, description="The user or group's storage quota, in gibibytes (GiB).\n")
    type: typing.Optional[str] = pydantic.Field(None, description='Specifies whether the quota applies to a user or group.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-userandgroupquotas.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    user_and_group_quotas_property = fsx.CfnFileSystem.UserAndGroupQuotasProperty(\n        id=123,\n        storage_capacity_quota_gi_b=123,\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_capacity_quota_gib', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.UserAndGroupQuotasProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem.WindowsConfigurationProperty
class CfnFileSystem_WindowsConfigurationPropertyDef(BaseStruct):
    throughput_capacity: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Sets the throughput capacity of an Amazon FSx file system, measured in megabytes per second (MB/s), in 2 to the *n* th increments, between 2^3 (8) and 2^11 (2048). .. epigraph:: To increase storage capacity, a file system must have a minimum throughput capacity of 16 MB/s.\n')
    active_directory_id: typing.Optional[str] = pydantic.Field(None, description="The ID for an existing AWS Managed Microsoft Active Directory (AD) instance that the file system should join when it's created. Required if you are joining the file system to an existing AWS Managed Microsoft AD.\n")
    aliases: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of one or more DNS alias names that you want to associate with the Amazon FSx file system. Aliases allow you to use existing DNS names to access the data in your Amazon FSx file system. You can associate up to 50 aliases with a file system at any time. For more information, see `Working with DNS Aliases <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-dns-aliases.html>`_ and `Walkthrough 5: Using DNS aliases to access your file system <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/walkthrough05-file-system-custom-CNAME.html>`_ , including additional steps you must take to be able to access your file system using a DNS alias. An alias name has to meet the following requirements: - Formatted as a fully-qualified domain name (FQDN), ``hostname.domain`` , for example, ``accounting.example.com`` . - Can contain alphanumeric characters, the underscore (_), and the hyphen (-). - Cannot start or end with a hyphen. - Can start with a numeric. For DNS alias names, Amazon FSx stores alphabetical characters as lowercase letters (a-z), regardless of how you specify them: as uppercase letters, lowercase letters, or the corresponding letters in escape codes.\n')
    audit_log_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_AuditLogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration that Amazon FSx for Windows File Server uses to audit and log user accesses of files, folders, and file shares on the Amazon FSx for Windows File Server file system.\n')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days to retain automatic backups. Setting this property to ``0`` disables automatic backups. You can retain automatic backups for a maximum of 90 days. The default is ``30`` .\n')
    copy_tags_to_backups: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A boolean flag indicating whether tags for the file system should be copied to backups. This value defaults to false. If it's set to true, all tags for the file system are copied to all automatic and user-initiated backups where the user doesn't specify tags. If this value is true, and you specify one or more tags, only the specified tags are copied to backups. If you specify one or more tags when creating a user-initiated backup, no tags are copied from the file system, regardless of this value.\n")
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring daily time, in the format ``HH:MM`` . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``05:00`` specifies 5 AM daily.\n')
    deployment_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the file system deployment type, valid values are the following:. - ``MULTI_AZ_1`` - Deploys a high availability file system that is configured for Multi-AZ redundancy to tolerate temporary Availability Zone (AZ) unavailability. You can only deploy a Multi-AZ file system in AWS Regions that have a minimum of three Availability Zones. Also supports HDD storage type - ``SINGLE_AZ_1`` - (Default) Choose to deploy a file system that is configured for single AZ redundancy. - ``SINGLE_AZ_2`` - The latest generation Single AZ file system. Specifies a file system that is configured for single AZ redundancy and supports HDD storage type. For more information, see `Availability and Durability: Single-AZ and Multi-AZ File Systems <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html>`_ .\n')
    disk_iops_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_DiskIopsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The SSD IOPS (input/output operations per second) configuration for an Amazon FSx for Windows file system. By default, Amazon FSx automatically provisions 3 IOPS per GiB of storage capacity. You can provision additional IOPS per GiB of storage, up to the maximum limit associated with your chosen throughput capacity.\n')
    preferred_subnet_id: typing.Optional[str] = pydantic.Field(None, description='Required when ``DeploymentType`` is set to ``MULTI_AZ_1`` . This specifies the subnet in which you want the preferred file server to be located. For in- AWS applications, we recommend that you launch your clients in the same availability zone as your preferred file server to reduce cross-availability zone data transfer costs and minimize latency.\n')
    self_managed_active_directory_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_SelfManagedActiveDirectoryConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration that Amazon FSx uses to join a FSx for Windows File Server file system or an FSx for ONTAP storage virtual machine (SVM) to a self-managed (including on-premises) Microsoft Active Directory (AD) directory. For more information, see `Using Amazon FSx for Windows with your self-managed Microsoft Active Directory <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD.html>`_ or `Managing FSx for ONTAP SVMs <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/managing-svms.html>`_ .\n')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='A recurring weekly time, in the format ``D:HH:MM`` . ``D`` is the day of the week, for which 1 represents Monday and 7 represents Sunday. For further details, see `the ISO-8601 spec as described on Wikipedia <https://docs.aws.amazon.com/https://en.wikipedia.org/wiki/ISO_week_date>`_ . ``HH`` is the zero-padded hour of the day (0-23), and ``MM`` is the zero-padded minute of the hour. For example, ``1:05:00`` specifies maintenance at 5 AM Monday.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-windowsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    windows_configuration_property = fsx.CfnFileSystem.WindowsConfigurationProperty(\n        throughput_capacity=123,\n\n        # the properties below are optional\n        active_directory_id="activeDirectoryId",\n        aliases=["aliases"],\n        audit_log_configuration=fsx.CfnFileSystem.AuditLogConfigurationProperty(\n            file_access_audit_log_level="fileAccessAuditLogLevel",\n            file_share_access_audit_log_level="fileShareAccessAuditLogLevel",\n\n            # the properties below are optional\n            audit_log_destination="auditLogDestination"\n        ),\n        automatic_backup_retention_days=123,\n        copy_tags_to_backups=False,\n        daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n        deployment_type="deploymentType",\n        disk_iops_configuration=fsx.CfnFileSystem.DiskIopsConfigurationProperty(\n            iops=123,\n            mode="mode"\n        ),\n        preferred_subnet_id="preferredSubnetId",\n        self_managed_active_directory_configuration=fsx.CfnFileSystem.SelfManagedActiveDirectoryConfigurationProperty(\n            dns_ips=["dnsIps"],\n            domain_name="domainName",\n            file_system_administrators_group="fileSystemAdministratorsGroup",\n            organizational_unit_distinguished_name="organizationalUnitDistinguishedName",\n            password="password",\n            user_name="userName"\n        ),\n        weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['throughput_capacity', 'active_directory_id', 'aliases', 'audit_log_configuration', 'automatic_backup_retention_days', 'copy_tags_to_backups', 'daily_automatic_backup_start_time', 'deployment_type', 'disk_iops_configuration', 'preferred_subnet_id', 'self_managed_active_directory_configuration', 'weekly_maintenance_start_time']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem.WindowsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnStorageVirtualMachine.ActiveDirectoryConfigurationProperty
class CfnStorageVirtualMachine_ActiveDirectoryConfigurationPropertyDef(BaseStruct):
    net_bios_name: typing.Optional[str] = pydantic.Field(None, description='The NetBIOS name of the Active Directory computer object that will be created for your SVM.\n')
    self_managed_active_directory_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnStorageVirtualMachine_SelfManagedActiveDirectoryConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration that Amazon FSx uses to join the ONTAP storage virtual machine (SVM) to your self-managed (including on-premises) Microsoft Active Directory directory.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-storagevirtualmachine-activedirectoryconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    active_directory_configuration_property = fsx.CfnStorageVirtualMachine.ActiveDirectoryConfigurationProperty(\n        net_bios_name="netBiosName",\n        self_managed_active_directory_configuration=fsx.CfnStorageVirtualMachine.SelfManagedActiveDirectoryConfigurationProperty(\n            dns_ips=["dnsIps"],\n            domain_name="domainName",\n            file_system_administrators_group="fileSystemAdministratorsGroup",\n            organizational_unit_distinguished_name="organizationalUnitDistinguishedName",\n            password="password",\n            user_name="userName"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['net_bios_name', 'self_managed_active_directory_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnStorageVirtualMachine.ActiveDirectoryConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnStorageVirtualMachine.SelfManagedActiveDirectoryConfigurationProperty
class CfnStorageVirtualMachine_SelfManagedActiveDirectoryConfigurationPropertyDef(BaseStruct):
    dns_ips: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of up to three IP addresses of DNS servers or domain controllers in the self-managed AD directory.\n')
    domain_name: typing.Optional[str] = pydantic.Field(None, description='The fully qualified domain name of the self-managed AD directory, such as ``corp.example.com`` .\n')
    file_system_administrators_group: typing.Optional[str] = pydantic.Field(None, description="(Optional) The name of the domain group whose members are granted administrative privileges for the file system. Administrative privileges include taking ownership of files and folders, setting audit controls (audit ACLs) on files and folders, and administering the file system remotely by using the FSx Remote PowerShell. The group that you specify must already exist in your domain. If you don't provide one, your AD domain's Domain Admins group is used.\n")
    organizational_unit_distinguished_name: typing.Optional[str] = pydantic.Field(None, description="(Optional) The fully qualified distinguished name of the organizational unit within your self-managed AD directory. Amazon FSx only accepts OU as the direct parent of the file system. An example is ``OU=FSx,DC=yourdomain,DC=corp,DC=com`` . To learn more, see `RFC 2253 <https://docs.aws.amazon.com/https://tools.ietf.org/html/rfc2253>`_ . If none is provided, the FSx file system is created in the default location of your self-managed AD directory. .. epigraph:: Only Organizational Unit (OU) objects can be the direct parent of the file system that you're creating.\n")
    password: typing.Optional[str] = pydantic.Field(None, description='The password for the service account on your self-managed AD domain that Amazon FSx will use to join to your AD domain.\n')
    user_name: typing.Optional[str] = pydantic.Field(None, description='The user name for the service account on your self-managed AD domain that Amazon FSx will use to join to your AD domain. This account must have the permission to join computers to the domain in the organizational unit provided in ``OrganizationalUnitDistinguishedName`` , or in the default location of your AD domain.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-storagevirtualmachine-selfmanagedactivedirectoryconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    self_managed_active_directory_configuration_property = fsx.CfnStorageVirtualMachine.SelfManagedActiveDirectoryConfigurationProperty(\n        dns_ips=["dnsIps"],\n        domain_name="domainName",\n        file_system_administrators_group="fileSystemAdministratorsGroup",\n        organizational_unit_distinguished_name="organizationalUnitDistinguishedName",\n        password="password",\n        user_name="userName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dns_ips', 'domain_name', 'file_system_administrators_group', 'organizational_unit_distinguished_name', 'password', 'user_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnStorageVirtualMachine.SelfManagedActiveDirectoryConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.AggregateConfigurationProperty
class CfnVolume_AggregateConfigurationPropertyDef(BaseStruct):
    aggregates: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of aggregates that this volume resides on. Aggregates are storage pools which make up your primary storage tier. Each high-availability (HA) pair has one aggregate. The names of the aggregates map to the names of the aggregates in the ONTAP CLI and REST API. For FlexVols, there will always be a single entry. Amazon FSx responds with an HTTP status code 400 (Bad Request) for the following conditions: - The strings in the value of ``Aggregates`` are not are not formatted as ``aggrX`` , where X is a number between 1 and 6. - The value of ``Aggregates`` contains aggregates that are not present. - One or more of the aggregates supplied are too close to the volume limit to support adding more volumes.\n')
    constituents_per_aggregate: typing.Union[int, float, None] = pydantic.Field(None, description='Used to explicitly set the number of constituents within the FlexGroup per storage aggregate. This field is optional when creating a FlexGroup volume. If unspecified, the default value will be 8. This field cannot be provided when creating a FlexVol volume.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-aggregateconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    aggregate_configuration_property = fsx.CfnVolume.AggregateConfigurationProperty(\n        aggregates=["aggregates"],\n        constituents_per_aggregate=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['aggregates', 'constituents_per_aggregate']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.AggregateConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.AutocommitPeriodProperty
class CfnVolume_AutocommitPeriodPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Defines the type of time for the autocommit period of a file in an FSx for ONTAP SnapLock volume. Setting this value to ``NONE`` disables autocommit. The default value is ``NONE`` .\n')
    value: typing.Union[int, float, None] = pydantic.Field(None, description='Defines the amount of time for the autocommit period of a file in an FSx for ONTAP SnapLock volume. The following ranges are valid: - ``Minutes`` : 5 - 65,535 - ``Hours`` : 1 - 65,535 - ``Days`` : 1 - 3,650 - ``Months`` : 1 - 120 - ``Years`` : 1 - 10\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-autocommitperiod.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    autocommit_period_property = fsx.CfnVolume.AutocommitPeriodProperty(\n        type="type",\n\n        # the properties below are optional\n        value=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.AutocommitPeriodProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.ClientConfigurationsProperty
class CfnVolume_ClientConfigurationsPropertyDef(BaseStruct):
    clients: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A value that specifies who can mount the file system. You can provide a wildcard character ( ``*`` ), an IP address ( ``0.0.0.0`` ), or a CIDR address ( ``192.0.2.0/24`` ). By default, Amazon FSx uses the wildcard character when specifying the client.\n')
    options: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The options to use when mounting the file system. For a list of options that you can use with Network File System (NFS), see the `exports(5) - Linux man page <https://docs.aws.amazon.com/https://linux.die.net/man/5/exports>`_ . When choosing your options, consider the following: - ``crossmnt`` is used by default. If you don\'t specify ``crossmnt`` when changing the client configuration, you won\'t be able to see or access snapshots in your file system\'s snapshot directory. - ``sync`` is used by default. If you instead specify ``async`` , the system acknowledges writes before writing to disk. If the system crashes before the writes are finished, you lose the unwritten data.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-clientconfigurations.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    client_configurations_property = fsx.CfnVolume.ClientConfigurationsProperty(\n        clients="clients",\n        options=["options"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['clients', 'options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.ClientConfigurationsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.NfsExportsProperty
class CfnVolume_NfsExportsPropertyDef(BaseStruct):
    client_configurations: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_ClientConfigurationsPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A list of configuration objects that contain the client and options for mounting the OpenZFS file system.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-nfsexports.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    nfs_exports_property = fsx.CfnVolume.NfsExportsProperty(\n        client_configurations=[fsx.CfnVolume.ClientConfigurationsProperty(\n            clients="clients",\n            options=["options"]\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['client_configurations']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.NfsExportsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.OntapConfigurationProperty
class CfnVolume_OntapConfigurationPropertyDef(BaseStruct):
    storage_virtual_machine_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the ONTAP SVM in which to create the volume.\n')
    aggregate_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_AggregateConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Used to specify the configuration options for an FSx for ONTAP volume's storage aggregate or aggregates.\n")
    copy_tags_to_backups: typing.Optional[str] = pydantic.Field(None, description="A boolean flag indicating whether tags for the volume should be copied to backups. This value defaults to false. If it's set to true, all tags for the volume are copied to all automatic and user-initiated backups where the user doesn't specify tags. If this value is true, and you specify one or more tags, only the specified tags are copied to backups. If you specify one or more tags when creating a user-initiated backup, no tags are copied from the volume, regardless of this value.\n")
    junction_path: typing.Optional[str] = pydantic.Field(None, description="Specifies the location in the SVM's namespace where the volume is mounted. This parameter is required. The ``JunctionPath`` must have a leading forward slash, such as ``/vol3`` .\n")
    ontap_volume_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the type of volume you are creating. Valid values are the following:. - ``RW`` specifies a read/write volume. ``RW`` is the default. - ``DP`` specifies a data-protection volume. A ``DP`` volume is read-only and can be used as the destination of a NetApp SnapMirror relationship. For more information, see `Volume types <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/managing-volumes.html#volume-types>`_ in the Amazon FSx for NetApp ONTAP User Guide.\n')
    security_style: typing.Optional[str] = pydantic.Field(None, description="Specifies the security style for the volume. If a volume's security style is not specified, it is automatically set to the root volume's security style. The security style determines the type of permissions that FSx for ONTAP uses to control data access. Specify one of the following values: - ``UNIX`` if the file system is managed by a UNIX administrator, the majority of users are NFS clients, and an application accessing the data uses a UNIX user as the service account. - ``NTFS`` if the file system is managed by a Windows administrator, the majority of users are SMB clients, and an application accessing the data uses a Windows user as the service account. - ``MIXED`` This is an advanced setting. For more information, see the topic `What the security styles and their effects are <https://docs.aws.amazon.com/https://docs.netapp.com/us-en/ontap/nfs-admin/security-styles-their-effects-concept.html>`_ in the NetApp Documentation Center. For more information, see `Volume security style <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/managing-volumes.html#volume-security-style>`_ in the FSx for ONTAP User Guide.\n")
    size_in_bytes: typing.Optional[str] = pydantic.Field(None, description='Specifies the configured size of the volume, in bytes.\n')
    size_in_megabytes: typing.Optional[str] = pydantic.Field(None, description='Use ``SizeInBytes`` instead. Specifies the size of the volume, in megabytes (MB), that you are creating.\n')
    snaplock_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_SnaplockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The SnapLock configuration object for an FSx for ONTAP SnapLock volume.\n')
    snapshot_policy: typing.Optional[str] = pydantic.Field(None, description='Specifies the snapshot policy for the volume. There are three built-in snapshot policies:. - ``default`` : This is the default policy. A maximum of six hourly snapshots taken five minutes past the hour. A maximum of two daily snapshots taken Monday through Saturday at 10 minutes after midnight. A maximum of two weekly snapshots taken every Sunday at 15 minutes after midnight. - ``default-1weekly`` : This policy is the same as the ``default`` policy except that it only retains one snapshot from the weekly schedule. - ``none`` : This policy does not take any snapshots. This policy can be assigned to volumes to prevent automatic snapshots from being taken. You can also provide the name of a custom policy that you created with the ONTAP CLI or REST API. For more information, see `Snapshot policies <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/snapshots-ontap.html#snapshot-policies>`_ in the Amazon FSx for NetApp ONTAP User Guide.\n')
    storage_efficiency_enabled: typing.Optional[str] = pydantic.Field(None, description='Set to true to enable deduplication, compression, and compaction storage efficiency features on the volume, or set to false to disable them. ``StorageEfficiencyEnabled`` is required when creating a ``RW`` volume ( ``OntapVolumeType`` set to ``RW`` ).\n')
    tiering_policy: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_TieringPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Describes the data tiering policy for an ONTAP volume. When enabled, Amazon FSx for ONTAP's intelligent tiering automatically transitions a volume's data between the file system's primary storage and capacity pool storage based on your access patterns. Valid tiering policies are the following: - ``SNAPSHOT_ONLY`` - (Default value) moves cold snapshots to the capacity pool storage tier. - ``AUTO`` - moves cold user data and snapshots to the capacity pool storage tier based on your access patterns. - ``ALL`` - moves all user data blocks in both the active file system and Snapshot copies to the storage pool tier. - ``NONE`` - keeps a volume's data in the primary storage tier, preventing it from being moved to the capacity pool tier.\n")
    volume_style: typing.Optional[str] = pydantic.Field(None, description='Use to specify the style of an ONTAP volume. FSx for ONTAP offers two styles of volumes that you can use for different purposes, FlexVol and FlexGroup volumes. For more information, see `Volume styles <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/managing-volumes.html#volume-styles>`_ in the Amazon FSx for NetApp ONTAP User Guide.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-ontapconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    ontap_configuration_property = fsx.CfnVolume.OntapConfigurationProperty(\n        storage_virtual_machine_id="storageVirtualMachineId",\n\n        # the properties below are optional\n        aggregate_configuration=fsx.CfnVolume.AggregateConfigurationProperty(\n            aggregates=["aggregates"],\n            constituents_per_aggregate=123\n        ),\n        copy_tags_to_backups="copyTagsToBackups",\n        junction_path="junctionPath",\n        ontap_volume_type="ontapVolumeType",\n        security_style="securityStyle",\n        size_in_bytes="sizeInBytes",\n        size_in_megabytes="sizeInMegabytes",\n        snaplock_configuration=fsx.CfnVolume.SnaplockConfigurationProperty(\n            snaplock_type="snaplockType",\n\n            # the properties below are optional\n            audit_log_volume="auditLogVolume",\n            autocommit_period=fsx.CfnVolume.AutocommitPeriodProperty(\n                type="type",\n\n                # the properties below are optional\n                value=123\n            ),\n            privileged_delete="privilegedDelete",\n            retention_period=fsx.CfnVolume.SnaplockRetentionPeriodProperty(\n                default_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                    type="type",\n\n                    # the properties below are optional\n                    value=123\n                ),\n                maximum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                    type="type",\n\n                    # the properties below are optional\n                    value=123\n                ),\n                minimum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                    type="type",\n\n                    # the properties below are optional\n                    value=123\n                )\n            ),\n            volume_append_mode_enabled="volumeAppendModeEnabled"\n        ),\n        snapshot_policy="snapshotPolicy",\n        storage_efficiency_enabled="storageEfficiencyEnabled",\n        tiering_policy=fsx.CfnVolume.TieringPolicyProperty(\n            cooling_period=123,\n            name="name"\n        ),\n        volume_style="volumeStyle"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_virtual_machine_id', 'aggregate_configuration', 'copy_tags_to_backups', 'junction_path', 'ontap_volume_type', 'security_style', 'size_in_bytes', 'size_in_megabytes', 'snaplock_configuration', 'snapshot_policy', 'storage_efficiency_enabled', 'tiering_policy', 'volume_style']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.OntapConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.OpenZFSConfigurationProperty
class CfnVolume_OpenZFSConfigurationPropertyDef(BaseStruct):
    parent_volume_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ID of the volume to use as the parent volume of the volume that you are creating.\n')
    copy_tags_to_snapshots: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A Boolean value indicating whether tags for the volume should be copied to snapshots. This value defaults to ``false`` . If it's set to ``true`` , all tags for the volume are copied to snapshots where the user doesn't specify tags. If this value is ``true`` , and you specify one or more tags, only the specified tags are copied to snapshots. If you specify one or more tags when creating the snapshot, no tags are copied from the volume, regardless of this value.\n")
    data_compression_type: typing.Optional[str] = pydantic.Field(None, description="Specifies the method used to compress the data on the volume. The compression type is ``NONE`` by default. - ``NONE`` - Doesn't compress the data on the volume. ``NONE`` is the default. - ``ZSTD`` - Compresses the data in the volume using the Zstandard (ZSTD) compression algorithm. Compared to LZ4, Z-Standard provides a better compression ratio to minimize on-disk storage utilization. - ``LZ4`` - Compresses the data in the volume using the LZ4 compression algorithm. Compared to Z-Standard, LZ4 is less compute-intensive and delivers higher write throughput speeds.\n")
    nfs_exports: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_NfsExportsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The configuration object for mounting a Network File System (NFS) file system.\n')
    options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="To delete the volume's child volumes, snapshots, and clones, use the string ``DELETE_CHILD_VOLUMES_AND_SNAPSHOTS`` .\n")
    origin_snapshot: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_OriginSnapshotPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration object that specifies the snapshot to use as the origin of the data for the volume.\n')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A Boolean value indicating whether the volume is read-only.\n')
    record_size_kib: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the suggested block size for a volume in a ZFS dataset, in kibibytes (KiB). Valid values are 4, 8, 16, 32, 64, 128, 256, 512, or 1024 KiB. The default is 128 KiB. We recommend using the default setting for the majority of use cases. Generally, workloads that write in fixed small or large record sizes may benefit from setting a custom record size, like database workloads (small record size) or media streaming workloads (large record size). For additional guidance on when to set a custom record size, see `ZFS Record size <https://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/performance.html#record-size-performance>`_ in the *Amazon FSx for OpenZFS User Guide* .\n')
    storage_capacity_quota_gib: typing.Union[int, float, None] = pydantic.Field(None, description='Sets the maximum storage size in gibibytes (GiB) for the volume. You can specify a quota that is larger than the storage on the parent volume. A volume quota limits the amount of storage that the volume can consume to the configured amount, but does not guarantee the space will be available on the parent volume. To guarantee quota space, you must also set ``StorageCapacityReservationGiB`` . To *not* specify a storage capacity quota, set this to ``-1`` . For more information, see `Volume properties <https://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/managing-volumes.html#volume-properties>`_ in the *Amazon FSx for OpenZFS User Guide* .\n')
    storage_capacity_reservation_gib: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies the amount of storage in gibibytes (GiB) to reserve from the parent volume. Setting ``StorageCapacityReservationGiB`` guarantees that the specified amount of storage space on the parent volume will always be available for the volume. You can't reserve more storage than the parent volume has. To *not* specify a storage capacity reservation, set this to ``0`` or ``-1`` . For more information, see `Volume properties <https://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/managing-volumes.html#volume-properties>`_ in the *Amazon FSx for OpenZFS User Guide* .\n")
    user_and_group_quotas: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_UserAndGroupQuotasPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Configures how much storage users and groups can use on the volume.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-openzfsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    open_zFSConfiguration_property = fsx.CfnVolume.OpenZFSConfigurationProperty(\n        parent_volume_id="parentVolumeId",\n\n        # the properties below are optional\n        copy_tags_to_snapshots=False,\n        data_compression_type="dataCompressionType",\n        nfs_exports=[fsx.CfnVolume.NfsExportsProperty(\n            client_configurations=[fsx.CfnVolume.ClientConfigurationsProperty(\n                clients="clients",\n                options=["options"]\n            )]\n        )],\n        options=["options"],\n        origin_snapshot=fsx.CfnVolume.OriginSnapshotProperty(\n            copy_strategy="copyStrategy",\n            snapshot_arn="snapshotArn"\n        ),\n        read_only=False,\n        record_size_ki_b=123,\n        storage_capacity_quota_gi_b=123,\n        storage_capacity_reservation_gi_b=123,\n        user_and_group_quotas=[fsx.CfnVolume.UserAndGroupQuotasProperty(\n            id=123,\n            storage_capacity_quota_gi_b=123,\n            type="type"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['parent_volume_id', 'copy_tags_to_snapshots', 'data_compression_type', 'nfs_exports', 'options', 'origin_snapshot', 'read_only', 'record_size_kib', 'storage_capacity_quota_gib', 'storage_capacity_reservation_gib', 'user_and_group_quotas']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.OpenZFSConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.OriginSnapshotProperty
class CfnVolume_OriginSnapshotPropertyDef(BaseStruct):
    copy_strategy: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Specifies the strategy used when copying data from the snapshot to the new volume. - ``CLONE`` - The new volume references the data in the origin snapshot. Cloning a snapshot is faster than copying data from the snapshot to a new volume and doesn't consume disk throughput. However, the origin snapshot can't be deleted if there is a volume using its copied data. - ``FULL_COPY`` - Copies all data from the snapshot to the new volume. Specify this option to create the volume from a snapshot on another FSx for OpenZFS file system. .. epigraph:: The ``INCREMENTAL_COPY`` option is only for updating an existing volume by using a snapshot from another FSx for OpenZFS file system. For more information, see `CopySnapshotAndUpdateVolume <https://docs.aws.amazon.com/fsx/latest/APIReference/API_CopySnapshotAndUpdateVolume.html>`_ .\n")
    snapshot_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the snapshot to use when creating an OpenZFS volume from a snapshot.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-originsnapshot.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    origin_snapshot_property = fsx.CfnVolume.OriginSnapshotProperty(\n        copy_strategy="copyStrategy",\n        snapshot_arn="snapshotArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['copy_strategy', 'snapshot_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.OriginSnapshotProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.RetentionPeriodProperty
class CfnVolume_RetentionPeriodPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Defines the type of time for the retention period of an FSx for ONTAP SnapLock volume. Set it to one of the valid types. If you set it to ``INFINITE`` , the files are retained forever. If you set it to ``UNSPECIFIED`` , the files are retained until you set an explicit retention period.\n')
    value: typing.Union[int, float, None] = pydantic.Field(None, description='Defines the amount of time for the retention period of an FSx for ONTAP SnapLock volume. You can\'t set a value for ``INFINITE`` or ``UNSPECIFIED`` . For all other options, the following ranges are valid: - ``Seconds`` : 0 - 65,535 - ``Minutes`` : 0 - 65,535 - ``Hours`` : 0 - 24 - ``Days`` : 0 - 365 - ``Months`` : 0 - 12 - ``Years`` : 0 - 100\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-retentionperiod.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    retention_period_property = fsx.CfnVolume.RetentionPeriodProperty(\n        type="type",\n\n        # the properties below are optional\n        value=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.RetentionPeriodProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.SnaplockConfigurationProperty
class CfnVolume_SnaplockConfigurationPropertyDef(BaseStruct):
    snaplock_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Specifies the retention mode of an FSx for ONTAP SnapLock volume. After it is set, it can't be changed. You can choose one of the following retention modes: - ``COMPLIANCE`` : Files transitioned to write once, read many (WORM) on a Compliance volume can't be deleted until their retention periods expire. This retention mode is used to address government or industry-specific mandates or to protect against ransomware attacks. For more information, see `SnapLock Compliance <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/snaplock-compliance.html>`_ . - ``ENTERPRISE`` : Files transitioned to WORM on an Enterprise volume can be deleted by authorized users before their retention periods expire using privileged delete. This retention mode is used to advance an organization's data integrity and internal compliance or to test retention settings before using SnapLock Compliance. For more information, see `SnapLock Enterprise <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/snaplock-enterprise.html>`_ .\n")
    audit_log_volume: typing.Optional[str] = pydantic.Field(None, description='Enables or disables the audit log volume for an FSx for ONTAP SnapLock volume. The default value is ``false`` . If you set ``AuditLogVolume`` to ``true`` , the SnapLock volume is created as an audit log volume. The minimum retention period for an audit log volume is six months. For more information, see `SnapLock audit log volumes <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/how-snaplock-works.html#snaplock-audit-log-volume>`_ .\n')
    autocommit_period: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_AutocommitPeriodPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration object for setting the autocommit period of files in an FSx for ONTAP SnapLock volume.\n')
    privileged_delete: typing.Optional[str] = pydantic.Field(None, description="Enables, disables, or permanently disables privileged delete on an FSx for ONTAP SnapLock Enterprise volume. Enabling privileged delete allows SnapLock administrators to delete write once, read many (WORM) files even if they have active retention periods. ``PERMANENTLY_DISABLED`` is a terminal state. If privileged delete is permanently disabled on a SnapLock volume, you can't re-enable it. The default value is ``DISABLED`` . For more information, see `Privileged delete <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/snaplock-enterprise.html#privileged-delete>`_ .\n")
    retention_period: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_SnaplockRetentionPeriodPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the retention period of an FSx for ONTAP SnapLock volume.\n')
    volume_append_mode_enabled: typing.Optional[str] = pydantic.Field(None, description='Enables or disables volume-append mode on an FSx for ONTAP SnapLock volume. Volume-append mode allows you to create WORM-appendable files and write data to them incrementally. The default value is ``false`` . For more information, see `Volume-append mode <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/worm-state.html#worm-state-append>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-snaplockconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    snaplock_configuration_property = fsx.CfnVolume.SnaplockConfigurationProperty(\n        snaplock_type="snaplockType",\n\n        # the properties below are optional\n        audit_log_volume="auditLogVolume",\n        autocommit_period=fsx.CfnVolume.AutocommitPeriodProperty(\n            type="type",\n\n            # the properties below are optional\n            value=123\n        ),\n        privileged_delete="privilegedDelete",\n        retention_period=fsx.CfnVolume.SnaplockRetentionPeriodProperty(\n            default_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                type="type",\n\n                # the properties below are optional\n                value=123\n            ),\n            maximum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                type="type",\n\n                # the properties below are optional\n                value=123\n            ),\n            minimum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                type="type",\n\n                # the properties below are optional\n                value=123\n            )\n        ),\n        volume_append_mode_enabled="volumeAppendModeEnabled"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['snaplock_type', 'audit_log_volume', 'autocommit_period', 'privileged_delete', 'retention_period', 'volume_append_mode_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.SnaplockConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.SnaplockRetentionPeriodProperty
class CfnVolume_SnaplockRetentionPeriodPropertyDef(BaseStruct):
    default_retention: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_fsx.CfnVolume_RetentionPeriodPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The retention period assigned to a write once, read many (WORM) file by default if an explicit retention period is not set for an FSx for ONTAP SnapLock volume. The default retention period must be greater than or equal to the minimum retention period and less than or equal to the maximum retention period.\n')
    maximum_retention: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_fsx.CfnVolume_RetentionPeriodPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The longest retention period that can be assigned to a WORM file on an FSx for ONTAP SnapLock volume.\n')
    minimum_retention: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_fsx.CfnVolume_RetentionPeriodPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The shortest retention period that can be assigned to a WORM file on an FSx for ONTAP SnapLock volume.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-snaplockretentionperiod.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    snaplock_retention_period_property = fsx.CfnVolume.SnaplockRetentionPeriodProperty(\n        default_retention=fsx.CfnVolume.RetentionPeriodProperty(\n            type="type",\n\n            # the properties below are optional\n            value=123\n        ),\n        maximum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n            type="type",\n\n            # the properties below are optional\n            value=123\n        ),\n        minimum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n            type="type",\n\n            # the properties below are optional\n            value=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['default_retention', 'maximum_retention', 'minimum_retention']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.SnaplockRetentionPeriodProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.TieringPolicyProperty
class CfnVolume_TieringPolicyPropertyDef(BaseStruct):
    cooling_period: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the number of days that user data in a volume must remain inactive before it is considered "cold" and moved to the capacity pool. Used with the ``AUTO`` and ``SNAPSHOT_ONLY`` tiering policies. Enter a whole number between 2 and 183. Default values are 31 days for ``AUTO`` and 2 days for ``SNAPSHOT_ONLY`` .\n')
    name: typing.Optional[str] = pydantic.Field(None, description='Specifies the tiering policy used to transition data. Default value is ``SNAPSHOT_ONLY`` . - ``SNAPSHOT_ONLY`` - moves cold snapshots to the capacity pool storage tier. - ``AUTO`` - moves cold user data and snapshots to the capacity pool storage tier based on your access patterns. - ``ALL`` - moves all user data blocks in both the active file system and Snapshot copies to the storage pool tier. - ``NONE`` - keeps a volume\'s data in the primary storage tier, preventing it from being moved to the capacity pool tier.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-tieringpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    tiering_policy_property = fsx.CfnVolume.TieringPolicyProperty(\n        cooling_period=123,\n        name="name"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cooling_period', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.TieringPolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolume.UserAndGroupQuotasProperty
class CfnVolume_UserAndGroupQuotasPropertyDef(BaseStruct):
    storage_capacity_quota_gib: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description="The user or group's storage quota, in gibibytes (GiB).\n")
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether the quota applies to a user or group.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-volume-userandgroupquotas.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    user_and_group_quotas_property = fsx.CfnVolume.UserAndGroupQuotasProperty(\n        id=123,\n        storage_capacity_quota_gi_b=123,\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_capacity_quota_gib', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume.UserAndGroupQuotasProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.FileSystemAttributes
class FileSystemAttributesDef(BaseStruct):
    dns_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The DNS name assigned to this file system.\n')
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ID of the file system, assigned by Amazon FSx.\n')
    security_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.SecurityGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The security group of the file system.\n\n:exampleMetadata: infused\n\nExample::\n\n    sg = ec2.SecurityGroup.from_security_group_id(self, "FsxSecurityGroup", "{SECURITY-GROUP-ID}")\n    fs = fsx.LustreFileSystem.from_lustre_file_system_attributes(self, "FsxLustreFileSystem",\n        dns_name="{FILE-SYSTEM-DNS-NAME}",\n        file_system_id="{FILE-SYSTEM-ID}",\n        security_group=sg\n    )\n\n    vpc = ec2.Vpc.from_vpc_attributes(self, "Vpc",\n        availability_zones=["us-west-2a", "us-west-2b"],\n        public_subnet_ids=["{US-WEST-2A-SUBNET-ID}", "{US-WEST-2B-SUBNET-ID}"],\n        vpc_id="{VPC-ID}"\n    )\n\n    inst = ec2.Instance(self, "inst",\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.T2, ec2.InstanceSize.LARGE),\n        machine_image=ec2.AmazonLinuxImage(\n            generation=ec2.AmazonLinuxGeneration.AMAZON_LINUX_2\n        ),\n        vpc=vpc,\n        vpc_subnets=ec2.SubnetSelection(\n            subnet_type=ec2.SubnetType.PUBLIC\n        )\n    )\n\n    fs.connections.allow_default_port_from(inst)\n')
    _init_params: typing.ClassVar[list[str]] = ['dns_name', 'file_system_id', 'security_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.FileSystemAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.FileSystemAttributesDefConfig] = pydantic.Field(None)


class FileSystemAttributesDefConfig(pydantic.BaseModel):
    security_group_config: typing.Optional[models._interface_methods.AwsEc2ISecurityGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_fsx.FileSystemProps
class FileSystemPropsDef(BaseStruct):
    storage_capacity_gib: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The storage capacity of the file system being created. For Windows file systems, valid values are 32 GiB to 65,536 GiB. For SCRATCH_1 deployment types, valid values are 1,200, 2,400, 3,600, then continuing in increments of 3,600 GiB. For SCRATCH_2 and PERSISTENT_1 types, valid values are 1,200, 2,400, then continuing in increments of 2,400 GiB.\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The VPC to launch the file system in.\n')
    backup_id: typing.Optional[str] = pydantic.Field(None, description="The ID of the backup. Specifies the backup to use if you're creating a file system from an existing backup. Default: - no backup will be used.\n")
    kms_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key used for encryption to protect your data at rest. Default: - the aws/fsx default KMS key for the AWS account being deployed into.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Policy to apply when the file system is removed from the stack. Default: RemovalPolicy.RETAIN\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='Security Group to assign to this file system. Default: - creates new security group which allows all outbound traffic.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-fsx-filesystem.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_fsx as fsx\n    from aws_cdk import aws_kms as kms\n\n    # key: kms.Key\n    # security_group: ec2.SecurityGroup\n    # vpc: ec2.Vpc\n\n    file_system_props = fsx.FileSystemProps(\n        storage_capacity_gi_b=123,\n        vpc=vpc,\n\n        # the properties below are optional\n        backup_id="backupId",\n        kms_key=key,\n        removal_policy=cdk.RemovalPolicy.DESTROY,\n        security_group=security_group\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_capacity_gib', 'vpc', 'backup_id', 'kms_key', 'removal_policy', 'security_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.FileSystemProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.FileSystemPropsDefConfig] = pydantic.Field(None)


class FileSystemPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_fsx.LustreConfiguration
class LustreConfigurationDef(BaseStruct):
    deployment_type: typing.Union[aws_cdk.aws_fsx.LustreDeploymentType, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of backing file system deployment used by FSx.\n')
    auto_import_policy: typing.Optional[aws_cdk.aws_fsx.LustreAutoImportPolicy] = pydantic.Field(None, description='Available with ``Scratch`` and ``Persistent_1`` deployment types. When you create your file system, your existing S3 objects appear as file and directory listings. Use this property to choose how Amazon FSx keeps your file and directory listings up to date as you add or modify objects in your linked S3 bucket. ``AutoImportPolicy`` can have the following values: For more information, see `Automatically import updates from your S3 bucket <https://docs.aws.amazon.com/fsx/latest/LustreGuide/autoimport-data-repo.html>`_ . .. epigraph:: This parameter is not supported for Lustre file systems using the ``Persistent_2`` deployment type. Default: - no import policy\n')
    automatic_backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days to retain automatic backups. Setting this property to 0 disables automatic backups. You can retain automatic backups for a maximum of 90 days. Automatic Backups is not supported on scratch file systems. Default: Duration.days(0)\n')
    copy_tags_to_backups: typing.Optional[bool] = pydantic.Field(None, description='A boolean flag indicating whether tags for the file system should be copied to backups. Default: - false\n')
    daily_automatic_backup_start_time: typing.Optional[aws_cdk.aws_fsx.DailyAutomaticBackupStartTime] = pydantic.Field(None, description='Start time for 30-minute daily automatic backup window in Coordinated Universal Time (UTC). Default: - no backup window\n')
    data_compression_type: typing.Optional[aws_cdk.aws_fsx.LustreDataCompressionType] = pydantic.Field(None, description='Sets the data compression configuration for the file system. For more information, see `Lustre data compression <https://docs.aws.amazon.com/fsx/latest/LustreGuide/data-compression.html>`_ in the *Amazon FSx for Lustre User Guide* . Default: - no compression\n')
    export_path: typing.Optional[str] = pydantic.Field(None, description='The path in Amazon S3 where the root of your Amazon FSx file system is exported. The path must use the same Amazon S3 bucket as specified in ImportPath. If you only specify a bucket name, such as s3://import-bucket, you get a 1:1 mapping of file system objects to S3 bucket objects. This mapping means that the input data in S3 is overwritten on export. If you provide a custom prefix in the export path, such as s3://import-bucket/[custom-optional-prefix], Amazon FSx exports the contents of your file system to that export prefix in the Amazon S3 bucket. Default: s3://import-bucket/FSxLustre[creation-timestamp]\n')
    imported_file_chunk_size_mib: typing.Union[int, float, None] = pydantic.Field(None, description='For files imported from a data repository, this value determines the stripe count and maximum amount of data per file (in MiB) stored on a single physical disk. Allowed values are between 1 and 512,000. Default: 1024\n')
    import_path: typing.Optional[str] = pydantic.Field(None, description='The path to the Amazon S3 bucket (including the optional prefix) that you\'re using as the data repository for your Amazon FSx for Lustre file system. Must be of the format "s3://{bucketName}/optional-prefix" and cannot exceed 900 characters. Default: - no bucket is imported\n')
    per_unit_storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='Required for the PERSISTENT_1 deployment type, describes the amount of read and write throughput for each 1 tebibyte of storage, in MB/s/TiB. Valid values are 50, 100, 200. Default: - no default, conditionally required for PERSISTENT_1 deployment type\n')
    weekly_maintenance_start_time: typing.Optional[models.aws_fsx.LustreMaintenanceTimeDef] = pydantic.Field(None, description='The preferred day and time to perform weekly maintenance. The first digit is the day of the week, starting at 1 for Monday, then the following are hours and minutes in the UTC time zone, 24 hour clock. For example: \'2:20:30\' is Tuesdays at 20:30. Default: - no preference\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-fsx-filesystem-lustreconfiguration.html\n:exampleMetadata: infused\n\nExample::\n\n    from aws_cdk import aws_s3 as s3\n\n    # vpc: ec2.Vpc\n    # bucket: s3.Bucket\n\n\n    lustre_configuration = {\n        "deployment_type": fsx.LustreDeploymentType.SCRATCH_2,\n        "export_path": bucket.s3_url_for_object(),\n        "import_path": bucket.s3_url_for_object(),\n        "auto_import_policy": fsx.LustreAutoImportPolicy.NEW_CHANGED_DELETED\n    }\n\n    fs = fsx.LustreFileSystem(self, "FsxLustreFileSystem",\n        vpc=vpc,\n        vpc_subnet=vpc.private_subnets[0],\n        storage_capacity_gi_b=1200,\n        lustre_configuration=lustre_configuration\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['deployment_type', 'auto_import_policy', 'automatic_backup_retention', 'copy_tags_to_backups', 'daily_automatic_backup_start_time', 'data_compression_type', 'export_path', 'imported_file_chunk_size_mib', 'import_path', 'per_unit_storage_throughput', 'weekly_maintenance_start_time']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.LustreConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.LustreFileSystemProps
class LustreFileSystemPropsDef(BaseStruct):
    storage_capacity_gib: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The storage capacity of the file system being created. For Windows file systems, valid values are 32 GiB to 65,536 GiB. For SCRATCH_1 deployment types, valid values are 1,200, 2,400, 3,600, then continuing in increments of 3,600 GiB. For SCRATCH_2 and PERSISTENT_1 types, valid values are 1,200, 2,400, then continuing in increments of 2,400 GiB.\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The VPC to launch the file system in.\n')
    backup_id: typing.Optional[str] = pydantic.Field(None, description="The ID of the backup. Specifies the backup to use if you're creating a file system from an existing backup. Default: - no backup will be used.\n")
    kms_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key used for encryption to protect your data at rest. Default: - the aws/fsx default KMS key for the AWS account being deployed into.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Policy to apply when the file system is removed from the stack. Default: RemovalPolicy.RETAIN\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='Security Group to assign to this file system. Default: - creates new security group which allows all outbound traffic.\n')
    lustre_configuration: typing.Union[_REQUIRED_INIT_PARAM, models.aws_fsx.LustreConfigurationDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Additional configuration for FSx specific to Lustre.\n')
    vpc_subnet: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.PrivateSubnetDef, models.aws_ec2.PublicSubnetDef, models.aws_ec2.SubnetDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The subnet that the file system will be accessible from.\n\n:exampleMetadata: infused\n\nExample::\n\n    from aws_cdk import aws_s3 as s3\n\n    # vpc: ec2.Vpc\n    # bucket: s3.Bucket\n\n\n    lustre_configuration = {\n        "deployment_type": fsx.LustreDeploymentType.SCRATCH_2,\n        "export_path": bucket.s3_url_for_object(),\n        "import_path": bucket.s3_url_for_object(),\n        "auto_import_policy": fsx.LustreAutoImportPolicy.NEW_CHANGED_DELETED\n    }\n\n    fs = fsx.LustreFileSystem(self, "FsxLustreFileSystem",\n        vpc=vpc,\n        vpc_subnet=vpc.private_subnets[0],\n        storage_capacity_gi_b=1200,\n        lustre_configuration=lustre_configuration\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_capacity_gib', 'vpc', 'backup_id', 'kms_key', 'removal_policy', 'security_group', 'lustre_configuration', 'vpc_subnet']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.LustreFileSystemProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.LustreFileSystemPropsDefConfig] = pydantic.Field(None)


class LustreFileSystemPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)
    vpc_subnet_config: typing.Optional[models._interface_methods.AwsEc2ISubnetDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_fsx.LustreMaintenanceTimeProps
class LustreMaintenanceTimePropsDef(BaseStruct):
    day: typing.Union[aws_cdk.aws_fsx.Weekday, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The day of the week for maintenance to be performed.\n')
    hour: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The hour of the day (from 0-23) for maintenance to be performed.\n')
    minute: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The minute of the hour (from 0-59) for maintenance to be performed.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    lustre_maintenance_time_props = fsx.LustreMaintenanceTimeProps(\n        day=fsx.Weekday.MONDAY,\n        hour=123,\n        minute=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['day', 'hour', 'minute']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.LustreMaintenanceTimeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.LustreAutoImportPolicy
# skipping emum

#  autogenerated from aws_cdk.aws_fsx.LustreDataCompressionType
# skipping emum

#  autogenerated from aws_cdk.aws_fsx.LustreDeploymentType
# skipping emum

#  autogenerated from aws_cdk.aws_fsx.Weekday
# skipping emum

#  autogenerated from aws_cdk.aws_fsx.IFileSystem
#  skipping Interface

#  autogenerated from aws_cdk.aws_fsx.CfnDataRepositoryAssociation
class CfnDataRepositoryAssociationDef(BaseCfnResource):
    data_repository_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path to the Amazon S3 data repository that will be linked to the file system. The path can be an S3 bucket or prefix in the format ``s3://myBucket/myPrefix/`` . This path specifies where in the S3 data repository files will be imported from or exported to.\n')
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ID of the file system on which the data repository association is configured.\n')
    file_system_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A path on the Amazon FSx for Lustre file system that points to a high-level directory (such as ``/ns1/`` ) or subdirectory (such as ``/ns1/subdir/`` ) that will be mapped 1-1 with ``DataRepositoryPath`` . The leading forward slash in the name is required. Two data repository associations cannot have overlapping file system paths. For example, if a data repository is associated with file system path ``/ns1/`` , then you cannot link another data repository with file system path ``/ns1/ns2`` . This path specifies where in your file system files will be exported from or imported to. This file system directory can be linked to only one Amazon S3 bucket, and no other S3 bucket can be linked to the directory. .. epigraph:: If you specify only a forward slash ( ``/`` ) as the file system path, you can link only one data repository to the file system. You can only specify "/" as the file system path for the first data repository associated with a file system.\n')
    batch_import_meta_data_on_create: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A boolean flag indicating whether an import data repository task to import metadata should run after the data repository association is created. The task runs if this flag is set to ``true`` .\n')
    imported_file_chunk_size: typing.Union[int, float, None] = pydantic.Field(None, description='For files imported from a data repository, this value determines the stripe count and maximum amount of data per file (in MiB) stored on a single physical disk. The maximum number of disks that a single file can be striped across is limited by the total number of disks that make up the file system or cache. The default chunk size is 1,024 MiB (1 GiB) and can go as high as 512,000 MiB (500 GiB). Amazon S3 objects have a maximum size of 5 TB.\n')
    s3: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnDataRepositoryAssociation_S3PropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration for an Amazon S3 data repository linked to an Amazon FSx Lustre file system with a data repository association. The configuration defines which file events (new, changed, or deleted files or directories) are automatically imported from the linked data repository to the file system or automatically exported from the file system to the data repository.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of ``Tag`` values, with a maximum of 50 elements.')
    _init_params: typing.ClassVar[list[str]] = ['data_repository_path', 'file_system_id', 'file_system_path', 'batch_import_meta_data_on_create', 'imported_file_chunk_size', 's3', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['AutoExportPolicyProperty', 'AutoImportPolicyProperty', 'S3Property', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnDataRepositoryAssociation'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.CfnDataRepositoryAssociationDefConfig] = pydantic.Field(None)


class CfnDataRepositoryAssociationDefConfig(pydantic.BaseModel):
    AutoExportPolicyProperty: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAutoexportpolicypropertyParams]] = pydantic.Field(None, description='')
    AutoImportPolicyProperty: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAutoimportpolicypropertyParams]] = pydantic.Field(None, description='')
    S3Property: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefS3PropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_fsx.CfnDataRepositoryAssociationDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDataRepositoryAssociationDefAutoexportpolicypropertyParams(pydantic.BaseModel):
    events: typing.Sequence[str] = pydantic.Field(..., description='')
    ...

class CfnDataRepositoryAssociationDefAutoimportpolicypropertyParams(pydantic.BaseModel):
    events: typing.Sequence[str] = pydantic.Field(..., description='')
    ...

class CfnDataRepositoryAssociationDefS3PropertyParams(pydantic.BaseModel):
    auto_export_policy: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnDataRepositoryAssociation_AutoExportPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    auto_import_policy: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnDataRepositoryAssociation_AutoImportPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnDataRepositoryAssociationDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDataRepositoryAssociationDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDataRepositoryAssociationDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDataRepositoryAssociationDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDataRepositoryAssociationDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDataRepositoryAssociationDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDataRepositoryAssociationDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDataRepositoryAssociationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDataRepositoryAssociationDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDataRepositoryAssociationDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDataRepositoryAssociationDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnDataRepositoryAssociationDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDataRepositoryAssociationDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDataRepositoryAssociationDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_fsx.CfnFileSystem
class CfnFileSystemDef(BaseCfnResource):
    file_system_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of Amazon FSx file system, which can be ``LUSTRE`` , ``WINDOWS`` , ``ONTAP`` , or ``OPENZFS`` .\n')
    subnet_ids: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Specifies the IDs of the subnets that the file system will be accessible from. For Windows and ONTAP ``MULTI_AZ_1`` deployment types,provide exactly two subnet IDs, one for the preferred file server and one for the standby file server. You specify one of these subnets as the preferred subnet using the ``WindowsConfiguration > PreferredSubnetID`` or ``OntapConfiguration > PreferredSubnetID`` properties. For more information about Multi-AZ file system configuration, see `Availability and durability: Single-AZ and Multi-AZ file systems <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html>`_ in the *Amazon FSx for Windows User Guide* and `Availability and durability <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/high-availability-multiAZ.html>`_ in the *Amazon FSx for ONTAP User Guide* . For Windows ``SINGLE_AZ_1`` and ``SINGLE_AZ_2`` and all Lustre deployment types, provide exactly one subnet ID. The file server is launched in that subnet's Availability Zone.\n")
    backup_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the file system backup that you are using to create a file system. For more information, see `CreateFileSystemFromBackup <https://docs.aws.amazon.com/fsx/latest/APIReference/API_CreateFileSystemFromBackup.html>`_ .\n')
    file_system_type_version: typing.Optional[str] = pydantic.Field(None, description="For FSx for Lustre file systems, sets the Lustre version for the file system that you're creating. Valid values are ``2.10`` , ``2.12`` , and ``2.15`` : - ``2.10`` is supported by the Scratch and Persistent_1 Lustre deployment types. - ``2.12`` is supported by all Lustre deployment types, except for ``PERSISTENT_2`` with a metadata configuration mode. - ``2.15`` is supported by all Lustre deployment types and is recommended for all new file systems. Default value is ``2.10`` , except for the following deployments: - Default value is ``2.12`` when ``DeploymentType`` is set to ``PERSISTENT_2`` without a metadata configuration mode. - Default value is ``2.15`` when ``DeploymentType`` is set to ``PERSISTENT_2`` with a metadata configuration mode.\n")
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the AWS Key Management Service ( AWS KMS ) key used to encrypt Amazon FSx file system data. Used as follows with Amazon FSx file system types: - Amazon FSx for Lustre ``PERSISTENT_1`` and ``PERSISTENT_2`` deployment types only. ``SCRATCH_1`` and ``SCRATCH_2`` types are encrypted using the Amazon FSx service AWS KMS key for your account. - Amazon FSx for NetApp ONTAP - Amazon FSx for OpenZFS - Amazon FSx for Windows File Server\n')
    lustre_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_LustreConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Lustre configuration for the file system being created. .. epigraph:: The following parameters are not supported when creating Lustre file systems with a data repository association. - ``AutoImportPolicy`` - ``ExportPath`` - ``ImportedChunkSize`` - ``ImportPath``\n')
    ontap_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_OntapConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ONTAP configuration properties of the FSx for ONTAP file system that you are creating.\n')
    open_zfs_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_OpenZFSConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Amazon FSx for OpenZFS configuration properties for the file system that you are creating.\n')
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of IDs specifying the security groups to apply to all network interfaces created for file system access. This list isn't returned in later requests to describe the file system. .. epigraph:: You must specify a security group if you are creating a Multi-AZ FSx for ONTAP file system in a VPC subnet that has been shared with you.\n")
    storage_capacity: typing.Union[int, float, None] = pydantic.Field(None, description="Sets the storage capacity of the file system that you're creating. ``StorageCapacity`` is required if you are creating a new file system. It is not required if you are creating a file system by restoring a backup. *FSx for Lustre file systems* - The amount of storage capacity that you can configure depends on the value that you set for ``StorageType`` and the Lustre ``DeploymentType`` , as follows: - For ``SCRATCH_2`` , ``PERSISTENT_2`` and ``PERSISTENT_1`` deployment types using SSD storage type, the valid values are 1200 GiB, 2400 GiB, and increments of 2400 GiB. - For ``PERSISTENT_1`` HDD file systems, valid values are increments of 6000 GiB for 12 MB/s/TiB file systems and increments of 1800 GiB for 40 MB/s/TiB file systems. - For ``SCRATCH_1`` deployment type, valid values are 1200 GiB, 2400 GiB, and increments of 3600 GiB. *FSx for ONTAP file systems* - The amount of SSD storage capacity that you can configure depends on the value of the ``HAPairs`` property. The minimum value is calculated as 1,024 GiB * HAPairs and the maximum is calculated as 524,288 GiB * HAPairs, up to a maximum amount of SSD storage capacity of 1,048,576 GiB (1 pebibyte). *FSx for OpenZFS file systems* - The amount of storage capacity that you can configure is from 64 GiB up to 524,288 GiB (512 TiB). If you are creating a file system from a backup, you can specify a storage capacity equal to or greater than the original file system's storage capacity. *FSx for Windows File Server file systems* - The amount of storage capacity that you can configure depends on the value that you set for ``StorageType`` as follows: - For SSD storage, valid values are 32 GiB-65,536 GiB (64 TiB). - For HDD storage, valid values are 2000 GiB-65,536 GiB (64 TiB).\n")
    storage_type: typing.Optional[str] = pydantic.Field(None, description="Sets the storage type for the file system that you're creating. Valid values are ``SSD`` and ``HDD`` . - Set to ``SSD`` to use solid state drive storage. SSD is supported on all Windows, Lustre, ONTAP, and OpenZFS deployment types. - Set to ``HDD`` to use hard disk drive storage. HDD is supported on ``SINGLE_AZ_2`` and ``MULTI_AZ_1`` Windows file system deployment types, and on ``PERSISTENT_1`` Lustre file system deployment types. Default value is ``SSD`` . For more information, see `Storage type options <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/optimize-fsx-costs.html#storage-type-options>`_ in the *FSx for Windows File Server User Guide* and `Multiple storage options <https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html#storage-options>`_ in the *FSx for Lustre User Guide* .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The tags to associate with the file system. For more information, see `Tagging your Amazon FSx resources <https://docs.aws.amazon.com/fsx/latest/LustreGuide/tag-resources.html>`_ in the *Amazon FSx for Lustre User Guide* .\n')
    windows_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_WindowsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration object for the Microsoft Windows file system you are creating. This value is required if ``FileSystemType`` is set to ``WINDOWS`` .')
    _init_params: typing.ClassVar[list[str]] = ['file_system_type', 'subnet_ids', 'backup_id', 'file_system_type_version', 'kms_key_id', 'lustre_configuration', 'ontap_configuration', 'open_zfs_configuration', 'security_group_ids', 'storage_capacity', 'storage_type', 'tags', 'windows_configuration']
    _method_names: typing.ClassVar[list[str]] = ['AuditLogConfigurationProperty', 'ClientConfigurationsProperty', 'DiskIopsConfigurationProperty', 'LustreConfigurationProperty', 'MetadataConfigurationProperty', 'NfsExportsProperty', 'OntapConfigurationProperty', 'OpenZFSConfigurationProperty', 'RootVolumeConfigurationProperty', 'SelfManagedActiveDirectoryConfigurationProperty', 'UserAndGroupQuotasProperty', 'WindowsConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystem'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.CfnFileSystemDefConfig] = pydantic.Field(None)


class CfnFileSystemDefConfig(pydantic.BaseModel):
    AuditLogConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAuditlogconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ClientConfigurationsProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefClientconfigurationspropertyParams]] = pydantic.Field(None, description='')
    DiskIopsConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefDiskiopsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    LustreConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefLustreconfigurationpropertyParams]] = pydantic.Field(None, description='')
    MetadataConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefMetadataconfigurationpropertyParams]] = pydantic.Field(None, description='')
    NfsExportsProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefNfsexportspropertyParams]] = pydantic.Field(None, description='')
    OntapConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefOntapconfigurationpropertyParams]] = pydantic.Field(None, description='')
    OpenZFSConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefOpenzfsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    RootVolumeConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefRootvolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    SelfManagedActiveDirectoryConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefSelfmanagedactivedirectoryconfigurationpropertyParams]] = pydantic.Field(None, description='')
    UserAndGroupQuotasProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefUserandgroupquotaspropertyParams]] = pydantic.Field(None, description='')
    WindowsConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnFileSystemDefWindowsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_fsx.CfnFileSystemDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_fsx.CfnFileSystemDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_fsx.CfnFileSystemDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_fsx.CfnFileSystemDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_fsx.CfnFileSystemDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_fsx.CfnFileSystemDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_fsx.CfnFileSystemDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnFileSystemDefAuditlogconfigurationpropertyParams(pydantic.BaseModel):
    file_access_audit_log_level: str = pydantic.Field(..., description='')
    file_share_access_audit_log_level: str = pydantic.Field(..., description='')
    audit_log_destination: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefClientconfigurationspropertyParams(pydantic.BaseModel):
    clients: typing.Optional[str] = pydantic.Field(None, description='')
    options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefDiskiopsconfigurationpropertyParams(pydantic.BaseModel):
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='')
    mode: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefLustreconfigurationpropertyParams(pydantic.BaseModel):
    auto_import_policy: typing.Optional[str] = pydantic.Field(None, description='')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    copy_tags_to_backups: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    data_compression_type: typing.Optional[str] = pydantic.Field(None, description='')
    deployment_type: typing.Optional[str] = pydantic.Field(None, description='')
    drive_cache_type: typing.Optional[str] = pydantic.Field(None, description='')
    export_path: typing.Optional[str] = pydantic.Field(None, description='')
    imported_file_chunk_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    import_path: typing.Optional[str] = pydantic.Field(None, description='')
    metadata_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_MetadataConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    per_unit_storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefMetadataconfigurationpropertyParams(pydantic.BaseModel):
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='')
    mode: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefNfsexportspropertyParams(pydantic.BaseModel):
    client_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_ClientConfigurationsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefOntapconfigurationpropertyParams(pydantic.BaseModel):
    deployment_type: str = pydantic.Field(..., description='')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    disk_iops_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_DiskIopsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    endpoint_ip_address_range: typing.Optional[str] = pydantic.Field(None, description='')
    fsx_admin_password: typing.Optional[str] = pydantic.Field(None, description='')
    ha_pairs: typing.Union[int, float, None] = pydantic.Field(None, description='')
    preferred_subnet_id: typing.Optional[str] = pydantic.Field(None, description='')
    route_table_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    throughput_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    throughput_capacity_per_ha_pair: typing.Union[int, float, None] = pydantic.Field(None, description='')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefOpenzfsconfigurationpropertyParams(pydantic.BaseModel):
    deployment_type: str = pydantic.Field(..., description='')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    copy_tags_to_backups: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    copy_tags_to_volumes: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    disk_iops_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_DiskIopsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    endpoint_ip_address_range: typing.Optional[str] = pydantic.Field(None, description='')
    options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    preferred_subnet_id: typing.Optional[str] = pydantic.Field(None, description='')
    root_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_RootVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    route_table_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    throughput_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefRootvolumeconfigurationpropertyParams(pydantic.BaseModel):
    copy_tags_to_snapshots: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    data_compression_type: typing.Optional[str] = pydantic.Field(None, description='')
    nfs_exports: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_NfsExportsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    record_size_kib: typing.Union[int, float, None] = pydantic.Field(None, description='')
    user_and_group_quotas: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_UserAndGroupQuotasPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefSelfmanagedactivedirectoryconfigurationpropertyParams(pydantic.BaseModel):
    dns_ips: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    domain_name: typing.Optional[str] = pydantic.Field(None, description='')
    file_system_administrators_group: typing.Optional[str] = pydantic.Field(None, description='')
    organizational_unit_distinguished_name: typing.Optional[str] = pydantic.Field(None, description='')
    password: typing.Optional[str] = pydantic.Field(None, description='')
    user_name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefUserandgroupquotaspropertyParams(pydantic.BaseModel):
    id: typing.Union[int, float, None] = pydantic.Field(None, description='')
    storage_capacity_quota_gib: typing.Union[int, float, None] = pydantic.Field(None, description='')
    type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefWindowsconfigurationpropertyParams(pydantic.BaseModel):
    throughput_capacity: typing.Union[int, float] = pydantic.Field(..., description='')
    active_directory_id: typing.Optional[str] = pydantic.Field(None, description='')
    aliases: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    audit_log_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_AuditLogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    automatic_backup_retention_days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    copy_tags_to_backups: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    daily_automatic_backup_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    deployment_type: typing.Optional[str] = pydantic.Field(None, description='')
    disk_iops_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_DiskIopsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    preferred_subnet_id: typing.Optional[str] = pydantic.Field(None, description='')
    self_managed_active_directory_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_SelfManagedActiveDirectoryConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    weekly_maintenance_start_time: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFileSystemDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnFileSystemDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnFileSystemDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnFileSystemDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnFileSystemDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnFileSystemDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnFileSystemDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnFileSystemDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnFileSystemDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnFileSystemDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnFileSystemDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnFileSystemDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnFileSystemDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnFileSystemDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_fsx.CfnSnapshot
class CfnSnapshotDef(BaseCfnResource):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the snapshot.\n')
    volume_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ID of the volume that the snapshot is of.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of ``Tag`` values, with a maximum of 50 elements.')
    _init_params: typing.ClassVar[list[str]] = ['name', 'volume_id', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnSnapshot'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.CfnSnapshotDefConfig] = pydantic.Field(None)


class CfnSnapshotDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_fsx.CfnSnapshotDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_fsx.CfnSnapshotDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_fsx.CfnSnapshotDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_fsx.CfnSnapshotDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_fsx.CfnSnapshotDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_fsx.CfnSnapshotDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_fsx.CfnSnapshotDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_fsx.CfnSnapshotDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_fsx.CfnSnapshotDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_fsx.CfnSnapshotDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_fsx.CfnSnapshotDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_fsx.CfnSnapshotDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_fsx.CfnSnapshotDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnSnapshotDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnSnapshotDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSnapshotDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnSnapshotDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSnapshotDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnSnapshotDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnSnapshotDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnSnapshotDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnSnapshotDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnSnapshotDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSnapshotDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnSnapshotDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnSnapshotDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSnapshotDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_fsx.CfnStorageVirtualMachine
class CfnStorageVirtualMachineDef(BaseCfnResource):
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the FSx for ONTAP file system on which to create the SVM.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the SVM.\n')
    active_directory_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnStorageVirtualMachine_ActiveDirectoryConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Describes the Microsoft Active Directory configuration to which the SVM is joined, if applicable.\n')
    root_volume_security_style: typing.Optional[str] = pydantic.Field(None, description='The security style of the root volume of the SVM. Specify one of the following values:. - ``UNIX`` if the file system is managed by a UNIX administrator, the majority of users are NFS clients, and an application accessing the data uses a UNIX user as the service account. - ``NTFS`` if the file system is managed by a Microsoft Windows administrator, the majority of users are SMB clients, and an application accessing the data uses a Microsoft Windows user as the service account. - ``MIXED`` This is an advanced setting. For more information, see `Volume security style <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/volume-security-style.html>`_ in the Amazon FSx for NetApp ONTAP User Guide.\n')
    svm_admin_password: typing.Optional[str] = pydantic.Field(None, description="Specifies the password to use when logging on to the SVM using a secure shell (SSH) connection to the SVM's management endpoint. Doing so enables you to manage the SVM using the NetApp ONTAP CLI or REST API. If you do not specify a password, you can still use the file system's ``fsxadmin`` user to manage the SVM. For more information, see `Managing SVMs using the NetApp ONTAP CLI <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/managing-resources-ontap-apps.html#vsadmin-ontap-cli>`_ in the *FSx for ONTAP User Guide* .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of ``Tag`` values, with a maximum of 50 elements.')
    _init_params: typing.ClassVar[list[str]] = ['file_system_id', 'name', 'active_directory_configuration', 'root_volume_security_style', 'svm_admin_password', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['ActiveDirectoryConfigurationProperty', 'SelfManagedActiveDirectoryConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnStorageVirtualMachine'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.CfnStorageVirtualMachineDefConfig] = pydantic.Field(None)


class CfnStorageVirtualMachineDefConfig(pydantic.BaseModel):
    ActiveDirectoryConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefActivedirectoryconfigurationpropertyParams]] = pydantic.Field(None, description='')
    SelfManagedActiveDirectoryConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefSelfmanagedactivedirectoryconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_fsx.CfnStorageVirtualMachineDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnStorageVirtualMachineDefActivedirectoryconfigurationpropertyParams(pydantic.BaseModel):
    net_bios_name: typing.Optional[str] = pydantic.Field(None, description='')
    self_managed_active_directory_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnStorageVirtualMachine_SelfManagedActiveDirectoryConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnStorageVirtualMachineDefSelfmanagedactivedirectoryconfigurationpropertyParams(pydantic.BaseModel):
    dns_ips: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    domain_name: typing.Optional[str] = pydantic.Field(None, description='')
    file_system_administrators_group: typing.Optional[str] = pydantic.Field(None, description='')
    organizational_unit_distinguished_name: typing.Optional[str] = pydantic.Field(None, description='')
    password: typing.Optional[str] = pydantic.Field(None, description='')
    user_name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnStorageVirtualMachineDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnStorageVirtualMachineDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnStorageVirtualMachineDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnStorageVirtualMachineDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnStorageVirtualMachineDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnStorageVirtualMachineDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnStorageVirtualMachineDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnStorageVirtualMachineDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnStorageVirtualMachineDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnStorageVirtualMachineDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnStorageVirtualMachineDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnStorageVirtualMachineDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnStorageVirtualMachineDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnStorageVirtualMachineDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_fsx.CfnVolume
class CfnVolumeDef(BaseCfnResource):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume.\n')
    backup_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the ID of the volume backup to use to create a new volume.\n')
    ontap_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_OntapConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration of an Amazon FSx for NetApp ONTAP volume.\n')
    open_zfs_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_OpenZFSConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration of an Amazon FSx for OpenZFS volume.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource. For more information, see `Tag <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-resource-tags.html>`_ .\n')
    volume_type: typing.Optional[str] = pydantic.Field(None, description='The type of the volume.')
    _init_params: typing.ClassVar[list[str]] = ['name', 'backup_id', 'ontap_configuration', 'open_zfs_configuration', 'tags', 'volume_type']
    _method_names: typing.ClassVar[list[str]] = ['AggregateConfigurationProperty', 'AutocommitPeriodProperty', 'ClientConfigurationsProperty', 'NfsExportsProperty', 'OntapConfigurationProperty', 'OpenZFSConfigurationProperty', 'OriginSnapshotProperty', 'RetentionPeriodProperty', 'SnaplockConfigurationProperty', 'SnaplockRetentionPeriodProperty', 'TieringPolicyProperty', 'UserAndGroupQuotasProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_fsx.CfnVolumeDefConfig] = pydantic.Field(None)


class CfnVolumeDefConfig(pydantic.BaseModel):
    AggregateConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefAggregateconfigurationpropertyParams]] = pydantic.Field(None, description='')
    AutocommitPeriodProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefAutocommitperiodpropertyParams]] = pydantic.Field(None, description='')
    ClientConfigurationsProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefClientconfigurationspropertyParams]] = pydantic.Field(None, description='')
    NfsExportsProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefNfsexportspropertyParams]] = pydantic.Field(None, description='')
    OntapConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefOntapconfigurationpropertyParams]] = pydantic.Field(None, description='')
    OpenZFSConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefOpenzfsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    OriginSnapshotProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefOriginsnapshotpropertyParams]] = pydantic.Field(None, description='')
    RetentionPeriodProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefRetentionperiodpropertyParams]] = pydantic.Field(None, description='')
    SnaplockConfigurationProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefSnaplockconfigurationpropertyParams]] = pydantic.Field(None, description='')
    SnaplockRetentionPeriodProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefSnaplockretentionperiodpropertyParams]] = pydantic.Field(None, description='')
    TieringPolicyProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefTieringpolicypropertyParams]] = pydantic.Field(None, description='')
    UserAndGroupQuotasProperty: typing.Optional[list[models.aws_fsx.CfnVolumeDefUserandgroupquotaspropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_fsx.CfnVolumeDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_fsx.CfnVolumeDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_fsx.CfnVolumeDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_fsx.CfnVolumeDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_fsx.CfnVolumeDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_fsx.CfnVolumeDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_fsx.CfnVolumeDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_fsx.CfnVolumeDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_fsx.CfnVolumeDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_fsx.CfnVolumeDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_fsx.CfnVolumeDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_fsx.CfnVolumeDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_fsx.CfnVolumeDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnVolumeDefAggregateconfigurationpropertyParams(pydantic.BaseModel):
    aggregates: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    constituents_per_aggregate: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnVolumeDefAutocommitperiodpropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    value: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnVolumeDefClientconfigurationspropertyParams(pydantic.BaseModel):
    clients: str = pydantic.Field(..., description='')
    options: typing.Sequence[str] = pydantic.Field(..., description='')
    ...

class CfnVolumeDefNfsexportspropertyParams(pydantic.BaseModel):
    client_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_ClientConfigurationsPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnVolumeDefOntapconfigurationpropertyParams(pydantic.BaseModel):
    storage_virtual_machine_id: str = pydantic.Field(..., description='')
    aggregate_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_AggregateConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    copy_tags_to_backups: typing.Optional[str] = pydantic.Field(None, description='')
    junction_path: typing.Optional[str] = pydantic.Field(None, description='')
    ontap_volume_type: typing.Optional[str] = pydantic.Field(None, description='')
    security_style: typing.Optional[str] = pydantic.Field(None, description='')
    size_in_bytes: typing.Optional[str] = pydantic.Field(None, description='')
    size_in_megabytes: typing.Optional[str] = pydantic.Field(None, description='')
    snaplock_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_SnaplockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    snapshot_policy: typing.Optional[str] = pydantic.Field(None, description='')
    storage_efficiency_enabled: typing.Optional[str] = pydantic.Field(None, description='')
    tiering_policy: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_TieringPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    volume_style: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnVolumeDefOpenzfsconfigurationpropertyParams(pydantic.BaseModel):
    parent_volume_id: str = pydantic.Field(..., description='')
    copy_tags_to_snapshots: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    data_compression_type: typing.Optional[str] = pydantic.Field(None, description='')
    nfs_exports: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_NfsExportsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    origin_snapshot: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_OriginSnapshotPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    record_size_kib: typing.Union[int, float, None] = pydantic.Field(None, description='')
    storage_capacity_quota_gib: typing.Union[int, float, None] = pydantic.Field(None, description='')
    storage_capacity_reservation_gib: typing.Union[int, float, None] = pydantic.Field(None, description='')
    user_and_group_quotas: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_UserAndGroupQuotasPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnVolumeDefOriginsnapshotpropertyParams(pydantic.BaseModel):
    copy_strategy: str = pydantic.Field(..., description='')
    snapshot_arn: str = pydantic.Field(..., description='')
    ...

class CfnVolumeDefRetentionperiodpropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    value: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnVolumeDefSnaplockconfigurationpropertyParams(pydantic.BaseModel):
    snaplock_type: str = pydantic.Field(..., description='')
    audit_log_volume: typing.Optional[str] = pydantic.Field(None, description='')
    autocommit_period: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_AutocommitPeriodPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    privileged_delete: typing.Optional[str] = pydantic.Field(None, description='')
    retention_period: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_SnaplockRetentionPeriodPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    volume_append_mode_enabled: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnVolumeDefSnaplockretentionperiodpropertyParams(pydantic.BaseModel):
    default_retention: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_RetentionPeriodPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    maximum_retention: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_RetentionPeriodPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    minimum_retention: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_RetentionPeriodPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnVolumeDefTieringpolicypropertyParams(pydantic.BaseModel):
    cooling_period: typing.Union[int, float, None] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnVolumeDefUserandgroupquotaspropertyParams(pydantic.BaseModel):
    id: typing.Union[int, float] = pydantic.Field(..., description='')
    storage_capacity_quota_gib: typing.Union[int, float] = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    ...

class CfnVolumeDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnVolumeDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnVolumeDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnVolumeDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnVolumeDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnVolumeDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnVolumeDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnVolumeDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnVolumeDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnVolumeDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnVolumeDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnVolumeDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnVolumeDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnVolumeDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_fsx.CfnDataRepositoryAssociationProps
class CfnDataRepositoryAssociationPropsDef(BaseCfnProperty):
    data_repository_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path to the Amazon S3 data repository that will be linked to the file system. The path can be an S3 bucket or prefix in the format ``s3://myBucket/myPrefix/`` . This path specifies where in the S3 data repository files will be imported from or exported to.\n')
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ID of the file system on which the data repository association is configured.\n')
    file_system_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A path on the Amazon FSx for Lustre file system that points to a high-level directory (such as ``/ns1/`` ) or subdirectory (such as ``/ns1/subdir/`` ) that will be mapped 1-1 with ``DataRepositoryPath`` . The leading forward slash in the name is required. Two data repository associations cannot have overlapping file system paths. For example, if a data repository is associated with file system path ``/ns1/`` , then you cannot link another data repository with file system path ``/ns1/ns2`` . This path specifies where in your file system files will be exported from or imported to. This file system directory can be linked to only one Amazon S3 bucket, and no other S3 bucket can be linked to the directory. .. epigraph:: If you specify only a forward slash ( ``/`` ) as the file system path, you can link only one data repository to the file system. You can only specify "/" as the file system path for the first data repository associated with a file system.\n')
    batch_import_meta_data_on_create: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A boolean flag indicating whether an import data repository task to import metadata should run after the data repository association is created. The task runs if this flag is set to ``true`` .\n')
    imported_file_chunk_size: typing.Union[int, float, None] = pydantic.Field(None, description='For files imported from a data repository, this value determines the stripe count and maximum amount of data per file (in MiB) stored on a single physical disk. The maximum number of disks that a single file can be striped across is limited by the total number of disks that make up the file system or cache. The default chunk size is 1,024 MiB (1 GiB) and can go as high as 512,000 MiB (500 GiB). Amazon S3 objects have a maximum size of 5 TB.\n')
    s3: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnDataRepositoryAssociation_S3PropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration for an Amazon S3 data repository linked to an Amazon FSx Lustre file system with a data repository association. The configuration defines which file events (new, changed, or deleted files or directories) are automatically imported from the linked data repository to the file system or automatically exported from the file system to the data repository.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of ``Tag`` values, with a maximum of 50 elements.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-fsx-datarepositoryassociation.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    cfn_data_repository_association_props = fsx.CfnDataRepositoryAssociationProps(\n        data_repository_path="dataRepositoryPath",\n        file_system_id="fileSystemId",\n        file_system_path="fileSystemPath",\n\n        # the properties below are optional\n        batch_import_meta_data_on_create=False,\n        imported_file_chunk_size=123,\n        s3=fsx.CfnDataRepositoryAssociation.S3Property(\n            auto_export_policy=fsx.CfnDataRepositoryAssociation.AutoExportPolicyProperty(\n                events=["events"]\n            ),\n            auto_import_policy=fsx.CfnDataRepositoryAssociation.AutoImportPolicyProperty(\n                events=["events"]\n            )\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_repository_path', 'file_system_id', 'file_system_path', 'batch_import_meta_data_on_create', 'imported_file_chunk_size', 's3', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnDataRepositoryAssociationProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnFileSystemProps
class CfnFileSystemPropsDef(BaseCfnProperty):
    file_system_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of Amazon FSx file system, which can be ``LUSTRE`` , ``WINDOWS`` , ``ONTAP`` , or ``OPENZFS`` .\n')
    subnet_ids: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Specifies the IDs of the subnets that the file system will be accessible from. For Windows and ONTAP ``MULTI_AZ_1`` deployment types,provide exactly two subnet IDs, one for the preferred file server and one for the standby file server. You specify one of these subnets as the preferred subnet using the ``WindowsConfiguration > PreferredSubnetID`` or ``OntapConfiguration > PreferredSubnetID`` properties. For more information about Multi-AZ file system configuration, see `Availability and durability: Single-AZ and Multi-AZ file systems <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html>`_ in the *Amazon FSx for Windows User Guide* and `Availability and durability <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/high-availability-multiAZ.html>`_ in the *Amazon FSx for ONTAP User Guide* . For Windows ``SINGLE_AZ_1`` and ``SINGLE_AZ_2`` and all Lustre deployment types, provide exactly one subnet ID. The file server is launched in that subnet's Availability Zone.\n")
    backup_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the file system backup that you are using to create a file system. For more information, see `CreateFileSystemFromBackup <https://docs.aws.amazon.com/fsx/latest/APIReference/API_CreateFileSystemFromBackup.html>`_ .\n')
    file_system_type_version: typing.Optional[str] = pydantic.Field(None, description="For FSx for Lustre file systems, sets the Lustre version for the file system that you're creating. Valid values are ``2.10`` , ``2.12`` , and ``2.15`` : - ``2.10`` is supported by the Scratch and Persistent_1 Lustre deployment types. - ``2.12`` is supported by all Lustre deployment types, except for ``PERSISTENT_2`` with a metadata configuration mode. - ``2.15`` is supported by all Lustre deployment types and is recommended for all new file systems. Default value is ``2.10`` , except for the following deployments: - Default value is ``2.12`` when ``DeploymentType`` is set to ``PERSISTENT_2`` without a metadata configuration mode. - Default value is ``2.15`` when ``DeploymentType`` is set to ``PERSISTENT_2`` with a metadata configuration mode.\n")
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the AWS Key Management Service ( AWS KMS ) key used to encrypt Amazon FSx file system data. Used as follows with Amazon FSx file system types: - Amazon FSx for Lustre ``PERSISTENT_1`` and ``PERSISTENT_2`` deployment types only. ``SCRATCH_1`` and ``SCRATCH_2`` types are encrypted using the Amazon FSx service AWS KMS key for your account. - Amazon FSx for NetApp ONTAP - Amazon FSx for OpenZFS - Amazon FSx for Windows File Server\n')
    lustre_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_LustreConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Lustre configuration for the file system being created. .. epigraph:: The following parameters are not supported when creating Lustre file systems with a data repository association. - ``AutoImportPolicy`` - ``ExportPath`` - ``ImportedChunkSize`` - ``ImportPath``\n')
    ontap_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_OntapConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ONTAP configuration properties of the FSx for ONTAP file system that you are creating.\n')
    open_zfs_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_OpenZFSConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Amazon FSx for OpenZFS configuration properties for the file system that you are creating.\n')
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of IDs specifying the security groups to apply to all network interfaces created for file system access. This list isn't returned in later requests to describe the file system. .. epigraph:: You must specify a security group if you are creating a Multi-AZ FSx for ONTAP file system in a VPC subnet that has been shared with you.\n")
    storage_capacity: typing.Union[int, float, None] = pydantic.Field(None, description="Sets the storage capacity of the file system that you're creating. ``StorageCapacity`` is required if you are creating a new file system. It is not required if you are creating a file system by restoring a backup. *FSx for Lustre file systems* - The amount of storage capacity that you can configure depends on the value that you set for ``StorageType`` and the Lustre ``DeploymentType`` , as follows: - For ``SCRATCH_2`` , ``PERSISTENT_2`` and ``PERSISTENT_1`` deployment types using SSD storage type, the valid values are 1200 GiB, 2400 GiB, and increments of 2400 GiB. - For ``PERSISTENT_1`` HDD file systems, valid values are increments of 6000 GiB for 12 MB/s/TiB file systems and increments of 1800 GiB for 40 MB/s/TiB file systems. - For ``SCRATCH_1`` deployment type, valid values are 1200 GiB, 2400 GiB, and increments of 3600 GiB. *FSx for ONTAP file systems* - The amount of SSD storage capacity that you can configure depends on the value of the ``HAPairs`` property. The minimum value is calculated as 1,024 GiB * HAPairs and the maximum is calculated as 524,288 GiB * HAPairs, up to a maximum amount of SSD storage capacity of 1,048,576 GiB (1 pebibyte). *FSx for OpenZFS file systems* - The amount of storage capacity that you can configure is from 64 GiB up to 524,288 GiB (512 TiB). If you are creating a file system from a backup, you can specify a storage capacity equal to or greater than the original file system's storage capacity. *FSx for Windows File Server file systems* - The amount of storage capacity that you can configure depends on the value that you set for ``StorageType`` as follows: - For SSD storage, valid values are 32 GiB-65,536 GiB (64 TiB). - For HDD storage, valid values are 2000 GiB-65,536 GiB (64 TiB).\n")
    storage_type: typing.Optional[str] = pydantic.Field(None, description="Sets the storage type for the file system that you're creating. Valid values are ``SSD`` and ``HDD`` . - Set to ``SSD`` to use solid state drive storage. SSD is supported on all Windows, Lustre, ONTAP, and OpenZFS deployment types. - Set to ``HDD`` to use hard disk drive storage. HDD is supported on ``SINGLE_AZ_2`` and ``MULTI_AZ_1`` Windows file system deployment types, and on ``PERSISTENT_1`` Lustre file system deployment types. Default value is ``SSD`` . For more information, see `Storage type options <https://docs.aws.amazon.com/fsx/latest/WindowsGuide/optimize-fsx-costs.html#storage-type-options>`_ in the *FSx for Windows File Server User Guide* and `Multiple storage options <https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html#storage-options>`_ in the *FSx for Lustre User Guide* .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The tags to associate with the file system. For more information, see `Tagging your Amazon FSx resources <https://docs.aws.amazon.com/fsx/latest/LustreGuide/tag-resources.html>`_ in the *Amazon FSx for Lustre User Guide* .\n')
    windows_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnFileSystem_WindowsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration object for the Microsoft Windows file system you are creating. This value is required if ``FileSystemType`` is set to ``WINDOWS`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-fsx-filesystem.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    cfn_file_system_props = fsx.CfnFileSystemProps(\n        file_system_type="fileSystemType",\n        subnet_ids=["subnetIds"],\n\n        # the properties below are optional\n        backup_id="backupId",\n        file_system_type_version="fileSystemTypeVersion",\n        kms_key_id="kmsKeyId",\n        lustre_configuration=fsx.CfnFileSystem.LustreConfigurationProperty(\n            auto_import_policy="autoImportPolicy",\n            automatic_backup_retention_days=123,\n            copy_tags_to_backups=False,\n            daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n            data_compression_type="dataCompressionType",\n            deployment_type="deploymentType",\n            drive_cache_type="driveCacheType",\n            export_path="exportPath",\n            imported_file_chunk_size=123,\n            import_path="importPath",\n            metadata_configuration=fsx.CfnFileSystem.MetadataConfigurationProperty(\n                iops=123,\n                mode="mode"\n            ),\n            per_unit_storage_throughput=123,\n            weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n        ),\n        ontap_configuration=fsx.CfnFileSystem.OntapConfigurationProperty(\n            deployment_type="deploymentType",\n\n            # the properties below are optional\n            automatic_backup_retention_days=123,\n            daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n            disk_iops_configuration=fsx.CfnFileSystem.DiskIopsConfigurationProperty(\n                iops=123,\n                mode="mode"\n            ),\n            endpoint_ip_address_range="endpointIpAddressRange",\n            fsx_admin_password="fsxAdminPassword",\n            ha_pairs=123,\n            preferred_subnet_id="preferredSubnetId",\n            route_table_ids=["routeTableIds"],\n            throughput_capacity=123,\n            throughput_capacity_per_ha_pair=123,\n            weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n        ),\n        open_zfs_configuration=fsx.CfnFileSystem.OpenZFSConfigurationProperty(\n            deployment_type="deploymentType",\n\n            # the properties below are optional\n            automatic_backup_retention_days=123,\n            copy_tags_to_backups=False,\n            copy_tags_to_volumes=False,\n            daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n            disk_iops_configuration=fsx.CfnFileSystem.DiskIopsConfigurationProperty(\n                iops=123,\n                mode="mode"\n            ),\n            endpoint_ip_address_range="endpointIpAddressRange",\n            options=["options"],\n            preferred_subnet_id="preferredSubnetId",\n            root_volume_configuration=fsx.CfnFileSystem.RootVolumeConfigurationProperty(\n                copy_tags_to_snapshots=False,\n                data_compression_type="dataCompressionType",\n                nfs_exports=[fsx.CfnFileSystem.NfsExportsProperty(\n                    client_configurations=[fsx.CfnFileSystem.ClientConfigurationsProperty(\n                        clients="clients",\n                        options=["options"]\n                    )]\n                )],\n                read_only=False,\n                record_size_ki_b=123,\n                user_and_group_quotas=[fsx.CfnFileSystem.UserAndGroupQuotasProperty(\n                    id=123,\n                    storage_capacity_quota_gi_b=123,\n                    type="type"\n                )]\n            ),\n            route_table_ids=["routeTableIds"],\n            throughput_capacity=123,\n            weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n        ),\n        security_group_ids=["securityGroupIds"],\n        storage_capacity=123,\n        storage_type="storageType",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        windows_configuration=fsx.CfnFileSystem.WindowsConfigurationProperty(\n            throughput_capacity=123,\n\n            # the properties below are optional\n            active_directory_id="activeDirectoryId",\n            aliases=["aliases"],\n            audit_log_configuration=fsx.CfnFileSystem.AuditLogConfigurationProperty(\n                file_access_audit_log_level="fileAccessAuditLogLevel",\n                file_share_access_audit_log_level="fileShareAccessAuditLogLevel",\n\n                # the properties below are optional\n                audit_log_destination="auditLogDestination"\n            ),\n            automatic_backup_retention_days=123,\n            copy_tags_to_backups=False,\n            daily_automatic_backup_start_time="dailyAutomaticBackupStartTime",\n            deployment_type="deploymentType",\n            disk_iops_configuration=fsx.CfnFileSystem.DiskIopsConfigurationProperty(\n                iops=123,\n                mode="mode"\n            ),\n            preferred_subnet_id="preferredSubnetId",\n            self_managed_active_directory_configuration=fsx.CfnFileSystem.SelfManagedActiveDirectoryConfigurationProperty(\n                dns_ips=["dnsIps"],\n                domain_name="domainName",\n                file_system_administrators_group="fileSystemAdministratorsGroup",\n                organizational_unit_distinguished_name="organizationalUnitDistinguishedName",\n                password="password",\n                user_name="userName"\n            ),\n            weekly_maintenance_start_time="weeklyMaintenanceStartTime"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_system_type', 'subnet_ids', 'backup_id', 'file_system_type_version', 'kms_key_id', 'lustre_configuration', 'ontap_configuration', 'open_zfs_configuration', 'security_group_ids', 'storage_capacity', 'storage_type', 'tags', 'windows_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnFileSystemProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnSnapshotProps
class CfnSnapshotPropsDef(BaseCfnProperty):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the snapshot.\n')
    volume_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ID of the volume that the snapshot is of.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of ``Tag`` values, with a maximum of 50 elements.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-fsx-snapshot.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    cfn_snapshot_props = fsx.CfnSnapshotProps(\n        name="name",\n        volume_id="volumeId",\n\n        # the properties below are optional\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'volume_id', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnSnapshotProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnStorageVirtualMachineProps
class CfnStorageVirtualMachinePropsDef(BaseCfnProperty):
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the FSx for ONTAP file system on which to create the SVM.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the SVM.\n')
    active_directory_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnStorageVirtualMachine_ActiveDirectoryConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Describes the Microsoft Active Directory configuration to which the SVM is joined, if applicable.\n')
    root_volume_security_style: typing.Optional[str] = pydantic.Field(None, description='The security style of the root volume of the SVM. Specify one of the following values:. - ``UNIX`` if the file system is managed by a UNIX administrator, the majority of users are NFS clients, and an application accessing the data uses a UNIX user as the service account. - ``NTFS`` if the file system is managed by a Microsoft Windows administrator, the majority of users are SMB clients, and an application accessing the data uses a Microsoft Windows user as the service account. - ``MIXED`` This is an advanced setting. For more information, see `Volume security style <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/volume-security-style.html>`_ in the Amazon FSx for NetApp ONTAP User Guide.\n')
    svm_admin_password: typing.Optional[str] = pydantic.Field(None, description="Specifies the password to use when logging on to the SVM using a secure shell (SSH) connection to the SVM's management endpoint. Doing so enables you to manage the SVM using the NetApp ONTAP CLI or REST API. If you do not specify a password, you can still use the file system's ``fsxadmin`` user to manage the SVM. For more information, see `Managing SVMs using the NetApp ONTAP CLI <https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/managing-resources-ontap-apps.html#vsadmin-ontap-cli>`_ in the *FSx for ONTAP User Guide* .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of ``Tag`` values, with a maximum of 50 elements.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-fsx-storagevirtualmachine.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    cfn_storage_virtual_machine_props = fsx.CfnStorageVirtualMachineProps(\n        file_system_id="fileSystemId",\n        name="name",\n\n        # the properties below are optional\n        active_directory_configuration=fsx.CfnStorageVirtualMachine.ActiveDirectoryConfigurationProperty(\n            net_bios_name="netBiosName",\n            self_managed_active_directory_configuration=fsx.CfnStorageVirtualMachine.SelfManagedActiveDirectoryConfigurationProperty(\n                dns_ips=["dnsIps"],\n                domain_name="domainName",\n                file_system_administrators_group="fileSystemAdministratorsGroup",\n                organizational_unit_distinguished_name="organizationalUnitDistinguishedName",\n                password="password",\n                user_name="userName"\n            )\n        ),\n        root_volume_security_style="rootVolumeSecurityStyle",\n        svm_admin_password="svmAdminPassword",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_system_id', 'name', 'active_directory_configuration', 'root_volume_security_style', 'svm_admin_password', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnStorageVirtualMachineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_fsx.CfnVolumeProps
class CfnVolumePropsDef(BaseCfnProperty):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume.\n')
    backup_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the ID of the volume backup to use to create a new volume.\n')
    ontap_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_OntapConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration of an Amazon FSx for NetApp ONTAP volume.\n')
    open_zfs_configuration: typing.Union[models.UnsupportedResource, models.aws_fsx.CfnVolume_OpenZFSConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration of an Amazon FSx for OpenZFS volume.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource. For more information, see `Tag <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-resource-tags.html>`_ .\n')
    volume_type: typing.Optional[str] = pydantic.Field(None, description='The type of the volume.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-fsx-volume.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_fsx as fsx\n\n    cfn_volume_props = fsx.CfnVolumeProps(\n        name="name",\n\n        # the properties below are optional\n        backup_id="backupId",\n        ontap_configuration=fsx.CfnVolume.OntapConfigurationProperty(\n            storage_virtual_machine_id="storageVirtualMachineId",\n\n            # the properties below are optional\n            aggregate_configuration=fsx.CfnVolume.AggregateConfigurationProperty(\n                aggregates=["aggregates"],\n                constituents_per_aggregate=123\n            ),\n            copy_tags_to_backups="copyTagsToBackups",\n            junction_path="junctionPath",\n            ontap_volume_type="ontapVolumeType",\n            security_style="securityStyle",\n            size_in_bytes="sizeInBytes",\n            size_in_megabytes="sizeInMegabytes",\n            snaplock_configuration=fsx.CfnVolume.SnaplockConfigurationProperty(\n                snaplock_type="snaplockType",\n\n                # the properties below are optional\n                audit_log_volume="auditLogVolume",\n                autocommit_period=fsx.CfnVolume.AutocommitPeriodProperty(\n                    type="type",\n\n                    # the properties below are optional\n                    value=123\n                ),\n                privileged_delete="privilegedDelete",\n                retention_period=fsx.CfnVolume.SnaplockRetentionPeriodProperty(\n                    default_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                        type="type",\n\n                        # the properties below are optional\n                        value=123\n                    ),\n                    maximum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                        type="type",\n\n                        # the properties below are optional\n                        value=123\n                    ),\n                    minimum_retention=fsx.CfnVolume.RetentionPeriodProperty(\n                        type="type",\n\n                        # the properties below are optional\n                        value=123\n                    )\n                ),\n                volume_append_mode_enabled="volumeAppendModeEnabled"\n            ),\n            snapshot_policy="snapshotPolicy",\n            storage_efficiency_enabled="storageEfficiencyEnabled",\n            tiering_policy=fsx.CfnVolume.TieringPolicyProperty(\n                cooling_period=123,\n                name="name"\n            ),\n            volume_style="volumeStyle"\n        ),\n        open_zfs_configuration=fsx.CfnVolume.OpenZFSConfigurationProperty(\n            parent_volume_id="parentVolumeId",\n\n            # the properties below are optional\n            copy_tags_to_snapshots=False,\n            data_compression_type="dataCompressionType",\n            nfs_exports=[fsx.CfnVolume.NfsExportsProperty(\n                client_configurations=[fsx.CfnVolume.ClientConfigurationsProperty(\n                    clients="clients",\n                    options=["options"]\n                )]\n            )],\n            options=["options"],\n            origin_snapshot=fsx.CfnVolume.OriginSnapshotProperty(\n                copy_strategy="copyStrategy",\n                snapshot_arn="snapshotArn"\n            ),\n            read_only=False,\n            record_size_ki_b=123,\n            storage_capacity_quota_gi_b=123,\n            storage_capacity_reservation_gi_b=123,\n            user_and_group_quotas=[fsx.CfnVolume.UserAndGroupQuotasProperty(\n                id=123,\n                storage_capacity_quota_gi_b=123,\n                type="type"\n            )]\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        volume_type="volumeType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'backup_id', 'ontap_configuration', 'open_zfs_configuration', 'tags', 'volume_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_fsx.CfnVolumeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    FileSystemBase: typing.Optional[dict[str, models.aws_fsx.FileSystemBaseDef]] = pydantic.Field(None)
    LustreMaintenanceTime: typing.Optional[dict[str, models.aws_fsx.LustreMaintenanceTimeDef]] = pydantic.Field(None)
    LustreFileSystem: typing.Optional[dict[str, models.aws_fsx.LustreFileSystemDef]] = pydantic.Field(None)
    CfnDataRepositoryAssociation_AutoExportPolicyProperty: typing.Optional[dict[str, models.aws_fsx.CfnDataRepositoryAssociation_AutoExportPolicyPropertyDef]] = pydantic.Field(None)
    CfnDataRepositoryAssociation_AutoImportPolicyProperty: typing.Optional[dict[str, models.aws_fsx.CfnDataRepositoryAssociation_AutoImportPolicyPropertyDef]] = pydantic.Field(None)
    CfnDataRepositoryAssociation_S3Property: typing.Optional[dict[str, models.aws_fsx.CfnDataRepositoryAssociation_S3PropertyDef]] = pydantic.Field(None)
    CfnFileSystem_AuditLogConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_AuditLogConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_ClientConfigurationsProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_ClientConfigurationsPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_DiskIopsConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_DiskIopsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_LustreConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_LustreConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_MetadataConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_MetadataConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_NfsExportsProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_NfsExportsPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_OntapConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_OntapConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_OpenZFSConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_OpenZFSConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_RootVolumeConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_RootVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_SelfManagedActiveDirectoryConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_SelfManagedActiveDirectoryConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_UserAndGroupQuotasProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_UserAndGroupQuotasPropertyDef]] = pydantic.Field(None)
    CfnFileSystem_WindowsConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnFileSystem_WindowsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnStorageVirtualMachine_ActiveDirectoryConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnStorageVirtualMachine_ActiveDirectoryConfigurationPropertyDef]] = pydantic.Field(None)
    CfnStorageVirtualMachine_SelfManagedActiveDirectoryConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnStorageVirtualMachine_SelfManagedActiveDirectoryConfigurationPropertyDef]] = pydantic.Field(None)
    CfnVolume_AggregateConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_AggregateConfigurationPropertyDef]] = pydantic.Field(None)
    CfnVolume_AutocommitPeriodProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_AutocommitPeriodPropertyDef]] = pydantic.Field(None)
    CfnVolume_ClientConfigurationsProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_ClientConfigurationsPropertyDef]] = pydantic.Field(None)
    CfnVolume_NfsExportsProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_NfsExportsPropertyDef]] = pydantic.Field(None)
    CfnVolume_OntapConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_OntapConfigurationPropertyDef]] = pydantic.Field(None)
    CfnVolume_OpenZFSConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_OpenZFSConfigurationPropertyDef]] = pydantic.Field(None)
    CfnVolume_OriginSnapshotProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_OriginSnapshotPropertyDef]] = pydantic.Field(None)
    CfnVolume_RetentionPeriodProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_RetentionPeriodPropertyDef]] = pydantic.Field(None)
    CfnVolume_SnaplockConfigurationProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_SnaplockConfigurationPropertyDef]] = pydantic.Field(None)
    CfnVolume_SnaplockRetentionPeriodProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_SnaplockRetentionPeriodPropertyDef]] = pydantic.Field(None)
    CfnVolume_TieringPolicyProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_TieringPolicyPropertyDef]] = pydantic.Field(None)
    CfnVolume_UserAndGroupQuotasProperty: typing.Optional[dict[str, models.aws_fsx.CfnVolume_UserAndGroupQuotasPropertyDef]] = pydantic.Field(None)
    FileSystemAttributes: typing.Optional[dict[str, models.aws_fsx.FileSystemAttributesDef]] = pydantic.Field(None)
    FileSystemProps: typing.Optional[dict[str, models.aws_fsx.FileSystemPropsDef]] = pydantic.Field(None)
    LustreConfiguration: typing.Optional[dict[str, models.aws_fsx.LustreConfigurationDef]] = pydantic.Field(None)
    LustreFileSystemProps: typing.Optional[dict[str, models.aws_fsx.LustreFileSystemPropsDef]] = pydantic.Field(None)
    LustreMaintenanceTimeProps: typing.Optional[dict[str, models.aws_fsx.LustreMaintenanceTimePropsDef]] = pydantic.Field(None)
    CfnDataRepositoryAssociation: typing.Optional[dict[str, models.aws_fsx.CfnDataRepositoryAssociationDef]] = pydantic.Field(None)
    CfnFileSystem: typing.Optional[dict[str, models.aws_fsx.CfnFileSystemDef]] = pydantic.Field(None)
    CfnSnapshot: typing.Optional[dict[str, models.aws_fsx.CfnSnapshotDef]] = pydantic.Field(None)
    CfnStorageVirtualMachine: typing.Optional[dict[str, models.aws_fsx.CfnStorageVirtualMachineDef]] = pydantic.Field(None)
    CfnVolume: typing.Optional[dict[str, models.aws_fsx.CfnVolumeDef]] = pydantic.Field(None)
    CfnDataRepositoryAssociationProps: typing.Optional[dict[str, models.aws_fsx.CfnDataRepositoryAssociationPropsDef]] = pydantic.Field(None)
    CfnFileSystemProps: typing.Optional[dict[str, models.aws_fsx.CfnFileSystemPropsDef]] = pydantic.Field(None)
    CfnSnapshotProps: typing.Optional[dict[str, models.aws_fsx.CfnSnapshotPropsDef]] = pydantic.Field(None)
    CfnStorageVirtualMachineProps: typing.Optional[dict[str, models.aws_fsx.CfnStorageVirtualMachinePropsDef]] = pydantic.Field(None)
    CfnVolumeProps: typing.Optional[dict[str, models.aws_fsx.CfnVolumePropsDef]] = pydantic.Field(None)
    ...

import models
