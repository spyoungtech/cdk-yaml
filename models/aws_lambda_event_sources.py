from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_lambda_event_sources.ApiEventSource
class ApiEventSourceDef(BaseClass):
    method: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-\n')
    api_key_required: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the method requires clients to submit a valid API key. Default: false\n')
    authorization_scopes: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of authorization scopes configured on the method. The scopes are used with a COGNITO_USER_POOLS authorizer to authorize the method invocation. Default: - no authorization scopes\n')
    authorization_type: typing.Optional[aws_cdk.aws_apigateway.AuthorizationType] = pydantic.Field(None, description="Method authorization. If the value is set of ``Custom``, an ``authorizer`` must also be specified. If you're using one of the authorizers that are available via the ``Authorizer`` class, such as ``Authorizer#token()``, it is recommended that this option not be specified. The authorizer will take care of setting the correct authorization type. However, specifying an authorization type using this property that conflicts with what is expected by the ``Authorizer`` will result in an error. Default: - open access unless ``authorizer`` is specified\n")
    authorizer: typing.Optional[typing.Union[models.aws_apigateway.AuthorizerDef, models.aws_apigateway.CognitoUserPoolsAuthorizerDef, models.aws_apigateway.CognitoUserPoolsAuthorizerDef, models.aws_apigateway.RequestAuthorizerDef, models.aws_apigateway.RequestAuthorizerDef, models.aws_apigateway.TokenAuthorizerDef, models.aws_apigateway.TokenAuthorizerDef]] = pydantic.Field(None, description='If ``authorizationType`` is ``Custom``, this specifies the ID of the method authorizer resource. If specified, the value of ``authorizationType`` must be set to ``Custom``\n')
    method_responses: typing.Optional[typing.Sequence[typing.Union[models.aws_apigateway.MethodResponseDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The responses that can be sent to the client who calls the method. Default: None This property is not required, but if these are not supplied for a Lambda proxy integration, the Lambda function must return a value of the correct format, for the integration response to be correctly mapped to a response to the client.\n')
    operation_name: typing.Optional[str] = pydantic.Field(None, description='A friendly operation name for the method. For example, you can assign the OperationName of ListPets for the GET /pets method.\n')
    request_models: typing.Optional[typing.Mapping[str, typing.Union[models.aws_apigateway.ModelDef]]] = pydantic.Field(None, description="The models which describe data structure of request payload. When combined with ``requestValidator`` or ``requestValidatorOptions``, the service will validate the API request payload before it reaches the API's Integration (including proxies). Specify ``requestModels`` as key-value pairs, with a content type (e.g. ``'application/json'``) as the key and an API Gateway Model as the value.\n")
    request_parameters: typing.Optional[typing.Mapping[str, bool]] = pydantic.Field(None, description='The request parameters that API Gateway accepts. Specify request parameters as key-value pairs (string-to-Boolean mapping), with a source as the key and a Boolean as the value. The Boolean specifies whether a parameter is required. A source must match the format method.request.location.name, where the location is querystring, path, or header, and name is a valid, unique parameter name. Default: None\n')
    request_validator: typing.Optional[typing.Union[models.aws_apigateway.RequestValidatorDef]] = pydantic.Field(None, description='The ID of the associated request validator. Only one of ``requestValidator`` or ``requestValidatorOptions`` must be specified. Works together with ``requestModels`` or ``requestParameters`` to validate the request before it reaches integration like Lambda Proxy Integration. Default: - No default validator\n')
    request_validator_options: typing.Union[models.aws_apigateway.RequestValidatorOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Request validator options to create new validator Only one of ``requestValidator`` or ``requestValidatorOptions`` must be specified. Works together with ``requestModels`` or ``requestParameters`` to validate the request before it reaches integration like Lambda Proxy Integration. Default: - No default validator')
    _init_params: typing.ClassVar[list[str]] = ['method', 'path', 'api_key_required', 'authorization_scopes', 'authorization_type', 'authorizer', 'method_responses', 'operation_name', 'request_models', 'request_parameters', 'request_validator', 'request_validator_options']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.ApiEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ApiEventSourceDefConfig] = pydantic.Field(None)


class ApiEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ApiEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')

class ApiEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.DynamoEventSource
class DynamoEventSourceDef(BaseClass):
    table: typing.Union[_REQUIRED_INIT_PARAM, models.aws_dynamodb.TableBaseDef, models.aws_dynamodb.TableDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - the retention period configured on the stream\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: - discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of retry attempts Valid Range: * Minimum value of 0 * Maximum value of 10000. Default: - retry until the record expires\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis Valid Range: 0 - 15 minutes. Default: - None\n')
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.')
    _init_params: typing.ClassVar[list[str]] = ['table', 'bisect_batch_on_error', 'filters', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'tumbling_window', 'starting_position', 'batch_size', 'enabled', 'max_batching_window']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.DynamoEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoEventSourceDefConfig] = pydantic.Field(None)


class DynamoEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[DynamoEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')

class DynamoEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.KinesisEventSource
class KinesisEventSourceDef(BaseClass):
    stream: typing.Union[_REQUIRED_INIT_PARAM, models.aws_kinesis.StreamDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - the retention period configured on the stream\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: - discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of retry attempts Valid Range: * Minimum value of 0 * Maximum value of 10000. Default: - retry until the record expires\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis Valid Range: 0 - 15 minutes. Default: - None\n')
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.')
    _init_params: typing.ClassVar[list[str]] = ['stream', 'starting_position_timestamp', 'bisect_batch_on_error', 'filters', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'tumbling_window', 'starting_position', 'batch_size', 'enabled', 'max_batching_window']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.KinesisEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[KinesisEventSourceDefConfig] = pydantic.Field(None)


class KinesisEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[KinesisEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')
    stream_config: typing.Optional[models._interface_methods.AwsKinesisIStreamDefConfig] = pydantic.Field(None)

class KinesisEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.ManagedKafkaEventSource
class ManagedKafkaEventSourceDef(BaseClass):
    cluster_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An MSK cluster construct.')
    topic: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Kafka topic to subscribe to.\n')
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. Default: - none\n")
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret with the Kafka credentials, see https://docs.aws.amazon.com/msk/latest/developerguide/msk-password.html for details This field is required if your Kafka brokers are accessed over the Internet. Default: none\n')
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.')
    _init_params: typing.ClassVar[list[str]] = ['cluster_arn', 'topic', 'consumer_group_id', 'filters', 'secret', 'starting_position', 'batch_size', 'enabled', 'max_batching_window']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.ManagedKafkaEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ManagedKafkaEventSourceDefConfig] = pydantic.Field(None)


class ManagedKafkaEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ManagedKafkaEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')

class ManagedKafkaEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.S3EventSource
class S3EventSourceDef(BaseClass):
    bucket: typing.Union[models.aws_s3.BucketDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    events: typing.Union[typing.Sequence[aws_cdk.aws_s3.EventType], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The s3 event types that will trigger the notification.\n')
    filters: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.NotificationKeyFilterDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='S3 object key filter rules to determine which objects trigger this event. Each filter must include a ``prefix`` and/or ``suffix`` that will be matched against the s3 object key. Refer to the S3 Developer Guide for details about allowed filter rules.')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'events', 'filters']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.S3EventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[S3EventSourceDefConfig] = pydantic.Field(None)


class S3EventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[S3EventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')
    bucket_config: typing.Optional[models.aws_s3.BucketDefConfig] = pydantic.Field(None)

class S3EventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.SelfManagedKafkaEventSource
class SelfManagedKafkaEventSourceDef(BaseClass):
    bootstrap_servers: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The list of host and port pairs that are the addresses of the Kafka brokers in a "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.xyz.com:xxxx``.')
    authentication_method: typing.Optional[aws_cdk.aws_lambda_event_sources.AuthenticationMethod] = pydantic.Field(None, description='The authentication method for your Kafka cluster. Default: AuthenticationMethod.SASL_SCRAM_512_AUTH\n')
    root_ca_certificate: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret with the root CA certificate used by your Kafka brokers for TLS encryption This field is required if your Kafka brokers use certificates signed by a private CA. Default: - none\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='If your Kafka brokers are only reachable via VPC, provide the security group here. Default: - none, required if setting vpc\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='If your Kafka brokers are only reachable via VPC provide the VPC here. Default: none\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If your Kafka brokers are only reachable via VPC, provide the subnets selection here. Default: - none, required if setting vpc\n')
    topic: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Kafka topic to subscribe to.\n')
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. Default: - none\n")
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret with the Kafka credentials, see https://docs.aws.amazon.com/msk/latest/developerguide/msk-password.html for details This field is required if your Kafka brokers are accessed over the Internet. Default: none\n')
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.')
    _init_params: typing.ClassVar[list[str]] = ['bootstrap_servers', 'authentication_method', 'root_ca_certificate', 'security_group', 'vpc', 'vpc_subnets', 'topic', 'consumer_group_id', 'filters', 'secret', 'starting_position', 'batch_size', 'enabled', 'max_batching_window']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SelfManagedKafkaEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SelfManagedKafkaEventSourceDefConfig] = pydantic.Field(None)


class SelfManagedKafkaEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[SelfManagedKafkaEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')

class SelfManagedKafkaEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.SnsDlq
class SnsDlqDef(BaseClass):
    topic: typing.Union[_REQUIRED_INIT_PARAM, models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['topic']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SnsDlq'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SnsDlqDefConfig] = pydantic.Field(None)


class SnsDlqDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[SnsDlqDefBindParams]] = pydantic.Field(None, description='Returns a destination configuration for the DLQ.')

class SnsDlqDefBindParams(pydantic.BaseModel):
    _target: typing.Union[models.aws_lambda.EventSourceMappingDef] = pydantic.Field(..., description='-\n')
    target_handler: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.SnsEventSource
class SnsEventSourceDef(BaseClass):
    topic: typing.Union[_REQUIRED_INIT_PARAM, models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='Queue to be used as dead letter queue. If not passed no dead letter queue is enabled. Default: - No dead letter queue enabled.\n')
    filter_policy: typing.Optional[typing.Mapping[str, models.aws_sns.SubscriptionFilterDef]] = pydantic.Field(None, description='The filter policy. Default: - all messages are delivered\n')
    filter_policy_with_message_body: typing.Optional[typing.Mapping[str, models.aws_sns.FilterOrPolicyDef]] = pydantic.Field(None, description='The filter policy that is applied on the message body. To apply a filter policy to the message attributes, use ``filterPolicy``. A maximum of one of ``filterPolicyWithMessageBody`` and ``filterPolicy`` may be used. Default: - all messages are delivered')
    _init_params: typing.ClassVar[list[str]] = ['topic', 'dead_letter_queue', 'filter_policy', 'filter_policy_with_message_body']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SnsEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SnsEventSourceDefConfig] = pydantic.Field(None)


class SnsEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[SnsEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')
    topic_config: typing.Optional[models._interface_methods.AwsSnsITopicDefConfig] = pydantic.Field(None)

class SnsEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.SqsDlq
class SqsDlqDef(BaseClass):
    queue: typing.Union[_REQUIRED_INIT_PARAM, models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['queue']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SqsDlq'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqsDlqDefConfig] = pydantic.Field(None)


class SqsDlqDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[SqsDlqDefBindParams]] = pydantic.Field(None, description='Returns a destination configuration for the DLQ.')

class SqsDlqDefBindParams(pydantic.BaseModel):
    _target: typing.Union[models.aws_lambda.EventSourceMappingDef] = pydantic.Field(..., description='-\n')
    target_handler: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.SqsEventSource
class SqsEventSourceDef(BaseClass):
    queue: typing.Union[_REQUIRED_INIT_PARAM, models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10. If ``maxBatchingWindow`` is configured, this value can go up to 10,000. Default: 10\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the SQS event source mapping should be enabled. Default: true\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Valid Range: Minimum value of 0 minutes. Maximum value of 5 minutes. Default: - no batching window. The lambda function will be invoked immediately with the records that are available.\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['queue', 'batch_size', 'enabled', 'filters', 'max_batching_window', 'max_concurrency', 'report_batch_item_failures']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SqsEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqsEventSourceDefConfig] = pydantic.Field(None)


class SqsEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[SqsEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')
    queue_config: typing.Optional[models._interface_methods.AwsSqsIQueueDefConfig] = pydantic.Field(None)

class SqsEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.StreamEventSource
class StreamEventSourceDef(BaseClass):
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - the retention period configured on the stream\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: - discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of retry attempts Valid Range: * Minimum value of 0 * Maximum value of 10000. Default: - retry until the record expires\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis Valid Range: 0 - 15 minutes. Default: - None\n')
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.')
    _init_params: typing.ClassVar[list[str]] = ['bisect_batch_on_error', 'filters', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'tumbling_window', 'starting_position', 'batch_size', 'enabled', 'max_batching_window']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.StreamEventSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StreamEventSourceDefConfig] = pydantic.Field(None)


class StreamEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[StreamEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')

class StreamEventSourceDefBindParams(pydantic.BaseModel):
    _target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda_event_sources.BaseStreamEventSourceProps
class BaseStreamEventSourcePropsDef(BaseStruct):
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n    from aws_cdk import aws_lambda_event_sources as lambda_event_sources\n\n    base_stream_event_source_props = lambda_event_sources.BaseStreamEventSourceProps(\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON,\n\n        # the properties below are optional\n        batch_size=123,\n        enabled=False,\n        max_batching_window=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['starting_position', 'batch_size', 'enabled', 'max_batching_window']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.BaseStreamEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.DynamoEventSourceProps
class DynamoEventSourcePropsDef(BaseStruct):
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - the retention period configured on the stream\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: - discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of retry attempts Valid Range: * Minimum value of 0 * Maximum value of 10000. Default: - retry until the record expires\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis Valid Range: 0 - 15 minutes. Default: - None\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_lambda_event_sources as eventsources\n    import aws_cdk.aws_dynamodb as dynamodb\n\n    # fn: lambda.Function\n\n    table = dynamodb.Table(self, "Table",\n        partition_key=dynamodb.Attribute(\n            name="id",\n            type=dynamodb.AttributeType.STRING\n        ),\n        stream=dynamodb.StreamViewType.NEW_IMAGE\n    )\n    fn.add_event_source(eventsources.DynamoEventSource(table,\n        starting_position=lambda_.StartingPosition.LATEST,\n        filters=[lambda_.FilterCriteria.filter({"event_name": lambda_.FilterRule.is_equal("INSERT")})]\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['starting_position', 'batch_size', 'enabled', 'max_batching_window', 'bisect_batch_on_error', 'filters', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'tumbling_window']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.DynamoEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.KafkaEventSourceProps
class KafkaEventSourcePropsDef(BaseStruct):
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.\n')
    topic: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Kafka topic to subscribe to.\n')
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. Default: - none\n")
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret with the Kafka credentials, see https://docs.aws.amazon.com/msk/latest/developerguide/msk-password.html for details This field is required if your Kafka brokers are accessed over the Internet. Default: none\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n    from aws_cdk import aws_lambda_event_sources as lambda_event_sources\n    from aws_cdk import aws_secretsmanager as secretsmanager\n\n    # filters: Any\n    # secret: secretsmanager.Secret\n\n    kafka_event_source_props = lambda_event_sources.KafkaEventSourceProps(\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON,\n        topic="topic",\n\n        # the properties below are optional\n        batch_size=123,\n        consumer_group_id="consumerGroupId",\n        enabled=False,\n        filters=[{\n            "filters_key": filters\n        }],\n        max_batching_window=cdk.Duration.minutes(30),\n        secret=secret\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['starting_position', 'batch_size', 'enabled', 'max_batching_window', 'topic', 'consumer_group_id', 'filters', 'secret']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.KafkaEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.KinesisEventSourceProps
class KinesisEventSourcePropsDef(BaseStruct):
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - the retention period configured on the stream\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: - discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of retry attempts Valid Range: * Minimum value of 0 * Maximum value of 10000. Default: - retry until the record expires\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis Valid Range: 0 - 15 minutes. Default: - None\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_kinesis as kinesis\n    from aws_cdk.aws_lambda_event_sources import KinesisEventSource\n\n    # my_function: lambda.Function\n\n\n    stream = kinesis.Stream(self, "MyStream")\n    my_function.add_event_source(KinesisEventSource(stream,\n        batch_size=100,  # default\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['starting_position', 'batch_size', 'enabled', 'max_batching_window', 'bisect_batch_on_error', 'filters', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'tumbling_window', 'starting_position_timestamp']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.KinesisEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.ManagedKafkaEventSourceProps
class ManagedKafkaEventSourcePropsDef(BaseStruct):
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.\n')
    topic: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Kafka topic to subscribe to.\n')
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. Default: - none\n")
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret with the Kafka credentials, see https://docs.aws.amazon.com/msk/latest/developerguide/msk-password.html for details This field is required if your Kafka brokers are accessed over the Internet. Default: none\n')
    cluster_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An MSK cluster construct.\n\n:exampleMetadata: infused\n\nExample::\n\n    from aws_cdk.aws_secretsmanager import Secret\n    from aws_cdk.aws_lambda_event_sources import ManagedKafkaEventSource\n\n    # my_function: lambda.Function\n\n\n    # Your MSK cluster arn\n    cluster_arn = "arn:aws:kafka:us-east-1:0123456789019:cluster/SalesCluster/abcd1234-abcd-cafe-abab-9876543210ab-4"\n\n    # The Kafka topic you want to subscribe to\n    topic = "some-cool-topic"\n\n    # The secret that allows access to your MSK cluster\n    # You still have to make sure that it is associated with your cluster as described in the documentation\n    secret = Secret(self, "Secret", secret_name="AmazonMSK_KafkaSecret")\n    my_function.add_event_source(ManagedKafkaEventSource(\n        cluster_arn=cluster_arn,\n        topic=topic,\n        secret=secret,\n        batch_size=100,  # default\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['starting_position', 'batch_size', 'enabled', 'max_batching_window', 'topic', 'consumer_group_id', 'filters', 'secret', 'cluster_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.ManagedKafkaEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.S3EventSourceProps
class S3EventSourcePropsDef(BaseStruct):
    events: typing.Union[typing.Sequence[aws_cdk.aws_s3.EventType], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The s3 event types that will trigger the notification.')
    filters: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.NotificationKeyFilterDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='S3 object key filter rules to determine which objects trigger this event. Each filter must include a ``prefix`` and/or ``suffix`` that will be matched against the s3 object key. Refer to the S3 Developer Guide for details about allowed filter rules.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_lambda_event_sources as eventsources\n    import aws_cdk.aws_s3 as s3\n\n    # fn: lambda.Function\n\n    bucket = s3.Bucket(self, "Bucket")\n    fn.add_event_source(eventsources.S3EventSource(bucket,\n        events=[s3.EventType.OBJECT_CREATED, s3.EventType.OBJECT_REMOVED],\n        filters=[s3.NotificationKeyFilter(prefix="subdir/")]\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['events', 'filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.S3EventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.SelfManagedKafkaEventSourceProps
class SelfManagedKafkaEventSourcePropsDef(BaseStruct):
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.\n')
    topic: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Kafka topic to subscribe to.\n')
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. Default: - none\n")
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret with the Kafka credentials, see https://docs.aws.amazon.com/msk/latest/developerguide/msk-password.html for details This field is required if your Kafka brokers are accessed over the Internet. Default: none\n')
    bootstrap_servers: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The list of host and port pairs that are the addresses of the Kafka brokers in a "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.xyz.com:xxxx``.\n')
    authentication_method: typing.Optional[aws_cdk.aws_lambda_event_sources.AuthenticationMethod] = pydantic.Field(None, description='The authentication method for your Kafka cluster. Default: AuthenticationMethod.SASL_SCRAM_512_AUTH\n')
    root_ca_certificate: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret with the root CA certificate used by your Kafka brokers for TLS encryption This field is required if your Kafka brokers use certificates signed by a private CA. Default: - none\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='If your Kafka brokers are only reachable via VPC, provide the security group here. Default: - none, required if setting vpc\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='If your Kafka brokers are only reachable via VPC provide the VPC here. Default: none\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If your Kafka brokers are only reachable via VPC, provide the subnets selection here. Default: - none, required if setting vpc\n\n:exampleMetadata: infused\n\nExample::\n\n    from aws_cdk.aws_secretsmanager import Secret\n    from aws_cdk.aws_lambda_event_sources import SelfManagedKafkaEventSource\n\n    # The secret that allows access to your self hosted Kafka cluster\n    # secret: Secret\n\n    # my_function: lambda.Function\n\n\n    # The list of Kafka brokers\n    bootstrap_servers = ["kafka-broker:9092"]\n\n    # The Kafka topic you want to subscribe to\n    topic = "some-cool-topic"\n\n    # (Optional) The consumer group id to use when connecting to the Kafka broker. If omitted the UUID of the event source mapping will be used.\n    consumer_group_id = "my-consumer-group-id"\n    my_function.add_event_source(SelfManagedKafkaEventSource(\n        bootstrap_servers=bootstrap_servers,\n        topic=topic,\n        consumer_group_id=consumer_group_id,\n        secret=secret,\n        batch_size=100,  # default\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['starting_position', 'batch_size', 'enabled', 'max_batching_window', 'topic', 'consumer_group_id', 'filters', 'secret', 'bootstrap_servers', 'authentication_method', 'root_ca_certificate', 'security_group', 'vpc', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SelfManagedKafkaEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.SnsEventSourceProps
class SnsEventSourcePropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='Queue to be used as dead letter queue. If not passed no dead letter queue is enabled. Default: - No dead letter queue enabled.\n')
    filter_policy: typing.Optional[typing.Mapping[str, models.aws_sns.SubscriptionFilterDef]] = pydantic.Field(None, description='The filter policy. Default: - all messages are delivered\n')
    filter_policy_with_message_body: typing.Optional[typing.Mapping[str, models.aws_sns.FilterOrPolicyDef]] = pydantic.Field(None, description='The filter policy that is applied on the message body. To apply a filter policy to the message attributes, use ``filterPolicy``. A maximum of one of ``filterPolicyWithMessageBody`` and ``filterPolicy`` may be used. Default: - all messages are delivered\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_sns as sns\n    from aws_cdk.aws_lambda_event_sources import SnsEventSource\n\n    # topic: sns.Topic\n\n    # fn: lambda.Function\n\n    dead_letter_queue = sqs.Queue(self, "deadLetterQueue")\n    fn.add_event_source(SnsEventSource(topic,\n        filter_policy={},\n        dead_letter_queue=dead_letter_queue\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'filter_policy', 'filter_policy_with_message_body']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SnsEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.SqsEventSourceProps
class SqsEventSourcePropsDef(BaseStruct):
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10. If ``maxBatchingWindow`` is configured, this value can go up to 10,000. Default: 10')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the SQS event source mapping should be enabled. Default: true\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Valid Range: Minimum value of 0 minutes. Maximum value of 5 minutes. Default: - no batching window. The lambda function will be invoked immediately with the records that are available.\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    from aws_cdk.aws_lambda_event_sources import SqsEventSource\n    # fn: lambda.Function\n\n\n    queue = sqs.Queue(self, "MyQueue",\n        visibility_timeout=Duration.seconds(30)\n    )\n\n    fn.add_event_source(SqsEventSource(queue,\n        batch_size=10,  # default\n        max_batching_window=Duration.minutes(5),\n        report_batch_item_failures=True\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['batch_size', 'enabled', 'filters', 'max_batching_window', 'max_concurrency', 'report_batch_item_failures']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.SqsEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.StreamEventSourceProps
class StreamEventSourcePropsDef(BaseStruct):
    starting_position: typing.Union[aws_cdk.aws_lambda.StartingPosition, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Where to begin consuming the stream.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: - Minimum value of 1 - Maximum value of: - 1000 for ``DynamoEventSource`` - 10000 for ``KinesisEventSource``, ``ManagedKafkaEventSource`` and ``SelfManagedKafkaEventSource`` Default: 100\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='If the stream event source mapping should be enabled. Default: true\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5). Default: - Duration.seconds(0) for Kinesis, DynamoDB, and SQS event sources, Duration.millis(500) for MSK, self-managed Kafka, and Amazon MQ.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria option. Default: - None\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - the retention period configured on the stream\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: - discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of retry attempts Valid Range: * Minimum value of 0 * Maximum value of 10000. Default: - retry until the record expires\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis Valid Range: 0 - 15 minutes. Default: - None\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n    from aws_cdk import aws_lambda_event_sources as lambda_event_sources\n\n    # event_source_dlq: lambda.IEventSourceDlq\n    # filters: Any\n\n    stream_event_source_props = lambda_event_sources.StreamEventSourceProps(\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON,\n\n        # the properties below are optional\n        batch_size=123,\n        bisect_batch_on_error=False,\n        enabled=False,\n        filters=[{\n            "filters_key": filters\n        }],\n        max_batching_window=cdk.Duration.minutes(30),\n        max_record_age=cdk.Duration.minutes(30),\n        on_failure=event_source_dlq,\n        parallelization_factor=123,\n        report_batch_item_failures=False,\n        retry_attempts=123,\n        tumbling_window=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['starting_position', 'batch_size', 'enabled', 'max_batching_window', 'bisect_batch_on_error', 'filters', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'tumbling_window']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_event_sources.StreamEventSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_event_sources.AuthenticationMethod
# skipping emum

import models

class ModuleModel(pydantic.BaseModel):
    ApiEventSource: typing.Optional[dict[str, ApiEventSourceDef]] = pydantic.Field(None)
    DynamoEventSource: typing.Optional[dict[str, DynamoEventSourceDef]] = pydantic.Field(None)
    KinesisEventSource: typing.Optional[dict[str, KinesisEventSourceDef]] = pydantic.Field(None)
    ManagedKafkaEventSource: typing.Optional[dict[str, ManagedKafkaEventSourceDef]] = pydantic.Field(None)
    S3EventSource: typing.Optional[dict[str, S3EventSourceDef]] = pydantic.Field(None)
    SelfManagedKafkaEventSource: typing.Optional[dict[str, SelfManagedKafkaEventSourceDef]] = pydantic.Field(None)
    SnsDlq: typing.Optional[dict[str, SnsDlqDef]] = pydantic.Field(None)
    SnsEventSource: typing.Optional[dict[str, SnsEventSourceDef]] = pydantic.Field(None)
    SqsDlq: typing.Optional[dict[str, SqsDlqDef]] = pydantic.Field(None)
    SqsEventSource: typing.Optional[dict[str, SqsEventSourceDef]] = pydantic.Field(None)
    StreamEventSource: typing.Optional[dict[str, StreamEventSourceDef]] = pydantic.Field(None)
    BaseStreamEventSourceProps: typing.Optional[dict[str, BaseStreamEventSourcePropsDef]] = pydantic.Field(None)
    DynamoEventSourceProps: typing.Optional[dict[str, DynamoEventSourcePropsDef]] = pydantic.Field(None)
    KafkaEventSourceProps: typing.Optional[dict[str, KafkaEventSourcePropsDef]] = pydantic.Field(None)
    KinesisEventSourceProps: typing.Optional[dict[str, KinesisEventSourcePropsDef]] = pydantic.Field(None)
    ManagedKafkaEventSourceProps: typing.Optional[dict[str, ManagedKafkaEventSourcePropsDef]] = pydantic.Field(None)
    S3EventSourceProps: typing.Optional[dict[str, S3EventSourcePropsDef]] = pydantic.Field(None)
    SelfManagedKafkaEventSourceProps: typing.Optional[dict[str, SelfManagedKafkaEventSourcePropsDef]] = pydantic.Field(None)
    SnsEventSourceProps: typing.Optional[dict[str, SnsEventSourcePropsDef]] = pydantic.Field(None)
    SqsEventSourceProps: typing.Optional[dict[str, SqsEventSourcePropsDef]] = pydantic.Field(None)
    StreamEventSourceProps: typing.Optional[dict[str, StreamEventSourcePropsDef]] = pydantic.Field(None)
    ...
