from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams

#  autogenerated from aws_cdk.aws_databrew.CfnDataset.CsvOptionsProperty
class CfnDataset_CsvOptionsPropertyDef(BaseStruct):
    delimiter: typing.Optional[str] = pydantic.Field(None, description='A single character that specifies the delimiter being used in the CSV file.\n')
    header_row: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-csvoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    csv_options_property = databrew.CfnDataset.CsvOptionsProperty(\n        delimiter="delimiter",\n        header_row=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['delimiter', 'header_row']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.CsvOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.DatabaseInputDefinitionProperty
class CfnDataset_DatabaseInputDefinitionPropertyDef(BaseStruct):
    glue_connection_name: str = pydantic.Field(..., description='The AWS Glue Connection that stores the connection information for the target database.\n')
    database_table_name: typing.Optional[str] = pydantic.Field(None, description='The table within the target database.\n')
    query_string: typing.Optional[str] = pydantic.Field(None, description='Custom SQL to run against the provided AWS Glue connection. This SQL will be used as the input for DataBrew projects and jobs.\n')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An Amazon location that AWS Glue Data Catalog can use as a temporary directory.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-databaseinputdefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    database_input_definition_property = databrew.CfnDataset.DatabaseInputDefinitionProperty(\n        glue_connection_name="glueConnectionName",\n\n        # the properties below are optional\n        database_table_name="databaseTableName",\n        query_string="queryString",\n        temp_directory=databrew.CfnDataset.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['glue_connection_name', 'database_table_name', 'query_string', 'temp_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.DatabaseInputDefinitionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.DataCatalogInputDefinitionProperty
class CfnDataset_DataCatalogInputDefinitionPropertyDef(BaseStruct):
    catalog_id: typing.Optional[str] = pydantic.Field(None, description='The unique identifier of the AWS account that holds the Data Catalog that stores the data.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of a database in the Data Catalog.\n')
    table_name: typing.Optional[str] = pydantic.Field(None, description='The name of a database table in the Data Catalog. This table corresponds to a DataBrew dataset.\n')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An Amazon location that AWS Glue Data Catalog can use as a temporary directory.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-datacataloginputdefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    data_catalog_input_definition_property = databrew.CfnDataset.DataCatalogInputDefinitionProperty(\n        catalog_id="catalogId",\n        database_name="databaseName",\n        table_name="tableName",\n        temp_directory=databrew.CfnDataset.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['catalog_id', 'database_name', 'table_name', 'temp_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.DataCatalogInputDefinitionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.DatasetParameterProperty
class CfnDataset_DatasetParameterPropertyDef(BaseStruct):
    name: str = pydantic.Field(..., description="The name of the parameter that is used in the dataset's Amazon S3 path.\n")
    type: str = pydantic.Field(..., description="The type of the dataset parameter, can be one of a 'String', 'Number' or 'Datetime'.\n")
    create_column: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Optional boolean value that defines whether the captured value of this parameter should be loaded as an additional column in the dataset.\n')
    datetime_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DatetimeOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional parameter options such as a format and a timezone. Required for datetime parameters.\n')
    filter: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilterExpressionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The optional filter expression structure to apply additional matching criteria to the parameter.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-datasetparameter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    dataset_parameter_property = databrew.CfnDataset.DatasetParameterProperty(\n        name="name",\n        type="type",\n\n        # the properties below are optional\n        create_column=False,\n        datetime_options=databrew.CfnDataset.DatetimeOptionsProperty(\n            format="format",\n\n            # the properties below are optional\n            locale_code="localeCode",\n            timezone_offset="timezoneOffset"\n        ),\n        filter=databrew.CfnDataset.FilterExpressionProperty(\n            expression="expression",\n            values_map=[databrew.CfnDataset.FilterValueProperty(\n                value="value",\n                value_reference="valueReference"\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'type', 'create_column', 'datetime_options', 'filter']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.DatasetParameterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.DatetimeOptionsProperty
class CfnDataset_DatetimeOptionsPropertyDef(BaseStruct):
    format: str = pydantic.Field(..., description='Required option, that defines the datetime format used for a date parameter in the Amazon S3 path. Should use only supported datetime specifiers and separation characters, all litera a-z or A-Z character should be escaped with single quotes. E.g. "MM.dd.yyyy-\'at\'-HH:mm".\n')
    locale_code: typing.Optional[str] = pydantic.Field(None, description='Optional value for a non-US locale code, needed for correct interpretation of some date formats.\n')
    timezone_offset: typing.Optional[str] = pydantic.Field(None, description='Optional value for a timezone offset of the datetime parameter value in the Amazon S3 path. Shouldn\'t be used if Format for this parameter includes timezone fields. If no offset specified, UTC is assumed.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-datetimeoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    datetime_options_property = databrew.CfnDataset.DatetimeOptionsProperty(\n        format="format",\n\n        # the properties below are optional\n        locale_code="localeCode",\n        timezone_offset="timezoneOffset"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['format', 'locale_code', 'timezone_offset']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.DatetimeOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.ExcelOptionsProperty
class CfnDataset_ExcelOptionsPropertyDef(BaseStruct):
    header_row: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n')
    sheet_indexes: typing.Union[typing.Sequence[typing.Union[int, float]], typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='One or more sheet numbers in the Excel file that will be included in the dataset.\n')
    sheet_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more named sheets in the Excel file that will be included in the dataset.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-exceloptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    excel_options_property = databrew.CfnDataset.ExcelOptionsProperty(\n        header_row=False,\n        sheet_indexes=[123],\n        sheet_names=["sheetNames"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['header_row', 'sheet_indexes', 'sheet_names']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.ExcelOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.FilesLimitProperty
class CfnDataset_FilesLimitPropertyDef(BaseStruct):
    max_files: typing.Union[int, float] = pydantic.Field(..., description='The number of Amazon S3 files to select.\n')
    order: typing.Optional[str] = pydantic.Field(None, description='A criteria to use for Amazon S3 files sorting before their selection. By default uses DESCENDING order, i.e. most recent files are selected first. Anotherpossible value is ASCENDING.\n')
    ordered_by: typing.Optional[str] = pydantic.Field(None, description='A criteria to use for Amazon S3 files sorting before their selection. By default uses LAST_MODIFIED_DATE as a sorting criteria. Currently it\'s the only allowed value.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-fileslimit.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    files_limit_property = databrew.CfnDataset.FilesLimitProperty(\n        max_files=123,\n\n        # the properties below are optional\n        order="order",\n        ordered_by="orderedBy"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_files', 'order', 'ordered_by']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.FilesLimitProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.FilterExpressionProperty
class CfnDataset_FilterExpressionPropertyDef(BaseStruct):
    expression: str = pydantic.Field(..., description='The expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, "(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)". Substitution variables should start with \':\' symbol.\n')
    values_map: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilterValuePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='The map of substitution variable names to their values used in this filter expression.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-filterexpression.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    filter_expression_property = databrew.CfnDataset.FilterExpressionProperty(\n        expression="expression",\n        values_map=[databrew.CfnDataset.FilterValueProperty(\n            value="value",\n            value_reference="valueReference"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['expression', 'values_map']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.FilterExpressionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.FilterValueProperty
class CfnDataset_FilterValuePropertyDef(BaseStruct):
    value: str = pydantic.Field(..., description='The value to be associated with the substitution variable.\n')
    value_reference: str = pydantic.Field(..., description='The substitution variable reference.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-filtervalue.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    filter_value_property = databrew.CfnDataset.FilterValueProperty(\n        value="value",\n        value_reference="valueReference"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['value', 'value_reference']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.FilterValueProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.FormatOptionsProperty
class CfnDataset_FormatOptionsPropertyDef(BaseStruct):
    csv: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_CsvOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options that define how CSV input is to be interpreted by DataBrew.\n')
    excel: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_ExcelOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options that define how Excel input is to be interpreted by DataBrew.\n')
    json_: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_JsonOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options that define how JSON input is to be interpreted by DataBrew.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-formatoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    format_options_property = databrew.CfnDataset.FormatOptionsProperty(\n        csv=databrew.CfnDataset.CsvOptionsProperty(\n            delimiter="delimiter",\n            header_row=False\n        ),\n        excel=databrew.CfnDataset.ExcelOptionsProperty(\n            header_row=False,\n            sheet_indexes=[123],\n            sheet_names=["sheetNames"]\n        ),\n        json=databrew.CfnDataset.JsonOptionsProperty(\n            multi_line=False\n        )\n    )\n', alias='json')
    _init_params: typing.ClassVar[list[str]] = ['csv', 'excel', 'json']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.FormatOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.InputProperty
class CfnDataset_InputPropertyDef(BaseStruct):
    database_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DatabaseInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Connection information for dataset input files stored in a database.\n')
    data_catalog_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DataCatalogInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The AWS Glue Data Catalog parameters for the data.\n')
    metadata: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_MetadataPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Contains additional resource information needed for specific datasets.\n')
    s3_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Amazon S3 location where the data is stored.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-input.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    input_property = databrew.CfnDataset.InputProperty(\n        database_input_definition=databrew.CfnDataset.DatabaseInputDefinitionProperty(\n            glue_connection_name="glueConnectionName",\n\n            # the properties below are optional\n            database_table_name="databaseTableName",\n            query_string="queryString",\n            temp_directory=databrew.CfnDataset.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                key="key"\n            )\n        ),\n        data_catalog_input_definition=databrew.CfnDataset.DataCatalogInputDefinitionProperty(\n            catalog_id="catalogId",\n            database_name="databaseName",\n            table_name="tableName",\n            temp_directory=databrew.CfnDataset.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                key="key"\n            )\n        ),\n        metadata=databrew.CfnDataset.MetadataProperty(\n            source_arn="sourceArn"\n        ),\n        s3_input_definition=databrew.CfnDataset.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['database_input_definition', 'data_catalog_input_definition', 'metadata', 's3_input_definition']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.InputProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.JsonOptionsProperty
class CfnDataset_JsonOptionsPropertyDef(BaseStruct):
    multi_line: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that specifies whether JSON input contains embedded new line characters.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-jsonoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    json_options_property = databrew.CfnDataset.JsonOptionsProperty(\n        multi_line=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['multi_line']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.JsonOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.MetadataProperty
class CfnDataset_MetadataPropertyDef(BaseStruct):
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) associated with the dataset. Currently, DataBrew only supports ARNs from Amazon AppFlow.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-metadata.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    metadata_property = databrew.CfnDataset.MetadataProperty(\n        source_arn="sourceArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['source_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.MetadataProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.PathOptionsProperty
class CfnDataset_PathOptionsPropertyDef(BaseStruct):
    files_limit: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilesLimitPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If provided, this structure imposes a limit on a number of files that should be selected.\n')
    last_modified_date_condition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilterExpressionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If provided, this structure defines a date range for matching Amazon S3 objects based on their LastModifiedDate attribute in Amazon S3 .\n')
    parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_PathParameterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A structure that maps names of parameters used in the Amazon S3 path of a dataset to their definitions.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-pathoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    path_options_property = databrew.CfnDataset.PathOptionsProperty(\n        files_limit=databrew.CfnDataset.FilesLimitProperty(\n            max_files=123,\n\n            # the properties below are optional\n            order="order",\n            ordered_by="orderedBy"\n        ),\n        last_modified_date_condition=databrew.CfnDataset.FilterExpressionProperty(\n            expression="expression",\n            values_map=[databrew.CfnDataset.FilterValueProperty(\n                value="value",\n                value_reference="valueReference"\n            )]\n        ),\n        parameters=[databrew.CfnDataset.PathParameterProperty(\n            dataset_parameter=databrew.CfnDataset.DatasetParameterProperty(\n                name="name",\n                type="type",\n\n                # the properties below are optional\n                create_column=False,\n                datetime_options=databrew.CfnDataset.DatetimeOptionsProperty(\n                    format="format",\n\n                    # the properties below are optional\n                    locale_code="localeCode",\n                    timezone_offset="timezoneOffset"\n                ),\n                filter=databrew.CfnDataset.FilterExpressionProperty(\n                    expression="expression",\n                    values_map=[databrew.CfnDataset.FilterValueProperty(\n                        value="value",\n                        value_reference="valueReference"\n                    )]\n                )\n            ),\n            path_parameter_name="pathParameterName"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['files_limit', 'last_modified_date_condition', 'parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.PathOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.PathParameterProperty
class CfnDataset_PathParameterPropertyDef(BaseStruct):
    dataset_parameter: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DatasetParameterPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The path parameter definition.\n')
    path_parameter_name: str = pydantic.Field(..., description='The name of the path parameter.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-pathparameter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    path_parameter_property = databrew.CfnDataset.PathParameterProperty(\n        dataset_parameter=databrew.CfnDataset.DatasetParameterProperty(\n            name="name",\n            type="type",\n\n            # the properties below are optional\n            create_column=False,\n            datetime_options=databrew.CfnDataset.DatetimeOptionsProperty(\n                format="format",\n\n                # the properties below are optional\n                locale_code="localeCode",\n                timezone_offset="timezoneOffset"\n            ),\n            filter=databrew.CfnDataset.FilterExpressionProperty(\n                expression="expression",\n                values_map=[databrew.CfnDataset.FilterValueProperty(\n                    value="value",\n                    value_reference="valueReference"\n                )]\n            )\n        ),\n        path_parameter_name="pathParameterName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dataset_parameter', 'path_parameter_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.PathParameterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset.S3LocationProperty
class CfnDataset_S3LocationPropertyDef(BaseStruct):
    bucket: str = pydantic.Field(..., description='The Amazon S3 bucket name.\n')
    key: typing.Optional[str] = pydantic.Field(None, description='The unique name of the object in the bucket.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-dataset-s3location.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    s3_location_property = databrew.CfnDataset.S3LocationProperty(\n        bucket="bucket",\n\n        # the properties below are optional\n        key="key"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset.S3LocationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.AllowedStatisticsProperty
class CfnJob_AllowedStatisticsPropertyDef(BaseStruct):
    statistics: typing.Sequence[str] = pydantic.Field(..., description='One or more column statistics to allow for columns that contain detected entities.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-allowedstatistics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    allowed_statistics_property = databrew.CfnJob.AllowedStatisticsProperty(\n        statistics=["statistics"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['statistics']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.AllowedStatisticsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.ColumnSelectorProperty
class CfnJob_ColumnSelectorPropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of a column from a dataset.\n')
    regex: typing.Optional[str] = pydantic.Field(None, description='A regular expression for selecting a column from a dataset.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-columnselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    column_selector_property = databrew.CfnJob.ColumnSelectorProperty(\n        name="name",\n        regex="regex"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'regex']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.ColumnSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.ColumnStatisticsConfigurationProperty
class CfnJob_ColumnStatisticsConfigurationPropertyDef(BaseStruct):
    statistics: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_StatisticsConfigurationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='Configuration for evaluations. Statistics can be used to select evaluations and override parameters of evaluations.\n')
    selectors: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ColumnSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='List of column selectors. Selectors can be used to select columns from the dataset. When selectors are undefined, configuration will be applied to all supported columns.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-columnstatisticsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    column_statistics_configuration_property = databrew.CfnJob.ColumnStatisticsConfigurationProperty(\n        statistics=databrew.CfnJob.StatisticsConfigurationProperty(\n            included_statistics=["includedStatistics"],\n            overrides=[databrew.CfnJob.StatisticOverrideProperty(\n                parameters={\n                    "parameters_key": "parameters"\n                },\n                statistic="statistic"\n            )]\n        ),\n\n        # the properties below are optional\n        selectors=[databrew.CfnJob.ColumnSelectorProperty(\n            name="name",\n            regex="regex"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['statistics', 'selectors']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.ColumnStatisticsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.CsvOutputOptionsProperty
class CfnJob_CsvOutputOptionsPropertyDef(BaseStruct):
    delimiter: typing.Optional[str] = pydantic.Field(None, description='A single character that specifies the delimiter used to create CSV job output.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-csvoutputoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    csv_output_options_property = databrew.CfnJob.CsvOutputOptionsProperty(\n        delimiter="delimiter"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['delimiter']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.CsvOutputOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.DatabaseOutputProperty
class CfnJob_DatabaseOutputPropertyDef(BaseStruct):
    database_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DatabaseTableOutputOptionsPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='Represents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n')
    glue_connection_name: str = pydantic.Field(..., description='The AWS Glue connection that stores the connection information for the target database.\n')
    database_output_mode: typing.Optional[str] = pydantic.Field(None, description='The output mode to write into the database. Currently supported option: NEW_TABLE.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-databaseoutput.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    database_output_property = databrew.CfnJob.DatabaseOutputProperty(\n        database_options=databrew.CfnJob.DatabaseTableOutputOptionsProperty(\n            table_name="tableName",\n\n            # the properties below are optional\n            temp_directory=databrew.CfnJob.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                bucket_owner="bucketOwner",\n                key="key"\n            )\n        ),\n        glue_connection_name="glueConnectionName",\n\n        # the properties below are optional\n        database_output_mode="databaseOutputMode"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['database_options', 'glue_connection_name', 'database_output_mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.DatabaseOutputProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.DatabaseTableOutputOptionsProperty
class CfnJob_DatabaseTableOutputOptionsPropertyDef(BaseStruct):
    table_name: str = pydantic.Field(..., description='A prefix for the name of a table DataBrew will create in the database.\n')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Represents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-databasetableoutputoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    database_table_output_options_property = databrew.CfnJob.DatabaseTableOutputOptionsProperty(\n        table_name="tableName",\n\n        # the properties below are optional\n        temp_directory=databrew.CfnJob.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            bucket_owner="bucketOwner",\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['table_name', 'temp_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.DatabaseTableOutputOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.DataCatalogOutputProperty
class CfnJob_DataCatalogOutputPropertyDef(BaseStruct):
    database_name: str = pydantic.Field(..., description='The name of a database in the Data Catalog.\n')
    table_name: str = pydantic.Field(..., description='The name of a table in the Data Catalog.\n')
    catalog_id: typing.Optional[str] = pydantic.Field(None, description='The unique identifier of the AWS account that holds the Data Catalog that stores the data.\n')
    database_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DatabaseTableOutputOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Represents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n')
    overwrite: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.\n')
    s3_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3TableOutputOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Represents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-datacatalogoutput.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    data_catalog_output_property = databrew.CfnJob.DataCatalogOutputProperty(\n        database_name="databaseName",\n        table_name="tableName",\n\n        # the properties below are optional\n        catalog_id="catalogId",\n        database_options=databrew.CfnJob.DatabaseTableOutputOptionsProperty(\n            table_name="tableName",\n\n            # the properties below are optional\n            temp_directory=databrew.CfnJob.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                bucket_owner="bucketOwner",\n                key="key"\n            )\n        ),\n        overwrite=False,\n        s3_options=databrew.CfnJob.S3TableOutputOptionsProperty(\n            location=databrew.CfnJob.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                bucket_owner="bucketOwner",\n                key="key"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['database_name', 'table_name', 'catalog_id', 'database_options', 'overwrite', 's3_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.DataCatalogOutputProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.EntityDetectorConfigurationProperty
class CfnJob_EntityDetectorConfigurationPropertyDef(BaseStruct):
    entity_types: typing.Sequence[str] = pydantic.Field(..., description='Entity types to detect. Can be any of the following:. - USA_SSN - EMAIL - USA_ITIN - USA_PASSPORT_NUMBER - PHONE_NUMBER - USA_DRIVING_LICENSE - BANK_ACCOUNT - CREDIT_CARD - IP_ADDRESS - MAC_ADDRESS - USA_DEA_NUMBER - USA_HCPCS_CODE - USA_NATIONAL_PROVIDER_IDENTIFIER - USA_NATIONAL_DRUG_CODE - USA_HEALTH_INSURANCE_CLAIM_NUMBER - USA_MEDICARE_BENEFICIARY_IDENTIFIER - USA_CPT_CODE - PERSON_NAME - DATE The Entity type group USA_ALL is also supported, and includes all of the above entity types except PERSON_NAME and DATE.\n')
    allowed_statistics: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_AllowedStatisticsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration of statistics that are allowed to be run on columns that contain detected entities. When undefined, no statistics will be computed on columns that contain detected entities.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-entitydetectorconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    entity_detector_configuration_property = databrew.CfnJob.EntityDetectorConfigurationProperty(\n        entity_types=["entityTypes"],\n\n        # the properties below are optional\n        allowed_statistics=databrew.CfnJob.AllowedStatisticsProperty(\n            statistics=["statistics"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['entity_types', 'allowed_statistics']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.EntityDetectorConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.JobSampleProperty
class CfnJob_JobSamplePropertyDef(BaseStruct):
    mode: typing.Optional[str] = pydantic.Field(None, description='A value that determines whether the profile job is run on the entire dataset or a specified number of rows. This value must be one of the following: - FULL_DATASET - The profile job is run on the entire dataset. - CUSTOM_ROWS - The profile job is run on the number of rows specified in the ``Size`` parameter.\n')
    size: typing.Union[int, float, None] = pydantic.Field(None, description='The ``Size`` parameter is only required when the mode is CUSTOM_ROWS. The profile job is run on the specified number of rows. The maximum value for size is Long.MAX_VALUE. Long.MAX_VALUE = 9223372036854775807\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-jobsample.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    job_sample_property = databrew.CfnJob.JobSampleProperty(\n        mode="mode",\n        size=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['mode', 'size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.JobSampleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.OutputFormatOptionsProperty
class CfnJob_OutputFormatOptionsPropertyDef(BaseStruct):
    csv: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_CsvOutputOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Represents a set of options that define the structure of comma-separated value (CSV) job output.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-outputformatoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    output_format_options_property = databrew.CfnJob.OutputFormatOptionsProperty(\n        csv=databrew.CfnJob.CsvOutputOptionsProperty(\n            delimiter="delimiter"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['csv']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.OutputFormatOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.OutputLocationProperty
class CfnJob_OutputLocationPropertyDef(BaseStruct):
    bucket: str = pydantic.Field(..., description='The Amazon S3 bucket name.\n')
    bucket_owner: typing.Optional[str] = pydantic.Field(None, description='``CfnJob.OutputLocationProperty.BucketOwner``.\n')
    key: typing.Optional[str] = pydantic.Field(None, description='The unique name of the object in the bucket.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-outputlocation.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    output_location_property = databrew.CfnJob.OutputLocationProperty(\n        bucket="bucket",\n\n        # the properties below are optional\n        bucket_owner="bucketOwner",\n        key="key"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_owner', 'key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.OutputLocationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.OutputProperty
class CfnJob_OutputPropertyDef(BaseStruct):
    location: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3LocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The location in Amazon S3 where the job writes its output.\n')
    compression_format: typing.Optional[str] = pydantic.Field(None, description='The compression algorithm used to compress the output text of the job.\n')
    format: typing.Optional[str] = pydantic.Field(None, description='The data format of the output of the job.\n')
    format_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_OutputFormatOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Represents options that define how DataBrew formats job output files.\n')
    max_output_files: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of files to be generated by the job and written to the output folder.\n')
    overwrite: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that, if true, means that any data in the location specified for output is overwritten with new output.\n')
    partition_columns: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The names of one or more partition columns for the output of the job.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-output.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    output_property = databrew.CfnJob.OutputProperty(\n        location=databrew.CfnJob.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            bucket_owner="bucketOwner",\n            key="key"\n        ),\n\n        # the properties below are optional\n        compression_format="compressionFormat",\n        format="format",\n        format_options=databrew.CfnJob.OutputFormatOptionsProperty(\n            csv=databrew.CfnJob.CsvOutputOptionsProperty(\n                delimiter="delimiter"\n            )\n        ),\n        max_output_files=123,\n        overwrite=False,\n        partition_columns=["partitionColumns"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['location', 'compression_format', 'format', 'format_options', 'max_output_files', 'overwrite', 'partition_columns']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.OutputProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.ProfileConfigurationProperty
class CfnJob_ProfileConfigurationPropertyDef(BaseStruct):
    column_statistics_configurations: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ColumnStatisticsConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='List of configurations for column evaluations. ColumnStatisticsConfigurations are used to select evaluations and override parameters of evaluations for particular columns. When ColumnStatisticsConfigurations is undefined, the profile job will profile all supported columns and run all supported evaluations.\n')
    dataset_statistics_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_StatisticsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for inter-column evaluations. Configuration can be used to select evaluations and override parameters of evaluations. When configuration is undefined, the profile job will run all supported inter-column evaluations.\n')
    entity_detector_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_EntityDetectorConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration of entity detection for a profile job. When undefined, entity detection is disabled.\n')
    profile_columns: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ColumnSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='List of column selectors. ProfileColumns can be used to select columns from the dataset. When ProfileColumns is undefined, the profile job will profile all supported columns.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-profileconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    profile_configuration_property = databrew.CfnJob.ProfileConfigurationProperty(\n        column_statistics_configurations=[databrew.CfnJob.ColumnStatisticsConfigurationProperty(\n            statistics=databrew.CfnJob.StatisticsConfigurationProperty(\n                included_statistics=["includedStatistics"],\n                overrides=[databrew.CfnJob.StatisticOverrideProperty(\n                    parameters={\n                        "parameters_key": "parameters"\n                    },\n                    statistic="statistic"\n                )]\n            ),\n\n            # the properties below are optional\n            selectors=[databrew.CfnJob.ColumnSelectorProperty(\n                name="name",\n                regex="regex"\n            )]\n        )],\n        dataset_statistics_configuration=databrew.CfnJob.StatisticsConfigurationProperty(\n            included_statistics=["includedStatistics"],\n            overrides=[databrew.CfnJob.StatisticOverrideProperty(\n                parameters={\n                    "parameters_key": "parameters"\n                },\n                statistic="statistic"\n            )]\n        ),\n        entity_detector_configuration=databrew.CfnJob.EntityDetectorConfigurationProperty(\n            entity_types=["entityTypes"],\n\n            # the properties below are optional\n            allowed_statistics=databrew.CfnJob.AllowedStatisticsProperty(\n                statistics=["statistics"]\n            )\n        ),\n        profile_columns=[databrew.CfnJob.ColumnSelectorProperty(\n            name="name",\n            regex="regex"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['column_statistics_configurations', 'dataset_statistics_configuration', 'entity_detector_configuration', 'profile_columns']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.ProfileConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.RecipeProperty
class CfnJob_RecipePropertyDef(BaseStruct):
    name: str = pydantic.Field(..., description='The unique name for the recipe.\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The identifier for the version for the recipe.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-recipe.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    recipe_property = databrew.CfnJob.RecipeProperty(\n        name="name",\n\n        # the properties below are optional\n        version="version"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.RecipeProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.S3LocationProperty
class CfnJob_S3LocationPropertyDef(BaseStruct):
    bucket: str = pydantic.Field(..., description='The Amazon S3 bucket name.\n')
    bucket_owner: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID of the bucket owner.\n')
    key: typing.Optional[str] = pydantic.Field(None, description='The unique name of the object in the bucket.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-s3location.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    s3_location_property = databrew.CfnJob.S3LocationProperty(\n        bucket="bucket",\n\n        # the properties below are optional\n        bucket_owner="bucketOwner",\n        key="key"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_owner', 'key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.S3LocationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.S3TableOutputOptionsProperty
class CfnJob_S3TableOutputOptionsPropertyDef(BaseStruct):
    location: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3LocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='Represents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-s3tableoutputoptions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    s3_table_output_options_property = databrew.CfnJob.S3TableOutputOptionsProperty(\n        location=databrew.CfnJob.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            bucket_owner="bucketOwner",\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.S3TableOutputOptionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.StatisticOverrideProperty
class CfnJob_StatisticOverridePropertyDef(BaseStruct):
    parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Mapping[str, str]] = pydantic.Field(..., description='A map that includes overrides of an evaluations parameters.\n')
    statistic: str = pydantic.Field(..., description='The name of an evaluation.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-statisticoverride.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    statistic_override_property = databrew.CfnJob.StatisticOverrideProperty(\n        parameters={\n            "parameters_key": "parameters"\n        },\n        statistic="statistic"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['parameters', 'statistic']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.StatisticOverrideProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.StatisticsConfigurationProperty
class CfnJob_StatisticsConfigurationPropertyDef(BaseStruct):
    included_statistics: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='List of included evaluations. When the list is undefined, all supported evaluations will be included.\n')
    overrides: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_StatisticOverridePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='List of overrides for evaluations.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-statisticsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    statistics_configuration_property = databrew.CfnJob.StatisticsConfigurationProperty(\n        included_statistics=["includedStatistics"],\n        overrides=[databrew.CfnJob.StatisticOverrideProperty(\n            parameters={\n                "parameters_key": "parameters"\n            },\n            statistic="statistic"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['included_statistics', 'overrides']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.StatisticsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJob.ValidationConfigurationProperty
class CfnJob_ValidationConfigurationPropertyDef(BaseStruct):
    ruleset_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) for the ruleset to be validated in the profile job. The TargetArn of the selected ruleset should be the same as the Amazon Resource Name (ARN) of the dataset that is associated with the profile job.\n')
    validation_mode: typing.Optional[str] = pydantic.Field(None, description='Mode of data quality validation. Default mode is CHECK_ALL which verifies all rules defined in the selected ruleset.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-job-validationconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    validation_configuration_property = databrew.CfnJob.ValidationConfigurationProperty(\n        ruleset_arn="rulesetArn",\n\n        # the properties below are optional\n        validation_mode="validationMode"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['ruleset_arn', 'validation_mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob.ValidationConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnProject.SampleProperty
class CfnProject_SamplePropertyDef(BaseStruct):
    type: str = pydantic.Field(..., description='The way in which DataBrew obtains rows from a dataset.\n')
    size: typing.Union[int, float, None] = pydantic.Field(None, description='The number of rows in the sample.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-project-sample.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    sample_property = databrew.CfnProject.SampleProperty(\n        type="type",\n\n        # the properties below are optional\n        size=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnProject.SampleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.ActionProperty
class CfnRecipe_ActionPropertyDef(BaseStruct):
    operation: str = pydantic.Field(..., description='The name of a valid DataBrew transformation to be performed on the data.\n')
    parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Mapping[str, str], None] = pydantic.Field(None, description='Contextual parameters for the transformation.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-action.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    action_property = databrew.CfnRecipe.ActionProperty(\n        operation="operation",\n\n        # the properties below are optional\n        parameters={\n            "parameters_key": "parameters"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['operation', 'parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.ActionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.ConditionExpressionProperty
class CfnRecipe_ConditionExpressionPropertyDef(BaseStruct):
    condition: str = pydantic.Field(..., description='A specific condition to apply to a recipe action. For more information, see `Recipe structure <https://docs.aws.amazon.com/databrew/latest/dg/recipe-structure.html>`_ in the *AWS Glue DataBrew Developer Guide* .\n')
    target_column: str = pydantic.Field(..., description='A column to apply this condition to.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='A value that the condition must evaluate to for the condition to succeed.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-conditionexpression.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    condition_expression_property = databrew.CfnRecipe.ConditionExpressionProperty(\n        condition="condition",\n        target_column="targetColumn",\n\n        # the properties below are optional\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['condition', 'target_column', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.ConditionExpressionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.DataCatalogInputDefinitionProperty
class CfnRecipe_DataCatalogInputDefinitionPropertyDef(BaseStruct):
    catalog_id: typing.Optional[str] = pydantic.Field(None, description='The unique identifier of the AWS account that holds the Data Catalog that stores the data.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of a database in the Data Catalog.\n')
    table_name: typing.Optional[str] = pydantic.Field(None, description='The name of a database table in the Data Catalog. This table corresponds to a DataBrew dataset.\n')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Represents an Amazon location where DataBrew can store intermediate results.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-datacataloginputdefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    data_catalog_input_definition_property = databrew.CfnRecipe.DataCatalogInputDefinitionProperty(\n        catalog_id="catalogId",\n        database_name="databaseName",\n        table_name="tableName",\n        temp_directory=databrew.CfnRecipe.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['catalog_id', 'database_name', 'table_name', 'temp_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.DataCatalogInputDefinitionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.InputProperty
class CfnRecipe_InputPropertyDef(BaseStruct):
    data_catalog_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_DataCatalogInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``CfnRecipe.InputProperty.DataCatalogInputDefinition``.')
    s3_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``CfnRecipe.InputProperty.S3InputDefinition``.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-input.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    input_property = databrew.CfnRecipe.InputProperty(\n        data_catalog_input_definition=databrew.CfnRecipe.DataCatalogInputDefinitionProperty(\n            catalog_id="catalogId",\n            database_name="databaseName",\n            table_name="tableName",\n            temp_directory=databrew.CfnRecipe.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                key="key"\n            )\n        ),\n        s3_input_definition=databrew.CfnRecipe.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_catalog_input_definition', 's3_input_definition']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.InputProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.RecipeParametersProperty
class CfnRecipe_RecipeParametersPropertyDef(BaseStruct):
    aggregate_function: typing.Optional[str] = pydantic.Field(None, description='The name of an aggregation function to apply.\n')
    base: typing.Optional[str] = pydantic.Field(None, description='The number of digits used in a counting system.\n')
    case_statement: typing.Optional[str] = pydantic.Field(None, description='A case statement associated with a recipe.\n')
    category_map: typing.Optional[str] = pydantic.Field(None, description='A category map used for one-hot encoding.\n')
    chars_to_remove: typing.Optional[str] = pydantic.Field(None, description='Characters to remove from a step that applies one-hot encoding or tokenization.\n')
    collapse_consecutive_whitespace: typing.Optional[str] = pydantic.Field(None, description='Remove any non-word non-punctuation character.\n')
    column_data_type: typing.Optional[str] = pydantic.Field(None, description='The data type of the column.\n')
    column_range: typing.Optional[str] = pydantic.Field(None, description='A range of columns to which a step is applied.\n')
    count: typing.Optional[str] = pydantic.Field(None, description='The number of times a string needs to be repeated.\n')
    custom_characters: typing.Optional[str] = pydantic.Field(None, description='One or more characters that can be substituted or removed, depending on the context.\n')
    custom_stop_words: typing.Optional[str] = pydantic.Field(None, description='A list of words to ignore in a step that applies word tokenization.\n')
    custom_value: typing.Optional[str] = pydantic.Field(None, description='A list of custom values to use in a step that requires that you provide a value to finish the operation.\n')
    datasets_columns: typing.Optional[str] = pydantic.Field(None, description='A list of the dataset columns included in a project.\n')
    date_add_value: typing.Optional[str] = pydantic.Field(None, description='A value that specifies how many units of time to add or subtract for a date math operation.\n')
    date_time_format: typing.Optional[str] = pydantic.Field(None, description='A date format to apply to a date.\n')
    date_time_parameters: typing.Optional[str] = pydantic.Field(None, description='A set of parameters associated with a datetime.\n')
    delete_other_rows: typing.Optional[str] = pydantic.Field(None, description='Determines whether unmapped rows in a categorical mapping should be deleted.\n')
    delimiter: typing.Optional[str] = pydantic.Field(None, description='The delimiter to use when parsing separated values in a text file.\n')
    end_pattern: typing.Optional[str] = pydantic.Field(None, description='The end pattern to locate.\n')
    end_position: typing.Optional[str] = pydantic.Field(None, description='The end position to locate.\n')
    end_value: typing.Optional[str] = pydantic.Field(None, description='The end value to locate.\n')
    expand_contractions: typing.Optional[str] = pydantic.Field(None, description="A list of word contractions and what they expand to. For eample: *can't* ; *cannot* ; *can not* .\n")
    exponent: typing.Optional[str] = pydantic.Field(None, description='The exponent to apply in an exponential operation.\n')
    false_string: typing.Optional[str] = pydantic.Field(None, description='A value that represents ``FALSE`` .\n')
    group_by_agg_function_options: typing.Optional[str] = pydantic.Field(None, description='Specifies options to apply to the ``GROUP BY`` used in an aggregation.\n')
    group_by_columns: typing.Optional[str] = pydantic.Field(None, description='The columns to use in the ``GROUP BY`` clause.\n')
    hidden_columns: typing.Optional[str] = pydantic.Field(None, description='A list of columns to hide.\n')
    ignore_case: typing.Optional[str] = pydantic.Field(None, description='Indicates that lower and upper case letters are treated equally.\n')
    include_in_split: typing.Optional[str] = pydantic.Field(None, description='Indicates if this column is participating in a split transform.\n')
    input: typing.Any = pydantic.Field(None, description='The input location to load the dataset from - Amazon S3 or AWS Glue Data Catalog .\n')
    interval: typing.Optional[str] = pydantic.Field(None, description='The number of characters to split by.\n')
    is_text: typing.Optional[str] = pydantic.Field(None, description='Indicates if the content is text.\n')
    join_keys: typing.Optional[str] = pydantic.Field(None, description='The keys or columns involved in a join.\n')
    join_type: typing.Optional[str] = pydantic.Field(None, description='The type of join to use, for example, ``INNER JOIN`` , ``OUTER JOIN`` , and so on.\n')
    left_columns: typing.Optional[str] = pydantic.Field(None, description='The columns on the left side of the join.\n')
    limit: typing.Optional[str] = pydantic.Field(None, description='The number of times to perform ``split`` or ``replaceBy`` in a string.\n')
    lower_bound: typing.Optional[str] = pydantic.Field(None, description='The lower boundary for a value.\n')
    map_type: typing.Optional[str] = pydantic.Field(None, description='The type of mappings to apply to construct a new dynamic frame.\n')
    mode_type: typing.Optional[str] = pydantic.Field(None, description='Determines the manner in which mode value is calculated, in case there is more than one mode value. Valid values: ``NONE`` | ``AVERAGE`` | ``MINIMUM`` | ``MAXIMUM``\n')
    multi_line: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Specifies whether JSON input contains embedded new line characters.\n')
    num_rows: typing.Optional[str] = pydantic.Field(None, description='The number of rows to consider in a window.\n')
    num_rows_after: typing.Optional[str] = pydantic.Field(None, description='The number of rows to consider after the current row in a window.\n')
    num_rows_before: typing.Optional[str] = pydantic.Field(None, description='The number of rows to consider before the current row in a window.\n')
    order_by_column: typing.Optional[str] = pydantic.Field(None, description='A column to sort the results by.\n')
    order_by_columns: typing.Optional[str] = pydantic.Field(None, description='The columns to sort the results by.\n')
    other: typing.Optional[str] = pydantic.Field(None, description='The value to assign to unmapped cells, in categorical mapping.\n')
    pattern: typing.Optional[str] = pydantic.Field(None, description='The pattern to locate.\n')
    pattern_option1: typing.Optional[str] = pydantic.Field(None, description='The starting pattern to split between.\n')
    pattern_option2: typing.Optional[str] = pydantic.Field(None, description='The ending pattern to split between.\n')
    pattern_options: typing.Optional[str] = pydantic.Field(None, description='For splitting by multiple delimiters: A JSON-encoded string that lists the patterns in the format. For example: ``[{\\"pattern\\":\\"1\\",\\"includeInSplit\\":true}]``\n')
    period: typing.Optional[str] = pydantic.Field(None, description='The size of the rolling window.\n')
    position: typing.Optional[str] = pydantic.Field(None, description='The character index within a string.\n')
    remove_all_punctuation: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all of the following characters: ``.`` ``.!`` ``.,`` ``.?``.\n')
    remove_all_quotes: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all single quotes and double quotes.\n')
    remove_all_whitespace: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all whitespaces from the value.\n')
    remove_custom_characters: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all chraracters specified by ``CustomCharacters`` .\n')
    remove_custom_value: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all chraracters specified by ``CustomValue`` .\n')
    remove_leading_and_trailing_punctuation: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes the following characters if they occur at the start or end of the value: ``.`` ``!`` ``,`` ``?``.\n')
    remove_leading_and_trailing_quotes: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes single quotes and double quotes from the beginning and end of the value.\n')
    remove_leading_and_trailing_whitespace: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all whitespaces from the beginning and end of the value.\n')
    remove_letters: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all uppercase and lowercase alphabetic characters (A through Z; a through z).\n')
    remove_numbers: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all numeric characters (0 through 9).\n')
    remove_source_column: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , the source column will be removed after un-nesting that column. (Used with nested column types, such as Map, Struct, or Array.)\n')
    remove_special_characters: typing.Optional[str] = pydantic.Field(None, description='If ``true`` , removes all of the following characters: `! " # $ % & \' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ `` { | } ~``\n')
    right_columns: typing.Optional[str] = pydantic.Field(None, description='The columns on the right side of a join.\n')
    sample_size: typing.Optional[str] = pydantic.Field(None, description='The number of rows in the sample.\n')
    sample_type: typing.Optional[str] = pydantic.Field(None, description='The sampling type to apply to the dataset. Valid values: ``FIRST_N`` | ``LAST_N`` | ``RANDOM``\n')
    secondary_inputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_SecondaryInputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of secondary inputs in a UNION transform.\n')
    second_input: typing.Optional[str] = pydantic.Field(None, description='A object value to indicate the second dataset used in a join.\n')
    sheet_indexes: typing.Union[typing.Sequence[typing.Union[int, float]], typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='One or more sheet numbers in the Excel file, which will be included in a dataset.\n')
    sheet_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Oone or more named sheets in the Excel file, which will be included in a dataset.\n')
    source_column: typing.Optional[str] = pydantic.Field(None, description='A source column needed for an operation, step, or transform.\n')
    source_column1: typing.Optional[str] = pydantic.Field(None, description='A source column needed for an operation, step, or transform.\n')
    source_column2: typing.Optional[str] = pydantic.Field(None, description='A source column needed for an operation, step, or transform.\n')
    source_columns: typing.Optional[str] = pydantic.Field(None, description='A list of source columns needed for an operation, step, or transform.\n')
    start_column_index: typing.Optional[str] = pydantic.Field(None, description='The index number of the first column used by an operation, step, or transform.\n')
    start_pattern: typing.Optional[str] = pydantic.Field(None, description='The starting pattern to locate.\n')
    start_position: typing.Optional[str] = pydantic.Field(None, description='The starting position to locate.\n')
    start_value: typing.Optional[str] = pydantic.Field(None, description='The starting value to locate.\n')
    stemming_mode: typing.Optional[str] = pydantic.Field(None, description='Indicates this operation uses stems and lemmas (base words) for word tokenization.\n')
    step_count: typing.Optional[str] = pydantic.Field(None, description='The total number of transforms in this recipe.\n')
    step_index: typing.Optional[str] = pydantic.Field(None, description='The index ID of a step.\n')
    stop_words_mode: typing.Optional[str] = pydantic.Field(None, description='Indicates this operation uses stop words as part of word tokenization.\n')
    strategy: typing.Optional[str] = pydantic.Field(None, description='The resolution strategy to apply in resolving ambiguities.\n')
    target_column: typing.Optional[str] = pydantic.Field(None, description='The column targeted by this operation.\n')
    target_column_names: typing.Optional[str] = pydantic.Field(None, description='The names to give columns altered by this operation.\n')
    target_date_format: typing.Optional[str] = pydantic.Field(None, description='The date format to convert to.\n')
    target_index: typing.Optional[str] = pydantic.Field(None, description='The index number of an object that is targeted by this operation.\n')
    time_zone: typing.Optional[str] = pydantic.Field(None, description='The current timezone that you want to use for dates.\n')
    tokenizer_pattern: typing.Optional[str] = pydantic.Field(None, description='A regex expression to use when splitting text into terms, also called words or tokens.\n')
    true_string: typing.Optional[str] = pydantic.Field(None, description='A value to use to represent ``TRUE`` .\n')
    udf_lang: typing.Optional[str] = pydantic.Field(None, description="The language that's used in the user-defined function.\n")
    units: typing.Optional[str] = pydantic.Field(None, description='Specifies a unit of time. For example: ``MINUTES`` ; ``SECONDS`` ; ``HOURS`` ; etc.\n')
    unpivot_column: typing.Optional[str] = pydantic.Field(None, description='Cast columns as rows, so that each value is a different row in a single column.\n')
    upper_bound: typing.Optional[str] = pydantic.Field(None, description='The upper boundary for a value.\n')
    use_new_data_frame: typing.Optional[str] = pydantic.Field(None, description='Create a new container to hold a dataset.\n')
    value: typing.Optional[str] = pydantic.Field(None, description="A static value that can be used in a comparison, a substitution, or in another context-specific way. A ``Value`` can be a number, string, or other datatype, depending on the recipe action in which it's used.\n")
    value1: typing.Optional[str] = pydantic.Field(None, description="A value that's used by this operation.\n")
    value2: typing.Optional[str] = pydantic.Field(None, description="A value that's used by this operation.\n")
    value_column: typing.Optional[str] = pydantic.Field(None, description="The column that is provided as a value that's used by this operation.\n")
    view_frame: typing.Optional[str] = pydantic.Field(None, description='The subset of rows currently available for viewing.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-recipeparameters.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    # input: Any\n\n    recipe_parameters_property = databrew.CfnRecipe.RecipeParametersProperty(\n        aggregate_function="aggregateFunction",\n        base="base",\n        case_statement="caseStatement",\n        category_map="categoryMap",\n        chars_to_remove="charsToRemove",\n        collapse_consecutive_whitespace="collapseConsecutiveWhitespace",\n        column_data_type="columnDataType",\n        column_range="columnRange",\n        count="count",\n        custom_characters="customCharacters",\n        custom_stop_words="customStopWords",\n        custom_value="customValue",\n        datasets_columns="datasetsColumns",\n        date_add_value="dateAddValue",\n        date_time_format="dateTimeFormat",\n        date_time_parameters="dateTimeParameters",\n        delete_other_rows="deleteOtherRows",\n        delimiter="delimiter",\n        end_pattern="endPattern",\n        end_position="endPosition",\n        end_value="endValue",\n        expand_contractions="expandContractions",\n        exponent="exponent",\n        false_string="falseString",\n        group_by_agg_function_options="groupByAggFunctionOptions",\n        group_by_columns="groupByColumns",\n        hidden_columns="hiddenColumns",\n        ignore_case="ignoreCase",\n        include_in_split="includeInSplit",\n        input=input,\n        interval="interval",\n        is_text="isText",\n        join_keys="joinKeys",\n        join_type="joinType",\n        left_columns="leftColumns",\n        limit="limit",\n        lower_bound="lowerBound",\n        map_type="mapType",\n        mode_type="modeType",\n        multi_line=False,\n        num_rows="numRows",\n        num_rows_after="numRowsAfter",\n        num_rows_before="numRowsBefore",\n        order_by_column="orderByColumn",\n        order_by_columns="orderByColumns",\n        other="other",\n        pattern="pattern",\n        pattern_option1="patternOption1",\n        pattern_option2="patternOption2",\n        pattern_options="patternOptions",\n        period="period",\n        position="position",\n        remove_all_punctuation="removeAllPunctuation",\n        remove_all_quotes="removeAllQuotes",\n        remove_all_whitespace="removeAllWhitespace",\n        remove_custom_characters="removeCustomCharacters",\n        remove_custom_value="removeCustomValue",\n        remove_leading_and_trailing_punctuation="removeLeadingAndTrailingPunctuation",\n        remove_leading_and_trailing_quotes="removeLeadingAndTrailingQuotes",\n        remove_leading_and_trailing_whitespace="removeLeadingAndTrailingWhitespace",\n        remove_letters="removeLetters",\n        remove_numbers="removeNumbers",\n        remove_source_column="removeSourceColumn",\n        remove_special_characters="removeSpecialCharacters",\n        right_columns="rightColumns",\n        sample_size="sampleSize",\n        sample_type="sampleType",\n        secondary_inputs=[databrew.CfnRecipe.SecondaryInputProperty(\n            data_catalog_input_definition=databrew.CfnRecipe.DataCatalogInputDefinitionProperty(\n                catalog_id="catalogId",\n                database_name="databaseName",\n                table_name="tableName",\n                temp_directory=databrew.CfnRecipe.S3LocationProperty(\n                    bucket="bucket",\n\n                    # the properties below are optional\n                    key="key"\n                )\n            ),\n            s3_input_definition=databrew.CfnRecipe.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                key="key"\n            )\n        )],\n        second_input="secondInput",\n        sheet_indexes=[123],\n        sheet_names=["sheetNames"],\n        source_column="sourceColumn",\n        source_column1="sourceColumn1",\n        source_column2="sourceColumn2",\n        source_columns="sourceColumns",\n        start_column_index="startColumnIndex",\n        start_pattern="startPattern",\n        start_position="startPosition",\n        start_value="startValue",\n        stemming_mode="stemmingMode",\n        step_count="stepCount",\n        step_index="stepIndex",\n        stop_words_mode="stopWordsMode",\n        strategy="strategy",\n        target_column="targetColumn",\n        target_column_names="targetColumnNames",\n        target_date_format="targetDateFormat",\n        target_index="targetIndex",\n        time_zone="timeZone",\n        tokenizer_pattern="tokenizerPattern",\n        true_string="trueString",\n        udf_lang="udfLang",\n        units="units",\n        unpivot_column="unpivotColumn",\n        upper_bound="upperBound",\n        use_new_data_frame="useNewDataFrame",\n        value="value",\n        value1="value1",\n        value2="value2",\n        value_column="valueColumn",\n        view_frame="viewFrame"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['aggregate_function', 'base', 'case_statement', 'category_map', 'chars_to_remove', 'collapse_consecutive_whitespace', 'column_data_type', 'column_range', 'count', 'custom_characters', 'custom_stop_words', 'custom_value', 'datasets_columns', 'date_add_value', 'date_time_format', 'date_time_parameters', 'delete_other_rows', 'delimiter', 'end_pattern', 'end_position', 'end_value', 'expand_contractions', 'exponent', 'false_string', 'group_by_agg_function_options', 'group_by_columns', 'hidden_columns', 'ignore_case', 'include_in_split', 'input', 'interval', 'is_text', 'join_keys', 'join_type', 'left_columns', 'limit', 'lower_bound', 'map_type', 'mode_type', 'multi_line', 'num_rows', 'num_rows_after', 'num_rows_before', 'order_by_column', 'order_by_columns', 'other', 'pattern', 'pattern_option1', 'pattern_option2', 'pattern_options', 'period', 'position', 'remove_all_punctuation', 'remove_all_quotes', 'remove_all_whitespace', 'remove_custom_characters', 'remove_custom_value', 'remove_leading_and_trailing_punctuation', 'remove_leading_and_trailing_quotes', 'remove_leading_and_trailing_whitespace', 'remove_letters', 'remove_numbers', 'remove_source_column', 'remove_special_characters', 'right_columns', 'sample_size', 'sample_type', 'secondary_inputs', 'second_input', 'sheet_indexes', 'sheet_names', 'source_column', 'source_column1', 'source_column2', 'source_columns', 'start_column_index', 'start_pattern', 'start_position', 'start_value', 'stemming_mode', 'step_count', 'step_index', 'stop_words_mode', 'strategy', 'target_column', 'target_column_names', 'target_date_format', 'target_index', 'time_zone', 'tokenizer_pattern', 'true_string', 'udf_lang', 'units', 'unpivot_column', 'upper_bound', 'use_new_data_frame', 'value', 'value1', 'value2', 'value_column', 'view_frame']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.RecipeParametersProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.RecipeStepProperty
class CfnRecipe_RecipeStepPropertyDef(BaseStruct):
    action: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_ActionPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The particular action to be performed in the recipe step.\n')
    condition_expressions: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_ConditionExpressionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more conditions that must be met for the recipe step to succeed. .. epigraph:: All of the conditions in the array must be met. In other words, all of the conditions must be combined using a logical AND operation.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-recipestep.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    recipe_step_property = databrew.CfnRecipe.RecipeStepProperty(\n        action=databrew.CfnRecipe.ActionProperty(\n            operation="operation",\n\n            # the properties below are optional\n            parameters={\n                "parameters_key": "parameters"\n            }\n        ),\n\n        # the properties below are optional\n        condition_expressions=[databrew.CfnRecipe.ConditionExpressionProperty(\n            condition="condition",\n            target_column="targetColumn",\n\n            # the properties below are optional\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'condition_expressions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.RecipeStepProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.S3LocationProperty
class CfnRecipe_S3LocationPropertyDef(BaseStruct):
    bucket: str = pydantic.Field(..., description='The Amazon S3 bucket name.\n')
    key: typing.Optional[str] = pydantic.Field(None, description='The unique name of the object in the bucket.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-s3location.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    s3_location_property = databrew.CfnRecipe.S3LocationProperty(\n        bucket="bucket",\n\n        # the properties below are optional\n        key="key"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.S3LocationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipe.SecondaryInputProperty
class CfnRecipe_SecondaryInputPropertyDef(BaseStruct):
    data_catalog_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_DataCatalogInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The AWS Glue Data Catalog parameters for the data.\n')
    s3_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Amazon S3 location where the data is stored.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-recipe-secondaryinput.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    secondary_input_property = databrew.CfnRecipe.SecondaryInputProperty(\n        data_catalog_input_definition=databrew.CfnRecipe.DataCatalogInputDefinitionProperty(\n            catalog_id="catalogId",\n            database_name="databaseName",\n            table_name="tableName",\n            temp_directory=databrew.CfnRecipe.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                key="key"\n            )\n        ),\n        s3_input_definition=databrew.CfnRecipe.S3LocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            key="key"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_catalog_input_definition', 's3_input_definition']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe.SecondaryInputProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRuleset.ColumnSelectorProperty
class CfnRuleset_ColumnSelectorPropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of a column from a dataset.\n')
    regex: typing.Optional[str] = pydantic.Field(None, description='A regular expression for selecting a column from a dataset.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-ruleset-columnselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    column_selector_property = databrew.CfnRuleset.ColumnSelectorProperty(\n        name="name",\n        regex="regex"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'regex']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRuleset.ColumnSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRuleset.RuleProperty
class CfnRuleset_RulePropertyDef(BaseStruct):
    check_expression: str = pydantic.Field(..., description="The expression which includes column references, condition names followed by variable references, possibly grouped and combined with other conditions. For example, ``(:col1 starts_with :prefix1 or :col1 starts_with :prefix2) and (:col1 ends_with :suffix1 or :col1 ends_with :suffix2)`` . Column and value references are substitution variables that should start with the ':' symbol. Depending on the context, substitution variables' values can be either an actual value or a column name. These values are defined in the SubstitutionMap. If a CheckExpression starts with a column reference, then ColumnSelectors in the rule should be null. If ColumnSelectors has been defined, then there should be no columnn reference in the left side of a condition, for example, ``is_between :val1 and :val2`` .\n")
    name: str = pydantic.Field(..., description='The name of the rule.\n')
    column_selectors: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_ColumnSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='List of column selectors. Selectors can be used to select columns using a name or regular expression from the dataset. Rule will be applied to selected columns.\n')
    disabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that specifies whether the rule is disabled. Once a rule is disabled, a profile job will not validate it during a job run. Default value is false.\n')
    substitution_map: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_SubstitutionValuePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The map of substitution variable names to their values used in a check expression. Variable names should start with a \':\' (colon). Variable values can either be actual values or column names. To differentiate between the two, column names should be enclosed in backticks, for example, ``":col1": "``Column A``".``\n')
    threshold: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_ThresholdPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The threshold used with a non-aggregate check expression. Non-aggregate check expressions will be applied to each row in a specific column, and the threshold will be used to determine whether the validation succeeds.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-ruleset-rule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    rule_property = databrew.CfnRuleset.RuleProperty(\n        check_expression="checkExpression",\n        name="name",\n\n        # the properties below are optional\n        column_selectors=[databrew.CfnRuleset.ColumnSelectorProperty(\n            name="name",\n            regex="regex"\n        )],\n        disabled=False,\n        substitution_map=[databrew.CfnRuleset.SubstitutionValueProperty(\n            value="value",\n            value_reference="valueReference"\n        )],\n        threshold=databrew.CfnRuleset.ThresholdProperty(\n            value=123,\n\n            # the properties below are optional\n            type="type",\n            unit="unit"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['check_expression', 'name', 'column_selectors', 'disabled', 'substitution_map', 'threshold']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRuleset.RuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRuleset.SubstitutionValueProperty
class CfnRuleset_SubstitutionValuePropertyDef(BaseStruct):
    value: str = pydantic.Field(..., description='Value or column name.\n')
    value_reference: str = pydantic.Field(..., description='Variable name.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-ruleset-substitutionvalue.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    substitution_value_property = databrew.CfnRuleset.SubstitutionValueProperty(\n        value="value",\n        value_reference="valueReference"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['value', 'value_reference']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRuleset.SubstitutionValueProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRuleset.ThresholdProperty
class CfnRuleset_ThresholdPropertyDef(BaseStruct):
    value: typing.Union[int, float] = pydantic.Field(..., description='The value of a threshold.\n')
    type: typing.Optional[str] = pydantic.Field(None, description='The type of a threshold. Used for comparison of an actual count of rows that satisfy the rule to the threshold value.\n')
    unit: typing.Optional[str] = pydantic.Field(None, description='Unit of threshold value. Can be either a COUNT or PERCENTAGE of the full sample size used for validation.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-databrew-ruleset-threshold.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    threshold_property = databrew.CfnRuleset.ThresholdProperty(\n        value=123,\n\n        # the properties below are optional\n        type="type",\n        unit="unit"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['value', 'type', 'unit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRuleset.ThresholdProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnDataset
class CfnDatasetDef(BaseCfnResource):
    input: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_InputPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='Information on how DataBrew can find the dataset, in either the AWS Glue Data Catalog or Amazon S3 .\n')
    name: str = pydantic.Field(..., description='The unique name of the dataset.\n')
    format: typing.Optional[str] = pydantic.Field(None, description='The file format of a dataset that is created from an Amazon S3 file or folder.\n')
    format_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FormatOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A set of options that define how DataBrew interprets the data in the dataset.\n')
    path_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_PathOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A set of options that defines how DataBrew interprets an Amazon S3 path of the dataset.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the dataset.')
    _init_params: typing.ClassVar[list[str]] = ['input', 'name', 'format', 'format_options', 'path_options', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['CsvOptionsProperty', 'DataCatalogInputDefinitionProperty', 'DatabaseInputDefinitionProperty', 'DatasetParameterProperty', 'DatetimeOptionsProperty', 'ExcelOptionsProperty', 'FilesLimitProperty', 'FilterExpressionProperty', 'FilterValueProperty', 'FormatOptionsProperty', 'InputProperty', 'JsonOptionsProperty', 'MetadataProperty', 'PathOptionsProperty', 'PathParameterProperty', 'S3LocationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDataset'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDatasetDefConfig] = pydantic.Field(None)


class CfnDatasetDefConfig(pydantic.BaseModel):
    CsvOptionsProperty: typing.Optional[list[CfnDatasetDefCsvoptionspropertyParams]] = pydantic.Field(None, description='')
    DataCatalogInputDefinitionProperty: typing.Optional[list[CfnDatasetDefDatacataloginputdefinitionpropertyParams]] = pydantic.Field(None, description='')
    DatabaseInputDefinitionProperty: typing.Optional[list[CfnDatasetDefDatabaseinputdefinitionpropertyParams]] = pydantic.Field(None, description='')
    DatasetParameterProperty: typing.Optional[list[CfnDatasetDefDatasetparameterpropertyParams]] = pydantic.Field(None, description='')
    DatetimeOptionsProperty: typing.Optional[list[CfnDatasetDefDatetimeoptionspropertyParams]] = pydantic.Field(None, description='')
    ExcelOptionsProperty: typing.Optional[list[CfnDatasetDefExceloptionspropertyParams]] = pydantic.Field(None, description='')
    FilesLimitProperty: typing.Optional[list[CfnDatasetDefFileslimitpropertyParams]] = pydantic.Field(None, description='')
    FilterExpressionProperty: typing.Optional[list[CfnDatasetDefFilterexpressionpropertyParams]] = pydantic.Field(None, description='')
    FilterValueProperty: typing.Optional[list[CfnDatasetDefFiltervaluepropertyParams]] = pydantic.Field(None, description='')
    FormatOptionsProperty: typing.Optional[list[CfnDatasetDefFormatoptionspropertyParams]] = pydantic.Field(None, description='')
    InputProperty: typing.Optional[list[CfnDatasetDefInputpropertyParams]] = pydantic.Field(None, description='')
    JsonOptionsProperty: typing.Optional[list[CfnDatasetDefJsonoptionspropertyParams]] = pydantic.Field(None, description='')
    MetadataProperty: typing.Optional[list[CfnDatasetDefMetadatapropertyParams]] = pydantic.Field(None, description='')
    PathOptionsProperty: typing.Optional[list[CfnDatasetDefPathoptionspropertyParams]] = pydantic.Field(None, description='')
    PathParameterProperty: typing.Optional[list[CfnDatasetDefPathparameterpropertyParams]] = pydantic.Field(None, description='')
    S3LocationProperty: typing.Optional[list[CfnDatasetDefS3LocationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnDatasetDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDatasetDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDatasetDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDatasetDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDatasetDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDatasetDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDatasetDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDatasetDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDatasetDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDatasetDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDatasetDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDatasetDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDatasetDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDatasetDefCsvoptionspropertyParams(pydantic.BaseModel):
    delimiter: typing.Optional[str] = pydantic.Field(None, description='')
    header_row: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefDatacataloginputdefinitionpropertyParams(pydantic.BaseModel):
    catalog_id: typing.Optional[str] = pydantic.Field(None, description='')
    database_name: typing.Optional[str] = pydantic.Field(None, description='')
    table_name: typing.Optional[str] = pydantic.Field(None, description='')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefDatabaseinputdefinitionpropertyParams(pydantic.BaseModel):
    glue_connection_name: str = pydantic.Field(..., description='')
    database_table_name: typing.Optional[str] = pydantic.Field(None, description='')
    query_string: typing.Optional[str] = pydantic.Field(None, description='')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefDatasetparameterpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    create_column: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    datetime_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DatetimeOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    filter: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilterExpressionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefDatetimeoptionspropertyParams(pydantic.BaseModel):
    format: str = pydantic.Field(..., description='')
    locale_code: typing.Optional[str] = pydantic.Field(None, description='')
    timezone_offset: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefExceloptionspropertyParams(pydantic.BaseModel):
    header_row: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    sheet_indexes: typing.Union[typing.Sequence[typing.Union[int, float]], typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    sheet_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefFileslimitpropertyParams(pydantic.BaseModel):
    max_files: typing.Union[int, float] = pydantic.Field(..., description='')
    order: typing.Optional[str] = pydantic.Field(None, description='')
    ordered_by: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefFilterexpressionpropertyParams(pydantic.BaseModel):
    expression: str = pydantic.Field(..., description='')
    values_map: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilterValuePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnDatasetDefFiltervaluepropertyParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='')
    value_reference: str = pydantic.Field(..., description='')
    ...

class CfnDatasetDefFormatoptionspropertyParams(pydantic.BaseModel):
    csv: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_CsvOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    excel: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_ExcelOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    json_: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_JsonOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='', alias='json')
    ...

class CfnDatasetDefInputpropertyParams(pydantic.BaseModel):
    database_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DatabaseInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    data_catalog_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DataCatalogInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    metadata: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_MetadataPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    s3_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefJsonoptionspropertyParams(pydantic.BaseModel):
    multi_line: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefMetadatapropertyParams(pydantic.BaseModel):
    source_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefPathoptionspropertyParams(pydantic.BaseModel):
    files_limit: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilesLimitPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    last_modified_date_condition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FilterExpressionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_PathParameterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefPathparameterpropertyParams(pydantic.BaseModel):
    dataset_parameter: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_DatasetParameterPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    path_parameter_name: str = pydantic.Field(..., description='')
    ...

class CfnDatasetDefS3LocationpropertyParams(pydantic.BaseModel):
    bucket: str = pydantic.Field(..., description='')
    key: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDatasetDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDatasetDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDatasetDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDatasetDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDatasetDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDatasetDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDatasetDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDatasetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDatasetDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDatasetDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDatasetDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDatasetDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDatasetDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDatasetDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_databrew.CfnJob
class CfnJobDef(BaseCfnResource):
    name: str = pydantic.Field(..., description='The unique name of the job.\n')
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the role to be assumed for this job.\n')
    type: str = pydantic.Field(..., description='The job type of the job, which must be one of the following:. - ``PROFILE`` - A job to analyze a dataset, to determine its size, data types, data distribution, and more. - ``RECIPE`` - A job to apply one or more transformations to a dataset.\n')
    database_outputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DatabaseOutputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Represents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.\n')
    data_catalog_outputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DataCatalogOutputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more artifacts that represent the AWS Glue Data Catalog output from running the job.\n')
    dataset_name: typing.Optional[str] = pydantic.Field(None, description='A dataset that the job is to process.\n')
    encryption_key_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of an encryption key that is used to protect the job output. For more information, see `Encrypting data written by DataBrew jobs <https://docs.aws.amazon.com/databrew/latest/dg/encryption-security-configuration.html>`_\n')
    encryption_mode: typing.Optional[str] = pydantic.Field(None, description='The encryption mode for the job, which can be one of the following:. - ``SSE-KMS`` - Server-side encryption with keys managed by AWS KMS . - ``SSE-S3`` - Server-side encryption with keys managed by Amazon S3.\n')
    job_sample: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_JobSamplePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="A sample configuration for profile jobs only, which determines the number of rows on which the profile job is run. If a ``JobSample`` value isn't provided, the default value is used. The default value is CUSTOM_ROWS for the mode parameter and 20,000 for the size parameter.\n")
    log_subscription: typing.Optional[str] = pydantic.Field(None, description='The current status of Amazon CloudWatch logging for the job.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of nodes that can be consumed when the job processes data.\n')
    max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry the job after a job run fails.\n')
    output_location: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_OutputLocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``AWS::DataBrew::Job.OutputLocation``.\n')
    outputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_OutputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more artifacts that represent output from running the job.\n')
    profile_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ProfileConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for profile jobs. Configuration can be used to select columns, do evaluations, and override default parameters of evaluations. When configuration is undefined, the profile job will apply default settings to all supported columns.\n')
    project_name: typing.Optional[str] = pydantic.Field(None, description='The name of the project that the job is associated with.\n')
    recipe: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_RecipePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A series of data transformation steps that the job runs.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the job.\n')
    timeout: typing.Union[int, float, None] = pydantic.Field(None, description="The job's timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of ``TIMEOUT`` .\n")
    validation_configurations: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ValidationConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='List of validation configurations that are applied to the profile job.')
    _init_params: typing.ClassVar[list[str]] = ['name', 'role_arn', 'type', 'database_outputs', 'data_catalog_outputs', 'dataset_name', 'encryption_key_arn', 'encryption_mode', 'job_sample', 'log_subscription', 'max_capacity', 'max_retries', 'output_location', 'outputs', 'profile_configuration', 'project_name', 'recipe', 'tags', 'timeout', 'validation_configurations']
    _method_names: typing.ClassVar[list[str]] = ['AllowedStatisticsProperty', 'ColumnSelectorProperty', 'ColumnStatisticsConfigurationProperty', 'CsvOutputOptionsProperty', 'DataCatalogOutputProperty', 'DatabaseOutputProperty', 'DatabaseTableOutputOptionsProperty', 'EntityDetectorConfigurationProperty', 'JobSampleProperty', 'OutputFormatOptionsProperty', 'OutputLocationProperty', 'OutputProperty', 'ProfileConfigurationProperty', 'RecipeProperty', 'S3LocationProperty', 'S3TableOutputOptionsProperty', 'StatisticOverrideProperty', 'StatisticsConfigurationProperty', 'ValidationConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJob'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnJobDefConfig] = pydantic.Field(None)


class CfnJobDefConfig(pydantic.BaseModel):
    AllowedStatisticsProperty: typing.Optional[list[CfnJobDefAllowedstatisticspropertyParams]] = pydantic.Field(None, description='')
    ColumnSelectorProperty: typing.Optional[list[CfnJobDefColumnselectorpropertyParams]] = pydantic.Field(None, description='')
    ColumnStatisticsConfigurationProperty: typing.Optional[list[CfnJobDefColumnstatisticsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    CsvOutputOptionsProperty: typing.Optional[list[CfnJobDefCsvoutputoptionspropertyParams]] = pydantic.Field(None, description='')
    DataCatalogOutputProperty: typing.Optional[list[CfnJobDefDatacatalogoutputpropertyParams]] = pydantic.Field(None, description='')
    DatabaseOutputProperty: typing.Optional[list[CfnJobDefDatabaseoutputpropertyParams]] = pydantic.Field(None, description='')
    DatabaseTableOutputOptionsProperty: typing.Optional[list[CfnJobDefDatabasetableoutputoptionspropertyParams]] = pydantic.Field(None, description='')
    EntityDetectorConfigurationProperty: typing.Optional[list[CfnJobDefEntitydetectorconfigurationpropertyParams]] = pydantic.Field(None, description='')
    JobSampleProperty: typing.Optional[list[CfnJobDefJobsamplepropertyParams]] = pydantic.Field(None, description='')
    OutputFormatOptionsProperty: typing.Optional[list[CfnJobDefOutputformatoptionspropertyParams]] = pydantic.Field(None, description='')
    OutputLocationProperty: typing.Optional[list[CfnJobDefOutputlocationpropertyParams]] = pydantic.Field(None, description='')
    OutputProperty: typing.Optional[list[CfnJobDefOutputpropertyParams]] = pydantic.Field(None, description='')
    ProfileConfigurationProperty: typing.Optional[list[CfnJobDefProfileconfigurationpropertyParams]] = pydantic.Field(None, description='')
    RecipeProperty: typing.Optional[list[CfnJobDefRecipepropertyParams]] = pydantic.Field(None, description='')
    S3LocationProperty: typing.Optional[list[CfnJobDefS3LocationpropertyParams]] = pydantic.Field(None, description='')
    S3TableOutputOptionsProperty: typing.Optional[list[CfnJobDefS3TableoutputoptionspropertyParams]] = pydantic.Field(None, description='')
    StatisticOverrideProperty: typing.Optional[list[CfnJobDefStatisticoverridepropertyParams]] = pydantic.Field(None, description='')
    StatisticsConfigurationProperty: typing.Optional[list[CfnJobDefStatisticsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ValidationConfigurationProperty: typing.Optional[list[CfnJobDefValidationconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnJobDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnJobDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnJobDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnJobDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnJobDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnJobDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnJobDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnJobDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnJobDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnJobDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnJobDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnJobDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnJobDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnJobDefAllowedstatisticspropertyParams(pydantic.BaseModel):
    statistics: typing.Sequence[str] = pydantic.Field(..., description='')
    ...

class CfnJobDefColumnselectorpropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    regex: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefColumnstatisticsconfigurationpropertyParams(pydantic.BaseModel):
    statistics: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_StatisticsConfigurationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    selectors: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ColumnSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefCsvoutputoptionspropertyParams(pydantic.BaseModel):
    delimiter: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefDatacatalogoutputpropertyParams(pydantic.BaseModel):
    database_name: str = pydantic.Field(..., description='')
    table_name: str = pydantic.Field(..., description='')
    catalog_id: typing.Optional[str] = pydantic.Field(None, description='')
    database_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DatabaseTableOutputOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    overwrite: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    s3_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3TableOutputOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefDatabaseoutputpropertyParams(pydantic.BaseModel):
    database_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DatabaseTableOutputOptionsPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    glue_connection_name: str = pydantic.Field(..., description='')
    database_output_mode: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefDatabasetableoutputoptionspropertyParams(pydantic.BaseModel):
    table_name: str = pydantic.Field(..., description='')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefEntitydetectorconfigurationpropertyParams(pydantic.BaseModel):
    entity_types: typing.Sequence[str] = pydantic.Field(..., description='')
    allowed_statistics: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_AllowedStatisticsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefJobsamplepropertyParams(pydantic.BaseModel):
    mode: typing.Optional[str] = pydantic.Field(None, description='')
    size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefOutputformatoptionspropertyParams(pydantic.BaseModel):
    csv: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_CsvOutputOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefOutputlocationpropertyParams(pydantic.BaseModel):
    bucket: str = pydantic.Field(..., description='')
    bucket_owner: typing.Optional[str] = pydantic.Field(None, description='')
    key: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefOutputpropertyParams(pydantic.BaseModel):
    location: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3LocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    compression_format: typing.Optional[str] = pydantic.Field(None, description='')
    format: typing.Optional[str] = pydantic.Field(None, description='')
    format_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_OutputFormatOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    max_output_files: typing.Union[int, float, None] = pydantic.Field(None, description='')
    overwrite: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    partition_columns: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnJobDefProfileconfigurationpropertyParams(pydantic.BaseModel):
    column_statistics_configurations: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ColumnStatisticsConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    dataset_statistics_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_StatisticsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    entity_detector_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_EntityDetectorConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    profile_columns: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ColumnSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefRecipepropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefS3LocationpropertyParams(pydantic.BaseModel):
    bucket: str = pydantic.Field(..., description='')
    bucket_owner: typing.Optional[str] = pydantic.Field(None, description='')
    key: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefS3TableoutputoptionspropertyParams(pydantic.BaseModel):
    location: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_S3LocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnJobDefStatisticoverridepropertyParams(pydantic.BaseModel):
    parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Mapping[str, str]] = pydantic.Field(..., description='')
    statistic: str = pydantic.Field(..., description='')
    ...

class CfnJobDefStatisticsconfigurationpropertyParams(pydantic.BaseModel):
    included_statistics: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    overrides: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_StatisticOverridePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefValidationconfigurationpropertyParams(pydantic.BaseModel):
    ruleset_arn: str = pydantic.Field(..., description='')
    validation_mode: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnJobDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnJobDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnJobDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnJobDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnJobDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnJobDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnJobDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnJobDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnJobDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_databrew.CfnProject
class CfnProjectDef(BaseCfnResource):
    dataset_name: str = pydantic.Field(..., description='The dataset that the project is to act upon.\n')
    name: str = pydantic.Field(..., description='The unique name of a project.\n')
    recipe_name: str = pydantic.Field(..., description='The name of a recipe that will be developed during a project session.\n')
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the role that will be assumed for this project.\n')
    sample: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnProject_SamplePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The sample size and sampling type to apply to the data. If this parameter isn't specified, then the sample consists of the first 500 rows from the dataset.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the project.')
    _init_params: typing.ClassVar[list[str]] = ['dataset_name', 'name', 'recipe_name', 'role_arn', 'sample', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['SampleProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnProject'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnProjectDefConfig] = pydantic.Field(None)


class CfnProjectDefConfig(pydantic.BaseModel):
    SampleProperty: typing.Optional[list[CfnProjectDefSamplepropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnProjectDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnProjectDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnProjectDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnProjectDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnProjectDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnProjectDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnProjectDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnProjectDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnProjectDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnProjectDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnProjectDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnProjectDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnProjectDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnProjectDefSamplepropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnProjectDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnProjectDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnProjectDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnProjectDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnProjectDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnProjectDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnProjectDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnProjectDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnProjectDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnProjectDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnProjectDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnProjectDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnProjectDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnProjectDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_databrew.CfnRecipe
class CfnRecipeDef(BaseCfnResource):
    name: str = pydantic.Field(..., description='The unique name for the recipe.\n')
    steps: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_RecipeStepPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='A list of steps that are defined by the recipe.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the recipe.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the recipe.')
    _init_params: typing.ClassVar[list[str]] = ['name', 'steps', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['ActionProperty', 'ConditionExpressionProperty', 'DataCatalogInputDefinitionProperty', 'InputProperty', 'RecipeParametersProperty', 'RecipeStepProperty', 'S3LocationProperty', 'SecondaryInputProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipe'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnRecipeDefConfig] = pydantic.Field(None)


class CfnRecipeDefConfig(pydantic.BaseModel):
    ActionProperty: typing.Optional[list[CfnRecipeDefActionpropertyParams]] = pydantic.Field(None, description='')
    ConditionExpressionProperty: typing.Optional[list[CfnRecipeDefConditionexpressionpropertyParams]] = pydantic.Field(None, description='')
    DataCatalogInputDefinitionProperty: typing.Optional[list[CfnRecipeDefDatacataloginputdefinitionpropertyParams]] = pydantic.Field(None, description='')
    InputProperty: typing.Optional[list[CfnRecipeDefInputpropertyParams]] = pydantic.Field(None, description='')
    RecipeParametersProperty: typing.Optional[list[CfnRecipeDefRecipeparameterspropertyParams]] = pydantic.Field(None, description='')
    RecipeStepProperty: typing.Optional[list[CfnRecipeDefRecipesteppropertyParams]] = pydantic.Field(None, description='')
    S3LocationProperty: typing.Optional[list[CfnRecipeDefS3LocationpropertyParams]] = pydantic.Field(None, description='')
    SecondaryInputProperty: typing.Optional[list[CfnRecipeDefSecondaryinputpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnRecipeDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnRecipeDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnRecipeDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnRecipeDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnRecipeDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnRecipeDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnRecipeDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnRecipeDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnRecipeDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnRecipeDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnRecipeDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnRecipeDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnRecipeDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnRecipeDefActionpropertyParams(pydantic.BaseModel):
    operation: str = pydantic.Field(..., description='')
    parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefConditionexpressionpropertyParams(pydantic.BaseModel):
    condition: str = pydantic.Field(..., description='')
    target_column: str = pydantic.Field(..., description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefDatacataloginputdefinitionpropertyParams(pydantic.BaseModel):
    catalog_id: typing.Optional[str] = pydantic.Field(None, description='')
    database_name: typing.Optional[str] = pydantic.Field(None, description='')
    table_name: typing.Optional[str] = pydantic.Field(None, description='')
    temp_directory: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefInputpropertyParams(pydantic.BaseModel):
    data_catalog_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_DataCatalogInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    s3_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefRecipeparameterspropertyParams(pydantic.BaseModel):
    aggregate_function: typing.Optional[str] = pydantic.Field(None, description='')
    base: typing.Optional[str] = pydantic.Field(None, description='')
    case_statement: typing.Optional[str] = pydantic.Field(None, description='')
    category_map: typing.Optional[str] = pydantic.Field(None, description='')
    chars_to_remove: typing.Optional[str] = pydantic.Field(None, description='')
    collapse_consecutive_whitespace: typing.Optional[str] = pydantic.Field(None, description='')
    column_data_type: typing.Optional[str] = pydantic.Field(None, description='')
    column_range: typing.Optional[str] = pydantic.Field(None, description='')
    count: typing.Optional[str] = pydantic.Field(None, description='')
    custom_characters: typing.Optional[str] = pydantic.Field(None, description='')
    custom_stop_words: typing.Optional[str] = pydantic.Field(None, description='')
    custom_value: typing.Optional[str] = pydantic.Field(None, description='')
    datasets_columns: typing.Optional[str] = pydantic.Field(None, description='')
    date_add_value: typing.Optional[str] = pydantic.Field(None, description='')
    date_time_format: typing.Optional[str] = pydantic.Field(None, description='')
    date_time_parameters: typing.Optional[str] = pydantic.Field(None, description='')
    delete_other_rows: typing.Optional[str] = pydantic.Field(None, description='')
    delimiter: typing.Optional[str] = pydantic.Field(None, description='')
    end_pattern: typing.Optional[str] = pydantic.Field(None, description='')
    end_position: typing.Optional[str] = pydantic.Field(None, description='')
    end_value: typing.Optional[str] = pydantic.Field(None, description='')
    expand_contractions: typing.Optional[str] = pydantic.Field(None, description='')
    exponent: typing.Optional[str] = pydantic.Field(None, description='')
    false_string: typing.Optional[str] = pydantic.Field(None, description='')
    group_by_agg_function_options: typing.Optional[str] = pydantic.Field(None, description='')
    group_by_columns: typing.Optional[str] = pydantic.Field(None, description='')
    hidden_columns: typing.Optional[str] = pydantic.Field(None, description='')
    ignore_case: typing.Optional[str] = pydantic.Field(None, description='')
    include_in_split: typing.Optional[str] = pydantic.Field(None, description='')
    input: typing.Any = pydantic.Field(None, description='')
    interval: typing.Optional[str] = pydantic.Field(None, description='')
    is_text: typing.Optional[str] = pydantic.Field(None, description='')
    join_keys: typing.Optional[str] = pydantic.Field(None, description='')
    join_type: typing.Optional[str] = pydantic.Field(None, description='')
    left_columns: typing.Optional[str] = pydantic.Field(None, description='')
    limit: typing.Optional[str] = pydantic.Field(None, description='')
    lower_bound: typing.Optional[str] = pydantic.Field(None, description='')
    map_type: typing.Optional[str] = pydantic.Field(None, description='')
    mode_type: typing.Optional[str] = pydantic.Field(None, description='')
    multi_line: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    num_rows: typing.Optional[str] = pydantic.Field(None, description='')
    num_rows_after: typing.Optional[str] = pydantic.Field(None, description='')
    num_rows_before: typing.Optional[str] = pydantic.Field(None, description='')
    order_by_column: typing.Optional[str] = pydantic.Field(None, description='')
    order_by_columns: typing.Optional[str] = pydantic.Field(None, description='')
    other: typing.Optional[str] = pydantic.Field(None, description='')
    pattern: typing.Optional[str] = pydantic.Field(None, description='')
    pattern_option1: typing.Optional[str] = pydantic.Field(None, description='')
    pattern_option2: typing.Optional[str] = pydantic.Field(None, description='')
    pattern_options: typing.Optional[str] = pydantic.Field(None, description='')
    period: typing.Optional[str] = pydantic.Field(None, description='')
    position: typing.Optional[str] = pydantic.Field(None, description='')
    remove_all_punctuation: typing.Optional[str] = pydantic.Field(None, description='')
    remove_all_quotes: typing.Optional[str] = pydantic.Field(None, description='')
    remove_all_whitespace: typing.Optional[str] = pydantic.Field(None, description='')
    remove_custom_characters: typing.Optional[str] = pydantic.Field(None, description='')
    remove_custom_value: typing.Optional[str] = pydantic.Field(None, description='')
    remove_leading_and_trailing_punctuation: typing.Optional[str] = pydantic.Field(None, description='')
    remove_leading_and_trailing_quotes: typing.Optional[str] = pydantic.Field(None, description='')
    remove_leading_and_trailing_whitespace: typing.Optional[str] = pydantic.Field(None, description='')
    remove_letters: typing.Optional[str] = pydantic.Field(None, description='')
    remove_numbers: typing.Optional[str] = pydantic.Field(None, description='')
    remove_source_column: typing.Optional[str] = pydantic.Field(None, description='')
    remove_special_characters: typing.Optional[str] = pydantic.Field(None, description='')
    right_columns: typing.Optional[str] = pydantic.Field(None, description='')
    sample_size: typing.Optional[str] = pydantic.Field(None, description='')
    sample_type: typing.Optional[str] = pydantic.Field(None, description='')
    secondary_inputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_SecondaryInputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    second_input: typing.Optional[str] = pydantic.Field(None, description='')
    sheet_indexes: typing.Union[typing.Sequence[typing.Union[int, float]], typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    sheet_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    source_column: typing.Optional[str] = pydantic.Field(None, description='')
    source_column1: typing.Optional[str] = pydantic.Field(None, description='')
    source_column2: typing.Optional[str] = pydantic.Field(None, description='')
    source_columns: typing.Optional[str] = pydantic.Field(None, description='')
    start_column_index: typing.Optional[str] = pydantic.Field(None, description='')
    start_pattern: typing.Optional[str] = pydantic.Field(None, description='')
    start_position: typing.Optional[str] = pydantic.Field(None, description='')
    start_value: typing.Optional[str] = pydantic.Field(None, description='')
    stemming_mode: typing.Optional[str] = pydantic.Field(None, description='')
    step_count: typing.Optional[str] = pydantic.Field(None, description='')
    step_index: typing.Optional[str] = pydantic.Field(None, description='')
    stop_words_mode: typing.Optional[str] = pydantic.Field(None, description='')
    strategy: typing.Optional[str] = pydantic.Field(None, description='')
    target_column: typing.Optional[str] = pydantic.Field(None, description='')
    target_column_names: typing.Optional[str] = pydantic.Field(None, description='')
    target_date_format: typing.Optional[str] = pydantic.Field(None, description='')
    target_index: typing.Optional[str] = pydantic.Field(None, description='')
    time_zone: typing.Optional[str] = pydantic.Field(None, description='')
    tokenizer_pattern: typing.Optional[str] = pydantic.Field(None, description='')
    true_string: typing.Optional[str] = pydantic.Field(None, description='')
    udf_lang: typing.Optional[str] = pydantic.Field(None, description='')
    units: typing.Optional[str] = pydantic.Field(None, description='')
    unpivot_column: typing.Optional[str] = pydantic.Field(None, description='')
    upper_bound: typing.Optional[str] = pydantic.Field(None, description='')
    use_new_data_frame: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    value1: typing.Optional[str] = pydantic.Field(None, description='')
    value2: typing.Optional[str] = pydantic.Field(None, description='')
    value_column: typing.Optional[str] = pydantic.Field(None, description='')
    view_frame: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefRecipesteppropertyParams(pydantic.BaseModel):
    action: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_ActionPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    condition_expressions: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_ConditionExpressionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefS3LocationpropertyParams(pydantic.BaseModel):
    bucket: str = pydantic.Field(..., description='')
    key: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefSecondaryinputpropertyParams(pydantic.BaseModel):
    data_catalog_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_DataCatalogInputDefinitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    s3_input_definition: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_S3LocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnRecipeDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnRecipeDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnRecipeDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnRecipeDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnRecipeDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnRecipeDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnRecipeDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnRecipeDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnRecipeDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnRecipeDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnRecipeDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnRecipeDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnRecipeDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnRecipeDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_databrew.CfnRuleset
class CfnRulesetDef(BaseCfnResource):
    name: str = pydantic.Field(..., description='The name of the ruleset.\n')
    rules: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_RulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='Contains metadata about the ruleset.\n')
    target_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of a resource (dataset) that the ruleset is associated with.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the ruleset.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource. For more information, see `Tag <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-resource-tags.html>`_ .')
    _init_params: typing.ClassVar[list[str]] = ['name', 'rules', 'target_arn', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['ColumnSelectorProperty', 'RuleProperty', 'SubstitutionValueProperty', 'ThresholdProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRuleset'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnRulesetDefConfig] = pydantic.Field(None)


class CfnRulesetDefConfig(pydantic.BaseModel):
    ColumnSelectorProperty: typing.Optional[list[CfnRulesetDefColumnselectorpropertyParams]] = pydantic.Field(None, description='')
    RuleProperty: typing.Optional[list[CfnRulesetDefRulepropertyParams]] = pydantic.Field(None, description='')
    SubstitutionValueProperty: typing.Optional[list[CfnRulesetDefSubstitutionvaluepropertyParams]] = pydantic.Field(None, description='')
    ThresholdProperty: typing.Optional[list[CfnRulesetDefThresholdpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnRulesetDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnRulesetDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnRulesetDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnRulesetDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnRulesetDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnRulesetDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnRulesetDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnRulesetDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnRulesetDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnRulesetDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnRulesetDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnRulesetDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnRulesetDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnRulesetDefColumnselectorpropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    regex: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnRulesetDefRulepropertyParams(pydantic.BaseModel):
    check_expression: str = pydantic.Field(..., description='')
    name: str = pydantic.Field(..., description='')
    column_selectors: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_ColumnSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    disabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    substitution_map: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_SubstitutionValuePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    threshold: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_ThresholdPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnRulesetDefSubstitutionvaluepropertyParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='')
    value_reference: str = pydantic.Field(..., description='')
    ...

class CfnRulesetDefThresholdpropertyParams(pydantic.BaseModel):
    value: typing.Union[int, float] = pydantic.Field(..., description='')
    type: typing.Optional[str] = pydantic.Field(None, description='')
    unit: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnRulesetDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnRulesetDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnRulesetDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnRulesetDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnRulesetDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnRulesetDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnRulesetDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnRulesetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnRulesetDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnRulesetDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnRulesetDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnRulesetDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnRulesetDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnRulesetDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_databrew.CfnSchedule
class CfnScheduleDef(BaseCfnResource):
    cron_expression: str = pydantic.Field(..., description='The dates and times when the job is to run. For more information, see `Working with cron expressions for recipe jobs <https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.html#jobs.cron>`_ in the *AWS Glue DataBrew Developer Guide* .\n')
    name: str = pydantic.Field(..., description='The name of the schedule.\n')
    job_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of jobs to be run, according to the schedule.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the schedule.')
    _init_params: typing.ClassVar[list[str]] = ['cron_expression', 'name', 'job_names', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnSchedule'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnScheduleDefConfig] = pydantic.Field(None)


class CfnScheduleDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnScheduleDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnScheduleDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnScheduleDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnScheduleDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnScheduleDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnScheduleDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnScheduleDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnScheduleDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnScheduleDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnScheduleDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnScheduleDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnScheduleDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnScheduleDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnScheduleDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnScheduleDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnScheduleDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnScheduleDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnScheduleDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnScheduleDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnScheduleDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnScheduleDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnScheduleDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnScheduleDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnScheduleDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnScheduleDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnScheduleDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnScheduleDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_databrew.CfnDatasetProps
class CfnDatasetPropsDef(BaseCfnProperty):
    input: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_InputPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='Information on how DataBrew can find the dataset, in either the AWS Glue Data Catalog or Amazon S3 .\n')
    name: str = pydantic.Field(..., description='The unique name of the dataset.\n')
    format: typing.Optional[str] = pydantic.Field(None, description='The file format of a dataset that is created from an Amazon S3 file or folder.\n')
    format_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_FormatOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A set of options that define how DataBrew interprets the data in the dataset.\n')
    path_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnDataset_PathOptionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A set of options that defines how DataBrew interprets an Amazon S3 path of the dataset.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the dataset.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-dataset.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    cfn_dataset_props = databrew.CfnDatasetProps(\n        input=databrew.CfnDataset.InputProperty(\n            database_input_definition=databrew.CfnDataset.DatabaseInputDefinitionProperty(\n                glue_connection_name="glueConnectionName",\n\n                # the properties below are optional\n                database_table_name="databaseTableName",\n                query_string="queryString",\n                temp_directory=databrew.CfnDataset.S3LocationProperty(\n                    bucket="bucket",\n\n                    # the properties below are optional\n                    key="key"\n                )\n            ),\n            data_catalog_input_definition=databrew.CfnDataset.DataCatalogInputDefinitionProperty(\n                catalog_id="catalogId",\n                database_name="databaseName",\n                table_name="tableName",\n                temp_directory=databrew.CfnDataset.S3LocationProperty(\n                    bucket="bucket",\n\n                    # the properties below are optional\n                    key="key"\n                )\n            ),\n            metadata=databrew.CfnDataset.MetadataProperty(\n                source_arn="sourceArn"\n            ),\n            s3_input_definition=databrew.CfnDataset.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                key="key"\n            )\n        ),\n        name="name",\n\n        # the properties below are optional\n        format="format",\n        format_options=databrew.CfnDataset.FormatOptionsProperty(\n            csv=databrew.CfnDataset.CsvOptionsProperty(\n                delimiter="delimiter",\n                header_row=False\n            ),\n            excel=databrew.CfnDataset.ExcelOptionsProperty(\n                header_row=False,\n                sheet_indexes=[123],\n                sheet_names=["sheetNames"]\n            ),\n            json=databrew.CfnDataset.JsonOptionsProperty(\n                multi_line=False\n            )\n        ),\n        path_options=databrew.CfnDataset.PathOptionsProperty(\n            files_limit=databrew.CfnDataset.FilesLimitProperty(\n                max_files=123,\n\n                # the properties below are optional\n                order="order",\n                ordered_by="orderedBy"\n            ),\n            last_modified_date_condition=databrew.CfnDataset.FilterExpressionProperty(\n                expression="expression",\n                values_map=[databrew.CfnDataset.FilterValueProperty(\n                    value="value",\n                    value_reference="valueReference"\n                )]\n            ),\n            parameters=[databrew.CfnDataset.PathParameterProperty(\n                dataset_parameter=databrew.CfnDataset.DatasetParameterProperty(\n                    name="name",\n                    type="type",\n\n                    # the properties below are optional\n                    create_column=False,\n                    datetime_options=databrew.CfnDataset.DatetimeOptionsProperty(\n                        format="format",\n\n                        # the properties below are optional\n                        locale_code="localeCode",\n                        timezone_offset="timezoneOffset"\n                    ),\n                    filter=databrew.CfnDataset.FilterExpressionProperty(\n                        expression="expression",\n                        values_map=[databrew.CfnDataset.FilterValueProperty(\n                            value="value",\n                            value_reference="valueReference"\n                        )]\n                    )\n                ),\n                path_parameter_name="pathParameterName"\n            )]\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['input', 'name', 'format', 'format_options', 'path_options', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnDatasetProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnJobProps
class CfnJobPropsDef(BaseCfnProperty):
    name: str = pydantic.Field(..., description='The unique name of the job.\n')
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the role to be assumed for this job.\n')
    type: str = pydantic.Field(..., description='The job type of the job, which must be one of the following:. - ``PROFILE`` - A job to analyze a dataset, to determine its size, data types, data distribution, and more. - ``RECIPE`` - A job to apply one or more transformations to a dataset.\n')
    database_outputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DatabaseOutputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Represents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.\n')
    data_catalog_outputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_DataCatalogOutputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more artifacts that represent the AWS Glue Data Catalog output from running the job.\n')
    dataset_name: typing.Optional[str] = pydantic.Field(None, description='A dataset that the job is to process.\n')
    encryption_key_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of an encryption key that is used to protect the job output. For more information, see `Encrypting data written by DataBrew jobs <https://docs.aws.amazon.com/databrew/latest/dg/encryption-security-configuration.html>`_\n')
    encryption_mode: typing.Optional[str] = pydantic.Field(None, description='The encryption mode for the job, which can be one of the following:. - ``SSE-KMS`` - Server-side encryption with keys managed by AWS KMS . - ``SSE-S3`` - Server-side encryption with keys managed by Amazon S3.\n')
    job_sample: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_JobSamplePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="A sample configuration for profile jobs only, which determines the number of rows on which the profile job is run. If a ``JobSample`` value isn't provided, the default value is used. The default value is CUSTOM_ROWS for the mode parameter and 20,000 for the size parameter.\n")
    log_subscription: typing.Optional[str] = pydantic.Field(None, description='The current status of Amazon CloudWatch logging for the job.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of nodes that can be consumed when the job processes data.\n')
    max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry the job after a job run fails.\n')
    output_location: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_OutputLocationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``AWS::DataBrew::Job.OutputLocation``.\n')
    outputs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_OutputPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more artifacts that represent output from running the job.\n')
    profile_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ProfileConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for profile jobs. Configuration can be used to select columns, do evaluations, and override default parameters of evaluations. When configuration is undefined, the profile job will apply default settings to all supported columns.\n')
    project_name: typing.Optional[str] = pydantic.Field(None, description='The name of the project that the job is associated with.\n')
    recipe: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_RecipePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A series of data transformation steps that the job runs.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the job.\n')
    timeout: typing.Union[int, float, None] = pydantic.Field(None, description="The job's timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of ``TIMEOUT`` .\n")
    validation_configurations: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnJob_ValidationConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='List of validation configurations that are applied to the profile job.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-job.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    cfn_job_props = databrew.CfnJobProps(\n        name="name",\n        role_arn="roleArn",\n        type="type",\n\n        # the properties below are optional\n        database_outputs=[databrew.CfnJob.DatabaseOutputProperty(\n            database_options=databrew.CfnJob.DatabaseTableOutputOptionsProperty(\n                table_name="tableName",\n\n                # the properties below are optional\n                temp_directory=databrew.CfnJob.S3LocationProperty(\n                    bucket="bucket",\n\n                    # the properties below are optional\n                    bucket_owner="bucketOwner",\n                    key="key"\n                )\n            ),\n            glue_connection_name="glueConnectionName",\n\n            # the properties below are optional\n            database_output_mode="databaseOutputMode"\n        )],\n        data_catalog_outputs=[databrew.CfnJob.DataCatalogOutputProperty(\n            database_name="databaseName",\n            table_name="tableName",\n\n            # the properties below are optional\n            catalog_id="catalogId",\n            database_options=databrew.CfnJob.DatabaseTableOutputOptionsProperty(\n                table_name="tableName",\n\n                # the properties below are optional\n                temp_directory=databrew.CfnJob.S3LocationProperty(\n                    bucket="bucket",\n\n                    # the properties below are optional\n                    bucket_owner="bucketOwner",\n                    key="key"\n                )\n            ),\n            overwrite=False,\n            s3_options=databrew.CfnJob.S3TableOutputOptionsProperty(\n                location=databrew.CfnJob.S3LocationProperty(\n                    bucket="bucket",\n\n                    # the properties below are optional\n                    bucket_owner="bucketOwner",\n                    key="key"\n                )\n            )\n        )],\n        dataset_name="datasetName",\n        encryption_key_arn="encryptionKeyArn",\n        encryption_mode="encryptionMode",\n        job_sample=databrew.CfnJob.JobSampleProperty(\n            mode="mode",\n            size=123\n        ),\n        log_subscription="logSubscription",\n        max_capacity=123,\n        max_retries=123,\n        output_location=databrew.CfnJob.OutputLocationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            bucket_owner="bucketOwner",\n            key="key"\n        ),\n        outputs=[databrew.CfnJob.OutputProperty(\n            location=databrew.CfnJob.S3LocationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                bucket_owner="bucketOwner",\n                key="key"\n            ),\n\n            # the properties below are optional\n            compression_format="compressionFormat",\n            format="format",\n            format_options=databrew.CfnJob.OutputFormatOptionsProperty(\n                csv=databrew.CfnJob.CsvOutputOptionsProperty(\n                    delimiter="delimiter"\n                )\n            ),\n            max_output_files=123,\n            overwrite=False,\n            partition_columns=["partitionColumns"]\n        )],\n        profile_configuration=databrew.CfnJob.ProfileConfigurationProperty(\n            column_statistics_configurations=[databrew.CfnJob.ColumnStatisticsConfigurationProperty(\n                statistics=databrew.CfnJob.StatisticsConfigurationProperty(\n                    included_statistics=["includedStatistics"],\n                    overrides=[databrew.CfnJob.StatisticOverrideProperty(\n                        parameters={\n                            "parameters_key": "parameters"\n                        },\n                        statistic="statistic"\n                    )]\n                ),\n\n                # the properties below are optional\n                selectors=[databrew.CfnJob.ColumnSelectorProperty(\n                    name="name",\n                    regex="regex"\n                )]\n            )],\n            dataset_statistics_configuration=databrew.CfnJob.StatisticsConfigurationProperty(\n                included_statistics=["includedStatistics"],\n                overrides=[databrew.CfnJob.StatisticOverrideProperty(\n                    parameters={\n                        "parameters_key": "parameters"\n                    },\n                    statistic="statistic"\n                )]\n            ),\n            entity_detector_configuration=databrew.CfnJob.EntityDetectorConfigurationProperty(\n                entity_types=["entityTypes"],\n\n                # the properties below are optional\n                allowed_statistics=databrew.CfnJob.AllowedStatisticsProperty(\n                    statistics=["statistics"]\n                )\n            ),\n            profile_columns=[databrew.CfnJob.ColumnSelectorProperty(\n                name="name",\n                regex="regex"\n            )]\n        ),\n        project_name="projectName",\n        recipe=databrew.CfnJob.RecipeProperty(\n            name="name",\n\n            # the properties below are optional\n            version="version"\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        timeout=123,\n        validation_configurations=[databrew.CfnJob.ValidationConfigurationProperty(\n            ruleset_arn="rulesetArn",\n\n            # the properties below are optional\n            validation_mode="validationMode"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'role_arn', 'type', 'database_outputs', 'data_catalog_outputs', 'dataset_name', 'encryption_key_arn', 'encryption_mode', 'job_sample', 'log_subscription', 'max_capacity', 'max_retries', 'output_location', 'outputs', 'profile_configuration', 'project_name', 'recipe', 'tags', 'timeout', 'validation_configurations']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnJobProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnProjectProps
class CfnProjectPropsDef(BaseCfnProperty):
    dataset_name: str = pydantic.Field(..., description='The dataset that the project is to act upon.\n')
    name: str = pydantic.Field(..., description='The unique name of a project.\n')
    recipe_name: str = pydantic.Field(..., description='The name of a recipe that will be developed during a project session.\n')
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the role that will be assumed for this project.\n')
    sample: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnProject_SamplePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The sample size and sampling type to apply to the data. If this parameter isn't specified, then the sample consists of the first 500 rows from the dataset.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the project.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-project.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    cfn_project_props = databrew.CfnProjectProps(\n        dataset_name="datasetName",\n        name="name",\n        recipe_name="recipeName",\n        role_arn="roleArn",\n\n        # the properties below are optional\n        sample=databrew.CfnProject.SampleProperty(\n            type="type",\n\n            # the properties below are optional\n            size=123\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dataset_name', 'name', 'recipe_name', 'role_arn', 'sample', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnProjectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRecipeProps
class CfnRecipePropsDef(BaseCfnProperty):
    name: str = pydantic.Field(..., description='The unique name for the recipe.\n')
    steps: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRecipe_RecipeStepPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='A list of steps that are defined by the recipe.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the recipe.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the recipe.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-recipe.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    cfn_recipe_props = databrew.CfnRecipeProps(\n        name="name",\n        steps=[databrew.CfnRecipe.RecipeStepProperty(\n            action=databrew.CfnRecipe.ActionProperty(\n                operation="operation",\n\n                # the properties below are optional\n                parameters={\n                    "parameters_key": "parameters"\n                }\n            ),\n\n            # the properties below are optional\n            condition_expressions=[databrew.CfnRecipe.ConditionExpressionProperty(\n                condition="condition",\n                target_column="targetColumn",\n\n                # the properties below are optional\n                value="value"\n            )]\n        )],\n\n        # the properties below are optional\n        description="description",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'steps', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRecipeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnRulesetProps
class CfnRulesetPropsDef(BaseCfnProperty):
    name: str = pydantic.Field(..., description='The name of the ruleset.\n')
    rules: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_databrew.CfnRuleset_RulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='Contains metadata about the ruleset.\n')
    target_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of a resource (dataset) that the ruleset is associated with.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the ruleset.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource. For more information, see `Tag <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-resource-tags.html>`_ .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-ruleset.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    cfn_ruleset_props = databrew.CfnRulesetProps(\n        name="name",\n        rules=[databrew.CfnRuleset.RuleProperty(\n            check_expression="checkExpression",\n            name="name",\n\n            # the properties below are optional\n            column_selectors=[databrew.CfnRuleset.ColumnSelectorProperty(\n                name="name",\n                regex="regex"\n            )],\n            disabled=False,\n            substitution_map=[databrew.CfnRuleset.SubstitutionValueProperty(\n                value="value",\n                value_reference="valueReference"\n            )],\n            threshold=databrew.CfnRuleset.ThresholdProperty(\n                value=123,\n\n                # the properties below are optional\n                type="type",\n                unit="unit"\n            )\n        )],\n        target_arn="targetArn",\n\n        # the properties below are optional\n        description="description",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'rules', 'target_arn', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnRulesetProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_databrew.CfnScheduleProps
class CfnSchedulePropsDef(BaseCfnProperty):
    cron_expression: str = pydantic.Field(..., description='The dates and times when the job is to run. For more information, see `Working with cron expressions for recipe jobs <https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.html#jobs.cron>`_ in the *AWS Glue DataBrew Developer Guide* .\n')
    name: str = pydantic.Field(..., description='The name of the schedule.\n')
    job_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of jobs to be run, according to the schedule.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Metadata tags that have been applied to the schedule.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-schedule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_databrew as databrew\n\n    cfn_schedule_props = databrew.CfnScheduleProps(\n        cron_expression="cronExpression",\n        name="name",\n\n        # the properties below are optional\n        job_names=["jobNames"],\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cron_expression', 'name', 'job_names', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_databrew.CfnScheduleProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




import models

class ModuleModel(pydantic.BaseModel):
    CfnDataset_CsvOptionsProperty: typing.Optional[dict[str, CfnDataset_CsvOptionsPropertyDef]] = pydantic.Field(None)
    CfnDataset_DatabaseInputDefinitionProperty: typing.Optional[dict[str, CfnDataset_DatabaseInputDefinitionPropertyDef]] = pydantic.Field(None)
    CfnDataset_DataCatalogInputDefinitionProperty: typing.Optional[dict[str, CfnDataset_DataCatalogInputDefinitionPropertyDef]] = pydantic.Field(None)
    CfnDataset_DatasetParameterProperty: typing.Optional[dict[str, CfnDataset_DatasetParameterPropertyDef]] = pydantic.Field(None)
    CfnDataset_DatetimeOptionsProperty: typing.Optional[dict[str, CfnDataset_DatetimeOptionsPropertyDef]] = pydantic.Field(None)
    CfnDataset_ExcelOptionsProperty: typing.Optional[dict[str, CfnDataset_ExcelOptionsPropertyDef]] = pydantic.Field(None)
    CfnDataset_FilesLimitProperty: typing.Optional[dict[str, CfnDataset_FilesLimitPropertyDef]] = pydantic.Field(None)
    CfnDataset_FilterExpressionProperty: typing.Optional[dict[str, CfnDataset_FilterExpressionPropertyDef]] = pydantic.Field(None)
    CfnDataset_FilterValueProperty: typing.Optional[dict[str, CfnDataset_FilterValuePropertyDef]] = pydantic.Field(None)
    CfnDataset_FormatOptionsProperty: typing.Optional[dict[str, CfnDataset_FormatOptionsPropertyDef]] = pydantic.Field(None)
    CfnDataset_InputProperty: typing.Optional[dict[str, CfnDataset_InputPropertyDef]] = pydantic.Field(None)
    CfnDataset_JsonOptionsProperty: typing.Optional[dict[str, CfnDataset_JsonOptionsPropertyDef]] = pydantic.Field(None)
    CfnDataset_MetadataProperty: typing.Optional[dict[str, CfnDataset_MetadataPropertyDef]] = pydantic.Field(None)
    CfnDataset_PathOptionsProperty: typing.Optional[dict[str, CfnDataset_PathOptionsPropertyDef]] = pydantic.Field(None)
    CfnDataset_PathParameterProperty: typing.Optional[dict[str, CfnDataset_PathParameterPropertyDef]] = pydantic.Field(None)
    CfnDataset_S3LocationProperty: typing.Optional[dict[str, CfnDataset_S3LocationPropertyDef]] = pydantic.Field(None)
    CfnJob_AllowedStatisticsProperty: typing.Optional[dict[str, CfnJob_AllowedStatisticsPropertyDef]] = pydantic.Field(None)
    CfnJob_ColumnSelectorProperty: typing.Optional[dict[str, CfnJob_ColumnSelectorPropertyDef]] = pydantic.Field(None)
    CfnJob_ColumnStatisticsConfigurationProperty: typing.Optional[dict[str, CfnJob_ColumnStatisticsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJob_CsvOutputOptionsProperty: typing.Optional[dict[str, CfnJob_CsvOutputOptionsPropertyDef]] = pydantic.Field(None)
    CfnJob_DatabaseOutputProperty: typing.Optional[dict[str, CfnJob_DatabaseOutputPropertyDef]] = pydantic.Field(None)
    CfnJob_DatabaseTableOutputOptionsProperty: typing.Optional[dict[str, CfnJob_DatabaseTableOutputOptionsPropertyDef]] = pydantic.Field(None)
    CfnJob_DataCatalogOutputProperty: typing.Optional[dict[str, CfnJob_DataCatalogOutputPropertyDef]] = pydantic.Field(None)
    CfnJob_EntityDetectorConfigurationProperty: typing.Optional[dict[str, CfnJob_EntityDetectorConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJob_JobSampleProperty: typing.Optional[dict[str, CfnJob_JobSamplePropertyDef]] = pydantic.Field(None)
    CfnJob_OutputFormatOptionsProperty: typing.Optional[dict[str, CfnJob_OutputFormatOptionsPropertyDef]] = pydantic.Field(None)
    CfnJob_OutputLocationProperty: typing.Optional[dict[str, CfnJob_OutputLocationPropertyDef]] = pydantic.Field(None)
    CfnJob_OutputProperty: typing.Optional[dict[str, CfnJob_OutputPropertyDef]] = pydantic.Field(None)
    CfnJob_ProfileConfigurationProperty: typing.Optional[dict[str, CfnJob_ProfileConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJob_RecipeProperty: typing.Optional[dict[str, CfnJob_RecipePropertyDef]] = pydantic.Field(None)
    CfnJob_S3LocationProperty: typing.Optional[dict[str, CfnJob_S3LocationPropertyDef]] = pydantic.Field(None)
    CfnJob_S3TableOutputOptionsProperty: typing.Optional[dict[str, CfnJob_S3TableOutputOptionsPropertyDef]] = pydantic.Field(None)
    CfnJob_StatisticOverrideProperty: typing.Optional[dict[str, CfnJob_StatisticOverridePropertyDef]] = pydantic.Field(None)
    CfnJob_StatisticsConfigurationProperty: typing.Optional[dict[str, CfnJob_StatisticsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJob_ValidationConfigurationProperty: typing.Optional[dict[str, CfnJob_ValidationConfigurationPropertyDef]] = pydantic.Field(None)
    CfnProject_SampleProperty: typing.Optional[dict[str, CfnProject_SamplePropertyDef]] = pydantic.Field(None)
    CfnRecipe_ActionProperty: typing.Optional[dict[str, CfnRecipe_ActionPropertyDef]] = pydantic.Field(None)
    CfnRecipe_ConditionExpressionProperty: typing.Optional[dict[str, CfnRecipe_ConditionExpressionPropertyDef]] = pydantic.Field(None)
    CfnRecipe_DataCatalogInputDefinitionProperty: typing.Optional[dict[str, CfnRecipe_DataCatalogInputDefinitionPropertyDef]] = pydantic.Field(None)
    CfnRecipe_InputProperty: typing.Optional[dict[str, CfnRecipe_InputPropertyDef]] = pydantic.Field(None)
    CfnRecipe_RecipeParametersProperty: typing.Optional[dict[str, CfnRecipe_RecipeParametersPropertyDef]] = pydantic.Field(None)
    CfnRecipe_RecipeStepProperty: typing.Optional[dict[str, CfnRecipe_RecipeStepPropertyDef]] = pydantic.Field(None)
    CfnRecipe_S3LocationProperty: typing.Optional[dict[str, CfnRecipe_S3LocationPropertyDef]] = pydantic.Field(None)
    CfnRecipe_SecondaryInputProperty: typing.Optional[dict[str, CfnRecipe_SecondaryInputPropertyDef]] = pydantic.Field(None)
    CfnRuleset_ColumnSelectorProperty: typing.Optional[dict[str, CfnRuleset_ColumnSelectorPropertyDef]] = pydantic.Field(None)
    CfnRuleset_RuleProperty: typing.Optional[dict[str, CfnRuleset_RulePropertyDef]] = pydantic.Field(None)
    CfnRuleset_SubstitutionValueProperty: typing.Optional[dict[str, CfnRuleset_SubstitutionValuePropertyDef]] = pydantic.Field(None)
    CfnRuleset_ThresholdProperty: typing.Optional[dict[str, CfnRuleset_ThresholdPropertyDef]] = pydantic.Field(None)
    CfnDataset: typing.Optional[dict[str, CfnDatasetDef]] = pydantic.Field(None)
    CfnJob: typing.Optional[dict[str, CfnJobDef]] = pydantic.Field(None)
    CfnProject: typing.Optional[dict[str, CfnProjectDef]] = pydantic.Field(None)
    CfnRecipe: typing.Optional[dict[str, CfnRecipeDef]] = pydantic.Field(None)
    CfnRuleset: typing.Optional[dict[str, CfnRulesetDef]] = pydantic.Field(None)
    CfnSchedule: typing.Optional[dict[str, CfnScheduleDef]] = pydantic.Field(None)
    CfnDatasetProps: typing.Optional[dict[str, CfnDatasetPropsDef]] = pydantic.Field(None)
    CfnJobProps: typing.Optional[dict[str, CfnJobPropsDef]] = pydantic.Field(None)
    CfnProjectProps: typing.Optional[dict[str, CfnProjectPropsDef]] = pydantic.Field(None)
    CfnRecipeProps: typing.Optional[dict[str, CfnRecipePropsDef]] = pydantic.Field(None)
    CfnRulesetProps: typing.Optional[dict[str, CfnRulesetPropsDef]] = pydantic.Field(None)
    CfnScheduleProps: typing.Optional[dict[str, CfnSchedulePropsDef]] = pydantic.Field(None)
    ...
