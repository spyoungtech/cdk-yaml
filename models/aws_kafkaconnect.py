from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.ApacheKafkaClusterProperty
class CfnConnector_ApacheKafkaClusterPropertyDef(BaseStruct):
    bootstrap_servers: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The bootstrap servers of the cluster.\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_VpcPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Details of an Amazon VPC which has network connectivity to the Apache Kafka cluster.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-apachekafkacluster.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    apache_kafka_cluster_property = kafkaconnect.CfnConnector.ApacheKafkaClusterProperty(\n        bootstrap_servers="bootstrapServers",\n        vpc=kafkaconnect.CfnConnector.VpcProperty(\n            security_groups=["securityGroups"],\n            subnets=["subnets"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bootstrap_servers', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.ApacheKafkaClusterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.AutoScalingProperty
class CfnConnector_AutoScalingPropertyDef(BaseStruct):
    max_worker_count: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The maximum number of workers allocated to the connector.\n')
    mcu_count: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of microcontroller units (MCUs) allocated to each connector worker. The valid values are 1,2,4,8.\n')
    min_worker_count: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The minimum number of workers allocated to the connector.\n')
    scale_in_policy: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ScaleInPolicyPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The sacle-in policy for the connector.\n')
    scale_out_policy: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ScaleOutPolicyPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The sacle-out policy for the connector.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-autoscaling.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    auto_scaling_property = kafkaconnect.CfnConnector.AutoScalingProperty(\n        max_worker_count=123,\n        mcu_count=123,\n        min_worker_count=123,\n        scale_in_policy=kafkaconnect.CfnConnector.ScaleInPolicyProperty(\n            cpu_utilization_percentage=123\n        ),\n        scale_out_policy=kafkaconnect.CfnConnector.ScaleOutPolicyProperty(\n            cpu_utilization_percentage=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_worker_count', 'mcu_count', 'min_worker_count', 'scale_in_policy', 'scale_out_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.AutoScalingProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.CapacityProperty
class CfnConnector_CapacityPropertyDef(BaseStruct):
    auto_scaling: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_AutoScalingPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about the auto scaling parameters for the connector.\n')
    provisioned_capacity: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ProvisionedCapacityPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Details about a fixed capacity allocated to a connector.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-capacity.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    capacity_property = kafkaconnect.CfnConnector.CapacityProperty(\n        auto_scaling=kafkaconnect.CfnConnector.AutoScalingProperty(\n            max_worker_count=123,\n            mcu_count=123,\n            min_worker_count=123,\n            scale_in_policy=kafkaconnect.CfnConnector.ScaleInPolicyProperty(\n                cpu_utilization_percentage=123\n            ),\n            scale_out_policy=kafkaconnect.CfnConnector.ScaleOutPolicyProperty(\n                cpu_utilization_percentage=123\n            )\n        ),\n        provisioned_capacity=kafkaconnect.CfnConnector.ProvisionedCapacityProperty(\n            worker_count=123,\n\n            # the properties below are optional\n            mcu_count=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_scaling', 'provisioned_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.CapacityProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.CloudWatchLogsLogDeliveryProperty
class CfnConnector_CloudWatchLogsLogDeliveryPropertyDef(BaseStruct):
    enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Whether log delivery to Amazon CloudWatch Logs is enabled.\n')
    log_group: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudWatch log group that is the destination for log delivery.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-cloudwatchlogslogdelivery.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    cloud_watch_logs_log_delivery_property = kafkaconnect.CfnConnector.CloudWatchLogsLogDeliveryProperty(\n        enabled=False,\n\n        # the properties below are optional\n        log_group="logGroup"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['enabled', 'log_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.CloudWatchLogsLogDeliveryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.CustomPluginProperty
class CfnConnector_CustomPluginPropertyDef(BaseStruct):
    custom_plugin_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the custom plugin.\n')
    revision: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The revision of the custom plugin.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-customplugin.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    custom_plugin_property = kafkaconnect.CfnConnector.CustomPluginProperty(\n        custom_plugin_arn="customPluginArn",\n        revision=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['custom_plugin_arn', 'revision']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.CustomPluginProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.FirehoseLogDeliveryProperty
class CfnConnector_FirehoseLogDeliveryPropertyDef(BaseStruct):
    enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether connector logs get delivered to Amazon Kinesis Data Firehose.\n')
    delivery_stream: typing.Optional[str] = pydantic.Field(None, description='The name of the Kinesis Data Firehose delivery stream that is the destination for log delivery.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-firehoselogdelivery.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    firehose_log_delivery_property = kafkaconnect.CfnConnector.FirehoseLogDeliveryProperty(\n        enabled=False,\n\n        # the properties below are optional\n        delivery_stream="deliveryStream"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['enabled', 'delivery_stream']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.FirehoseLogDeliveryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.KafkaClusterClientAuthenticationProperty
class CfnConnector_KafkaClusterClientAuthenticationPropertyDef(BaseStruct):
    authentication_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of client authentication used to connect to the Apache Kafka cluster. Value NONE means that no client authentication is used.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-kafkaclusterclientauthentication.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    kafka_cluster_client_authentication_property = kafkaconnect.CfnConnector.KafkaClusterClientAuthenticationProperty(\n        authentication_type="authenticationType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['authentication_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.KafkaClusterClientAuthenticationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.KafkaClusterEncryptionInTransitProperty
class CfnConnector_KafkaClusterEncryptionInTransitPropertyDef(BaseStruct):
    encryption_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of encryption in transit to the Apache Kafka cluster.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-kafkaclusterencryptionintransit.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    kafka_cluster_encryption_in_transit_property = kafkaconnect.CfnConnector.KafkaClusterEncryptionInTransitProperty(\n        encryption_type="encryptionType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['encryption_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.KafkaClusterEncryptionInTransitProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.KafkaClusterProperty
class CfnConnector_KafkaClusterPropertyDef(BaseStruct):
    apache_kafka_cluster: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ApacheKafkaClusterPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Apache Kafka cluster to which the connector is connected.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-kafkacluster.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    kafka_cluster_property = kafkaconnect.CfnConnector.KafkaClusterProperty(\n        apache_kafka_cluster=kafkaconnect.CfnConnector.ApacheKafkaClusterProperty(\n            bootstrap_servers="bootstrapServers",\n            vpc=kafkaconnect.CfnConnector.VpcProperty(\n                security_groups=["securityGroups"],\n                subnets=["subnets"]\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['apache_kafka_cluster']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.KafkaClusterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.LogDeliveryProperty
class CfnConnector_LogDeliveryPropertyDef(BaseStruct):
    worker_log_delivery: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_WorkerLogDeliveryPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The workers can send worker logs to different destination types. This configuration specifies the details of these destinations.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-logdelivery.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    log_delivery_property = kafkaconnect.CfnConnector.LogDeliveryProperty(\n        worker_log_delivery=kafkaconnect.CfnConnector.WorkerLogDeliveryProperty(\n            cloud_watch_logs=kafkaconnect.CfnConnector.CloudWatchLogsLogDeliveryProperty(\n                enabled=False,\n\n                # the properties below are optional\n                log_group="logGroup"\n            ),\n            firehose=kafkaconnect.CfnConnector.FirehoseLogDeliveryProperty(\n                enabled=False,\n\n                # the properties below are optional\n                delivery_stream="deliveryStream"\n            ),\n            s3=kafkaconnect.CfnConnector.S3LogDeliveryProperty(\n                enabled=False,\n\n                # the properties below are optional\n                bucket="bucket",\n                prefix="prefix"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['worker_log_delivery']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.LogDeliveryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.PluginProperty
class CfnConnector_PluginPropertyDef(BaseStruct):
    custom_plugin: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_CustomPluginPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Details about a custom plugin.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-plugin.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    plugin_property = kafkaconnect.CfnConnector.PluginProperty(\n        custom_plugin=kafkaconnect.CfnConnector.CustomPluginProperty(\n            custom_plugin_arn="customPluginArn",\n            revision=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['custom_plugin']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.PluginProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.ProvisionedCapacityProperty
class CfnConnector_ProvisionedCapacityPropertyDef(BaseStruct):
    worker_count: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of workers that are allocated to the connector.\n')
    mcu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of microcontroller units (MCUs) allocated to each connector worker. The valid values are 1,2,4,8.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-provisionedcapacity.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    provisioned_capacity_property = kafkaconnect.CfnConnector.ProvisionedCapacityProperty(\n        worker_count=123,\n\n        # the properties below are optional\n        mcu_count=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['worker_count', 'mcu_count']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.ProvisionedCapacityProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.S3LogDeliveryProperty
class CfnConnector_S3LogDeliveryPropertyDef(BaseStruct):
    enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether connector logs get sent to the specified Amazon S3 destination.\n')
    bucket: typing.Optional[str] = pydantic.Field(None, description='The name of the S3 bucket that is the destination for log delivery.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='The S3 prefix that is the destination for log delivery.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-s3logdelivery.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    s3_log_delivery_property = kafkaconnect.CfnConnector.S3LogDeliveryProperty(\n        enabled=False,\n\n        # the properties below are optional\n        bucket="bucket",\n        prefix="prefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['enabled', 'bucket', 'prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.S3LogDeliveryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.ScaleInPolicyProperty
class CfnConnector_ScaleInPolicyPropertyDef(BaseStruct):
    cpu_utilization_percentage: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the CPU utilization percentage threshold at which you want connector scale in to be triggered.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-scaleinpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    scale_in_policy_property = kafkaconnect.CfnConnector.ScaleInPolicyProperty(\n        cpu_utilization_percentage=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu_utilization_percentage']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.ScaleInPolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.ScaleOutPolicyProperty
class CfnConnector_ScaleOutPolicyPropertyDef(BaseStruct):
    cpu_utilization_percentage: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The CPU utilization percentage threshold at which you want connector scale out to be triggered.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-scaleoutpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    scale_out_policy_property = kafkaconnect.CfnConnector.ScaleOutPolicyProperty(\n        cpu_utilization_percentage=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu_utilization_percentage']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.ScaleOutPolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.VpcProperty
class CfnConnector_VpcPropertyDef(BaseStruct):
    security_groups: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The security groups for the connector.\n')
    subnets: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The subnets for the connector.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-vpc.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    vpc_property = kafkaconnect.CfnConnector.VpcProperty(\n        security_groups=["securityGroups"],\n        subnets=["subnets"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['security_groups', 'subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.VpcProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.WorkerConfigurationProperty
class CfnConnector_WorkerConfigurationPropertyDef(BaseStruct):
    revision: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The revision of the worker configuration.\n')
    worker_configuration_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the worker configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-workerconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    worker_configuration_property = kafkaconnect.CfnConnector.WorkerConfigurationProperty(\n        revision=123,\n        worker_configuration_arn="workerConfigurationArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['revision', 'worker_configuration_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.WorkerConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector.WorkerLogDeliveryProperty
class CfnConnector_WorkerLogDeliveryPropertyDef(BaseStruct):
    cloud_watch_logs: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_CloudWatchLogsLogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Details about delivering logs to Amazon CloudWatch Logs.\n')
    firehose: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_FirehoseLogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Details about delivering logs to Amazon Kinesis Data Firehose.\n')
    s3: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_S3LogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Details about delivering logs to Amazon S3.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-connector-workerlogdelivery.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    worker_log_delivery_property = kafkaconnect.CfnConnector.WorkerLogDeliveryProperty(\n        cloud_watch_logs=kafkaconnect.CfnConnector.CloudWatchLogsLogDeliveryProperty(\n            enabled=False,\n\n            # the properties below are optional\n            log_group="logGroup"\n        ),\n        firehose=kafkaconnect.CfnConnector.FirehoseLogDeliveryProperty(\n            enabled=False,\n\n            # the properties below are optional\n            delivery_stream="deliveryStream"\n        ),\n        s3=kafkaconnect.CfnConnector.S3LogDeliveryProperty(\n            enabled=False,\n\n            # the properties below are optional\n            bucket="bucket",\n            prefix="prefix"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_watch_logs', 'firehose', 's3']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector.WorkerLogDeliveryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnCustomPlugin.CustomPluginFileDescriptionProperty
class CfnCustomPlugin_CustomPluginFileDescriptionPropertyDef(BaseStruct):
    file_md5: typing.Optional[str] = pydantic.Field(None, description='The hex-encoded MD5 checksum of the custom plugin file. You can use it to validate the file.\n')
    file_size: typing.Union[int, float, None] = pydantic.Field(None, description='The size in bytes of the custom plugin file. You can use it to validate the file.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-customplugin-custompluginfiledescription.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    custom_plugin_file_description_property = kafkaconnect.CfnCustomPlugin.CustomPluginFileDescriptionProperty(\n        file_md5="fileMd5",\n        file_size=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_md5', 'file_size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnCustomPlugin.CustomPluginFileDescriptionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnCustomPlugin.CustomPluginLocationProperty
class CfnCustomPlugin_CustomPluginLocationPropertyDef(BaseStruct):
    s3_location: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnCustomPlugin_S3LocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The S3 bucket Amazon Resource Name (ARN), file key, and object version of the plugin file stored in Amazon S3.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-customplugin-custompluginlocation.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    custom_plugin_location_property = kafkaconnect.CfnCustomPlugin.CustomPluginLocationProperty(\n        s3_location=kafkaconnect.CfnCustomPlugin.S3LocationProperty(\n            bucket_arn="bucketArn",\n            file_key="fileKey",\n\n            # the properties below are optional\n            object_version="objectVersion"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnCustomPlugin.CustomPluginLocationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnCustomPlugin.S3LocationProperty
class CfnCustomPlugin_S3LocationPropertyDef(BaseStruct):
    bucket_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of an S3 bucket.\n')
    file_key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The file key for an object in an S3 bucket.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='The version of an object in an S3 bucket.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kafkaconnect-customplugin-s3location.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    s3_location_property = kafkaconnect.CfnCustomPlugin.S3LocationProperty(\n        bucket_arn="bucketArn",\n        file_key="fileKey",\n\n        # the properties below are optional\n        object_version="objectVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket_arn', 'file_key', 'object_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnCustomPlugin.S3LocationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnector
class CfnConnectorDef(BaseCfnResource):
    capacity: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_CapacityPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description="The connector's compute capacity settings.\n")
    connector_configuration: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Mapping[str, str]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The configuration of the connector.\n')
    connector_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the connector.\n')
    kafka_cluster: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_KafkaClusterPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The details of the Apache Kafka cluster to which the connector is connected.\n')
    kafka_cluster_client_authentication: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_KafkaClusterClientAuthenticationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of client authentication used to connect to the Apache Kafka cluster. The value is NONE when no client authentication is used.\n')
    kafka_cluster_encryption_in_transit: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_KafkaClusterEncryptionInTransitPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Details of encryption in transit to the Apache Kafka cluster.\n')
    kafka_connect_version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The version of Kafka Connect. It has to be compatible with both the Apache Kafka cluster's version and the plugins.\n")
    plugins: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_PluginPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies which plugin to use for the connector. You must specify a single-element list. Amazon MSK Connect does not currently support specifying multiple plugins.\n')
    service_execution_role_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the IAM role used by the connector to access Amazon Web Services resources.\n')
    connector_description: typing.Optional[str] = pydantic.Field(None, description='The description of the connector.\n')
    log_delivery: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_LogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The settings for delivering connector logs to Amazon CloudWatch Logs.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A collection of tags associated with a resource.\n')
    worker_configuration: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_WorkerConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The worker configurations that are in use with the connector.')
    _init_params: typing.ClassVar[list[str]] = ['capacity', 'connector_configuration', 'connector_name', 'kafka_cluster', 'kafka_cluster_client_authentication', 'kafka_cluster_encryption_in_transit', 'kafka_connect_version', 'plugins', 'service_execution_role_arn', 'connector_description', 'log_delivery', 'tags', 'worker_configuration']
    _method_names: typing.ClassVar[list[str]] = ['ApacheKafkaClusterProperty', 'AutoScalingProperty', 'CapacityProperty', 'CloudWatchLogsLogDeliveryProperty', 'CustomPluginProperty', 'FirehoseLogDeliveryProperty', 'KafkaClusterClientAuthenticationProperty', 'KafkaClusterEncryptionInTransitProperty', 'KafkaClusterProperty', 'LogDeliveryProperty', 'PluginProperty', 'ProvisionedCapacityProperty', 'S3LogDeliveryProperty', 'ScaleInPolicyProperty', 'ScaleOutPolicyProperty', 'VpcProperty', 'WorkerConfigurationProperty', 'WorkerLogDeliveryProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnector'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_kafkaconnect.CfnConnectorDefConfig] = pydantic.Field(None)


class CfnConnectorDefConfig(pydantic.BaseModel):
    ApacheKafkaClusterProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefApachekafkaclusterpropertyParams]] = pydantic.Field(None, description='')
    AutoScalingProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAutoscalingpropertyParams]] = pydantic.Field(None, description='')
    CapacityProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefCapacitypropertyParams]] = pydantic.Field(None, description='')
    CloudWatchLogsLogDeliveryProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefCloudwatchlogslogdeliverypropertyParams]] = pydantic.Field(None, description='')
    CustomPluginProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefCustompluginpropertyParams]] = pydantic.Field(None, description='')
    FirehoseLogDeliveryProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefFirehoselogdeliverypropertyParams]] = pydantic.Field(None, description='')
    KafkaClusterClientAuthenticationProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefKafkaclusterclientauthenticationpropertyParams]] = pydantic.Field(None, description='')
    KafkaClusterEncryptionInTransitProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefKafkaclusterencryptionintransitpropertyParams]] = pydantic.Field(None, description='')
    KafkaClusterProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefKafkaclusterpropertyParams]] = pydantic.Field(None, description='')
    LogDeliveryProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefLogdeliverypropertyParams]] = pydantic.Field(None, description='')
    PluginProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefPluginpropertyParams]] = pydantic.Field(None, description='')
    ProvisionedCapacityProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefProvisionedcapacitypropertyParams]] = pydantic.Field(None, description='')
    S3LogDeliveryProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefS3LogdeliverypropertyParams]] = pydantic.Field(None, description='')
    ScaleInPolicyProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefScaleinpolicypropertyParams]] = pydantic.Field(None, description='')
    ScaleOutPolicyProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefScaleoutpolicypropertyParams]] = pydantic.Field(None, description='')
    VpcProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefVpcpropertyParams]] = pydantic.Field(None, description='')
    WorkerConfigurationProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefWorkerconfigurationpropertyParams]] = pydantic.Field(None, description='')
    WorkerLogDeliveryProperty: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefWorkerlogdeliverypropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnConnectorDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnConnectorDefApachekafkaclusterpropertyParams(pydantic.BaseModel):
    bootstrap_servers: str = pydantic.Field(..., description='')
    vpc: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_VpcPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefAutoscalingpropertyParams(pydantic.BaseModel):
    max_worker_count: typing.Union[int, float] = pydantic.Field(..., description='')
    mcu_count: typing.Union[int, float] = pydantic.Field(..., description='')
    min_worker_count: typing.Union[int, float] = pydantic.Field(..., description='')
    scale_in_policy: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ScaleInPolicyPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    scale_out_policy: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ScaleOutPolicyPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefCapacitypropertyParams(pydantic.BaseModel):
    auto_scaling: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_AutoScalingPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    provisioned_capacity: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ProvisionedCapacityPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnConnectorDefCloudwatchlogslogdeliverypropertyParams(pydantic.BaseModel):
    enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    log_group: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnConnectorDefCustompluginpropertyParams(pydantic.BaseModel):
    custom_plugin_arn: str = pydantic.Field(..., description='')
    revision: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefFirehoselogdeliverypropertyParams(pydantic.BaseModel):
    enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    delivery_stream: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnConnectorDefKafkaclusterclientauthenticationpropertyParams(pydantic.BaseModel):
    authentication_type: str = pydantic.Field(..., description='')
    ...

class CfnConnectorDefKafkaclusterencryptionintransitpropertyParams(pydantic.BaseModel):
    encryption_type: str = pydantic.Field(..., description='')
    ...

class CfnConnectorDefKafkaclusterpropertyParams(pydantic.BaseModel):
    apache_kafka_cluster: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_ApacheKafkaClusterPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefLogdeliverypropertyParams(pydantic.BaseModel):
    worker_log_delivery: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_WorkerLogDeliveryPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefPluginpropertyParams(pydantic.BaseModel):
    custom_plugin: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_CustomPluginPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefProvisionedcapacitypropertyParams(pydantic.BaseModel):
    worker_count: typing.Union[int, float] = pydantic.Field(..., description='')
    mcu_count: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnConnectorDefS3LogdeliverypropertyParams(pydantic.BaseModel):
    enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    bucket: typing.Optional[str] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnConnectorDefScaleinpolicypropertyParams(pydantic.BaseModel):
    cpu_utilization_percentage: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefScaleoutpolicypropertyParams(pydantic.BaseModel):
    cpu_utilization_percentage: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefVpcpropertyParams(pydantic.BaseModel):
    security_groups: typing.Sequence[str] = pydantic.Field(..., description='')
    subnets: typing.Sequence[str] = pydantic.Field(..., description='')
    ...

class CfnConnectorDefWorkerconfigurationpropertyParams(pydantic.BaseModel):
    revision: typing.Union[int, float] = pydantic.Field(..., description='')
    worker_configuration_arn: str = pydantic.Field(..., description='')
    ...

class CfnConnectorDefWorkerlogdeliverypropertyParams(pydantic.BaseModel):
    cloud_watch_logs: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_CloudWatchLogsLogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    firehose: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_FirehoseLogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    s3: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_S3LogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnConnectorDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnConnectorDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnConnectorDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnConnectorDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnConnectorDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnConnectorDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnConnectorDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnConnectorDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnConnectorDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnConnectorDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnConnectorDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnConnectorDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnConnectorDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnConnectorDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_kafkaconnect.CfnCustomPlugin
class CfnCustomPluginDef(BaseCfnResource):
    content_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The format of the plugin file.\n')
    location: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnCustomPlugin_CustomPluginLocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Information about the location of the custom plugin.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the custom plugin.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the custom plugin.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.')
    _init_params: typing.ClassVar[list[str]] = ['content_type', 'location', 'name', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['CustomPluginFileDescriptionProperty', 'CustomPluginLocationProperty', 'S3LocationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnCustomPlugin'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_kafkaconnect.CfnCustomPluginDefConfig] = pydantic.Field(None)


class CfnCustomPluginDefConfig(pydantic.BaseModel):
    CustomPluginFileDescriptionProperty: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefCustompluginfiledescriptionpropertyParams]] = pydantic.Field(None, description='')
    CustomPluginLocationProperty: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefCustompluginlocationpropertyParams]] = pydantic.Field(None, description='')
    S3LocationProperty: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefS3LocationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnCustomPluginDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    attr_file_description_config: typing.Optional[models._interface_methods.CoreIResolvableDefConfig] = pydantic.Field(None)
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnCustomPluginDefCustompluginfiledescriptionpropertyParams(pydantic.BaseModel):
    file_md5: typing.Optional[str] = pydantic.Field(None, description='')
    file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnCustomPluginDefCustompluginlocationpropertyParams(pydantic.BaseModel):
    s3_location: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnCustomPlugin_S3LocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnCustomPluginDefS3LocationpropertyParams(pydantic.BaseModel):
    bucket_arn: str = pydantic.Field(..., description='')
    file_key: str = pydantic.Field(..., description='')
    object_version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnCustomPluginDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnCustomPluginDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCustomPluginDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnCustomPluginDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCustomPluginDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnCustomPluginDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnCustomPluginDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnCustomPluginDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnCustomPluginDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnCustomPluginDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCustomPluginDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnCustomPluginDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnCustomPluginDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCustomPluginDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_kafkaconnect.CfnWorkerConfiguration
class CfnWorkerConfigurationDef(BaseCfnResource):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the worker configuration.\n')
    properties_file_content: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Base64 encoded contents of the connect-distributed.properties file.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of a worker configuration.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A collection of tags associated with a resource.')
    _init_params: typing.ClassVar[list[str]] = ['name', 'properties_file_content', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnWorkerConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_kafkaconnect.CfnWorkerConfigurationDefConfig] = pydantic.Field(None)


class CfnWorkerConfigurationDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_kafkaconnect.CfnWorkerConfigurationDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnWorkerConfigurationDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnWorkerConfigurationDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnWorkerConfigurationDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnWorkerConfigurationDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnWorkerConfigurationDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnWorkerConfigurationDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnWorkerConfigurationDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnWorkerConfigurationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnWorkerConfigurationDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnWorkerConfigurationDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnWorkerConfigurationDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnWorkerConfigurationDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnWorkerConfigurationDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnWorkerConfigurationDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_kafkaconnect.CfnConnectorProps
class CfnConnectorPropsDef(BaseCfnProperty):
    capacity: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_CapacityPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description="The connector's compute capacity settings.\n")
    connector_configuration: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Mapping[str, str]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The configuration of the connector.\n')
    connector_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the connector.\n')
    kafka_cluster: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_KafkaClusterPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The details of the Apache Kafka cluster to which the connector is connected.\n')
    kafka_cluster_client_authentication: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_KafkaClusterClientAuthenticationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of client authentication used to connect to the Apache Kafka cluster. The value is NONE when no client authentication is used.\n')
    kafka_cluster_encryption_in_transit: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_KafkaClusterEncryptionInTransitPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Details of encryption in transit to the Apache Kafka cluster.\n')
    kafka_connect_version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The version of Kafka Connect. It has to be compatible with both the Apache Kafka cluster's version and the plugins.\n")
    plugins: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_PluginPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies which plugin to use for the connector. You must specify a single-element list. Amazon MSK Connect does not currently support specifying multiple plugins.\n')
    service_execution_role_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the IAM role used by the connector to access Amazon Web Services resources.\n')
    connector_description: typing.Optional[str] = pydantic.Field(None, description='The description of the connector.\n')
    log_delivery: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_LogDeliveryPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The settings for delivering connector logs to Amazon CloudWatch Logs.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A collection of tags associated with a resource.\n')
    worker_configuration: typing.Union[models.UnsupportedResource, models.aws_kafkaconnect.CfnConnector_WorkerConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The worker configurations that are in use with the connector.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-kafkaconnect-connector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    cfn_connector_props = kafkaconnect.CfnConnectorProps(\n        capacity=kafkaconnect.CfnConnector.CapacityProperty(\n            auto_scaling=kafkaconnect.CfnConnector.AutoScalingProperty(\n                max_worker_count=123,\n                mcu_count=123,\n                min_worker_count=123,\n                scale_in_policy=kafkaconnect.CfnConnector.ScaleInPolicyProperty(\n                    cpu_utilization_percentage=123\n                ),\n                scale_out_policy=kafkaconnect.CfnConnector.ScaleOutPolicyProperty(\n                    cpu_utilization_percentage=123\n                )\n            ),\n            provisioned_capacity=kafkaconnect.CfnConnector.ProvisionedCapacityProperty(\n                worker_count=123,\n\n                # the properties below are optional\n                mcu_count=123\n            )\n        ),\n        connector_configuration={\n            "connector_configuration_key": "connectorConfiguration"\n        },\n        connector_name="connectorName",\n        kafka_cluster=kafkaconnect.CfnConnector.KafkaClusterProperty(\n            apache_kafka_cluster=kafkaconnect.CfnConnector.ApacheKafkaClusterProperty(\n                bootstrap_servers="bootstrapServers",\n                vpc=kafkaconnect.CfnConnector.VpcProperty(\n                    security_groups=["securityGroups"],\n                    subnets=["subnets"]\n                )\n            )\n        ),\n        kafka_cluster_client_authentication=kafkaconnect.CfnConnector.KafkaClusterClientAuthenticationProperty(\n            authentication_type="authenticationType"\n        ),\n        kafka_cluster_encryption_in_transit=kafkaconnect.CfnConnector.KafkaClusterEncryptionInTransitProperty(\n            encryption_type="encryptionType"\n        ),\n        kafka_connect_version="kafkaConnectVersion",\n        plugins=[kafkaconnect.CfnConnector.PluginProperty(\n            custom_plugin=kafkaconnect.CfnConnector.CustomPluginProperty(\n                custom_plugin_arn="customPluginArn",\n                revision=123\n            )\n        )],\n        service_execution_role_arn="serviceExecutionRoleArn",\n\n        # the properties below are optional\n        connector_description="connectorDescription",\n        log_delivery=kafkaconnect.CfnConnector.LogDeliveryProperty(\n            worker_log_delivery=kafkaconnect.CfnConnector.WorkerLogDeliveryProperty(\n                cloud_watch_logs=kafkaconnect.CfnConnector.CloudWatchLogsLogDeliveryProperty(\n                    enabled=False,\n\n                    # the properties below are optional\n                    log_group="logGroup"\n                ),\n                firehose=kafkaconnect.CfnConnector.FirehoseLogDeliveryProperty(\n                    enabled=False,\n\n                    # the properties below are optional\n                    delivery_stream="deliveryStream"\n                ),\n                s3=kafkaconnect.CfnConnector.S3LogDeliveryProperty(\n                    enabled=False,\n\n                    # the properties below are optional\n                    bucket="bucket",\n                    prefix="prefix"\n                )\n            )\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        worker_configuration=kafkaconnect.CfnConnector.WorkerConfigurationProperty(\n            revision=123,\n            worker_configuration_arn="workerConfigurationArn"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capacity', 'connector_configuration', 'connector_name', 'kafka_cluster', 'kafka_cluster_client_authentication', 'kafka_cluster_encryption_in_transit', 'kafka_connect_version', 'plugins', 'service_execution_role_arn', 'connector_description', 'log_delivery', 'tags', 'worker_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnConnectorProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnCustomPluginProps
class CfnCustomPluginPropsDef(BaseCfnProperty):
    content_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The format of the plugin file.\n')
    location: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_kafkaconnect.CfnCustomPlugin_CustomPluginLocationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Information about the location of the custom plugin.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the custom plugin.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the custom plugin.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-kafkaconnect-customplugin.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    cfn_custom_plugin_props = kafkaconnect.CfnCustomPluginProps(\n        content_type="contentType",\n        location=kafkaconnect.CfnCustomPlugin.CustomPluginLocationProperty(\n            s3_location=kafkaconnect.CfnCustomPlugin.S3LocationProperty(\n                bucket_arn="bucketArn",\n                file_key="fileKey",\n\n                # the properties below are optional\n                object_version="objectVersion"\n            )\n        ),\n        name="name",\n\n        # the properties below are optional\n        description="description",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['content_type', 'location', 'name', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnCustomPluginProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_kafkaconnect.CfnWorkerConfigurationProps
class CfnWorkerConfigurationPropsDef(BaseCfnProperty):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the worker configuration.\n')
    properties_file_content: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Base64 encoded contents of the connect-distributed.properties file.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of a worker configuration.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A collection of tags associated with a resource.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-kafkaconnect-workerconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kafkaconnect as kafkaconnect\n\n    cfn_worker_configuration_props = kafkaconnect.CfnWorkerConfigurationProps(\n        name="name",\n        properties_file_content="propertiesFileContent",\n\n        # the properties below are optional\n        description="description",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'properties_file_content', 'description', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_kafkaconnect.CfnWorkerConfigurationProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    CfnConnector_ApacheKafkaClusterProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_ApacheKafkaClusterPropertyDef]] = pydantic.Field(None)
    CfnConnector_AutoScalingProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_AutoScalingPropertyDef]] = pydantic.Field(None)
    CfnConnector_CapacityProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_CapacityPropertyDef]] = pydantic.Field(None)
    CfnConnector_CloudWatchLogsLogDeliveryProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_CloudWatchLogsLogDeliveryPropertyDef]] = pydantic.Field(None)
    CfnConnector_CustomPluginProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_CustomPluginPropertyDef]] = pydantic.Field(None)
    CfnConnector_FirehoseLogDeliveryProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_FirehoseLogDeliveryPropertyDef]] = pydantic.Field(None)
    CfnConnector_KafkaClusterClientAuthenticationProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_KafkaClusterClientAuthenticationPropertyDef]] = pydantic.Field(None)
    CfnConnector_KafkaClusterEncryptionInTransitProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_KafkaClusterEncryptionInTransitPropertyDef]] = pydantic.Field(None)
    CfnConnector_KafkaClusterProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_KafkaClusterPropertyDef]] = pydantic.Field(None)
    CfnConnector_LogDeliveryProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_LogDeliveryPropertyDef]] = pydantic.Field(None)
    CfnConnector_PluginProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_PluginPropertyDef]] = pydantic.Field(None)
    CfnConnector_ProvisionedCapacityProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_ProvisionedCapacityPropertyDef]] = pydantic.Field(None)
    CfnConnector_S3LogDeliveryProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_S3LogDeliveryPropertyDef]] = pydantic.Field(None)
    CfnConnector_ScaleInPolicyProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_ScaleInPolicyPropertyDef]] = pydantic.Field(None)
    CfnConnector_ScaleOutPolicyProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_ScaleOutPolicyPropertyDef]] = pydantic.Field(None)
    CfnConnector_VpcProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_VpcPropertyDef]] = pydantic.Field(None)
    CfnConnector_WorkerConfigurationProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_WorkerConfigurationPropertyDef]] = pydantic.Field(None)
    CfnConnector_WorkerLogDeliveryProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnector_WorkerLogDeliveryPropertyDef]] = pydantic.Field(None)
    CfnCustomPlugin_CustomPluginFileDescriptionProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnCustomPlugin_CustomPluginFileDescriptionPropertyDef]] = pydantic.Field(None)
    CfnCustomPlugin_CustomPluginLocationProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnCustomPlugin_CustomPluginLocationPropertyDef]] = pydantic.Field(None)
    CfnCustomPlugin_S3LocationProperty: typing.Optional[dict[str, models.aws_kafkaconnect.CfnCustomPlugin_S3LocationPropertyDef]] = pydantic.Field(None)
    CfnConnector: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnectorDef]] = pydantic.Field(None)
    CfnCustomPlugin: typing.Optional[dict[str, models.aws_kafkaconnect.CfnCustomPluginDef]] = pydantic.Field(None)
    CfnWorkerConfiguration: typing.Optional[dict[str, models.aws_kafkaconnect.CfnWorkerConfigurationDef]] = pydantic.Field(None)
    CfnConnectorProps: typing.Optional[dict[str, models.aws_kafkaconnect.CfnConnectorPropsDef]] = pydantic.Field(None)
    CfnCustomPluginProps: typing.Optional[dict[str, models.aws_kafkaconnect.CfnCustomPluginPropsDef]] = pydantic.Field(None)
    CfnWorkerConfigurationProps: typing.Optional[dict[str, models.aws_kafkaconnect.CfnWorkerConfigurationPropsDef]] = pydantic.Field(None)
    ...

import models
