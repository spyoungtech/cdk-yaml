from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams

#  autogenerated from aws_cdk.aws_s3_deployment.CacheControl
class CacheControlDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_string', 'immutable', 'max_age', 'must_revalidate', 'must_understand', 'no_cache', 'no_store', 'no_transform', 'proxy_revalidate', 's_max_age', 'set_private', 'set_public', 'stale_if_error', 'stale_while_revalidate']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3_deployment.CacheControl'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CacheControlDefConfig] = pydantic.Field(None)


class CacheControlDefConfig(pydantic.BaseModel):
    from_string: typing.Optional[list[CacheControlDefFromStringParams]] = pydantic.Field(None, description='Constructs a custom cache control key from the literal value.')
    immutable: typing.Optional[list[CacheControlDefImmutableParams]] = pydantic.Field(None, description="Sets 'immutable'.")
    max_age: typing.Optional[list[CacheControlDefMaxAgeParams]] = pydantic.Field(None, description="Sets 'max-age='.")
    must_revalidate: typing.Optional[list[CacheControlDefMustRevalidateParams]] = pydantic.Field(None, description="Sets 'must-revalidate'.")
    must_understand: typing.Optional[list[CacheControlDefMustUnderstandParams]] = pydantic.Field(None, description="Sets 'must-understand'.")
    no_cache: typing.Optional[list[CacheControlDefNoCacheParams]] = pydantic.Field(None, description="Sets 'no-cache'.")
    no_store: typing.Optional[list[CacheControlDefNoStoreParams]] = pydantic.Field(None, description="Sets 'no-store'.")
    no_transform: typing.Optional[list[CacheControlDefNoTransformParams]] = pydantic.Field(None, description="Sets 'no-transform'.")
    proxy_revalidate: typing.Optional[list[CacheControlDefProxyRevalidateParams]] = pydantic.Field(None, description="Sets 'proxy-revalidate'.")
    s_max_age: typing.Optional[list[CacheControlDefSMaxAgeParams]] = pydantic.Field(None, description="Sets 's-maxage='.")
    set_private: typing.Optional[list[CacheControlDefSetPrivateParams]] = pydantic.Field(None, description="Sets 'private'.")
    set_public: typing.Optional[list[CacheControlDefSetPublicParams]] = pydantic.Field(None, description="Sets 'public'.")
    stale_if_error: typing.Optional[list[CacheControlDefStaleIfErrorParams]] = pydantic.Field(None, description="Sets 'stale-if-error='.")
    stale_while_revalidate: typing.Optional[list[CacheControlDefStaleWhileRevalidateParams]] = pydantic.Field(None, description="Sets 'stale-while-revalidate='.")

class CacheControlDefFromStringParams(pydantic.BaseModel):
    s: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefImmutableParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefMaxAgeParams(pydantic.BaseModel):
    t: models.DurationDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefMustRevalidateParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefMustUnderstandParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefNoCacheParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefNoStoreParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefNoTransformParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefProxyRevalidateParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefSMaxAgeParams(pydantic.BaseModel):
    t: models.DurationDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefSetPrivateParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefSetPublicParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefStaleIfErrorParams(pydantic.BaseModel):
    t: models.DurationDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefStaleWhileRevalidateParams(pydantic.BaseModel):
    t: models.DurationDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_s3_deployment.CacheControlDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_s3_deployment.Source
class SourceDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['asset', 'bucket', 'data', 'json_data', 'yaml_data']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3_deployment.Source'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['asset', 'bucket', 'data', 'json_data', 'yaml_data']
    ...


    asset: typing.Optional[SourceDefAssetParams] = pydantic.Field(None, description='Uses a local asset as the deployment source.\nIf the local asset is a .zip archive, make sure you trust the\nproducer of the archive.')
    bucket: typing.Optional[SourceDefBucketParams] = pydantic.Field(None, description='Uses a .zip file stored in an S3 bucket as the source for the destination bucket contents.\nMake sure you trust the producer of the archive.')
    data: typing.Optional[SourceDefDataParams] = pydantic.Field(None, description='Deploys an object with the specified string contents into the bucket.\nThe\ncontent can include deploy-time values (such as ``snsTopic.topicArn``) that\nwill get resolved only during deployment.\n\nTo store a JSON object use ``Source.jsonData()``.\nTo store YAML content use ``Source.yamlData()``.')
    json_data: typing.Optional[SourceDefJsonDataParams] = pydantic.Field(None, description='Deploys an object with the specified JSON object into the bucket.\nThe\nobject can include deploy-time values (such as ``snsTopic.topicArn``) that\nwill get resolved only during deployment.')
    yaml_data: typing.Optional[SourceDefYamlDataParams] = pydantic.Field(None, description='Deploys an object with the specified JSON object formatted as YAML into the bucket.\nThe object can include deploy-time values (such as ``snsTopic.topicArn``) that\nwill get resolved only during deployment.')

class SourceDefAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to a local .zip file or a directory.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef]]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class SourceDefBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 Bucket.\n')
    zip_object_key: str = pydantic.Field(..., description='The S3 object key of the zip file with contents.')
    ...

class SourceDefDataParams(pydantic.BaseModel):
    object_key: str = pydantic.Field(..., description='The destination S3 object key (relative to the root of the S3 deployment).\n')
    data: str = pydantic.Field(..., description='The data to be stored in the object.')
    ...

class SourceDefJsonDataParams(pydantic.BaseModel):
    object_key: str = pydantic.Field(..., description='The destination S3 object key (relative to the root of the S3 deployment).\n')
    obj: typing.Any = pydantic.Field(..., description='A JSON object.')
    ...

class SourceDefYamlDataParams(pydantic.BaseModel):
    object_key: str = pydantic.Field(..., description='The destination S3 object key (relative to the root of the S3 deployment).\n')
    obj: typing.Any = pydantic.Field(..., description='A JSON object.')
    ...


#  autogenerated from aws_cdk.aws_s3_deployment.BucketDeployment
class BucketDeploymentDef(BaseConstruct):
    destination_bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket to sync the contents of the zip file to.\n')
    sources: typing.Sequence[models.UnsupportedResource] = pydantic.Field(..., description='The sources from which to deploy the contents of this bucket.\n')
    access_control: typing.Optional[aws_cdk.aws_s3.BucketAccessControl] = pydantic.Field(None, description='System-defined x-amz-acl metadata to be set on all objects in the deployment. Default: - Not set.\n')
    cache_control: typing.Optional[typing.Sequence[models.aws_s3_deployment.CacheControlDef]] = pydantic.Field(None, description='System-defined cache-control metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_disposition: typing.Optional[str] = pydantic.Field(None, description='System-defined cache-disposition metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_encoding: typing.Optional[str] = pydantic.Field(None, description='System-defined content-encoding metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_language: typing.Optional[str] = pydantic.Field(None, description='System-defined content-language metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_type: typing.Optional[str] = pydantic.Field(None, description='System-defined content-type metadata to be set on all objects in the deployment. Default: - Not set.\n')
    destination_key_prefix: typing.Optional[str] = pydantic.Field(None, description='Key prefix in the destination bucket. Must be <=104 characters Default: "/" (unzip to root of the destination bucket)\n')
    distribution: typing.Optional[typing.Union[models.aws_cloudfront.CloudFrontWebDistributionDef, models.aws_cloudfront.DistributionDef]] = pydantic.Field(None, description="The CloudFront distribution using the destination bucket as an origin. Files in the distribution's edge caches will be invalidated after files are uploaded to the destination bucket. Default: - No invalidation occurs\n")
    distribution_paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The file paths to invalidate in the CloudFront distribution. Default: - All files under the destination bucket key prefix will be invalidated.\n')
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the AWS Lambda function’s /tmp directory in MiB. Default: 512 MiB\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="If this is set, matching files or objects will be excluded from the deployment's sync command. This can be used to exclude a file from being pruned in the destination bucket. If you want to just exclude files from the deployment package (which excludes these files evaluated when invalidating the asset), you should leverage the ``exclude`` property of ``AssetOptions`` when defining your source. Default: - No exclude filters are used\n")
    expires: typing.Optional[models.ExpirationDef] = pydantic.Field(None, description='System-defined expires metadata to be set on all objects in the deployment. Default: - The objects in the distribution will not expire.\n')
    extract: typing.Optional[bool] = pydantic.Field(None, description='If this is set, the zip file will be synced to the destination S3 bucket and extracted. If false, the file will remain zipped in the destination bucket. Default: true\n')
    include: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="If this is set, matching files or objects will be included with the deployment's sync command. Since all files from the deployment package are included by default, this property is usually leveraged alongside an ``exclude`` filter. Default: - No include filters are used and all files are included with the sync command\n")
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days that the lambda function's log events are kept in CloudWatch Logs. Default: logs.RetentionDays.INFINITE\n")
    memory_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory (in MiB) to allocate to the AWS Lambda function which replicates the files from the CDK bucket to the destination bucket. If you are deploying large files, you will need to increase this number accordingly. Default: 128\n')
    metadata: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='User-defined object metadata to be set on all objects in the deployment. Default: - No user metadata is set\n')
    prune: typing.Optional[bool] = pydantic.Field(None, description='If this is set to false, files in the destination bucket that do not exist in the asset, will NOT be deleted during deployment (create/update). Default: true\n')
    retain_on_delete: typing.Optional[bool] = pydantic.Field(None, description='If this is set to "false", the destination files will be deleted when the resource is deleted or the destination is updated. NOTICE: Configuring this to "false" might have operational implications. Please visit to the package documentation referred below to make sure you fully understand those implications. Default: true - when resource is deleted/updated, files are retained\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Execution role associated with this function. Default: - A role is automatically created\n')
    server_side_encryption: typing.Optional[aws_cdk.aws_s3_deployment.ServerSideEncryption] = pydantic.Field(None, description='System-defined x-amz-server-side-encryption metadata to be set on all objects in the deployment. Default: - Server side encryption is not used.\n')
    server_side_encryption_aws_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='System-defined x-amz-server-side-encryption-aws-kms-key-id metadata to be set on all objects in the deployment. Default: - Not set.\n')
    server_side_encryption_customer_algorithm: typing.Optional[str] = pydantic.Field(None, description='System-defined x-amz-server-side-encryption-customer-algorithm metadata to be set on all objects in the deployment. Warning: This is not a useful parameter until this bug is fixed: https://github.com/aws/aws-cdk/issues/6080 Default: - Not set.\n')
    sign_content: typing.Optional[bool] = pydantic.Field(None, description='If set to true, uploads will precompute the value of ``x-amz-content-sha256`` and include it in the signed S3 request headers. Default: - ``x-amz-content-sha256`` will not be computed\n')
    storage_class: typing.Optional[aws_cdk.aws_s3_deployment.StorageClass] = pydantic.Field(None, description='System-defined x-amz-storage-class metadata to be set on all objects in the deployment. Default: - Default storage-class for the bucket is used.\n')
    use_efs: typing.Optional[bool] = pydantic.Field(None, description='Mount an EFS file system. Enable this if your assets are large and you encounter disk space errors. Enabling this option will require a VPC to be specified. Default: - No EFS. Lambda has access only to 512MB of disk space.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC network to place the deployment lambda handler in. This is required if ``useEfs`` is set. Default: None\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Where in the VPC to place the deployment lambda handler. Only used if 'vpc' is supplied. Default: - the Vpc default strategy if not specified\n")
    website_redirect_location: typing.Optional[str] = pydantic.Field(None, description='System-defined x-amz-website-redirect-location metadata to be set on all objects in the deployment. Default: - No website redirection.')
    _init_params: typing.ClassVar[list[str]] = ['destination_bucket', 'sources', 'access_control', 'cache_control', 'content_disposition', 'content_encoding', 'content_language', 'content_type', 'destination_key_prefix', 'distribution', 'distribution_paths', 'ephemeral_storage_size', 'exclude', 'expires', 'extract', 'include', 'log_retention', 'memory_limit', 'metadata', 'prune', 'retain_on_delete', 'role', 'server_side_encryption', 'server_side_encryption_aws_kms_key_id', 'server_side_encryption_customer_algorithm', 'sign_content', 'storage_class', 'use_efs', 'vpc', 'vpc_subnets', 'website_redirect_location']
    _method_names: typing.ClassVar[list[str]] = ['add_source']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3_deployment.BucketDeployment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[BucketDeploymentDefConfig] = pydantic.Field(None)


class BucketDeploymentDefConfig(pydantic.BaseModel):
    add_source: typing.Optional[list[BucketDeploymentDefAddSourceParams]] = pydantic.Field(None, description='Add an additional source to the bucket deployment.')
    deployed_bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)

class BucketDeploymentDefAddSourceParams(pydantic.BaseModel):
    source: models.UnsupportedResource = pydantic.Field(..., description='-\n\nExample::\n\n    # website_bucket: s3.IBucket\n\n    deployment = s3deploy.BucketDeployment(self, "Deployment",\n        sources=[s3deploy.Source.asset("./website-dist")],\n        destination_bucket=website_bucket\n    )\n\n    deployment.add_source(s3deploy.Source.asset("./another-asset"))\n')
    ...


#  autogenerated from aws_cdk.aws_s3_deployment.BucketDeploymentProps
class BucketDeploymentPropsDef(BaseStruct):
    destination_bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket to sync the contents of the zip file to.\n')
    sources: typing.Sequence[models.UnsupportedResource] = pydantic.Field(..., description='The sources from which to deploy the contents of this bucket.\n')
    access_control: typing.Optional[aws_cdk.aws_s3.BucketAccessControl] = pydantic.Field(None, description='System-defined x-amz-acl metadata to be set on all objects in the deployment. Default: - Not set.\n')
    cache_control: typing.Optional[typing.Sequence[models.aws_s3_deployment.CacheControlDef]] = pydantic.Field(None, description='System-defined cache-control metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_disposition: typing.Optional[str] = pydantic.Field(None, description='System-defined cache-disposition metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_encoding: typing.Optional[str] = pydantic.Field(None, description='System-defined content-encoding metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_language: typing.Optional[str] = pydantic.Field(None, description='System-defined content-language metadata to be set on all objects in the deployment. Default: - Not set.\n')
    content_type: typing.Optional[str] = pydantic.Field(None, description='System-defined content-type metadata to be set on all objects in the deployment. Default: - Not set.\n')
    destination_key_prefix: typing.Optional[str] = pydantic.Field(None, description='Key prefix in the destination bucket. Must be <=104 characters Default: "/" (unzip to root of the destination bucket)\n')
    distribution: typing.Optional[typing.Union[models.aws_cloudfront.CloudFrontWebDistributionDef, models.aws_cloudfront.DistributionDef]] = pydantic.Field(None, description="The CloudFront distribution using the destination bucket as an origin. Files in the distribution's edge caches will be invalidated after files are uploaded to the destination bucket. Default: - No invalidation occurs\n")
    distribution_paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The file paths to invalidate in the CloudFront distribution. Default: - All files under the destination bucket key prefix will be invalidated.\n')
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the AWS Lambda function’s /tmp directory in MiB. Default: 512 MiB\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="If this is set, matching files or objects will be excluded from the deployment's sync command. This can be used to exclude a file from being pruned in the destination bucket. If you want to just exclude files from the deployment package (which excludes these files evaluated when invalidating the asset), you should leverage the ``exclude`` property of ``AssetOptions`` when defining your source. Default: - No exclude filters are used\n")
    expires: typing.Optional[models.ExpirationDef] = pydantic.Field(None, description='System-defined expires metadata to be set on all objects in the deployment. Default: - The objects in the distribution will not expire.\n')
    extract: typing.Optional[bool] = pydantic.Field(None, description='If this is set, the zip file will be synced to the destination S3 bucket and extracted. If false, the file will remain zipped in the destination bucket. Default: true\n')
    include: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="If this is set, matching files or objects will be included with the deployment's sync command. Since all files from the deployment package are included by default, this property is usually leveraged alongside an ``exclude`` filter. Default: - No include filters are used and all files are included with the sync command\n")
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days that the lambda function's log events are kept in CloudWatch Logs. Default: logs.RetentionDays.INFINITE\n")
    memory_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory (in MiB) to allocate to the AWS Lambda function which replicates the files from the CDK bucket to the destination bucket. If you are deploying large files, you will need to increase this number accordingly. Default: 128\n')
    metadata: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='User-defined object metadata to be set on all objects in the deployment. Default: - No user metadata is set\n')
    prune: typing.Optional[bool] = pydantic.Field(None, description='If this is set to false, files in the destination bucket that do not exist in the asset, will NOT be deleted during deployment (create/update). Default: true\n')
    retain_on_delete: typing.Optional[bool] = pydantic.Field(None, description='If this is set to "false", the destination files will be deleted when the resource is deleted or the destination is updated. NOTICE: Configuring this to "false" might have operational implications. Please visit to the package documentation referred below to make sure you fully understand those implications. Default: true - when resource is deleted/updated, files are retained\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Execution role associated with this function. Default: - A role is automatically created\n')
    server_side_encryption: typing.Optional[aws_cdk.aws_s3_deployment.ServerSideEncryption] = pydantic.Field(None, description='System-defined x-amz-server-side-encryption metadata to be set on all objects in the deployment. Default: - Server side encryption is not used.\n')
    server_side_encryption_aws_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='System-defined x-amz-server-side-encryption-aws-kms-key-id metadata to be set on all objects in the deployment. Default: - Not set.\n')
    server_side_encryption_customer_algorithm: typing.Optional[str] = pydantic.Field(None, description='System-defined x-amz-server-side-encryption-customer-algorithm metadata to be set on all objects in the deployment. Warning: This is not a useful parameter until this bug is fixed: https://github.com/aws/aws-cdk/issues/6080 Default: - Not set.\n')
    sign_content: typing.Optional[bool] = pydantic.Field(None, description='If set to true, uploads will precompute the value of ``x-amz-content-sha256`` and include it in the signed S3 request headers. Default: - ``x-amz-content-sha256`` will not be computed\n')
    storage_class: typing.Optional[aws_cdk.aws_s3_deployment.StorageClass] = pydantic.Field(None, description='System-defined x-amz-storage-class metadata to be set on all objects in the deployment. Default: - Default storage-class for the bucket is used.\n')
    use_efs: typing.Optional[bool] = pydantic.Field(None, description='Mount an EFS file system. Enable this if your assets are large and you encounter disk space errors. Enabling this option will require a VPC to be specified. Default: - No EFS. Lambda has access only to 512MB of disk space.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC network to place the deployment lambda handler in. This is required if ``useEfs`` is set. Default: None\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Where in the VPC to place the deployment lambda handler. Only used if 'vpc' is supplied. Default: - the Vpc default strategy if not specified\n")
    website_redirect_location: typing.Optional[str] = pydantic.Field(None, description='System-defined x-amz-website-redirect-location metadata to be set on all objects in the deployment. Default: - No website redirection.\n\n:exampleMetadata: infused\n\nExample::\n\n    # website_bucket: s3.Bucket\n\n\n    deployment = s3deploy.BucketDeployment(self, "DeployWebsite",\n        sources=[s3deploy.Source.asset(path.join(__dirname, "my-website"))],\n        destination_bucket=website_bucket\n    )\n\n    ConstructThatReadsFromTheBucket(self, "Consumer", {\n        # Use \'deployment.deployedBucket\' instead of \'websiteBucket\' here\n        "bucket": deployment.deployed_bucket\n    })\n')
    _init_params: typing.ClassVar[list[str]] = ['destination_bucket', 'sources', 'access_control', 'cache_control', 'content_disposition', 'content_encoding', 'content_language', 'content_type', 'destination_key_prefix', 'distribution', 'distribution_paths', 'ephemeral_storage_size', 'exclude', 'expires', 'extract', 'include', 'log_retention', 'memory_limit', 'metadata', 'prune', 'retain_on_delete', 'role', 'server_side_encryption', 'server_side_encryption_aws_kms_key_id', 'server_side_encryption_customer_algorithm', 'sign_content', 'storage_class', 'use_efs', 'vpc', 'vpc_subnets', 'website_redirect_location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3_deployment.BucketDeploymentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[BucketDeploymentPropsDefConfig] = pydantic.Field(None)


class BucketDeploymentPropsDefConfig(pydantic.BaseModel):
    destination_bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_s3_deployment.DeploymentSourceContext
class DeploymentSourceContextDef(BaseStruct):
    handler_role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='The role for the handler.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_s3_deployment as s3_deployment\n\n    # role: iam.Role\n\n    deployment_source_context = s3_deployment.DeploymentSourceContext(\n        handler_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['handler_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3_deployment.DeploymentSourceContext'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DeploymentSourceContextDefConfig] = pydantic.Field(None)


class DeploymentSourceContextDefConfig(pydantic.BaseModel):
    handler_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_s3_deployment.SourceConfig
class SourceConfigDef(BaseStruct):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The source bucket to deploy from.\n')
    zip_object_key: str = pydantic.Field(..., description='An S3 object key in the source bucket that points to a zip file.\n')
    markers: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='A set of markers to substitute in the source content. Default: - no markers\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n    from aws_cdk import aws_s3_deployment as s3_deployment\n\n    # bucket: s3.Bucket\n    # markers: Any\n\n    source_config = s3_deployment.SourceConfig(\n        bucket=bucket,\n        zip_object_key="zipObjectKey",\n\n        # the properties below are optional\n        markers={\n            "markers_key": markers\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'zip_object_key', 'markers']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3_deployment.SourceConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SourceConfigDefConfig] = pydantic.Field(None)


class SourceConfigDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_s3_deployment.UserDefinedObjectMetadata
class UserDefinedObjectMetadataDef(BaseStruct):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3_deployment.UserDefinedObjectMetadata'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3_deployment.ServerSideEncryption
# skipping emum

#  autogenerated from aws_cdk.aws_s3_deployment.StorageClass
# skipping emum

#  autogenerated from aws_cdk.aws_s3_deployment.ISource
#  skipping Interface

import models

class ModuleModel(pydantic.BaseModel):
    CacheControl: typing.Optional[dict[str, CacheControlDef]] = pydantic.Field(None)
    Source: typing.Optional[dict[str, SourceDef]] = pydantic.Field(None)
    BucketDeployment: typing.Optional[dict[str, BucketDeploymentDef]] = pydantic.Field(None)
    BucketDeploymentProps: typing.Optional[dict[str, BucketDeploymentPropsDef]] = pydantic.Field(None)
    DeploymentSourceContext: typing.Optional[dict[str, DeploymentSourceContextDef]] = pydantic.Field(None)
    SourceConfig: typing.Optional[dict[str, SourceConfigDef]] = pydantic.Field(None)
    UserDefinedObjectMetadata: typing.Optional[dict[str, UserDefinedObjectMetadataDef]] = pydantic.Field(None)
    ...
