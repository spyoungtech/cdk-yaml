from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AcceleratorClass
class AcceleratorClassDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AcceleratorClass'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AcceleratorClassDefConfig] = pydantic.Field(None)


class AcceleratorClassDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[AcceleratorClassDefOfParams]] = pydantic.Field(None, description='Custom AcceleratorType.')

class AcceleratorClassDefOfParams(pydantic.BaseModel):
    version: str = pydantic.Field(..., description='- Elastic Inference accelerator generation.')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.AcceleratorClassDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AcceleratorType
class AcceleratorTypeDef(BaseClass):
    instance_type_identifier: str = pydantic.Field(..., description='-')
    _init_params: typing.ClassVar[list[str]] = ['instance_type_identifier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AcceleratorType'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AcceleratorTypeDefConfig] = pydantic.Field(None)


class AcceleratorTypeDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[AcceleratorTypeDefOfParams]] = pydantic.Field(None, description='AcceleratorType.\nThis class takes a combination of a class and size.')

class AcceleratorTypeDefOfParams(pydantic.BaseModel):
    accelerator_class: models.aws_stepfunctions_tasks.AcceleratorClassDef = pydantic.Field(..., description='-\n')
    instance_size: aws_cdk.aws_ec2.InstanceSize = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.AcceleratorTypeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.Classification
class ClassificationDef(BaseClass):
    classification_statement: str = pydantic.Field(..., description='A literal string in case a new EMR classification is released, if not already defined.')
    _init_params: typing.ClassVar[list[str]] = ['classification_statement']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.Classification'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ContainerDefinition
class ContainerDefinitionDef(BaseClass):
    container_host_name: typing.Optional[str] = pydantic.Field(None, description='This parameter is ignored for models that contain only a PrimaryContainer. When a ContainerDefinition is part of an inference pipeline, the value of the parameter uniquely identifies the container for the purposes of logging and metrics. Default: - None')
    environment_variables: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='The environment variables to set in the Docker container. Default: - No variables\n')
    image: typing.Optional[models.aws_stepfunctions_tasks.DockerImageDef] = pydantic.Field(None, description='The Amazon EC2 Container Registry (Amazon ECR) path where inference code is stored. Default: - None\n')
    mode: typing.Optional[aws_cdk.aws_stepfunctions_tasks.Mode] = pydantic.Field(None, description='Defines how many models the container hosts. Default: - Mode.SINGLE_MODEL\n')
    model_package_name: typing.Optional[str] = pydantic.Field(None, description='The name or Amazon Resource Name (ARN) of the model package to use to create the model. Default: - None\n')
    model_s3_location: typing.Optional[models.aws_stepfunctions_tasks.S3LocationDef] = pydantic.Field(None, description='The S3 path where the model artifacts, which result from model training, are stored. This path must point to a single gzip compressed tar archive (.tar.gz suffix). The S3 path is required for Amazon SageMaker built-in algorithms, but not if you use your own algorithms. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['container_host_name', 'environment_variables', 'image', 'mode', 'model_package_name', 'model_s3_location']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ContainerDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ContainerDefinitionDefConfig] = pydantic.Field(None)


class ContainerDefinitionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ContainerDefinitionDefBindParams]] = pydantic.Field(None, description='Called when the ContainerDefinition type configured on Sagemaker Task.')

class ContainerDefinitionDefBindParams(pydantic.BaseModel):
    task: models.UnsupportedResource = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DockerImage
class DockerImageDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_ecr_repository', 'from_json_expression', 'from_registry']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DockerImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_ecr_repository', 'from_json_expression', 'from_registry']
    ...


    from_asset: typing.Optional[DockerImageDefFromAssetParams] = pydantic.Field(None, description='Reference a Docker image that is provided as an Asset in the current app.')
    from_ecr_repository: typing.Optional[DockerImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='Reference a Docker image stored in an ECR repository.')
    from_json_expression: typing.Optional[DockerImageDefFromJsonExpressionParams] = pydantic.Field(None, description="Reference a Docker image which URI is obtained from the task's input.")
    from_registry: typing.Optional[DockerImageDefFromRegistryParams] = pydantic.Field(None, description="Reference a Docker image by it's URI.\nWhen referencing ECR images, prefer using ``inEcr``.")
    resource_config: typing.Optional[DockerImageDefConfig] = pydantic.Field(None)


class DockerImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[DockerImageDefBindParams]] = pydantic.Field(None, description='Called when the image is used by a SageMaker task.')

class DockerImageDefBindParams(pydantic.BaseModel):
    task: models.UnsupportedResource = pydantic.Field(..., description='-')
    ...

class DockerImageDefFromAssetParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='the scope in which to create the Asset.\n')
    id: str = pydantic.Field(..., description='the ID for the asset in the construct tree.\n')
    directory: str = pydantic.Field(..., description='The directory where the Dockerfile is stored. Any directory inside with a name that matches the CDK output folder (cdk.out by default) will be excluded from the asset\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class DockerImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository where the image is hosted.\n')
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description='an optional tag or digest (digests must start with ``sha256:``).')
    ...

class DockerImageDefFromJsonExpressionParams(pydantic.BaseModel):
    expression: str = pydantic.Field(..., description='the JSON path expression with the task input.\n')
    allow_any_ecr_image_pull: typing.Optional[bool] = pydantic.Field(None, description='whether ECR access should be permitted (set to ``false`` if the image will never be in ECR).')
    ...

class DockerImageDefFromRegistryParams(pydantic.BaseModel):
    image_uri: str = pydantic.Field(..., description='the URI to the docker image.')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoAttributeValue
class DynamoAttributeValueDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['boolean_from_json_path', 'from_binary', 'from_binary_set', 'from_boolean', 'from_list', 'from_map', 'from_null', 'from_number', 'from_number_set', 'from_string', 'from_string_set', 'list_from_json_path', 'map_from_json_path', 'number_from_string', 'number_set_from_strings']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoAttributeValue'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_binary', 'from_binary_set', 'from_boolean', 'from_list', 'from_map', 'from_null', 'from_number', 'from_number_set', 'from_string', 'from_string_set']
    ...


    from_binary: typing.Optional[DynamoAttributeValueDefFromBinaryParams] = pydantic.Field(None, description='Sets an attribute of type Binary.\nFor example:  "B": "dGhpcyB0ZXh0IGlzIGJhc2U2NC1lbmNvZGVk"')
    from_binary_set: typing.Optional[DynamoAttributeValueDefFromBinarySetParams] = pydantic.Field(None, description='Sets an attribute of type Binary Set.\nFor example:  "BS": ["U3Vubnk=", "UmFpbnk=", "U25vd3k="]')
    from_boolean: typing.Optional[DynamoAttributeValueDefFromBooleanParams] = pydantic.Field(None, description='Sets an attribute of type Boolean.\nFor example:  "BOOL": true')
    from_list: typing.Optional[DynamoAttributeValueDefFromListParams] = pydantic.Field(None, description='Sets an attribute of type List.\nFor example:  "L": [ {"S": "Cookies"} , {"S": "Coffee"}, {"N", "3.14159"}]')
    from_map: typing.Optional[DynamoAttributeValueDefFromMapParams] = pydantic.Field(None, description='Sets an attribute of type Map.\nFor example:  "M": {"Name": {"S": "Joe"}, "Age": {"N": "35"}}')
    from_null: typing.Optional[DynamoAttributeValueDefFromNullParams] = pydantic.Field(None, description='Sets an attribute of type Null.\nFor example:  "NULL": true')
    from_number: typing.Optional[DynamoAttributeValueDefFromNumberParams] = pydantic.Field(None, description='Sets a literal number.\nFor example: 1234\nNumbers are sent across the network to DynamoDB as strings,\nto maximize compatibility across languages and libraries.\nHowever, DynamoDB treats them as number type attributes for mathematical operations.')
    from_number_set: typing.Optional[DynamoAttributeValueDefFromNumberSetParams] = pydantic.Field(None, description='Sets an attribute of type Number Set.\nFor example:  "NS": ["42.2", "-19", "7.5", "3.14"]\nNumbers are sent across the network to DynamoDB as strings,\nto maximize compatibility across languages and libraries.\nHowever, DynamoDB treats them as number type attributes for mathematical operations.')
    from_string: typing.Optional[DynamoAttributeValueDefFromStringParams] = pydantic.Field(None, description='Sets an attribute of type String.\nFor example:  "S": "Hello"\nStrings may be literal values or as JsonPath. Example values:\n\n- ``DynamoAttributeValue.fromString(\'someValue\')``\n- ``DynamoAttributeValue.fromString(JsonPath.stringAt(\'$.bar\'))``')
    from_string_set: typing.Optional[DynamoAttributeValueDefFromStringSetParams] = pydantic.Field(None, description='Sets an attribute of type String Set.\nFor example:  "SS": ["Giraffe", "Hippo" ,"Zebra"]')
    resource_config: typing.Optional[DynamoAttributeValueDefConfig] = pydantic.Field(None)


class DynamoAttributeValueDefConfig(pydantic.BaseModel):
    boolean_from_json_path: typing.Optional[list[DynamoAttributeValueDefBooleanFromJsonPathParams]] = pydantic.Field(None, description='Sets an attribute of type Boolean from state input through Json path.\nFor example:  "BOOL": true')
    list_from_json_path: typing.Optional[list[DynamoAttributeValueDefListFromJsonPathParams]] = pydantic.Field(None, description='Sets an attribute of type List.\nFor example:  "L": [ {"S": "Cookies"} , {"S": "Coffee"}, {"S", "Veggies"}]')
    map_from_json_path: typing.Optional[list[DynamoAttributeValueDefMapFromJsonPathParams]] = pydantic.Field(None, description='Sets an attribute of type Map.\nFor example:  "M": {"Name": {"S": "Joe"}, "Age": {"N": "35"}}')
    number_from_string: typing.Optional[list[DynamoAttributeValueDefNumberFromStringParams]] = pydantic.Field(None, description='Sets an attribute of type Number.\nFor example:  "N": "123.45"\nNumbers are sent across the network to DynamoDB as strings,\nto maximize compatibility across languages and libraries.\nHowever, DynamoDB treats them as number type attributes for mathematical operations.\n\nNumbers may be expressed as literal strings or as JsonPath')
    number_set_from_strings: typing.Optional[list[DynamoAttributeValueDefNumberSetFromStringsParams]] = pydantic.Field(None, description='Sets an attribute of type Number Set.\nFor example:  "NS": ["42.2", "-19", "7.5", "3.14"]\nNumbers are sent across the network to DynamoDB as strings,\nto maximize compatibility across languages and libraries.\nHowever, DynamoDB treats them as number type attributes for mathematical operations.\n\nNumbers may be expressed as literal strings or as JsonPath')

class DynamoAttributeValueDefBooleanFromJsonPathParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='Json path that specifies state input to be used.')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.DynamoAttributeValueDefConfig]] = pydantic.Field(None)
    ...

class DynamoAttributeValueDefFromBinaryParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='base-64 encoded string.')
    ...

class DynamoAttributeValueDefFromBinarySetParams(pydantic.BaseModel):
    value: typing.Sequence[str] = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromBooleanParams(pydantic.BaseModel):
    value: bool = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromListParams(pydantic.BaseModel):
    value: typing.Sequence[models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromMapParams(pydantic.BaseModel):
    value: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromNullParams(pydantic.BaseModel):
    value: bool = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromNumberParams(pydantic.BaseModel):
    value: typing.Union[int, float] = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromNumberSetParams(pydantic.BaseModel):
    value: typing.Sequence[typing.Union[int, float]] = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromStringParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefFromStringSetParams(pydantic.BaseModel):
    value: typing.Sequence[str] = pydantic.Field(..., description='-')
    ...

class DynamoAttributeValueDefListFromJsonPathParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='Json path that specifies state input to be used.')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.DynamoAttributeValueDefConfig]] = pydantic.Field(None)
    ...

class DynamoAttributeValueDefMapFromJsonPathParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='Json path that specifies state input to be used.')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.DynamoAttributeValueDefConfig]] = pydantic.Field(None)
    ...

class DynamoAttributeValueDefNumberFromStringParams(pydantic.BaseModel):
    value: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.DynamoAttributeValueDefConfig]] = pydantic.Field(None)
    ...

class DynamoAttributeValueDefNumberSetFromStringsParams(pydantic.BaseModel):
    value: typing.Sequence[str] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.DynamoAttributeValueDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoProjectionExpression
class DynamoProjectionExpressionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['at_index', 'with_attribute']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoProjectionExpression'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoProjectionExpressionDefConfig] = pydantic.Field(None)


class DynamoProjectionExpressionDefConfig(pydantic.BaseModel):
    at_index: typing.Optional[list[DynamoProjectionExpressionDefAtIndexParams]] = pydantic.Field(None, description='Adds the array literal access for passed index.')
    with_attribute: typing.Optional[list[DynamoProjectionExpressionDefWithAttributeParams]] = pydantic.Field(None, description='Adds the passed attribute to the chain.')

class DynamoProjectionExpressionDefAtIndexParams(pydantic.BaseModel):
    index: typing.Union[int, float] = pydantic.Field(..., description='array index.')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.DynamoProjectionExpressionDefConfig]] = pydantic.Field(None)
    ...

class DynamoProjectionExpressionDefWithAttributeParams(pydantic.BaseModel):
    attr: str = pydantic.Field(..., description='Attribute name.')
    return_config: typing.Optional[list[models.aws_stepfunctions_tasks.DynamoProjectionExpressionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EcsEc2LaunchTarget
class EcsEc2LaunchTargetDef(BaseClass):
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='Placement constraints. Default: - None')
    placement_strategies: typing.Optional[typing.Sequence[models.aws_ecs.PlacementStrategyDef]] = pydantic.Field(None, description='Placement strategies. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['placement_constraints', 'placement_strategies']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EcsEc2LaunchTarget'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcsEc2LaunchTargetDefConfig] = pydantic.Field(None)


class EcsEc2LaunchTargetDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[EcsEc2LaunchTargetDefBindParams]] = pydantic.Field(None, description='Called when the EC2 launch type is configured on RunTask.')

class EcsEc2LaunchTargetDefBindParams(pydantic.BaseModel):
    _task: models.aws_stepfunctions_tasks.EcsRunTaskDef = pydantic.Field(..., description='-\n')
    task_definition: typing.Union[models.aws_ecs.Ec2TaskDefinitionDef, models.aws_ecs.ExternalTaskDefinitionDef, models.aws_ecs.FargateTaskDefinitionDef, models.aws_ecs.TaskDefinitionDef] = pydantic.Field(..., description='Task definition to run Docker containers in Amazon ECS.\n')
    cluster: typing.Optional[typing.Union[models.aws_ecs.ClusterDef]] = pydantic.Field(None, description='A regional grouping of one or more container instances on which you can run tasks and services. Default: - No cluster')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EcsFargateLaunchTarget
class EcsFargateLaunchTargetDef(BaseClass):
    platform_version: aws_cdk.aws_ecs.FargatePlatformVersion = pydantic.Field(..., description='Refers to a specific runtime environment for Fargate task infrastructure. Fargate platform version is a combination of the kernel and container runtime versions.')
    _init_params: typing.ClassVar[list[str]] = ['platform_version']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EcsFargateLaunchTarget'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcsFargateLaunchTargetDefConfig] = pydantic.Field(None)


class EcsFargateLaunchTargetDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[EcsFargateLaunchTargetDefBindParams]] = pydantic.Field(None, description='Called when the Fargate launch type configured on RunTask.')

class EcsFargateLaunchTargetDefBindParams(pydantic.BaseModel):
    _task: models.aws_stepfunctions_tasks.EcsRunTaskDef = pydantic.Field(..., description='-\n')
    task_definition: typing.Union[models.aws_ecs.Ec2TaskDefinitionDef, models.aws_ecs.ExternalTaskDefinitionDef, models.aws_ecs.FargateTaskDefinitionDef, models.aws_ecs.TaskDefinitionDef] = pydantic.Field(..., description='Task definition to run Docker containers in Amazon ECS.\n')
    cluster: typing.Optional[typing.Union[models.aws_ecs.ClusterDef]] = pydantic.Field(None, description='A regional grouping of one or more container instances on which you can run tasks and services. Default: - No cluster')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EksClusterInput
class EksClusterInputDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_cluster', 'from_task_input']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EksClusterInput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_cluster', 'from_task_input']
    ...


    from_cluster: typing.Optional[EksClusterInputDefFromClusterParams] = pydantic.Field(None, description='Specify an existing EKS Cluster as the name for this Cluster.')
    from_task_input: typing.Optional[EksClusterInputDefFromTaskInputParams] = pydantic.Field(None, description='Specify a Task Input as the name for this Cluster.')

class EksClusterInputDefFromClusterParams(pydantic.BaseModel):
    cluster: typing.Union[models.aws_eks.ClusterDef, models.aws_eks.FargateClusterDef] = pydantic.Field(..., description='-')
    ...

class EksClusterInputDefFromTaskInputParams(pydantic.BaseModel):
    task_input: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ReleaseLabel
class ReleaseLabelDef(BaseClass):
    label: str = pydantic.Field(..., description="A literal string that contains the release-version ex. 'emr-x.x.x-latest'")
    _init_params: typing.ClassVar[list[str]] = ['label']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ReleaseLabel'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.S3Location
class S3LocationDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_bucket', 'from_json_expression']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.S3Location'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_bucket', 'from_json_expression']
    ...


    from_bucket: typing.Optional[S3LocationDefFromBucketParams] = pydantic.Field(None, description='An ``IS3Location`` built with a determined bucket and key prefix.')
    from_json_expression: typing.Optional[S3LocationDefFromJsonExpressionParams] = pydantic.Field(None, description='An ``IS3Location`` determined fully by a JSON Path from the task input.\nDue to the dynamic nature of those locations, the IAM grants that will be set by ``grantRead`` and ``grantWrite``\napply to the ``*`` resource.')
    resource_config: typing.Optional[S3LocationDefConfig] = pydantic.Field(None)


class S3LocationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[S3LocationDefBindParams]] = pydantic.Field(None, description='Called when the S3Location is bound to a StepFunctions task.')

class S3LocationDefBindParams(pydantic.BaseModel):
    task: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    for_reading: typing.Optional[bool] = pydantic.Field(None, description='Allow reading from the S3 Location. Default: false\n')
    for_writing: typing.Optional[bool] = pydantic.Field(None, description='Allow writing to the S3 Location. Default: false')
    ...

class S3LocationDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='is the bucket where the objects are to be stored.\n')
    key_prefix: str = pydantic.Field(..., description='is the key prefix used by the location.')
    ...

class S3LocationDefFromJsonExpressionParams(pydantic.BaseModel):
    expression: str = pydantic.Field(..., description='the JSON expression resolving to an S3 location URI.')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.VirtualClusterInput
class VirtualClusterInputDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_task_input', 'from_virtual_cluster_id']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.VirtualClusterInput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_task_input', 'from_virtual_cluster_id']
    ...


    from_task_input: typing.Optional[VirtualClusterInputDefFromTaskInputParams] = pydantic.Field(None, description='Input for a virtualClusterId from a Task Input.')
    from_virtual_cluster_id: typing.Optional[VirtualClusterInputDefFromVirtualClusterIdParams] = pydantic.Field(None, description='Input for virtualClusterId from a literal string.')

class VirtualClusterInputDefFromTaskInputParams(pydantic.BaseModel):
    task_input: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='-')
    ...

class VirtualClusterInputDefFromVirtualClusterIdParams(pydantic.BaseModel):
    virtual_cluster_id: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryExecution
class AthenaGetQueryExecutionDef(BaseConstruct):
    query_execution_id: str = pydantic.Field(..., description='Query that will be retrieved. Example value: ``adfsaf-23trf23-f23rt23``\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['query_execution_id', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryExecution'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AthenaGetQueryExecutionDefConfig] = pydantic.Field(None)


class AthenaGetQueryExecutionDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[AthenaGetQueryExecutionDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[AthenaGetQueryExecutionDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[AthenaGetQueryExecutionDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[AthenaGetQueryExecutionDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[AthenaGetQueryExecutionDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[AthenaGetQueryExecutionDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[AthenaGetQueryExecutionDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[AthenaGetQueryExecutionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[AthenaGetQueryExecutionDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[AthenaGetQueryExecutionDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[AthenaGetQueryExecutionDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[AthenaGetQueryExecutionDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[AthenaGetQueryExecutionDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[AthenaGetQueryExecutionDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[AthenaGetQueryExecutionDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[AthenaGetQueryExecutionDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[AthenaGetQueryExecutionDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[AthenaGetQueryExecutionDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[AthenaGetQueryExecutionDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class AthenaGetQueryExecutionDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class AthenaGetQueryExecutionDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class AthenaGetQueryExecutionDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class AthenaGetQueryExecutionDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaGetQueryExecutionDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaGetQueryExecutionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryExecutionDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryResults
class AthenaGetQueryResultsDef(BaseConstruct):
    query_execution_id: str = pydantic.Field(..., description='Query that will be retrieved. Example value: ``adfsaf-23trf23-f23rt23``\n')
    max_results: typing.Union[int, float, None] = pydantic.Field(None, description='Max number of results. Default: 1000\n')
    next_token: typing.Optional[str] = pydantic.Field(None, description='Pagination token. Default: - No next token\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['query_execution_id', 'max_results', 'next_token', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryResults'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AthenaGetQueryResultsDefConfig] = pydantic.Field(None)


class AthenaGetQueryResultsDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[AthenaGetQueryResultsDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[AthenaGetQueryResultsDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[AthenaGetQueryResultsDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[AthenaGetQueryResultsDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[AthenaGetQueryResultsDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[AthenaGetQueryResultsDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[AthenaGetQueryResultsDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[AthenaGetQueryResultsDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[AthenaGetQueryResultsDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[AthenaGetQueryResultsDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[AthenaGetQueryResultsDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[AthenaGetQueryResultsDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[AthenaGetQueryResultsDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[AthenaGetQueryResultsDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[AthenaGetQueryResultsDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[AthenaGetQueryResultsDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[AthenaGetQueryResultsDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[AthenaGetQueryResultsDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[AthenaGetQueryResultsDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class AthenaGetQueryResultsDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class AthenaGetQueryResultsDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class AthenaGetQueryResultsDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class AthenaGetQueryResultsDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaGetQueryResultsDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaGetQueryResultsDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class AthenaGetQueryResultsDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaStartQueryExecution
class AthenaStartQueryExecutionDef(BaseConstruct):
    query_string: str = pydantic.Field(..., description='Query that will be started.\n')
    client_request_token: typing.Optional[str] = pydantic.Field(None, description='Unique string string to ensure idempotence. Default: - No client request token\n')
    query_execution_context: typing.Union[models.aws_stepfunctions_tasks.QueryExecutionContextDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Database within which query executes. Default: - No query execution context\n')
    result_configuration: typing.Union[models.aws_stepfunctions_tasks.ResultConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration on how and where to save query. Default: - No result configuration\n')
    work_group: typing.Optional[str] = pydantic.Field(None, description='Configuration on how and where to save query. Default: - No work group\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['query_string', 'client_request_token', 'query_execution_context', 'result_configuration', 'work_group', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaStartQueryExecution'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AthenaStartQueryExecutionDefConfig] = pydantic.Field(None)


class AthenaStartQueryExecutionDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[AthenaStartQueryExecutionDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[AthenaStartQueryExecutionDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[AthenaStartQueryExecutionDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[AthenaStartQueryExecutionDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[AthenaStartQueryExecutionDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[AthenaStartQueryExecutionDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[AthenaStartQueryExecutionDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[AthenaStartQueryExecutionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[AthenaStartQueryExecutionDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[AthenaStartQueryExecutionDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[AthenaStartQueryExecutionDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[AthenaStartQueryExecutionDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[AthenaStartQueryExecutionDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[AthenaStartQueryExecutionDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[AthenaStartQueryExecutionDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[AthenaStartQueryExecutionDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[AthenaStartQueryExecutionDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[AthenaStartQueryExecutionDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[AthenaStartQueryExecutionDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class AthenaStartQueryExecutionDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class AthenaStartQueryExecutionDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class AthenaStartQueryExecutionDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class AthenaStartQueryExecutionDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaStartQueryExecutionDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaStartQueryExecutionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class AthenaStartQueryExecutionDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaStopQueryExecution
class AthenaStopQueryExecutionDef(BaseConstruct):
    query_execution_id: str = pydantic.Field(..., description='Query that will be stopped.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['query_execution_id', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaStopQueryExecution'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AthenaStopQueryExecutionDefConfig] = pydantic.Field(None)


class AthenaStopQueryExecutionDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[AthenaStopQueryExecutionDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[AthenaStopQueryExecutionDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[AthenaStopQueryExecutionDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[AthenaStopQueryExecutionDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[AthenaStopQueryExecutionDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[AthenaStopQueryExecutionDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[AthenaStopQueryExecutionDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[AthenaStopQueryExecutionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[AthenaStopQueryExecutionDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[AthenaStopQueryExecutionDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[AthenaStopQueryExecutionDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[AthenaStopQueryExecutionDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[AthenaStopQueryExecutionDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[AthenaStopQueryExecutionDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[AthenaStopQueryExecutionDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[AthenaStopQueryExecutionDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[AthenaStopQueryExecutionDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[AthenaStopQueryExecutionDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[AthenaStopQueryExecutionDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class AthenaStopQueryExecutionDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class AthenaStopQueryExecutionDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class AthenaStopQueryExecutionDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class AthenaStopQueryExecutionDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaStopQueryExecutionDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class AthenaStopQueryExecutionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class AthenaStopQueryExecutionDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.BatchSubmitJob
class BatchSubmitJobDef(BaseConstruct):
    job_definition_arn: str = pydantic.Field(..., description='The arn of the job definition used by this job.\n')
    job_name: str = pydantic.Field(..., description='The name of the job. The first character must be alphanumeric, and up to 128 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed.\n')
    job_queue_arn: str = pydantic.Field(..., description='The arn of the job queue into which the job is submitted.\n')
    array_size: typing.Union[int, float, None] = pydantic.Field(None, description='The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. For more information, see Array Jobs in the AWS Batch User Guide. Default: - No array size\n')
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to move a job to the RUNNABLE status. You may specify between 1 and 10 attempts. If the value of attempts is greater than one, the job is retried on failure the same number of attempts as the value. Default: 1\n')
    container_overrides: typing.Union[models.aws_stepfunctions_tasks.BatchContainerOverridesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A list of container overrides in JSON format that specify the name of a container in the specified job definition and the overrides it should receive. Default: - No container overrides\n')
    depends_on: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.BatchJobDependencyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. Default: - No dependencies\n')
    payload: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='The payload to be passed as parameters to the batch job. Default: - No parameters are passed\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['job_definition_arn', 'job_name', 'job_queue_arn', 'array_size', 'attempts', 'container_overrides', 'depends_on', 'payload', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.BatchSubmitJob'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[BatchSubmitJobDefConfig] = pydantic.Field(None)


class BatchSubmitJobDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[BatchSubmitJobDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[BatchSubmitJobDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[BatchSubmitJobDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[BatchSubmitJobDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[BatchSubmitJobDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[BatchSubmitJobDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[BatchSubmitJobDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[BatchSubmitJobDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[BatchSubmitJobDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[BatchSubmitJobDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[BatchSubmitJobDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[BatchSubmitJobDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[BatchSubmitJobDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[BatchSubmitJobDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[BatchSubmitJobDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[BatchSubmitJobDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[BatchSubmitJobDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[BatchSubmitJobDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[BatchSubmitJobDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class BatchSubmitJobDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class BatchSubmitJobDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class BatchSubmitJobDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class BatchSubmitJobDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class BatchSubmitJobDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class BatchSubmitJobDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class BatchSubmitJobDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpoint
class CallApiGatewayHttpApiEndpointDef(BaseConstruct):
    api_id: str = pydantic.Field(..., description='The Id of the API to call.\n')
    api_stack: models.StackDef = pydantic.Field(..., description='The Stack in which the API is defined.\n')
    stage_name: typing.Optional[str] = pydantic.Field(None, description="Name of the stage where the API is deployed to in API Gateway. Default: '$default'\n")
    method: aws_cdk.aws_stepfunctions_tasks.HttpMethod = pydantic.Field(..., description='Http method for the API.\n')
    api_path: typing.Optional[str] = pydantic.Field(None, description='Path parameters appended after API endpoint. Default: - No path\n')
    auth_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.AuthType] = pydantic.Field(None, description='Authentication methods. Default: AuthType.NO_AUTH\n')
    headers: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP request information that does not relate to contents of the request. Default: - No headers\n')
    query_parameters: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Query strings attatched to end of request. Default: - No query parameters\n')
    request_body: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP Request body. Default: - No request body\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['api_id', 'api_stack', 'stage_name', 'method', 'api_path', 'auth_type', 'headers', 'query_parameters', 'request_body', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CallApiGatewayHttpApiEndpointDefConfig] = pydantic.Field(None)


class CallApiGatewayHttpApiEndpointDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[CallApiGatewayHttpApiEndpointDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[CallApiGatewayHttpApiEndpointDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[CallApiGatewayHttpApiEndpointDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[CallApiGatewayHttpApiEndpointDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[CallApiGatewayHttpApiEndpointDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[CallApiGatewayHttpApiEndpointDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[CallApiGatewayHttpApiEndpointDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[CallApiGatewayHttpApiEndpointDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[CallApiGatewayHttpApiEndpointDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[CallApiGatewayHttpApiEndpointDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class CallApiGatewayHttpApiEndpointDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class CallApiGatewayHttpApiEndpointDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class CallApiGatewayHttpApiEndpointDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class CallApiGatewayHttpApiEndpointDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CallApiGatewayHttpApiEndpointDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CallApiGatewayHttpApiEndpointDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayHttpApiEndpointDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpoint
class CallApiGatewayRestApiEndpointDef(BaseConstruct):
    api: typing.Union[models.aws_apigateway.RestApiBaseDef, models.aws_apigateway.LambdaRestApiDef, models.aws_apigateway.RestApiDef, models.aws_apigateway.SpecRestApiDef, models.aws_apigateway.StepFunctionsRestApiDef] = pydantic.Field(..., description='API to call.\n')
    stage_name: str = pydantic.Field(..., description='Name of the stage where the API is deployed to in API Gateway.\n')
    method: aws_cdk.aws_stepfunctions_tasks.HttpMethod = pydantic.Field(..., description='Http method for the API.\n')
    api_path: typing.Optional[str] = pydantic.Field(None, description='Path parameters appended after API endpoint. Default: - No path\n')
    auth_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.AuthType] = pydantic.Field(None, description='Authentication methods. Default: AuthType.NO_AUTH\n')
    headers: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP request information that does not relate to contents of the request. Default: - No headers\n')
    query_parameters: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Query strings attatched to end of request. Default: - No query parameters\n')
    request_body: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP Request body. Default: - No request body\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['api', 'stage_name', 'method', 'api_path', 'auth_type', 'headers', 'query_parameters', 'request_body', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CallApiGatewayRestApiEndpointDefConfig] = pydantic.Field(None)


class CallApiGatewayRestApiEndpointDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[CallApiGatewayRestApiEndpointDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[CallApiGatewayRestApiEndpointDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[CallApiGatewayRestApiEndpointDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[CallApiGatewayRestApiEndpointDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[CallApiGatewayRestApiEndpointDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[CallApiGatewayRestApiEndpointDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[CallApiGatewayRestApiEndpointDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[CallApiGatewayRestApiEndpointDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[CallApiGatewayRestApiEndpointDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[CallApiGatewayRestApiEndpointDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class CallApiGatewayRestApiEndpointDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class CallApiGatewayRestApiEndpointDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class CallApiGatewayRestApiEndpointDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class CallApiGatewayRestApiEndpointDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CallApiGatewayRestApiEndpointDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CallApiGatewayRestApiEndpointDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class CallApiGatewayRestApiEndpointDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CallAwsService
class CallAwsServiceDef(BaseConstruct):
    action: str = pydantic.Field(..., description='The API action to call. Use camelCase.\n')
    iam_resources: typing.Sequence[str] = pydantic.Field(..., description="The resources for the IAM statement that will be added to the state machine role's policy to allow the state machine to make the API call. By default the action for this IAM statement will be ``service:action``.\n")
    service: str = pydantic.Field(..., description='The AWS service to call.\n')
    additional_iam_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description="Additional IAM statements that will be added to the state machine role's policy. Use in the case where the call requires more than a single statement to be executed, e.g. ``rekognition:detectLabels`` requires also S3 permissions to read the object on which it must act. Default: - no additional statements are added\n")
    iam_action: typing.Optional[str] = pydantic.Field(None, description="The action for the IAM statement that will be added to the state machine role's policy to allow the state machine to make the API call. Use in the case where the IAM action name does not match with the API service/action name, e.g. ``s3:ListBuckets`` requires ``s3:ListAllMyBuckets``. Default: - service:action\n")
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Parameters for the API action call. Use PascalCase for the parameter names. Default: - no parameters\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['action', 'iam_resources', 'service', 'additional_iam_statements', 'iam_action', 'parameters', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CallAwsService'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CallAwsServiceDefConfig] = pydantic.Field(None)


class CallAwsServiceDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[CallAwsServiceDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[CallAwsServiceDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[CallAwsServiceDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[CallAwsServiceDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[CallAwsServiceDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[CallAwsServiceDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[CallAwsServiceDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[CallAwsServiceDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[CallAwsServiceDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[CallAwsServiceDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[CallAwsServiceDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[CallAwsServiceDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[CallAwsServiceDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[CallAwsServiceDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[CallAwsServiceDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[CallAwsServiceDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[CallAwsServiceDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[CallAwsServiceDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[CallAwsServiceDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class CallAwsServiceDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class CallAwsServiceDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class CallAwsServiceDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class CallAwsServiceDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CallAwsServiceDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CallAwsServiceDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class CallAwsServiceDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CodeBuildStartBuild
class CodeBuildStartBuildDef(BaseConstruct):
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description='CodeBuild project to start.\n')
    environment_variables_override: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A set of environment variables to be used for this build only. Default: - the latest environment variables already defined in the build project.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['project', 'environment_variables_override', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CodeBuildStartBuild'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeBuildStartBuildDefConfig] = pydantic.Field(None)


class CodeBuildStartBuildDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[CodeBuildStartBuildDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[CodeBuildStartBuildDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[CodeBuildStartBuildDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[CodeBuildStartBuildDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[CodeBuildStartBuildDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[CodeBuildStartBuildDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[CodeBuildStartBuildDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[CodeBuildStartBuildDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[CodeBuildStartBuildDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[CodeBuildStartBuildDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[CodeBuildStartBuildDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[CodeBuildStartBuildDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[CodeBuildStartBuildDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[CodeBuildStartBuildDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[CodeBuildStartBuildDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[CodeBuildStartBuildDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[CodeBuildStartBuildDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[CodeBuildStartBuildDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[CodeBuildStartBuildDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class CodeBuildStartBuildDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class CodeBuildStartBuildDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class CodeBuildStartBuildDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class CodeBuildStartBuildDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CodeBuildStartBuildDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class CodeBuildStartBuildDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildStartBuildDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoDeleteItem
class DynamoDeleteItemDef(BaseConstruct):
    key: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='Primary key of the item to retrieve. For the primary key, you must provide all of the attributes. For example, with a simple primary key, you only need to provide a value for the partition key. For a composite primary key, you must provide values for both the partition key and the sort key.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table containing the requested item.\n')
    condition_expression: typing.Optional[str] = pydantic.Field(None, description='A condition that must be satisfied in order for a conditional DeleteItem to succeed. Default: - No condition expression\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attribute names\n')
    expression_attribute_values: typing.Optional[typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef]] = pydantic.Field(None, description='One or more values that can be substituted in an expression. Default: - No expression attribute values\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n')
    return_item_collection_metrics: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoItemCollectionMetrics] = pydantic.Field(None, description='Determines whether item collection metrics are returned. If set to SIZE, the response includes statistics about item collections, if any, that were modified during the operation are returned in the response. If set to NONE (the default), no statistics are returned. Default: DynamoItemCollectionMetrics.NONE\n')
    return_values: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoReturnValues] = pydantic.Field(None, description='Use ReturnValues if you want to get the item attributes as they appeared before they were deleted. Default: DynamoReturnValues.NONE\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['key', 'table', 'condition_expression', 'expression_attribute_names', 'expression_attribute_values', 'return_consumed_capacity', 'return_item_collection_metrics', 'return_values', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoDeleteItem'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoDeleteItemDefConfig] = pydantic.Field(None)


class DynamoDeleteItemDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[DynamoDeleteItemDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[DynamoDeleteItemDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[DynamoDeleteItemDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[DynamoDeleteItemDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[DynamoDeleteItemDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[DynamoDeleteItemDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[DynamoDeleteItemDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[DynamoDeleteItemDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[DynamoDeleteItemDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[DynamoDeleteItemDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[DynamoDeleteItemDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[DynamoDeleteItemDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[DynamoDeleteItemDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[DynamoDeleteItemDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[DynamoDeleteItemDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[DynamoDeleteItemDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[DynamoDeleteItemDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[DynamoDeleteItemDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[DynamoDeleteItemDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class DynamoDeleteItemDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class DynamoDeleteItemDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class DynamoDeleteItemDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class DynamoDeleteItemDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoDeleteItemDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoDeleteItemDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class DynamoDeleteItemDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoGetItem
class DynamoGetItemDef(BaseConstruct):
    key: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='Primary key of the item to retrieve. For the primary key, you must provide all of the attributes. For example, with a simple primary key, you only need to provide a value for the partition key. For a composite primary key, you must provide values for both the partition key and the sort key.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table containing the requested item.\n')
    consistent_read: typing.Optional[bool] = pydantic.Field(None, description='Determines the read consistency model: If set to true, then the operation uses strongly consistent reads; otherwise, the operation uses eventually consistent reads. Default: false\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attributes\n')
    projection_expression: typing.Optional[typing.Sequence[models.aws_stepfunctions_tasks.DynamoProjectionExpressionDef]] = pydantic.Field(None, description='An array of DynamoProjectionExpression that identifies one or more attributes to retrieve from the table. These attributes can include scalars, sets, or elements of a JSON document. Default: - No projection expression\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['key', 'table', 'consistent_read', 'expression_attribute_names', 'projection_expression', 'return_consumed_capacity', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoGetItem'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoGetItemDefConfig] = pydantic.Field(None)


class DynamoGetItemDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[DynamoGetItemDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[DynamoGetItemDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[DynamoGetItemDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[DynamoGetItemDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[DynamoGetItemDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[DynamoGetItemDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[DynamoGetItemDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[DynamoGetItemDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[DynamoGetItemDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[DynamoGetItemDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[DynamoGetItemDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[DynamoGetItemDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[DynamoGetItemDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[DynamoGetItemDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[DynamoGetItemDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[DynamoGetItemDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[DynamoGetItemDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[DynamoGetItemDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[DynamoGetItemDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class DynamoGetItemDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class DynamoGetItemDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class DynamoGetItemDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class DynamoGetItemDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoGetItemDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoGetItemDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class DynamoGetItemDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoPutItem
class DynamoPutItemDef(BaseConstruct):
    item: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='A map of attribute name/value pairs, one for each attribute. Only the primary key attributes are required; you can optionally provide other attribute name-value pairs for the item.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table where the item should be written .\n')
    condition_expression: typing.Optional[str] = pydantic.Field(None, description='A condition that must be satisfied in order for a conditional PutItem operation to succeed. Default: - No condition expression\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attribute names\n')
    expression_attribute_values: typing.Optional[typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef]] = pydantic.Field(None, description='One or more values that can be substituted in an expression. Default: - No expression attribute values\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n')
    return_item_collection_metrics: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoItemCollectionMetrics] = pydantic.Field(None, description='The item collection metrics to returned in the response. Default: DynamoItemCollectionMetrics.NONE\n')
    return_values: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoReturnValues] = pydantic.Field(None, description='Use ReturnValues if you want to get the item attributes as they appeared before they were updated with the PutItem request. Default: DynamoReturnValues.NONE\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['item', 'table', 'condition_expression', 'expression_attribute_names', 'expression_attribute_values', 'return_consumed_capacity', 'return_item_collection_metrics', 'return_values', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoPutItem'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoPutItemDefConfig] = pydantic.Field(None)


class DynamoPutItemDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[DynamoPutItemDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[DynamoPutItemDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[DynamoPutItemDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[DynamoPutItemDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[DynamoPutItemDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[DynamoPutItemDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[DynamoPutItemDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[DynamoPutItemDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[DynamoPutItemDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[DynamoPutItemDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[DynamoPutItemDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[DynamoPutItemDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[DynamoPutItemDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[DynamoPutItemDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[DynamoPutItemDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[DynamoPutItemDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[DynamoPutItemDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[DynamoPutItemDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[DynamoPutItemDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class DynamoPutItemDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class DynamoPutItemDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class DynamoPutItemDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class DynamoPutItemDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoPutItemDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoPutItemDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class DynamoPutItemDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoUpdateItem
class DynamoUpdateItemDef(BaseConstruct):
    key: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='Primary key of the item to retrieve. For the primary key, you must provide all of the attributes. For example, with a simple primary key, you only need to provide a value for the partition key. For a composite primary key, you must provide values for both the partition key and the sort key.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table containing the requested item.\n')
    condition_expression: typing.Optional[str] = pydantic.Field(None, description='A condition that must be satisfied in order for a conditional DeleteItem to succeed. Default: - No condition expression\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attribute names\n')
    expression_attribute_values: typing.Optional[typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef]] = pydantic.Field(None, description='One or more values that can be substituted in an expression. Default: - No expression attribute values\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n')
    return_item_collection_metrics: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoItemCollectionMetrics] = pydantic.Field(None, description='Determines whether item collection metrics are returned. If set to SIZE, the response includes statistics about item collections, if any, that were modified during the operation are returned in the response. If set to NONE (the default), no statistics are returned. Default: DynamoItemCollectionMetrics.NONE\n')
    return_values: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoReturnValues] = pydantic.Field(None, description='Use ReturnValues if you want to get the item attributes as they appeared before they were deleted. Default: DynamoReturnValues.NONE\n')
    update_expression: typing.Optional[str] = pydantic.Field(None, description='An expression that defines one or more attributes to be updated, the action to be performed on them, and new values for them. Default: - No update expression\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['key', 'table', 'condition_expression', 'expression_attribute_names', 'expression_attribute_values', 'return_consumed_capacity', 'return_item_collection_metrics', 'return_values', 'update_expression', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoUpdateItem'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoUpdateItemDefConfig] = pydantic.Field(None)


class DynamoUpdateItemDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[DynamoUpdateItemDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[DynamoUpdateItemDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[DynamoUpdateItemDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[DynamoUpdateItemDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[DynamoUpdateItemDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[DynamoUpdateItemDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[DynamoUpdateItemDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[DynamoUpdateItemDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[DynamoUpdateItemDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[DynamoUpdateItemDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[DynamoUpdateItemDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[DynamoUpdateItemDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[DynamoUpdateItemDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[DynamoUpdateItemDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[DynamoUpdateItemDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[DynamoUpdateItemDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[DynamoUpdateItemDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[DynamoUpdateItemDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[DynamoUpdateItemDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class DynamoUpdateItemDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class DynamoUpdateItemDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class DynamoUpdateItemDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class DynamoUpdateItemDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoUpdateItemDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class DynamoUpdateItemDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class DynamoUpdateItemDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EcsRunTask
class EcsRunTaskDef(BaseConstruct, ConnectableMixin):
    cluster: typing.Union[models.aws_ecs.ClusterDef] = pydantic.Field(..., description='The ECS cluster to run the task on.\n')
    launch_target: typing.Union[models.aws_stepfunctions_tasks.EcsEc2LaunchTargetDef, models.aws_stepfunctions_tasks.EcsFargateLaunchTargetDef] = pydantic.Field(..., description='An Amazon ECS launch type determines the type of infrastructure on which your tasks and services are hosted.\n')
    task_definition: models.aws_ecs.TaskDefinitionDef = pydantic.Field(..., description='[disable-awslint:ref-via-interface] Task Definition used for running tasks in the service. Note: this must be TaskDefinition, and not ITaskDefinition, as it requires properties that are not known for imported task definitions If you want to run a RunTask with an imported task definition, consider using CustomState\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description='Assign public IP addresses to each task. Default: false\n')
    container_overrides: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ContainerOverrideDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Container setting overrides. Specify the container to use and the overrides to apply. Default: - No overrides\n')
    propagated_tag_source: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition to the task. An error will be received if you specify the SERVICE option when running a task. Default: - No tags are propagated.\n')
    revision_number: typing.Union[int, float, None] = pydantic.Field(None, description="The revision number of ECS task definiton family. Default: - '$latest'\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Existing security groups to use for the tasks. Default: - A new security group is created\n')
    subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Subnets to place the task's ENIs. Default: - Public subnets if assignPublicIp is set. Private subnets otherwise.\n")
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'launch_target', 'task_definition', 'assign_public_ip', 'container_overrides', 'propagated_tag_source', 'revision_number', 'security_groups', 'subnets', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EcsRunTask'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcsRunTaskDefConfig] = pydantic.Field(None)


class EcsRunTaskDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EcsRunTaskDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EcsRunTaskDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EcsRunTaskDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EcsRunTaskDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EcsRunTaskDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EcsRunTaskDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EcsRunTaskDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EcsRunTaskDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EcsRunTaskDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EcsRunTaskDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EcsRunTaskDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EcsRunTaskDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EcsRunTaskDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EcsRunTaskDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EcsRunTaskDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EcsRunTaskDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EcsRunTaskDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EcsRunTaskDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EcsRunTaskDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class EcsRunTaskDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EcsRunTaskDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EcsRunTaskDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EcsRunTaskDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EcsRunTaskDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EcsRunTaskDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EcsRunTaskDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EksCall
class EksCallDef(BaseConstruct):
    cluster: typing.Union[models.aws_eks.ClusterDef, models.aws_eks.FargateClusterDef] = pydantic.Field(..., description='The EKS cluster.\n')
    http_method: aws_cdk.aws_stepfunctions_tasks.HttpMethods = pydantic.Field(..., description='HTTP method ("GET", "POST", "PUT", ...) part of HTTP request.\n')
    http_path: str = pydantic.Field(..., description='HTTP path of the Kubernetes REST API operation For example: /api/v1/namespaces/default/pods.\n')
    query_parameters: typing.Optional[typing.Mapping[str, typing.Sequence[str]]] = pydantic.Field(None, description='Query Parameters part of HTTP request. Default: - no query parameters\n')
    request_body: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Request body part of HTTP request. Default: - No request body\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'http_method', 'http_path', 'query_parameters', 'request_body', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EksCall'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EksCallDefConfig] = pydantic.Field(None)


class EksCallDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EksCallDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EksCallDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EksCallDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EksCallDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EksCallDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EksCallDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EksCallDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EksCallDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EksCallDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EksCallDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EksCallDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EksCallDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EksCallDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EksCallDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EksCallDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EksCallDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EksCallDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EksCallDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EksCallDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EksCallDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EksCallDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EksCallDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EksCallDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EksCallDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EksCallDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EksCallDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrAddStep
class EmrAddStepDef(BaseConstruct):
    cluster_id: str = pydantic.Field(..., description='The ClusterId to add the Step to.\n')
    jar: str = pydantic.Field(..., description='A path to a JAR file run during the step.\n')
    name: str = pydantic.Field(..., description='The name of the Step.\n')
    action_on_failure: typing.Optional[aws_cdk.aws_stepfunctions_tasks.ActionOnFailure] = pydantic.Field(None, description='The action to take when the cluster step fails. Default: ActionOnFailure.CONTINUE\n')
    main_class: typing.Optional[str] = pydantic.Field(None, description='The name of the main class in the specified Java file. If not specified, the JAR file should specify a Main-Class in its manifest file. Default: - No mainClass\n')
    properties: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of Java properties that are set when the step runs. You can use these properties to pass key value pairs to your main function. Default: - No properties\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster_id', 'jar', 'name', 'action_on_failure', 'main_class', 'properties', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrAddStep'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrAddStepDefConfig] = pydantic.Field(None)


class EmrAddStepDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrAddStepDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrAddStepDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrAddStepDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrAddStepDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrAddStepDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrAddStepDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrAddStepDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrAddStepDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrAddStepDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrAddStepDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrAddStepDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrAddStepDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrAddStepDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrAddStepDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrAddStepDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrAddStepDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrAddStepDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrAddStepDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrAddStepDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrAddStepDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrAddStepDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrAddStepDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrAddStepDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrAddStepDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrAddStepDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrAddStepDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCancelStep
class EmrCancelStepDef(BaseConstruct):
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    step_id: str = pydantic.Field(..., description='The StepId to cancel.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster_id', 'step_id', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCancelStep'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrCancelStepDefConfig] = pydantic.Field(None)


class EmrCancelStepDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrCancelStepDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrCancelStepDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrCancelStepDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrCancelStepDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrCancelStepDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrCancelStepDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrCancelStepDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrCancelStepDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrCancelStepDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrCancelStepDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrCancelStepDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrCancelStepDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrCancelStepDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrCancelStepDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrCancelStepDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrCancelStepDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrCancelStepDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrCancelStepDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrCancelStepDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrCancelStepDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrCancelStepDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrCancelStepDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrCancelStepDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrCancelStepDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrCancelStepDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrCancelStepDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrContainersCreateVirtualCluster
class EmrContainersCreateVirtualClusterDef(BaseConstruct):
    eks_cluster: models.aws_stepfunctions_tasks.EksClusterInputDef = pydantic.Field(..., description='EKS Cluster or task input that contains the name of the cluster.\n')
    eks_namespace: typing.Optional[str] = pydantic.Field(None, description="The namespace of an EKS cluster. Default: - 'default'\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags assigned to the virtual cluster. Default: {}\n')
    virtual_cluster_name: typing.Optional[str] = pydantic.Field(None, description='Name of the virtual cluster that will be created. Default: - the name of the state machine execution that runs this task and state name\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['eks_cluster', 'eks_namespace', 'tags', 'virtual_cluster_name', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrContainersCreateVirtualCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrContainersCreateVirtualClusterDefConfig] = pydantic.Field(None)


class EmrContainersCreateVirtualClusterDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrContainersCreateVirtualClusterDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrContainersCreateVirtualClusterDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrContainersCreateVirtualClusterDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrContainersCreateVirtualClusterDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrContainersCreateVirtualClusterDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrContainersCreateVirtualClusterDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrContainersCreateVirtualClusterDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrContainersCreateVirtualClusterDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrContainersCreateVirtualClusterDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrContainersCreateVirtualClusterDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrContainersCreateVirtualClusterDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrContainersCreateVirtualClusterDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrContainersCreateVirtualClusterDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrContainersCreateVirtualClusterDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrContainersCreateVirtualClusterDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrContainersCreateVirtualClusterDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersCreateVirtualClusterDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrContainersDeleteVirtualCluster
class EmrContainersDeleteVirtualClusterDef(BaseConstruct):
    virtual_cluster_id: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The ID of the virtual cluster that will be deleted.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['virtual_cluster_id', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrContainersDeleteVirtualCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrContainersDeleteVirtualClusterDefConfig] = pydantic.Field(None)


class EmrContainersDeleteVirtualClusterDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrContainersDeleteVirtualClusterDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrContainersDeleteVirtualClusterDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrContainersDeleteVirtualClusterDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrContainersDeleteVirtualClusterDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrContainersDeleteVirtualClusterDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrContainersDeleteVirtualClusterDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrContainersDeleteVirtualClusterDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrContainersDeleteVirtualClusterDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrContainersDeleteVirtualClusterDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrContainersDeleteVirtualClusterDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrContainersDeleteVirtualClusterDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrContainersDeleteVirtualClusterDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrContainersDeleteVirtualClusterDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrContainersDeleteVirtualClusterDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrContainersDeleteVirtualClusterDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrContainersDeleteVirtualClusterDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersDeleteVirtualClusterDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrContainersStartJobRun
class EmrContainersStartJobRunDef(BaseConstruct):
    job_driver: typing.Union[models.aws_stepfunctions_tasks.JobDriverDef, dict[str, typing.Any]] = pydantic.Field(..., description='The job driver for the job run.\n')
    release_label: models.aws_stepfunctions_tasks.ReleaseLabelDef = pydantic.Field(..., description='The Amazon EMR release version to use for the job run.\n')
    virtual_cluster: models.aws_stepfunctions_tasks.VirtualClusterInputDef = pydantic.Field(..., description='The ID of the virtual cluster where the job will be run.\n')
    application_config: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ApplicationConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The configurations for the application running in the job run. Maximum of 100 items Default: - No application config\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The execution role for the job run. If ``virtualClusterId`` is from a JSON input path, an execution role must be provided. If an execution role is provided, follow the documentation to update the role trust policy. Default: - Automatically generated only when the provided ``virtualClusterId`` is not an encoded JSON path\n')
    job_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job run. Default: - No job run name\n')
    monitoring: typing.Union[models.aws_stepfunctions_tasks.MonitoringDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for monitoring the job run. Default: - logging enabled and resources automatically generated if ``monitoring.logging`` is set to ``true``\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags assigned to job runs. Default: - None\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['job_driver', 'release_label', 'virtual_cluster', 'application_config', 'execution_role', 'job_name', 'monitoring', 'tags', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrContainersStartJobRun'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrContainersStartJobRunDefConfig] = pydantic.Field(None)


class EmrContainersStartJobRunDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrContainersStartJobRunDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrContainersStartJobRunDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrContainersStartJobRunDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrContainersStartJobRunDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrContainersStartJobRunDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrContainersStartJobRunDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrContainersStartJobRunDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrContainersStartJobRunDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrContainersStartJobRunDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrContainersStartJobRunDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrContainersStartJobRunDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrContainersStartJobRunDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrContainersStartJobRunDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrContainersStartJobRunDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrContainersStartJobRunDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrContainersStartJobRunDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrContainersStartJobRunDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrContainersStartJobRunDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrContainersStartJobRunDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)

class EmrContainersStartJobRunDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrContainersStartJobRunDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrContainersStartJobRunDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrContainersStartJobRunDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrContainersStartJobRunDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrContainersStartJobRunDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrContainersStartJobRunDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster
class EmrCreateClusterDef(BaseConstruct):
    instances: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstancesConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='A specification of the number and type of Amazon EC2 instances.\n')
    name: str = pydantic.Field(..., description='The Name of the Cluster.\n')
    additional_info: typing.Optional[str] = pydantic.Field(None, description='A JSON string for selecting additional features. Default: - None\n')
    applications: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ApplicationConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A case-insensitive list of applications for Amazon EMR to install and configure when launching the cluster. Default: - EMR selected default\n')
    auto_scaling_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role for automatic scaling policies. Default: - A role will be created.\n')
    bootstrap_actions: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_BootstrapActionConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of bootstrap actions to run before Hadoop starts on the cluster nodes. Default: - None\n')
    cluster_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Also called instance profile and EC2 role. An IAM role for an EMR cluster. The EC2 instances of the cluster assume this role. This attribute has been renamed from jobFlowRole to clusterRole to align with other ERM/StepFunction integration parameters. Default: - - A Role will be created\n')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of configurations supplied for the EMR cluster you are creating. Default: - None\n')
    custom_ami_id: typing.Optional[str] = pydantic.Field(None, description='The ID of a custom Amazon EBS-backed Linux AMI. Default: - None\n')
    ebs_root_volume_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the EBS root device volume of the Linux AMI that is used for each EC2 instance. Default: - EMR selected default\n')
    kerberos_attributes: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_KerberosAttributesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Attributes for Kerberos configuration when Kerberos authentication is enabled using a security configuration. Default: - None\n')
    log_uri: typing.Optional[str] = pydantic.Field(None, description='The location in Amazon S3 to write the log files of the job flow. Default: - None\n')
    release_label: typing.Optional[str] = pydantic.Field(None, description='The Amazon EMR release label, which determines the version of open-source application packages installed on the cluster. Default: - EMR selected default\n')
    scale_down_behavior: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EmrClusterScaleDownBehavior] = pydantic.Field(None, description='Specifies the way that individual Amazon EC2 instances terminate when an automatic scale-in activity occurs or an instance group is resized. Default: - EMR selected default\n')
    security_configuration: typing.Optional[str] = pydantic.Field(None, description='The name of a security configuration to apply to the cluster. Default: - None\n')
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that will be assumed by the Amazon EMR service to access AWS resources on your behalf. Default: - A role will be created that Amazon EMR service can assume.\n')
    step_concurrency_level: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the step concurrency level to allow multiple steps to run in parallel. Requires EMR release label 5.28.0 or above. Must be in range [1, 256]. Default: 1 - no step concurrency allowed\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of tags to associate with a cluster and propagate to Amazon EC2 instances. Default: - None\n')
    visible_to_all_users: typing.Optional[bool] = pydantic.Field(None, description='A value of true indicates that all IAM users in the AWS account can perform cluster actions if they have the proper IAM policy permissions. Default: true\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['instances', 'name', 'additional_info', 'applications', 'auto_scaling_role', 'bootstrap_actions', 'cluster_role', 'configurations', 'custom_ami_id', 'ebs_root_volume_size', 'kerberos_attributes', 'log_uri', 'release_label', 'scale_down_behavior', 'security_configuration', 'service_role', 'step_concurrency_level', 'tags', 'visible_to_all_users', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['ApplicationConfigProperty', 'AutoScalingPolicyProperty', 'BootstrapActionConfigProperty', 'CloudWatchAlarmComparisonOperator', 'CloudWatchAlarmDefinitionProperty', 'CloudWatchAlarmStatistic', 'CloudWatchAlarmUnit', 'ConfigurationProperty', 'EbsBlockDeviceConfigProperty', 'EbsBlockDeviceVolumeType', 'EbsConfigurationProperty', 'EmrClusterScaleDownBehavior', 'InstanceFleetConfigProperty', 'InstanceFleetProvisioningSpecificationsProperty', 'InstanceGroupConfigProperty', 'InstanceMarket', 'InstanceRoleType', 'InstanceTypeConfigProperty', 'InstancesConfigProperty', 'KerberosAttributesProperty', 'MetricDimensionProperty', 'PlacementTypeProperty', 'ScalingActionProperty', 'ScalingAdjustmentType', 'ScalingConstraintsProperty', 'ScalingRuleProperty', 'ScalingTriggerProperty', 'ScriptBootstrapActionConfigProperty', 'SimpleScalingPolicyConfigurationProperty', 'SpotAllocationStrategy', 'SpotProvisioningSpecificationProperty', 'SpotTimeoutAction', 'VolumeSpecificationProperty', 'add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrCreateClusterDefConfig] = pydantic.Field(None)


class EmrCreateClusterDefConfig(pydantic.BaseModel):
    ApplicationConfigProperty: typing.Optional[list[EmrCreateClusterDefApplicationconfigpropertyParams]] = pydantic.Field(None, description='')
    AutoScalingPolicyProperty: typing.Optional[list[EmrCreateClusterDefAutoscalingpolicypropertyParams]] = pydantic.Field(None, description='')
    BootstrapActionConfigProperty: typing.Optional[list[EmrCreateClusterDefBootstrapactionconfigpropertyParams]] = pydantic.Field(None, description='')
    CloudWatchAlarmComparisonOperator: typing.Optional[list[EmrCreateClusterDefCloudwatchalarmcomparisonoperatorParams]] = pydantic.Field(None, description='CloudWatch Alarm Comparison Operators.')
    CloudWatchAlarmDefinitionProperty: typing.Optional[list[EmrCreateClusterDefCloudwatchalarmdefinitionpropertyParams]] = pydantic.Field(None, description='')
    CloudWatchAlarmStatistic: typing.Optional[list[EmrCreateClusterDefCloudwatchalarmstatisticParams]] = pydantic.Field(None, description='CloudWatch Alarm Statistics.')
    CloudWatchAlarmUnit: typing.Optional[list[EmrCreateClusterDefCloudwatchalarmunitParams]] = pydantic.Field(None, description='CloudWatch Alarm Units.')
    ConfigurationProperty: typing.Optional[list[EmrCreateClusterDefConfigurationpropertyParams]] = pydantic.Field(None, description='')
    EbsBlockDeviceConfigProperty: typing.Optional[list[EmrCreateClusterDefEbsblockdeviceconfigpropertyParams]] = pydantic.Field(None, description='')
    EbsBlockDeviceVolumeType: typing.Optional[list[EmrCreateClusterDefEbsblockdevicevolumetypeParams]] = pydantic.Field(None, description='EBS Volume Types.')
    EbsConfigurationProperty: typing.Optional[list[EmrCreateClusterDefEbsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    EmrClusterScaleDownBehavior: typing.Optional[list[EmrCreateClusterDefEmrclusterscaledownbehaviorParams]] = pydantic.Field(None, description='The Cluster ScaleDownBehavior specifies the way that individual Amazon EC2 instances terminate when an automatic scale-in activity occurs or an instance group is resized.\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html#EMR-RunJobFlow-request-ScaleDownBehavior')
    InstanceFleetConfigProperty: typing.Optional[list[EmrCreateClusterDefInstancefleetconfigpropertyParams]] = pydantic.Field(None, description='')
    InstanceFleetProvisioningSpecificationsProperty: typing.Optional[list[EmrCreateClusterDefInstancefleetprovisioningspecificationspropertyParams]] = pydantic.Field(None, description='')
    InstanceGroupConfigProperty: typing.Optional[list[EmrCreateClusterDefInstancegroupconfigpropertyParams]] = pydantic.Field(None, description='')
    InstanceMarket: typing.Optional[list[EmrCreateClusterDefInstancemarketParams]] = pydantic.Field(None, description='EC2 Instance Market.')
    InstanceRoleType: typing.Optional[list[EmrCreateClusterDefInstanceroletypeParams]] = pydantic.Field(None, description='Instance Role Types.')
    InstanceTypeConfigProperty: typing.Optional[list[EmrCreateClusterDefInstancetypeconfigpropertyParams]] = pydantic.Field(None, description='')
    InstancesConfigProperty: typing.Optional[list[EmrCreateClusterDefInstancesconfigpropertyParams]] = pydantic.Field(None, description='')
    KerberosAttributesProperty: typing.Optional[list[EmrCreateClusterDefKerberosattributespropertyParams]] = pydantic.Field(None, description='')
    MetricDimensionProperty: typing.Optional[list[EmrCreateClusterDefMetricdimensionpropertyParams]] = pydantic.Field(None, description='')
    PlacementTypeProperty: typing.Optional[list[EmrCreateClusterDefPlacementtypepropertyParams]] = pydantic.Field(None, description='')
    ScalingActionProperty: typing.Optional[list[EmrCreateClusterDefScalingactionpropertyParams]] = pydantic.Field(None, description='')
    ScalingAdjustmentType: typing.Optional[list[EmrCreateClusterDefScalingadjustmenttypeParams]] = pydantic.Field(None, description='AutoScaling Adjustment Type.')
    ScalingConstraintsProperty: typing.Optional[list[EmrCreateClusterDefScalingconstraintspropertyParams]] = pydantic.Field(None, description='')
    ScalingRuleProperty: typing.Optional[list[EmrCreateClusterDefScalingrulepropertyParams]] = pydantic.Field(None, description='')
    ScalingTriggerProperty: typing.Optional[list[EmrCreateClusterDefScalingtriggerpropertyParams]] = pydantic.Field(None, description='')
    ScriptBootstrapActionConfigProperty: typing.Optional[list[EmrCreateClusterDefScriptbootstrapactionconfigpropertyParams]] = pydantic.Field(None, description='')
    SimpleScalingPolicyConfigurationProperty: typing.Optional[list[EmrCreateClusterDefSimplescalingpolicyconfigurationpropertyParams]] = pydantic.Field(None, description='')
    SpotAllocationStrategy: typing.Optional[list[EmrCreateClusterDefSpotallocationstrategyParams]] = pydantic.Field(None, description='Spot Allocation Strategies.\nSpecifies the strategy to use in launching Spot Instance fleets. For example, "capacity-optimized" launches instances from Spot Instance pools with optimal capacity for the number of instances that are launching.\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_SpotProvisioningSpecification.html')
    SpotProvisioningSpecificationProperty: typing.Optional[list[EmrCreateClusterDefSpotprovisioningspecificationpropertyParams]] = pydantic.Field(None, description='')
    SpotTimeoutAction: typing.Optional[list[EmrCreateClusterDefSpottimeoutactionParams]] = pydantic.Field(None, description='Spot Timeout Actions.')
    VolumeSpecificationProperty: typing.Optional[list[EmrCreateClusterDefVolumespecificationpropertyParams]] = pydantic.Field(None, description='')
    add_catch: typing.Optional[list[EmrCreateClusterDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrCreateClusterDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrCreateClusterDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrCreateClusterDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrCreateClusterDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrCreateClusterDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrCreateClusterDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrCreateClusterDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrCreateClusterDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrCreateClusterDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrCreateClusterDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrCreateClusterDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrCreateClusterDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrCreateClusterDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrCreateClusterDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrCreateClusterDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrCreateClusterDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrCreateClusterDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrCreateClusterDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')
    auto_scaling_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)
    cluster_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)
    service_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class EmrCreateClusterDefApplicationconfigpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    additional_info: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='')
    args: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefAutoscalingpolicypropertyParams(pydantic.BaseModel):
    constraints: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingConstraintsPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    rules: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingRulePropertyDef, dict[str, typing.Any]]] = pydantic.Field(..., description='')
    ...

class EmrCreateClusterDefBootstrapactionconfigpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    script_bootstrap_action: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScriptBootstrapActionConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class EmrCreateClusterDefCloudwatchalarmcomparisonoperatorParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefCloudwatchalarmdefinitionpropertyParams(pydantic.BaseModel):
    comparison_operator: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator = pydantic.Field(..., description='')
    metric_name: str = pydantic.Field(..., description='')
    period: models.DurationDef = pydantic.Field(..., description='')
    dimensions: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_MetricDimensionPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    evaluation_periods: typing.Union[int, float, None] = pydantic.Field(None, description='')
    namespace: typing.Optional[str] = pydantic.Field(None, description='')
    statistic: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic] = pydantic.Field(None, description='')
    threshold: typing.Union[int, float, None] = pydantic.Field(None, description='')
    unit: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefCloudwatchalarmstatisticParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefCloudwatchalarmunitParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefConfigurationpropertyParams(pydantic.BaseModel):
    classification: typing.Optional[str] = pydantic.Field(None, description='')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    properties: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefEbsblockdeviceconfigpropertyParams(pydantic.BaseModel):
    volume_specification: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_VolumeSpecificationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    volumes_per_instance: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefEbsblockdevicevolumetypeParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefEbsconfigurationpropertyParams(pydantic.BaseModel):
    ebs_block_device_configs: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_EbsBlockDeviceConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    ebs_optimized: typing.Optional[bool] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefEmrclusterscaledownbehaviorParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefInstancefleetconfigpropertyParams(pydantic.BaseModel):
    instance_fleet_type: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceRoleType = pydantic.Field(..., description='')
    instance_type_configs: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceTypeConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    launch_specifications: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceFleetProvisioningSpecificationsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    target_on_demand_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    target_spot_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefInstancefleetprovisioningspecificationspropertyParams(pydantic.BaseModel):
    spot_specification: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_SpotProvisioningSpecificationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class EmrCreateClusterDefInstancegroupconfigpropertyParams(pydantic.BaseModel):
    instance_count: typing.Union[int, float] = pydantic.Field(..., description='')
    instance_role: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceRoleType = pydantic.Field(..., description='')
    instance_type: str = pydantic.Field(..., description='')
    auto_scaling_policy: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_AutoScalingPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    bid_price: typing.Optional[str] = pydantic.Field(None, description='')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    ebs_configuration: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_EbsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    market: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceMarket] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefInstancemarketParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefInstanceroletypeParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefInstancetypeconfigpropertyParams(pydantic.BaseModel):
    instance_type: str = pydantic.Field(..., description='')
    bid_price: typing.Optional[str] = pydantic.Field(None, description='')
    bid_price_as_percentage_of_on_demand_price: typing.Union[int, float, None] = pydantic.Field(None, description='')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    ebs_configuration: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_EbsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    weighted_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefInstancesconfigpropertyParams(pydantic.BaseModel):
    additional_master_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    additional_slave_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ec2_key_name: typing.Optional[str] = pydantic.Field(None, description='')
    ec2_subnet_id: typing.Optional[str] = pydantic.Field(None, description='')
    ec2_subnet_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    emr_managed_master_security_group: typing.Optional[str] = pydantic.Field(None, description='')
    emr_managed_slave_security_group: typing.Optional[str] = pydantic.Field(None, description='')
    hadoop_version: typing.Optional[str] = pydantic.Field(None, description='')
    instance_count: typing.Union[int, float, None] = pydantic.Field(None, description='')
    instance_fleets: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceFleetConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    instance_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceGroupConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    master_instance_type: typing.Optional[str] = pydantic.Field(None, description='')
    placement: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_PlacementTypePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    service_access_security_group: typing.Optional[str] = pydantic.Field(None, description='')
    slave_instance_type: typing.Optional[str] = pydantic.Field(None, description='')
    termination_protected: typing.Optional[bool] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefKerberosattributespropertyParams(pydantic.BaseModel):
    realm: str = pydantic.Field(..., description='')
    ad_domain_join_password: typing.Optional[str] = pydantic.Field(None, description='')
    ad_domain_join_user: typing.Optional[str] = pydantic.Field(None, description='')
    cross_realm_trust_principal_password: typing.Optional[str] = pydantic.Field(None, description='')
    kdc_admin_password: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefMetricdimensionpropertyParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='')
    value: str = pydantic.Field(..., description='')
    ...

class EmrCreateClusterDefPlacementtypepropertyParams(pydantic.BaseModel):
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='')
    availability_zones: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefScalingactionpropertyParams(pydantic.BaseModel):
    simple_scaling_policy_configuration: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_SimpleScalingPolicyConfigurationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    market: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceMarket] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefScalingadjustmenttypeParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefScalingconstraintspropertyParams(pydantic.BaseModel):
    max_capacity: typing.Union[int, float] = pydantic.Field(..., description='')
    min_capacity: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class EmrCreateClusterDefScalingrulepropertyParams(pydantic.BaseModel):
    action: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingActionPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    name: str = pydantic.Field(..., description='')
    trigger: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingTriggerPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    description: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefScalingtriggerpropertyParams(pydantic.BaseModel):
    cloud_watch_alarm_definition: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_CloudWatchAlarmDefinitionPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class EmrCreateClusterDefScriptbootstrapactionconfigpropertyParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='')
    args: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefSimplescalingpolicyconfigurationpropertyParams(pydantic.BaseModel):
    scaling_adjustment: typing.Union[int, float] = pydantic.Field(..., description='')
    adjustment_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType] = pydantic.Field(None, description='')
    cool_down: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefSpotallocationstrategyParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefSpotprovisioningspecificationpropertyParams(pydantic.BaseModel):
    timeout_action: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotTimeoutAction = pydantic.Field(..., description='')
    timeout_duration_minutes: typing.Union[int, float] = pydantic.Field(..., description='')
    allocation_strategy: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotAllocationStrategy] = pydantic.Field(None, description='')
    block_duration_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefSpottimeoutactionParams(pydantic.BaseModel):
    value: typing.Any = pydantic.Field(..., description='')
    names: typing.Any = pydantic.Field(None, description='')
    module: typing.Any = pydantic.Field(None, description='')
    qualname: typing.Any = pydantic.Field(None, description='')
    type: typing.Any = pydantic.Field(None, description='')
    start: typing.Any = pydantic.Field(1, description='')
    boundary: typing.Any = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefVolumespecificationpropertyParams(pydantic.BaseModel):
    volume_size: models.SizeDef = pydantic.Field(..., description='')
    volume_type: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType = pydantic.Field(..., description='')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class EmrCreateClusterDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrCreateClusterDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrCreateClusterDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrCreateClusterDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrCreateClusterDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrCreateClusterDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrCreateClusterDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceFleetByName
class EmrModifyInstanceFleetByNameDef(BaseConstruct):
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    instance_fleet_name: str = pydantic.Field(..., description='The InstanceFleetName to update.\n')
    target_on_demand_capacity: typing.Union[int, float] = pydantic.Field(..., description='The target capacity of On-Demand units for the instance fleet. Default: - None\n')
    target_spot_capacity: typing.Union[int, float] = pydantic.Field(..., description='The target capacity of Spot units for the instance fleet. Default: - None\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster_id', 'instance_fleet_name', 'target_on_demand_capacity', 'target_spot_capacity', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceFleetByName'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrModifyInstanceFleetByNameDefConfig] = pydantic.Field(None)


class EmrModifyInstanceFleetByNameDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrModifyInstanceFleetByNameDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrModifyInstanceFleetByNameDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrModifyInstanceFleetByNameDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrModifyInstanceFleetByNameDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrModifyInstanceFleetByNameDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrModifyInstanceFleetByNameDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrModifyInstanceFleetByNameDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrModifyInstanceFleetByNameDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrModifyInstanceFleetByNameDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrModifyInstanceFleetByNameDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrModifyInstanceFleetByNameDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrModifyInstanceFleetByNameDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrModifyInstanceFleetByNameDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrModifyInstanceFleetByNameDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrModifyInstanceFleetByNameDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrModifyInstanceFleetByNameDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceFleetByNameDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName
class EmrModifyInstanceGroupByNameDef(BaseConstruct):
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    instance_group: typing.Union[models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName_InstanceGroupModifyConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The JSON that you want to provide to your ModifyInstanceGroup call as input. This uses the same syntax as the ModifyInstanceGroups API.\n')
    instance_group_name: str = pydantic.Field(..., description='The InstanceGroupName to update.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster_id', 'instance_group', 'instance_group_name', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['InstanceGroupModifyConfigProperty', 'InstanceResizePolicyProperty', 'ShrinkPolicyProperty', 'add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrModifyInstanceGroupByNameDefConfig] = pydantic.Field(None)


class EmrModifyInstanceGroupByNameDefConfig(pydantic.BaseModel):
    InstanceGroupModifyConfigProperty: typing.Optional[list[EmrModifyInstanceGroupByNameDefInstancegroupmodifyconfigpropertyParams]] = pydantic.Field(None, description='')
    InstanceResizePolicyProperty: typing.Optional[list[EmrModifyInstanceGroupByNameDefInstanceresizepolicypropertyParams]] = pydantic.Field(None, description='')
    ShrinkPolicyProperty: typing.Optional[list[EmrModifyInstanceGroupByNameDefShrinkpolicypropertyParams]] = pydantic.Field(None, description='')
    add_catch: typing.Optional[list[EmrModifyInstanceGroupByNameDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrModifyInstanceGroupByNameDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrModifyInstanceGroupByNameDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrModifyInstanceGroupByNameDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrModifyInstanceGroupByNameDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrModifyInstanceGroupByNameDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrModifyInstanceGroupByNameDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrModifyInstanceGroupByNameDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrModifyInstanceGroupByNameDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrModifyInstanceGroupByNameDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrModifyInstanceGroupByNameDefInstancegroupmodifyconfigpropertyParams(pydantic.BaseModel):
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    e_c2_instance_ids_to_terminate: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    instance_count: typing.Union[int, float, None] = pydantic.Field(None, description='')
    shrink_policy: typing.Union[models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName_ShrinkPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class EmrModifyInstanceGroupByNameDefInstanceresizepolicypropertyParams(pydantic.BaseModel):
    instances_to_protect: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    instances_to_terminate: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    instance_termination_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='')
    ...

class EmrModifyInstanceGroupByNameDefShrinkpolicypropertyParams(pydantic.BaseModel):
    decommission_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='')
    instance_resize_policy: typing.Union[models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName_InstanceResizePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class EmrModifyInstanceGroupByNameDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrModifyInstanceGroupByNameDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrModifyInstanceGroupByNameDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrModifyInstanceGroupByNameDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrModifyInstanceGroupByNameDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrModifyInstanceGroupByNameDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrModifyInstanceGroupByNameDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrSetClusterTerminationProtection
class EmrSetClusterTerminationProtectionDef(BaseConstruct):
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    termination_protected: bool = pydantic.Field(..., description='Termination protection indicator.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster_id', 'termination_protected', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrSetClusterTerminationProtection'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrSetClusterTerminationProtectionDefConfig] = pydantic.Field(None)


class EmrSetClusterTerminationProtectionDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrSetClusterTerminationProtectionDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrSetClusterTerminationProtectionDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrSetClusterTerminationProtectionDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrSetClusterTerminationProtectionDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrSetClusterTerminationProtectionDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrSetClusterTerminationProtectionDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrSetClusterTerminationProtectionDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrSetClusterTerminationProtectionDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrSetClusterTerminationProtectionDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrSetClusterTerminationProtectionDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrSetClusterTerminationProtectionDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrSetClusterTerminationProtectionDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrSetClusterTerminationProtectionDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrSetClusterTerminationProtectionDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrSetClusterTerminationProtectionDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrSetClusterTerminationProtectionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrSetClusterTerminationProtectionDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrTerminateCluster
class EmrTerminateClusterDef(BaseConstruct):
    cluster_id: str = pydantic.Field(..., description='The ClusterId to terminate.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['cluster_id', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrTerminateCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrTerminateClusterDefConfig] = pydantic.Field(None)


class EmrTerminateClusterDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EmrTerminateClusterDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EmrTerminateClusterDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EmrTerminateClusterDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EmrTerminateClusterDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EmrTerminateClusterDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EmrTerminateClusterDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EmrTerminateClusterDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EmrTerminateClusterDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EmrTerminateClusterDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EmrTerminateClusterDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EmrTerminateClusterDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EmrTerminateClusterDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EmrTerminateClusterDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EmrTerminateClusterDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EmrTerminateClusterDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EmrTerminateClusterDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EmrTerminateClusterDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EmrTerminateClusterDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EmrTerminateClusterDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EmrTerminateClusterDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EmrTerminateClusterDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EmrTerminateClusterDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EmrTerminateClusterDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrTerminateClusterDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EmrTerminateClusterDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EmrTerminateClusterDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EvaluateExpression
class EvaluateExpressionDef(BaseConstruct):
    expression: str = pydantic.Field(..., description="The expression to evaluate. The expression may contain state paths. Example value: ``'$.a + $.b'``\n")
    runtime: typing.Optional[models.aws_lambda.RuntimeDef] = pydantic.Field(None, description='The runtime language to use to evaluate the expression. Default: lambda.Runtime.NODEJS_16_X\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['expression', 'runtime', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EvaluateExpression'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EvaluateExpressionDefConfig] = pydantic.Field(None)


class EvaluateExpressionDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EvaluateExpressionDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EvaluateExpressionDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EvaluateExpressionDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EvaluateExpressionDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EvaluateExpressionDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EvaluateExpressionDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EvaluateExpressionDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EvaluateExpressionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EvaluateExpressionDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EvaluateExpressionDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EvaluateExpressionDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EvaluateExpressionDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EvaluateExpressionDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EvaluateExpressionDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EvaluateExpressionDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EvaluateExpressionDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EvaluateExpressionDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EvaluateExpressionDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EvaluateExpressionDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EvaluateExpressionDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EvaluateExpressionDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EvaluateExpressionDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EvaluateExpressionDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EvaluateExpressionDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EvaluateExpressionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EvaluateExpressionDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EventBridgePutEvents
class EventBridgePutEventsDef(BaseConstruct):
    entries: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EventBridgePutEventsEntryDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The entries that will be sent. Minimum number of entries is 1 and maximum is 10, unless `PutEvents API limit <https://docs.aws.amazon.com/eventbridge/latest/APIReference/API_PutEvents.html#API_PutEvents_RequestSyntax>`_ has changed.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['entries', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EventBridgePutEvents'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EventBridgePutEventsDefConfig] = pydantic.Field(None)


class EventBridgePutEventsDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[EventBridgePutEventsDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[EventBridgePutEventsDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[EventBridgePutEventsDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[EventBridgePutEventsDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[EventBridgePutEventsDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[EventBridgePutEventsDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[EventBridgePutEventsDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[EventBridgePutEventsDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[EventBridgePutEventsDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[EventBridgePutEventsDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[EventBridgePutEventsDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[EventBridgePutEventsDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[EventBridgePutEventsDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[EventBridgePutEventsDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[EventBridgePutEventsDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[EventBridgePutEventsDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[EventBridgePutEventsDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[EventBridgePutEventsDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[EventBridgePutEventsDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class EventBridgePutEventsDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class EventBridgePutEventsDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class EventBridgePutEventsDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class EventBridgePutEventsDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EventBridgePutEventsDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class EventBridgePutEventsDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class EventBridgePutEventsDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.GlueDataBrewStartJobRun
class GlueDataBrewStartJobRunDef(BaseConstruct):
    name: str = pydantic.Field(..., description='Glue DataBrew Job to run.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['name', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.GlueDataBrewStartJobRun'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[GlueDataBrewStartJobRunDefConfig] = pydantic.Field(None)


class GlueDataBrewStartJobRunDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[GlueDataBrewStartJobRunDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[GlueDataBrewStartJobRunDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[GlueDataBrewStartJobRunDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[GlueDataBrewStartJobRunDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[GlueDataBrewStartJobRunDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[GlueDataBrewStartJobRunDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[GlueDataBrewStartJobRunDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[GlueDataBrewStartJobRunDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[GlueDataBrewStartJobRunDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[GlueDataBrewStartJobRunDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[GlueDataBrewStartJobRunDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[GlueDataBrewStartJobRunDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[GlueDataBrewStartJobRunDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[GlueDataBrewStartJobRunDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[GlueDataBrewStartJobRunDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[GlueDataBrewStartJobRunDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[GlueDataBrewStartJobRunDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[GlueDataBrewStartJobRunDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[GlueDataBrewStartJobRunDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class GlueDataBrewStartJobRunDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class GlueDataBrewStartJobRunDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class GlueDataBrewStartJobRunDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class GlueDataBrewStartJobRunDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class GlueDataBrewStartJobRunDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class GlueDataBrewStartJobRunDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class GlueDataBrewStartJobRunDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.GlueStartJobRun
class GlueStartJobRunDef(BaseConstruct):
    glue_job_name: str = pydantic.Field(..., description='Glue job name.\n')
    arguments: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='The job arguments specifically for this run. For this job run, they replace the default arguments set in the job definition itself. Default: - Default arguments set in the job definition\n')
    notify_delay_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='After a job run starts, the number of minutes to wait before sending a job run delay notification. Must be at least 1 minute. Default: - Default delay set in the job definition\n')
    security_configuration: typing.Optional[str] = pydantic.Field(None, description='The name of the SecurityConfiguration structure to be used with this job run. This must match the Glue API Default: - Default configuration set in the job definition\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['glue_job_name', 'arguments', 'notify_delay_after', 'security_configuration', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.GlueStartJobRun'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[GlueStartJobRunDefConfig] = pydantic.Field(None)


class GlueStartJobRunDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[GlueStartJobRunDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[GlueStartJobRunDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[GlueStartJobRunDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[GlueStartJobRunDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[GlueStartJobRunDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[GlueStartJobRunDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[GlueStartJobRunDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[GlueStartJobRunDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[GlueStartJobRunDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[GlueStartJobRunDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[GlueStartJobRunDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[GlueStartJobRunDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[GlueStartJobRunDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[GlueStartJobRunDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[GlueStartJobRunDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[GlueStartJobRunDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[GlueStartJobRunDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[GlueStartJobRunDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[GlueStartJobRunDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class GlueStartJobRunDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class GlueStartJobRunDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class GlueStartJobRunDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class GlueStartJobRunDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class GlueStartJobRunDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class GlueStartJobRunDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class GlueStartJobRunDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.LambdaInvoke
class LambdaInvokeDef(BaseConstruct):
    lambda_function: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='Lambda function to invoke.\n')
    client_context: typing.Optional[str] = pydantic.Field(None, description='Up to 3583 bytes of base64-encoded data about the invoking client to pass to the function. Default: - No context\n')
    invocation_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.LambdaInvocationType] = pydantic.Field(None, description='Invocation type of the Lambda function. Default: InvocationType.REQUEST_RESPONSE\n')
    payload: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description="The JSON that will be supplied as input to the Lambda function. Default: - The state input (JSON path '$')\n")
    payload_response_only: typing.Optional[bool] = pydantic.Field(None, description='Invoke the Lambda in a way that only returns the payload response without additional metadata. The ``payloadResponseOnly`` property cannot be used if ``integrationPattern``, ``invocationType``, ``clientContext``, or ``qualifier`` are specified. It always uses the REQUEST_RESPONSE behavior. Default: false\n')
    qualifier: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Version or alias to invoke a published version of the function. You only need to supply this if you want the version of the Lambda Function to depend on data in the state machine state. If not, you can pass the appropriate Alias or Version object directly as the ``lambdaFunction`` argument. Default: - Version or alias inherent to the ``lambdaFunction`` object.\n')
    retry_on_service_exceptions: typing.Optional[bool] = pydantic.Field(None, description='Whether to retry on Lambda service exceptions. This handles ``Lambda.ServiceException``, ``Lambda.AWSLambdaException`` and ``Lambda.SdkClientException`` with an interval of 2 seconds, a back-off rate of 2 and 6 maximum attempts. Default: true\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['lambda_function', 'client_context', 'invocation_type', 'payload', 'payload_response_only', 'qualifier', 'retry_on_service_exceptions', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.LambdaInvoke'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[LambdaInvokeDefConfig] = pydantic.Field(None)


class LambdaInvokeDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[LambdaInvokeDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[LambdaInvokeDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[LambdaInvokeDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[LambdaInvokeDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[LambdaInvokeDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[LambdaInvokeDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[LambdaInvokeDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[LambdaInvokeDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[LambdaInvokeDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[LambdaInvokeDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[LambdaInvokeDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[LambdaInvokeDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[LambdaInvokeDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[LambdaInvokeDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[LambdaInvokeDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[LambdaInvokeDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[LambdaInvokeDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[LambdaInvokeDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[LambdaInvokeDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class LambdaInvokeDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class LambdaInvokeDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class LambdaInvokeDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class LambdaInvokeDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class LambdaInvokeDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class LambdaInvokeDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpoint
class SageMakerCreateEndpointDef(BaseConstruct):
    endpoint_config_name: str = pydantic.Field(..., description='The name of an endpoint configuration.\n')
    endpoint_name: str = pydantic.Field(..., description='The name of the endpoint. The name must be unique within an AWS Region in your AWS account.\n')
    tags: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Tags to be applied to the endpoint. Default: - No tags\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['endpoint_config_name', 'endpoint_name', 'tags', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SageMakerCreateEndpointDefConfig] = pydantic.Field(None)


class SageMakerCreateEndpointDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SageMakerCreateEndpointDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SageMakerCreateEndpointDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SageMakerCreateEndpointDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[SageMakerCreateEndpointDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SageMakerCreateEndpointDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SageMakerCreateEndpointDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SageMakerCreateEndpointDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SageMakerCreateEndpointDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SageMakerCreateEndpointDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SageMakerCreateEndpointDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SageMakerCreateEndpointDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SageMakerCreateEndpointDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SageMakerCreateEndpointDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SageMakerCreateEndpointDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SageMakerCreateEndpointDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SageMakerCreateEndpointDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SageMakerCreateEndpointDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SageMakerCreateEndpointDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SageMakerCreateEndpointDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class SageMakerCreateEndpointDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SageMakerCreateEndpointDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SageMakerCreateEndpointDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SageMakerCreateEndpointDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateEndpointDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateEndpointDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpointConfig
class SageMakerCreateEndpointConfigDef(BaseConstruct):
    endpoint_config_name: str = pydantic.Field(..., description='The name of the endpoint configuration.\n')
    production_variants: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ProductionVariantDef, dict[str, typing.Any]]] = pydantic.Field(..., description='An list of ProductionVariant objects, one for each model that you want to host at this endpoint. Identifies a model that you want to host and the resources to deploy for hosting it. If you are deploying multiple models, tell Amazon SageMaker how to distribute traffic among the models by specifying variant weights.\n')
    kms_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='AWS Key Management Service key that Amazon SageMaker uses to encrypt data on the storage volume attached to the ML compute instance that hosts the endpoint. Default: - None\n')
    tags: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Tags to be applied to the endpoint configuration. Default: - No tags\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['endpoint_config_name', 'production_variants', 'kms_key', 'tags', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpointConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SageMakerCreateEndpointConfigDefConfig] = pydantic.Field(None)


class SageMakerCreateEndpointConfigDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SageMakerCreateEndpointConfigDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SageMakerCreateEndpointConfigDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SageMakerCreateEndpointConfigDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[SageMakerCreateEndpointConfigDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SageMakerCreateEndpointConfigDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SageMakerCreateEndpointConfigDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SageMakerCreateEndpointConfigDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SageMakerCreateEndpointConfigDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SageMakerCreateEndpointConfigDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SageMakerCreateEndpointConfigDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class SageMakerCreateEndpointConfigDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SageMakerCreateEndpointConfigDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SageMakerCreateEndpointConfigDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SageMakerCreateEndpointConfigDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateEndpointConfigDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateEndpointConfigDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateEndpointConfigDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateModel
class SageMakerCreateModelDef(BaseConstruct, ConnectableMixin):
    model_name: str = pydantic.Field(..., description='The name of the new model.\n')
    primary_container: typing.Union[models.aws_stepfunctions_tasks.ContainerDefinitionDef] = pydantic.Field(..., description='The definition of the primary docker image containing inference code, associated artifacts, and custom environment map that the inference code uses when the model is deployed for predictions.\n')
    containers: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ContainerDefinitionDef]]] = pydantic.Field(None, description='Specifies the containers in the inference pipeline. Default: - None\n')
    enable_network_isolation: typing.Optional[bool] = pydantic.Field(None, description='Isolates the model container. No inbound or outbound network calls can be made to or from the model container. Default: false\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An execution role that you can pass in a CreateModel API request. Default: - a role will be created.\n')
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets of the VPC to which the hosted model is connected (Note this parameter is only used when VPC is provided). Default: - Private Subnets are selected\n')
    tags: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Tags to be applied to the model. Default: - No tags\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC that is accessible by the hosted model. Default: - None\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['model_name', 'primary_container', 'containers', 'enable_network_isolation', 'role', 'subnet_selection', 'tags', 'vpc', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'add_security_group', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateModel'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SageMakerCreateModelDefConfig] = pydantic.Field(None)


class SageMakerCreateModelDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SageMakerCreateModelDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SageMakerCreateModelDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SageMakerCreateModelDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    add_security_group: typing.Optional[list[SageMakerCreateModelDefAddSecurityGroupParams]] = pydantic.Field(None, description='Add the security group to all instances via the launch configuration security groups array.')
    bind_to_graph: typing.Optional[list[SageMakerCreateModelDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SageMakerCreateModelDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SageMakerCreateModelDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SageMakerCreateModelDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SageMakerCreateModelDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SageMakerCreateModelDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SageMakerCreateModelDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SageMakerCreateModelDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SageMakerCreateModelDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SageMakerCreateModelDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SageMakerCreateModelDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SageMakerCreateModelDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SageMakerCreateModelDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SageMakerCreateModelDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SageMakerCreateModelDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SageMakerCreateModelDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class SageMakerCreateModelDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SageMakerCreateModelDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefAddSecurityGroupParams(pydantic.BaseModel):
    security_group: typing.Union[models.aws_ec2.SecurityGroupDef] = pydantic.Field(..., description=': The security group to add.')
    ...

class SageMakerCreateModelDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SageMakerCreateModelDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SageMakerCreateModelDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateModelDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateModelDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateModelDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTrainingJob
class SageMakerCreateTrainingJobDef(BaseConstruct, ConnectableMixin):
    algorithm_specification: typing.Union[models.aws_stepfunctions_tasks.AlgorithmSpecificationDef, dict[str, typing.Any]] = pydantic.Field(..., description='Identifies the training algorithm to use.\n')
    input_data_config: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ChannelDef, dict[str, typing.Any]]] = pydantic.Field(..., description='Describes the various datasets (e.g. train, validation, test) and the Amazon S3 location where stored.\n')
    output_data_config: typing.Union[models.aws_stepfunctions_tasks.OutputDataConfigDef, dict[str, typing.Any]] = pydantic.Field(..., description='Identifies the Amazon S3 location where you want Amazon SageMaker to save the results of model training.\n')
    training_job_name: str = pydantic.Field(..., description='Training Job Name.\n')
    enable_network_isolation: typing.Optional[bool] = pydantic.Field(None, description='Isolates the training container. No inbound or outbound network calls can be made to or from the training container. Default: false\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set in the Docker container. Default: - No environment variables\n')
    hyperparameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Algorithm-specific parameters that influence the quality of the model. Set hyperparameters before you start the learning process. For a list of hyperparameters provided by Amazon SageMaker Default: - No hyperparameters\n')
    resource_config: typing.Union[models.aws_stepfunctions_tasks.ResourceConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the resources, ML compute instances, and ML storage volumes to deploy for model training. Default: - 1 instance of EC2 ``M4.XLarge`` with ``10GB`` volume\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role for the Training Job. The role must be granted all necessary permissions for the SageMaker training job to be able to operate. See https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms Default: - a role will be created.\n')
    stopping_condition: typing.Union[models.aws_stepfunctions_tasks.StoppingConditionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Sets a time limit for training. Default: - max runtime of 1 hour\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Tags to be applied to the train job. Default: - No tags\n')
    vpc_config: typing.Union[models.aws_stepfunctions_tasks.VpcConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the VPC that you want your training job to connect to. Default: - No VPC\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['algorithm_specification', 'input_data_config', 'output_data_config', 'training_job_name', 'enable_network_isolation', 'environment', 'hyperparameters', 'resource_config', 'role', 'stopping_condition', 'tags', 'vpc_config', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'add_security_group', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTrainingJob'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SageMakerCreateTrainingJobDefConfig] = pydantic.Field(None)


class SageMakerCreateTrainingJobDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SageMakerCreateTrainingJobDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SageMakerCreateTrainingJobDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SageMakerCreateTrainingJobDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    add_security_group: typing.Optional[list[SageMakerCreateTrainingJobDefAddSecurityGroupParams]] = pydantic.Field(None, description='Add the security group to all instances via the launch configuration security groups array.')
    bind_to_graph: typing.Optional[list[SageMakerCreateTrainingJobDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SageMakerCreateTrainingJobDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SageMakerCreateTrainingJobDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SageMakerCreateTrainingJobDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SageMakerCreateTrainingJobDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SageMakerCreateTrainingJobDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SageMakerCreateTrainingJobDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SageMakerCreateTrainingJobDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SageMakerCreateTrainingJobDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SageMakerCreateTrainingJobDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SageMakerCreateTrainingJobDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SageMakerCreateTrainingJobDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SageMakerCreateTrainingJobDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SageMakerCreateTrainingJobDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SageMakerCreateTrainingJobDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SageMakerCreateTrainingJobDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class SageMakerCreateTrainingJobDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SageMakerCreateTrainingJobDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefAddSecurityGroupParams(pydantic.BaseModel):
    security_group: typing.Union[models.aws_ec2.SecurityGroupDef] = pydantic.Field(..., description=': The security group to add.')
    ...

class SageMakerCreateTrainingJobDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SageMakerCreateTrainingJobDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SageMakerCreateTrainingJobDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateTrainingJobDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateTrainingJobDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTrainingJobDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTransformJob
class SageMakerCreateTransformJobDef(BaseConstruct):
    model_name: str = pydantic.Field(..., description='Name of the model that you want to use for the transform job.\n')
    transform_input: typing.Union[models.aws_stepfunctions_tasks.TransformInputDef, dict[str, typing.Any]] = pydantic.Field(..., description='Dataset to be transformed and the Amazon S3 location where it is stored.\n')
    transform_job_name: str = pydantic.Field(..., description='Transform Job Name.\n')
    transform_output: typing.Union[models.aws_stepfunctions_tasks.TransformOutputDef, dict[str, typing.Any]] = pydantic.Field(..., description='S3 location where you want Amazon SageMaker to save the results from the transform job.\n')
    batch_strategy: typing.Optional[aws_cdk.aws_stepfunctions_tasks.BatchStrategy] = pydantic.Field(None, description='Number of records to include in a mini-batch for an HTTP inference request. Default: - No batch strategy\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set in the Docker container. Default: - No environment variables\n')
    max_concurrent_transforms: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of parallel requests that can be sent to each instance in a transform job. Default: - Amazon SageMaker checks the optional execution-parameters to determine the settings for your chosen algorithm. If the execution-parameters endpoint is not enabled, the default value is 1.\n')
    max_payload: typing.Optional[models.SizeDef] = pydantic.Field(None, description='Maximum allowed size of the payload, in MB. Default: 6\n')
    model_client_options: typing.Union[models.aws_stepfunctions_tasks.ModelClientOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configures the timeout and maximum number of retries for processing a transform job invocation. Default: - 0 retries and 60 seconds of timeout\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role for the Transform Job. Default: - A role is created with ``AmazonSageMakerFullAccess`` managed policy\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Tags to be applied to the train job. Default: - No tags\n')
    transform_resources: typing.Union[models.aws_stepfunctions_tasks.TransformResourcesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='ML compute instances for the transform job. Default: - 1 instance of type M4.XLarge\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['model_name', 'transform_input', 'transform_job_name', 'transform_output', 'batch_strategy', 'environment', 'max_concurrent_transforms', 'max_payload', 'model_client_options', 'role', 'tags', 'transform_resources', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTransformJob'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SageMakerCreateTransformJobDefConfig] = pydantic.Field(None)


class SageMakerCreateTransformJobDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SageMakerCreateTransformJobDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SageMakerCreateTransformJobDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SageMakerCreateTransformJobDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[SageMakerCreateTransformJobDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SageMakerCreateTransformJobDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SageMakerCreateTransformJobDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SageMakerCreateTransformJobDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SageMakerCreateTransformJobDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SageMakerCreateTransformJobDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SageMakerCreateTransformJobDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SageMakerCreateTransformJobDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SageMakerCreateTransformJobDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SageMakerCreateTransformJobDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SageMakerCreateTransformJobDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SageMakerCreateTransformJobDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SageMakerCreateTransformJobDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SageMakerCreateTransformJobDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SageMakerCreateTransformJobDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SageMakerCreateTransformJobDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')
    role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class SageMakerCreateTransformJobDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SageMakerCreateTransformJobDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SageMakerCreateTransformJobDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SageMakerCreateTransformJobDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateTransformJobDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerCreateTransformJobDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SageMakerCreateTransformJobDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerUpdateEndpoint
class SageMakerUpdateEndpointDef(BaseConstruct):
    endpoint_config_name: str = pydantic.Field(..., description='The name of the new endpoint configuration.\n')
    endpoint_name: str = pydantic.Field(..., description='The name of the endpoint whose configuration you want to update.\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['endpoint_config_name', 'endpoint_name', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerUpdateEndpoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SageMakerUpdateEndpointDefConfig] = pydantic.Field(None)


class SageMakerUpdateEndpointDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SageMakerUpdateEndpointDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SageMakerUpdateEndpointDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SageMakerUpdateEndpointDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[SageMakerUpdateEndpointDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SageMakerUpdateEndpointDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SageMakerUpdateEndpointDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SageMakerUpdateEndpointDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SageMakerUpdateEndpointDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SageMakerUpdateEndpointDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SageMakerUpdateEndpointDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SageMakerUpdateEndpointDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SageMakerUpdateEndpointDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SageMakerUpdateEndpointDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SageMakerUpdateEndpointDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SageMakerUpdateEndpointDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SageMakerUpdateEndpointDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SageMakerUpdateEndpointDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SageMakerUpdateEndpointDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SageMakerUpdateEndpointDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class SageMakerUpdateEndpointDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SageMakerUpdateEndpointDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SageMakerUpdateEndpointDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SageMakerUpdateEndpointDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerUpdateEndpointDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SageMakerUpdateEndpointDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SageMakerUpdateEndpointDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SnsPublish
class SnsPublishDef(BaseConstruct):
    message: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The message you want to send. With the exception of SMS, messages must be UTF-8 encoded strings and at most 256 KB in size. For SMS, each message can contain up to 140 characters.\n')
    topic: typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef] = pydantic.Field(..., description='The SNS topic that the task will publish to.\n')
    message_attributes: typing.Optional[typing.Mapping[str, typing.Union[models.aws_stepfunctions_tasks.MessageAttributeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Add message attributes when publishing. These attributes carry additional metadata about the message and may be used for subscription filters. Default: {}\n')
    message_per_subscription_type: typing.Optional[bool] = pydantic.Field(None, description='Send different messages for each transport protocol. For example, you might want to send a shorter message to SMS subscribers and a more verbose message to email and SQS subscribers. Your message must be a JSON object with a top-level JSON key of "default" with a value that is a string You can define other top-level keys that define the message you want to send to a specific transport protocol (i.e. "sqs", "email", "http", etc) Default: false\n')
    subject: typing.Optional[str] = pydantic.Field(None, description='Used as the "Subject" line when the message is delivered to email endpoints. This field will also be included, if present, in the standard JSON messages delivered to other endpoints. Default: - No subject\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['message', 'topic', 'message_attributes', 'message_per_subscription_type', 'subject', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SnsPublish'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SnsPublishDefConfig] = pydantic.Field(None)


class SnsPublishDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SnsPublishDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SnsPublishDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SnsPublishDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[SnsPublishDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SnsPublishDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SnsPublishDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SnsPublishDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SnsPublishDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SnsPublishDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SnsPublishDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SnsPublishDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SnsPublishDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SnsPublishDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SnsPublishDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SnsPublishDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SnsPublishDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SnsPublishDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SnsPublishDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SnsPublishDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class SnsPublishDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SnsPublishDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SnsPublishDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SnsPublishDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SnsPublishDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SnsPublishDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SnsPublishDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SqsSendMessage
class SqsSendMessageDef(BaseConstruct):
    message_body: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The text message to send to the queue.\n')
    queue: typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef] = pydantic.Field(..., description='The SQS queue that messages will be sent to.\n')
    delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The length of time, for which to delay a message. Messages that you send to the queue remain invisible to consumers for the duration of the delay period. The maximum allowed delay is 15 minutes. Default: - delay set on the queue. If a delay is not set on the queue, messages are sent immediately (0 seconds).\n')
    message_deduplication_id: typing.Optional[str] = pydantic.Field(None, description="The token used for deduplication of sent messages. Any messages sent with the same deduplication ID are accepted successfully, but aren't delivered during the 5-minute deduplication interval. Default: - None\n")
    message_group_id: typing.Optional[str] = pydantic.Field(None, description='The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Messages in different message groups might be processed out of order. Default: - None\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['message_body', 'queue', 'delay', 'message_deduplication_id', 'message_group_id', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SqsSendMessage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqsSendMessageDefConfig] = pydantic.Field(None)


class SqsSendMessageDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[SqsSendMessageDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[SqsSendMessageDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[SqsSendMessageDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[SqsSendMessageDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[SqsSendMessageDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[SqsSendMessageDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[SqsSendMessageDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[SqsSendMessageDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[SqsSendMessageDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[SqsSendMessageDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[SqsSendMessageDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[SqsSendMessageDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[SqsSendMessageDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[SqsSendMessageDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[SqsSendMessageDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[SqsSendMessageDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[SqsSendMessageDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[SqsSendMessageDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[SqsSendMessageDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class SqsSendMessageDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class SqsSendMessageDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class SqsSendMessageDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class SqsSendMessageDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SqsSendMessageDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class SqsSendMessageDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class SqsSendMessageDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.StepFunctionsInvokeActivity
class StepFunctionsInvokeActivityDef(BaseConstruct):
    activity: typing.Union[models.aws_stepfunctions.ActivityDef] = pydantic.Field(..., description='Step Functions Activity to invoke.\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Parameters pass a collection of key-value pairs, either static values or JSONPath expressions that select from the input. Default: No parameters\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['activity', 'parameters', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.StepFunctionsInvokeActivity'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StepFunctionsInvokeActivityDefConfig] = pydantic.Field(None)


class StepFunctionsInvokeActivityDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[StepFunctionsInvokeActivityDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[StepFunctionsInvokeActivityDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[StepFunctionsInvokeActivityDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[StepFunctionsInvokeActivityDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[StepFunctionsInvokeActivityDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[StepFunctionsInvokeActivityDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[StepFunctionsInvokeActivityDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[StepFunctionsInvokeActivityDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[StepFunctionsInvokeActivityDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[StepFunctionsInvokeActivityDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[StepFunctionsInvokeActivityDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[StepFunctionsInvokeActivityDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[StepFunctionsInvokeActivityDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[StepFunctionsInvokeActivityDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[StepFunctionsInvokeActivityDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[StepFunctionsInvokeActivityDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[StepFunctionsInvokeActivityDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[StepFunctionsInvokeActivityDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[StepFunctionsInvokeActivityDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class StepFunctionsInvokeActivityDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class StepFunctionsInvokeActivityDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class StepFunctionsInvokeActivityDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class StepFunctionsInvokeActivityDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class StepFunctionsInvokeActivityDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class StepFunctionsInvokeActivityDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsInvokeActivityDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.StepFunctionsStartExecution
class StepFunctionsStartExecutionDef(BaseConstruct):
    state_machine: typing.Union[models.aws_stepfunctions.StateMachineDef] = pydantic.Field(..., description='The Step Functions state machine to start the execution on.\n')
    associate_with_parent: typing.Optional[bool] = pydantic.Field(None, description='Pass the execution ID from the context object to the execution input. This allows the Step Functions UI to link child executions from parent executions, making it easier to trace execution flow across state machines. If you set this property to ``true``, the ``input`` property must be an object (provided by ``sfn.TaskInput.fromObject``) or omitted entirely. Default: - false\n')
    input: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description="The JSON input for the execution, same as that of StartExecution. Default: - The state input (JSON path '$')\n")
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the execution, same as that of StartExecution. Default: - None\n')
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['state_machine', 'associate_with_parent', 'input', 'name', 'comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_catch', 'add_prefix', 'add_retry', 'bind_to_graph', 'metric', 'metric_failed', 'metric_heartbeat_timed_out', 'metric_run_time', 'metric_schedule_time', 'metric_scheduled', 'metric_started', 'metric_succeeded', 'metric_time', 'metric_timed_out', 'next']
    _classmethod_names: typing.ClassVar[list[str]] = ['filter_nextables', 'find_reachable_end_states', 'find_reachable_states', 'prefix_states']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.StepFunctionsStartExecution'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StepFunctionsStartExecutionDefConfig] = pydantic.Field(None)


class StepFunctionsStartExecutionDefConfig(pydantic.BaseModel):
    add_catch: typing.Optional[list[StepFunctionsStartExecutionDefAddCatchParams]] = pydantic.Field(None, description='Add a recovery handler for this state.\nWhen a particular error occurs, execution will continue at the error\nhandler instead of failing the state machine execution.')
    add_prefix: typing.Optional[list[StepFunctionsStartExecutionDefAddPrefixParams]] = pydantic.Field(None, description='Add a prefix to the stateId of this state.')
    add_retry: typing.Optional[list[StepFunctionsStartExecutionDefAddRetryParams]] = pydantic.Field(None, description='Add retry configuration for this state.\nThis controls if and how the execution will be retried if a particular\nerror occurs.')
    bind_to_graph: typing.Optional[list[StepFunctionsStartExecutionDefBindToGraphParams]] = pydantic.Field(None, description="Register this state as part of the given graph.\nDon't call this. It will be called automatically when you work\nwith states normally.")
    filter_nextables: typing.Optional[list[StepFunctionsStartExecutionDefFilterNextablesParams]] = pydantic.Field(None, description='Return only the states that allow chaining from an array of states.')
    find_reachable_end_states: typing.Optional[list[StepFunctionsStartExecutionDefFindReachableEndStatesParams]] = pydantic.Field(None, description='Find the set of end states states reachable through transitions from the given start state.')
    find_reachable_states: typing.Optional[list[StepFunctionsStartExecutionDefFindReachableStatesParams]] = pydantic.Field(None, description="Find the set of states reachable through transitions from the given start state.\nThis does not retrieve states from within sub-graphs, such as states within a Parallel state's branch.")
    metric: typing.Optional[list[StepFunctionsStartExecutionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Task.')
    metric_failed: typing.Optional[list[StepFunctionsStartExecutionDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of times this activity fails.')
    metric_heartbeat_timed_out: typing.Optional[list[StepFunctionsStartExecutionDefMetricHeartbeatTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times the heartbeat times out for this activity.')
    metric_run_time: typing.Optional[list[StepFunctionsStartExecutionDefMetricRunTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the Task starts and the time it closes.')
    metric_schedule_time: typing.Optional[list[StepFunctionsStartExecutionDefMetricScheduleTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, for which the activity stays in the schedule state.')
    metric_scheduled: typing.Optional[list[StepFunctionsStartExecutionDefMetricScheduledParams]] = pydantic.Field(None, description='Metric for the number of times this activity is scheduled.')
    metric_started: typing.Optional[list[StepFunctionsStartExecutionDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of times this activity is started.')
    metric_succeeded: typing.Optional[list[StepFunctionsStartExecutionDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of times this activity succeeds.')
    metric_time: typing.Optional[list[StepFunctionsStartExecutionDefMetricTimeParams]] = pydantic.Field(None, description='The interval, in milliseconds, between the time the activity is scheduled and the time it closes.')
    metric_timed_out: typing.Optional[list[StepFunctionsStartExecutionDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of times this activity times out.')
    next: typing.Optional[list[StepFunctionsStartExecutionDefNextParams]] = pydantic.Field(None, description='Continue normal execution with the given state.')
    prefix_states: typing.Optional[list[StepFunctionsStartExecutionDefPrefixStatesParams]] = pydantic.Field(None, description='Add a prefix to the stateId of all States found in a construct tree.')

class StepFunctionsStartExecutionDefAddCatchParams(pydantic.BaseModel):
    handler: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to recover from by going to the given state. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    result_path: typing.Optional[str] = pydantic.Field(None, description='JSONPath expression to indicate where to inject the error data. May also be the special value DISCARD, which will cause the error data to be discarded. Default: $')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefAddPrefixParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class StepFunctionsStartExecutionDefAddRetryParams(pydantic.BaseModel):
    backoff_rate: typing.Union[int, float, None] = pydantic.Field(None, description='Multiplication for how much longer the wait interval gets on every retry. Default: 2\n')
    errors: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Errors to retry. A list of error strings to retry, which can be either predefined errors (for example Errors.NoChoiceMatched) or a self-defined error. Default: All errors\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How many seconds to wait initially before retrying. Default: Duration.seconds(1)\n')
    max_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='How many times to retry this particular error. May be 0 to disable retry for specific errors (in case you have a catch-all retry policy). Default: 3')
    return_config: typing.Optional[list[models.aws_stepfunctions.TaskStateBaseDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefBindToGraphParams(pydantic.BaseModel):
    graph: models.aws_stepfunctions.StateGraphDef = pydantic.Field(..., description='-')
    ...

class StepFunctionsStartExecutionDefFilterNextablesParams(pydantic.BaseModel):
    states: typing.Sequence[models.aws_stepfunctions.StateDef] = pydantic.Field(..., description='-')
    ...

class StepFunctionsStartExecutionDefFindReachableEndStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class StepFunctionsStartExecutionDefFindReachableStatesParams(pydantic.BaseModel):
    start: models.aws_stepfunctions.StateDef = pydantic.Field(..., description='-\n')
    include_error_handlers: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to follow error-handling transitions. Default: false')
    ...

class StepFunctionsStartExecutionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricHeartbeatTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricRunTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricScheduleTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricScheduledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefNextParams(pydantic.BaseModel):
    next: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)
    ...

class StepFunctionsStartExecutionDefPrefixStatesParams(pydantic.BaseModel):
    root: models.AnyResource = pydantic.Field(..., description='-\n')
    prefix: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AlgorithmSpecification
class AlgorithmSpecificationDef(BaseStruct):
    algorithm_name: typing.Optional[str] = pydantic.Field(None, description="Name of the algorithm resource to use for the training job. This must be an algorithm resource that you created or subscribe to on AWS Marketplace. If you specify a value for this parameter, you can't specify a value for TrainingImage. Default: - No algorithm is specified\n")
    metric_definitions: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.MetricDefinitionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='List of metric definition objects. Each object specifies the metric name and regular expressions used to parse algorithm logs. Default: - No metrics\n')
    training_image: typing.Optional[models.aws_stepfunctions_tasks.DockerImageDef] = pydantic.Field(None, description='Registry path of the Docker image that contains the training algorithm. Default: - No Docker image is specified\n')
    training_input_mode: typing.Optional[aws_cdk.aws_stepfunctions_tasks.InputMode] = pydantic.Field(None, description='Input mode that the algorithm supports. Default: \'File\' mode\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTrainingJob(self, "TrainSagemaker",\n        training_job_name=sfn.JsonPath.string_at("$.JobName"),\n        algorithm_specification=tasks.AlgorithmSpecification(\n            algorithm_name="BlazingText",\n            training_input_mode=tasks.InputMode.FILE\n        ),\n        input_data_config=[tasks.Channel(\n            channel_name="train",\n            data_source=tasks.DataSource(\n                s3_data_source=tasks.S3DataSource(\n                    s3_data_type=tasks.S3DataType.S3_PREFIX,\n                    s3_location=tasks.S3Location.from_json_expression("$.S3Bucket")\n                )\n            )\n        )],\n        output_data_config=tasks.OutputDataConfig(\n            s3_output_location=tasks.S3Location.from_bucket(s3.Bucket.from_bucket_name(self, "Bucket", "mybucket"), "myoutputpath")\n        ),\n        resource_config=tasks.ResourceConfig(\n            instance_count=1,\n            instance_type=ec2.InstanceType(sfn.JsonPath.string_at("$.InstanceType")),\n            volume_size=Size.gibibytes(50)\n        ),  # optional: default is 1 instance of EC2 `M4.XLarge` with `10GB` volume\n        stopping_condition=tasks.StoppingCondition(\n            max_runtime=Duration.hours(2)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['algorithm_name', 'metric_definitions', 'training_image', 'training_input_mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AlgorithmSpecification'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ApplicationConfiguration
class ApplicationConfigurationDef(BaseStruct):
    classification: models.aws_stepfunctions_tasks.ClassificationDef = pydantic.Field(..., description='The classification within a configuration. Length Constraints: Minimum length of 1. Maximum length of 1024.\n')
    nested_config: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ApplicationConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of additional configurations to apply within a configuration object. Array Members: Maximum number of 100 items. Default: - No other configurations\n')
    properties: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A set of properties specified within a configuration classification. Map Entries: Maximum number of 100 items. Default: - No properties\n\n:see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # application_configuration_: stepfunctions_tasks.ApplicationConfiguration\n    # classification: stepfunctions_tasks.Classification\n\n    application_configuration = stepfunctions_tasks.ApplicationConfiguration(\n        classification=classification,\n\n        # the properties below are optional\n        nested_config=[stepfunctions_tasks.ApplicationConfiguration(\n            classification=classification,\n\n            # the properties below are optional\n            nested_config=[application_configuration_],\n            properties={\n                "properties_key": "properties"\n            }\n        )],\n        properties={\n            "properties_key": "properties"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['classification', 'nested_config', 'properties']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ApplicationConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryExecutionProps
class AthenaGetQueryExecutionPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    query_execution_id: str = pydantic.Field(..., description='Query that will be retrieved. Example value: ``adfsaf-23trf23-f23rt23``\n\n:exampleMetadata: infused\n\nExample::\n\n    get_query_execution_job = tasks.AthenaGetQueryExecution(self, "Get Query Execution",\n        query_execution_id=sfn.JsonPath.string_at("$.QueryExecutionId")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'query_execution_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryExecutionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryResultsProps
class AthenaGetQueryResultsPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    query_execution_id: str = pydantic.Field(..., description='Query that will be retrieved. Example value: ``adfsaf-23trf23-f23rt23``\n')
    max_results: typing.Union[int, float, None] = pydantic.Field(None, description='Max number of results. Default: 1000\n')
    next_token: typing.Optional[str] = pydantic.Field(None, description='Pagination token. Default: - No next token\n\n:exampleMetadata: infused\n\nExample::\n\n    get_query_results_job = tasks.AthenaGetQueryResults(self, "Get Query Results",\n        query_execution_id=sfn.JsonPath.string_at("$.QueryExecutionId")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'query_execution_id', 'max_results', 'next_token']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaGetQueryResultsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaStartQueryExecutionProps
class AthenaStartQueryExecutionPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    query_string: str = pydantic.Field(..., description='Query that will be started.\n')
    client_request_token: typing.Optional[str] = pydantic.Field(None, description='Unique string string to ensure idempotence. Default: - No client request token\n')
    query_execution_context: typing.Union[models.aws_stepfunctions_tasks.QueryExecutionContextDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Database within which query executes. Default: - No query execution context\n')
    result_configuration: typing.Union[models.aws_stepfunctions_tasks.ResultConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration on how and where to save query. Default: - No result configuration\n')
    work_group: typing.Optional[str] = pydantic.Field(None, description='Configuration on how and where to save query. Default: - No work group\n\n:exampleMetadata: infused\n\nExample::\n\n    start_query_execution_job = tasks.AthenaStartQueryExecution(self, "Start Athena Query",\n        query_string=sfn.JsonPath.string_at("$.queryString"),\n        query_execution_context=tasks.QueryExecutionContext(\n            database_name="mydatabase"\n        ),\n        result_configuration=tasks.ResultConfiguration(\n            encryption_configuration=tasks.EncryptionConfiguration(\n                encryption_option=tasks.EncryptionOption.S3_MANAGED\n            ),\n            output_location=s3.Location(\n                bucket_name="query-results-bucket",\n                object_key="folder"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'query_string', 'client_request_token', 'query_execution_context', 'result_configuration', 'work_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaStartQueryExecutionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AthenaStopQueryExecutionProps
class AthenaStopQueryExecutionPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    query_execution_id: str = pydantic.Field(..., description='Query that will be stopped.\n\n:exampleMetadata: infused\n\nExample::\n\n    stop_query_execution_job = tasks.AthenaStopQueryExecution(self, "Stop Query Execution",\n        query_execution_id=sfn.JsonPath.string_at("$.QueryExecutionId")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'query_execution_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.AthenaStopQueryExecutionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.BatchContainerOverrides
class BatchContainerOverridesDef(BaseStruct):
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command to send to the container that overrides the default command from the Docker image or the job definition. Default: - No command overrides\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the job definition. Default: - No environment overrides\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of physical GPUs to reserve for the container. The number of GPUs reserved for all containers in a job should not exceed the number of available GPUs on the compute resource that the job is launched on. Default: - No GPU reservation\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The instance type to use for a multi-node parallel job. This parameter is not valid for single-node container jobs. Default: - No instance type overrides\n')
    memory: typing.Optional[models.SizeDef] = pydantic.Field(None, description='Memory reserved for the job. Default: - No memory overrides. The memory supplied in the job definition will be used.\n')
    vcpus: typing.Union[int, float, None] = pydantic.Field(None, description='The number of vCPUs to reserve for the container. This value overrides the value set in the job definition. Default: - No vCPUs overrides\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # instance_type: ec2.InstanceType\n    # size: cdk.Size\n\n    batch_container_overrides = stepfunctions_tasks.BatchContainerOverrides(\n        command=["command"],\n        environment={\n            "environment_key": "environment"\n        },\n        gpu_count=123,\n        instance_type=instance_type,\n        memory=size,\n        vcpus=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['command', 'environment', 'gpu_count', 'instance_type', 'memory', 'vcpus']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.BatchContainerOverrides'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.BatchJobDependency
class BatchJobDependencyDef(BaseStruct):
    job_id: typing.Optional[str] = pydantic.Field(None, description='The job ID of the AWS Batch job associated with this dependency. Default: - No jobId\n')
    type: typing.Optional[str] = pydantic.Field(None, description='The type of the job dependency. Default: - No type\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    batch_job_dependency = stepfunctions_tasks.BatchJobDependency(\n        job_id="jobId",\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['job_id', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.BatchJobDependency'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.BatchSubmitJobProps
class BatchSubmitJobPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    job_definition_arn: str = pydantic.Field(..., description='The arn of the job definition used by this job.\n')
    job_name: str = pydantic.Field(..., description='The name of the job. The first character must be alphanumeric, and up to 128 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed.\n')
    job_queue_arn: str = pydantic.Field(..., description='The arn of the job queue into which the job is submitted.\n')
    array_size: typing.Union[int, float, None] = pydantic.Field(None, description='The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. For more information, see Array Jobs in the AWS Batch User Guide. Default: - No array size\n')
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to move a job to the RUNNABLE status. You may specify between 1 and 10 attempts. If the value of attempts is greater than one, the job is retried on failure the same number of attempts as the value. Default: 1\n')
    container_overrides: typing.Union[models.aws_stepfunctions_tasks.BatchContainerOverridesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A list of container overrides in JSON format that specify the name of a container in the specified job definition and the overrides it should receive. Default: - No container overrides\n')
    depends_on: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.BatchJobDependencyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. Default: - No dependencies\n')
    payload: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='The payload to be passed as parameters to the batch job. Default: - No parameters are passed\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_batch_alpha as batch\n    # batch_job_definition: batch.EcsJobDefinition\n    # batch_queue: batch.JobQueue\n\n\n    task = tasks.BatchSubmitJob(self, "Submit Job",\n        job_definition_arn=batch_job_definition.job_definition_arn,\n        job_name="MyJob",\n        job_queue_arn=batch_queue.job_queue_arn\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'job_definition_arn', 'job_name', 'job_queue_arn', 'array_size', 'attempts', 'container_overrides', 'depends_on', 'payload']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.BatchSubmitJobProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CallApiGatewayEndpointBaseProps
class CallApiGatewayEndpointBasePropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    method: aws_cdk.aws_stepfunctions_tasks.HttpMethod = pydantic.Field(..., description='Http method for the API.\n')
    api_path: typing.Optional[str] = pydantic.Field(None, description='Path parameters appended after API endpoint. Default: - No path\n')
    auth_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.AuthType] = pydantic.Field(None, description='Authentication methods. Default: AuthType.NO_AUTH\n')
    headers: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP request information that does not relate to contents of the request. Default: - No headers\n')
    query_parameters: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Query strings attatched to end of request. Default: - No query parameters\n')
    request_body: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP Request body. Default: - No request body\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions as stepfunctions\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # result_selector: Any\n    # task_input: stepfunctions.TaskInput\n    # task_role: stepfunctions.TaskRole\n    # timeout: stepfunctions.Timeout\n\n    call_api_gateway_endpoint_base_props = stepfunctions_tasks.CallApiGatewayEndpointBaseProps(\n        method=stepfunctions_tasks.HttpMethod.GET,\n\n        # the properties below are optional\n        api_path="apiPath",\n        auth_type=stepfunctions_tasks.AuthType.NO_AUTH,\n        comment="comment",\n        credentials=stepfunctions.Credentials(\n            role=task_role\n        ),\n        headers=task_input,\n        heartbeat=cdk.Duration.minutes(30),\n        heartbeat_timeout=timeout,\n        input_path="inputPath",\n        integration_pattern=stepfunctions.IntegrationPattern.REQUEST_RESPONSE,\n        output_path="outputPath",\n        query_parameters=task_input,\n        request_body=task_input,\n        result_path="resultPath",\n        result_selector={\n            "result_selector_key": result_selector\n        },\n        task_timeout=timeout,\n        timeout=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'method', 'api_path', 'auth_type', 'headers', 'query_parameters', 'request_body']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CallApiGatewayEndpointBaseProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointProps
class CallApiGatewayHttpApiEndpointPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    method: aws_cdk.aws_stepfunctions_tasks.HttpMethod = pydantic.Field(..., description='Http method for the API.\n')
    api_path: typing.Optional[str] = pydantic.Field(None, description='Path parameters appended after API endpoint. Default: - No path\n')
    auth_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.AuthType] = pydantic.Field(None, description='Authentication methods. Default: AuthType.NO_AUTH\n')
    headers: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP request information that does not relate to contents of the request. Default: - No headers\n')
    query_parameters: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Query strings attatched to end of request. Default: - No query parameters\n')
    request_body: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP Request body. Default: - No request body\n')
    api_id: str = pydantic.Field(..., description='The Id of the API to call.\n')
    api_stack: models.StackDef = pydantic.Field(..., description='The Stack in which the API is defined.\n')
    stage_name: typing.Optional[str] = pydantic.Field(None, description='Name of the stage where the API is deployed to in API Gateway. Default: \'$default\'\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_apigatewayv2_alpha as apigatewayv2\n\n    http_api = apigatewayv2.HttpApi(self, "MyHttpApi")\n\n    invoke_task = tasks.CallApiGatewayHttpApiEndpoint(self, "Call HTTP API",\n        api_id=http_api.api_id,\n        api_stack=Stack.of(http_api),\n        method=tasks.HttpMethod.GET\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'method', 'api_path', 'auth_type', 'headers', 'query_parameters', 'request_body', 'api_id', 'api_stack', 'stage_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CallApiGatewayHttpApiEndpointPropsDefConfig] = pydantic.Field(None)


class CallApiGatewayHttpApiEndpointPropsDefConfig(pydantic.BaseModel):
    api_stack_config: typing.Optional[models.core.StackDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointProps
class CallApiGatewayRestApiEndpointPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    method: aws_cdk.aws_stepfunctions_tasks.HttpMethod = pydantic.Field(..., description='Http method for the API.\n')
    api_path: typing.Optional[str] = pydantic.Field(None, description='Path parameters appended after API endpoint. Default: - No path\n')
    auth_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.AuthType] = pydantic.Field(None, description='Authentication methods. Default: AuthType.NO_AUTH\n')
    headers: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP request information that does not relate to contents of the request. Default: - No headers\n')
    query_parameters: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Query strings attatched to end of request. Default: - No query parameters\n')
    request_body: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='HTTP Request body. Default: - No request body\n')
    api: typing.Union[models.aws_apigateway.RestApiBaseDef, models.aws_apigateway.LambdaRestApiDef, models.aws_apigateway.RestApiDef, models.aws_apigateway.SpecRestApiDef, models.aws_apigateway.StepFunctionsRestApiDef] = pydantic.Field(..., description='API to call.\n')
    stage_name: str = pydantic.Field(..., description='Name of the stage where the API is deployed to in API Gateway.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_apigateway as apigateway\n    # api: apigateway.RestApi\n\n\n    tasks.CallApiGatewayRestApiEndpoint(self, "Endpoint",\n        api=api,\n        stage_name="Stage",\n        method=tasks.HttpMethod.PUT,\n        integration_pattern=sfn.IntegrationPattern.WAIT_FOR_TASK_TOKEN,\n        headers=sfn.TaskInput.from_object({\n            "TaskToken": sfn.JsonPath.array(sfn.JsonPath.task_token)\n        })\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'method', 'api_path', 'auth_type', 'headers', 'query_parameters', 'request_body', 'api', 'stage_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CallApiGatewayRestApiEndpointPropsDefConfig] = pydantic.Field(None)


class CallApiGatewayRestApiEndpointPropsDefConfig(pydantic.BaseModel):
    api_config: typing.Optional[models._interface_methods.AwsApigatewayIRestApiDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CallAwsServiceProps
class CallAwsServicePropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    action: str = pydantic.Field(..., description='The API action to call. Use camelCase.\n')
    iam_resources: typing.Sequence[str] = pydantic.Field(..., description="The resources for the IAM statement that will be added to the state machine role's policy to allow the state machine to make the API call. By default the action for this IAM statement will be ``service:action``.\n")
    service: str = pydantic.Field(..., description='The AWS service to call.\n')
    additional_iam_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description="Additional IAM statements that will be added to the state machine role's policy. Use in the case where the call requires more than a single statement to be executed, e.g. ``rekognition:detectLabels`` requires also S3 permissions to read the object on which it must act. Default: - no additional statements are added\n")
    iam_action: typing.Optional[str] = pydantic.Field(None, description="The action for the IAM statement that will be added to the state machine role's policy to allow the state machine to make the API call. Use in the case where the IAM action name does not match with the API service/action name, e.g. ``s3:ListBuckets`` requires ``s3:ListAllMyBuckets``. Default: - service:action\n")
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Parameters for the API action call. Use PascalCase for the parameter names. Default: - no parameters\n\n:see: https://docs.aws.amazon.com/step-functions/latest/dg/supported-services-awssdk.html\n:exampleMetadata: infused\n\nExample::\n\n    detect_labels = tasks.CallAwsService(self, "DetectLabels",\n        service="rekognition",\n        action="detectLabels",\n        iam_resources=["*"],\n        additional_iam_statements=[\n            iam.PolicyStatement(\n                actions=["s3:getObject"],\n                resources=["arn:aws:s3:::my-bucket/*"]\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'action', 'iam_resources', 'service', 'additional_iam_statements', 'iam_action', 'parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CallAwsServiceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.Channel
class ChannelDef(BaseStruct):
    channel_name: str = pydantic.Field(..., description='Name of the channel.\n')
    data_source: typing.Union[models.aws_stepfunctions_tasks.DataSourceDef, dict[str, typing.Any]] = pydantic.Field(..., description='Location of the channel data.\n')
    compression_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.CompressionType] = pydantic.Field(None, description='Compression type if training data is compressed. Default: - None\n')
    content_type: typing.Optional[str] = pydantic.Field(None, description='The MIME type of the data. Default: - None\n')
    input_mode: typing.Optional[aws_cdk.aws_stepfunctions_tasks.InputMode] = pydantic.Field(None, description='Input mode to use for the data channel in a training job. Default: - None\n')
    record_wrapper_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.RecordWrapperType] = pydantic.Field(None, description="Specify RecordIO as the value when input data is in raw format but the training algorithm requires the RecordIO format. In this case, Amazon SageMaker wraps each individual S3 object in a RecordIO record. If the input data is already in RecordIO format, you don't need to set this attribute. Default: - None\n")
    shuffle_config: typing.Union[models.aws_stepfunctions_tasks.ShuffleConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Shuffle config option for input data in a channel. Default: - None\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # s3_location: stepfunctions_tasks.S3Location\n\n    channel = stepfunctions_tasks.Channel(\n        channel_name="channelName",\n        data_source=stepfunctions_tasks.DataSource(\n            s3_data_source=stepfunctions_tasks.S3DataSource(\n                s3_location=s3_location,\n\n                # the properties below are optional\n                attribute_names=["attributeNames"],\n                s3_data_distribution_type=stepfunctions_tasks.S3DataDistributionType.FULLY_REPLICATED,\n                s3_data_type=stepfunctions_tasks.S3DataType.MANIFEST_FILE\n            )\n        ),\n\n        # the properties below are optional\n        compression_type=stepfunctions_tasks.CompressionType.NONE,\n        content_type="contentType",\n        input_mode=stepfunctions_tasks.InputMode.PIPE,\n        record_wrapper_type=stepfunctions_tasks.RecordWrapperType.NONE,\n        shuffle_config=stepfunctions_tasks.ShuffleConfig(\n            seed=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['channel_name', 'data_source', 'compression_type', 'content_type', 'input_mode', 'record_wrapper_type', 'shuffle_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.Channel'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CodeBuildStartBuildProps
class CodeBuildStartBuildPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description='CodeBuild project to start.\n')
    environment_variables_override: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A set of environment variables to be used for this build only. Default: - the latest environment variables already defined in the build project.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_codebuild as codebuild\n\n\n    codebuild_project = codebuild.Project(self, "Project",\n        project_name="MyTestProject",\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2",\n            "phases": {\n                "build": {\n                    "commands": ["echo "Hello, CodeBuild!""\n                    ]\n                }\n            }\n        })\n    )\n\n    task = tasks.CodeBuildStartBuild(self, "Task",\n        project=codebuild_project,\n        integration_pattern=sfn.IntegrationPattern.RUN_JOB,\n        environment_variables_override={\n            "ZONE": codebuild.BuildEnvironmentVariable(\n                type=codebuild.BuildEnvironmentVariableType.PLAINTEXT,\n                value=sfn.JsonPath.string_at("$.envVariables.zone")\n            )\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'project', 'environment_variables_override']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CodeBuildStartBuildProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeBuildStartBuildPropsDefConfig] = pydantic.Field(None)


class CodeBuildStartBuildPropsDefConfig(pydantic.BaseModel):
    project_config: typing.Optional[models._interface_methods.AwsCodebuildIProjectDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CommonEcsRunTaskProps
class CommonEcsRunTaskPropsDef(BaseStruct):
    cluster: typing.Union[models.aws_ecs.ClusterDef] = pydantic.Field(..., description='The topic to run the task on.\n')
    task_definition: models.aws_ecs.TaskDefinitionDef = pydantic.Field(..., description='Task Definition used for running tasks in the service. Note: this must be TaskDefinition, and not ITaskDefinition, as it requires properties that are not known for imported task definitions If you want to run a RunTask with an imported task definition, consider using CustomState\n')
    container_overrides: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ContainerOverrideDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Container setting overrides. Key is the name of the container to override, value is the values you want to override. Default: - No overrides\n')
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.ServiceIntegrationPattern] = pydantic.Field(None, description='The service integration pattern indicates different ways to call RunTask in ECS. The valid value for Lambda is FIRE_AND_FORGET, SYNC and WAIT_FOR_TASK_TOKEN. Default: FIRE_AND_FORGET\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_stepfunctions as stepfunctions\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # cluster: ecs.Cluster\n    # container_definition: ecs.ContainerDefinition\n    # task_definition: ecs.TaskDefinition\n\n    common_ecs_run_task_props = stepfunctions_tasks.CommonEcsRunTaskProps(\n        cluster=cluster,\n        task_definition=task_definition,\n\n        # the properties below are optional\n        container_overrides=[stepfunctions_tasks.ContainerOverride(\n            container_definition=container_definition,\n\n            # the properties below are optional\n            command=["command"],\n            cpu=123,\n            environment=[stepfunctions_tasks.TaskEnvironmentVariable(\n                name="name",\n                value="value"\n            )],\n            memory_limit=123,\n            memory_reservation=123\n        )],\n        integration_pattern=stepfunctions.ServiceIntegrationPattern.FIRE_AND_FORGET\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'task_definition', 'container_overrides', 'integration_pattern']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.CommonEcsRunTaskProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CommonEcsRunTaskPropsDefConfig] = pydantic.Field(None)


class CommonEcsRunTaskPropsDefConfig(pydantic.BaseModel):
    cluster_config: typing.Optional[models._interface_methods.AwsEcsIClusterDefConfig] = pydantic.Field(None)
    task_definition_config: typing.Optional[models.aws_ecs.TaskDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ContainerDefinitionConfig
class ContainerDefinitionConfigDef(BaseStruct):
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional parameters to pass to the base task. Default: - No additional parameters passed\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # parameters: Any\n\n    container_definition_config = stepfunctions_tasks.ContainerDefinitionConfig(\n        parameters={\n            "parameters_key": parameters\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ContainerDefinitionConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ContainerDefinitionOptions
class ContainerDefinitionOptionsDef(BaseStruct):
    container_host_name: typing.Optional[str] = pydantic.Field(None, description='This parameter is ignored for models that contain only a PrimaryContainer. When a ContainerDefinition is part of an inference pipeline, the value of the parameter uniquely identifies the container for the purposes of logging and metrics. Default: - None\n')
    environment_variables: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='The environment variables to set in the Docker container. Default: - No variables\n')
    image: typing.Optional[models.aws_stepfunctions_tasks.DockerImageDef] = pydantic.Field(None, description='The Amazon EC2 Container Registry (Amazon ECR) path where inference code is stored. Default: - None\n')
    mode: typing.Optional[aws_cdk.aws_stepfunctions_tasks.Mode] = pydantic.Field(None, description='Defines how many models the container hosts. Default: - Mode.SINGLE_MODEL\n')
    model_package_name: typing.Optional[str] = pydantic.Field(None, description='The name or Amazon Resource Name (ARN) of the model package to use to create the model. Default: - None\n')
    model_s3_location: typing.Optional[models.aws_stepfunctions_tasks.S3LocationDef] = pydantic.Field(None, description='The S3 path where the model artifacts, which result from model training, are stored. This path must point to a single gzip compressed tar archive (.tar.gz suffix). The S3 path is required for Amazon SageMaker built-in algorithms, but not if you use your own algorithms. Default: - None\n\n:see: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ContainerDefinition.html\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateModel(self, "Sagemaker",\n        model_name="MyModel",\n        primary_container=tasks.ContainerDefinition(\n            image=tasks.DockerImage.from_json_expression(sfn.JsonPath.string_at("$.Model.imageName")),\n            mode=tasks.Mode.SINGLE_MODEL,\n            model_s3_location=tasks.S3Location.from_json_expression("$.TrainingJob.ModelArtifacts.S3ModelArtifacts")\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_host_name', 'environment_variables', 'image', 'mode', 'model_package_name', 'model_s3_location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ContainerDefinitionOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ContainerOverride
class ContainerOverrideDef(BaseStruct):
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='Name of the container inside the task definition.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Command to run inside the container. Default: - Default command from the Docker image or the task definition\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The number of cpu units reserved for the container. Default: - The default value from the task definition.\n')
    environment: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.TaskEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Default: - The existing environment variables from the Docker image or the task definition\n')
    memory_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The hard limit (in MiB) of memory to present to the container. Default: - The default value from the task definition.\n')
    memory_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. Default: - The default value from the task definition.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # container_definition: ecs.ContainerDefinition\n\n    container_override = stepfunctions_tasks.ContainerOverride(\n        container_definition=container_definition,\n\n        # the properties below are optional\n        command=["command"],\n        cpu=123,\n        environment=[stepfunctions_tasks.TaskEnvironmentVariable(\n            name="name",\n            value="value"\n        )],\n        memory_limit=123,\n        memory_reservation=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_definition', 'command', 'cpu', 'environment', 'memory_limit', 'memory_reservation']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ContainerOverride'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ContainerOverrideDefConfig] = pydantic.Field(None)


class ContainerOverrideDefConfig(pydantic.BaseModel):
    container_definition_config: typing.Optional[models.aws_ecs.ContainerDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ContainerOverrides
class ContainerOverridesDef(BaseStruct):
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command to send to the container that overrides the default command from the Docker image or the job definition. Default: - No command overrides\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the job definition. Default: - No environment overrides\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of physical GPUs to reserve for the container. The number of GPUs reserved for all containers in a job should not exceed the number of available GPUs on the compute resource that the job is launched on. Default: - No GPU reservation\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The instance type to use for a multi-node parallel job. This parameter is not valid for single-node container jobs. Default: - No instance type overrides\n')
    memory: typing.Union[int, float, None] = pydantic.Field(None, description='The number of MiB of memory reserved for the job. This value overrides the value set in the job definition. Default: - No memory overrides\n')
    vcpus: typing.Union[int, float, None] = pydantic.Field(None, description='The number of vCPUs to reserve for the container. This value overrides the value set in the job definition. Default: - No vCPUs overrides\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # instance_type: ec2.InstanceType\n\n    container_overrides = stepfunctions_tasks.ContainerOverrides(\n        command=["command"],\n        environment={\n            "environment_key": "environment"\n        },\n        gpu_count=123,\n        instance_type=instance_type,\n        memory=123,\n        vcpus=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['command', 'environment', 'gpu_count', 'instance_type', 'memory', 'vcpus']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ContainerOverrides'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DataSource
class DataSourceDef(BaseStruct):
    s3_data_source: typing.Union[models.aws_stepfunctions_tasks.S3DataSourceDef, dict[str, typing.Any]] = pydantic.Field(..., description='S3 location of the data source that is associated with a channel.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTrainingJob(self, "TrainSagemaker",\n        training_job_name=sfn.JsonPath.string_at("$.JobName"),\n        algorithm_specification=tasks.AlgorithmSpecification(\n            algorithm_name="BlazingText",\n            training_input_mode=tasks.InputMode.FILE\n        ),\n        input_data_config=[tasks.Channel(\n            channel_name="train",\n            data_source=tasks.DataSource(\n                s3_data_source=tasks.S3DataSource(\n                    s3_data_type=tasks.S3DataType.S3_PREFIX,\n                    s3_location=tasks.S3Location.from_json_expression("$.S3Bucket")\n                )\n            )\n        )],\n        output_data_config=tasks.OutputDataConfig(\n            s3_output_location=tasks.S3Location.from_bucket(s3.Bucket.from_bucket_name(self, "Bucket", "mybucket"), "myoutputpath")\n        ),\n        resource_config=tasks.ResourceConfig(\n            instance_count=1,\n            instance_type=ec2.InstanceType(sfn.JsonPath.string_at("$.InstanceType")),\n            volume_size=Size.gibibytes(50)\n        ),  # optional: default is 1 instance of EC2 `M4.XLarge` with `10GB` volume\n        stopping_condition=tasks.StoppingCondition(\n            max_runtime=Duration.hours(2)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_data_source']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DataSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DockerImageConfig
class DockerImageConfigDef(BaseStruct):
    image_uri: str = pydantic.Field(..., description='The fully qualified URI of the Docker image.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    docker_image_config = stepfunctions_tasks.DockerImageConfig(\n        image_uri="imageUri"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image_uri']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DockerImageConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoDeleteItemProps
class DynamoDeleteItemPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    key: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='Primary key of the item to retrieve. For the primary key, you must provide all of the attributes. For example, with a simple primary key, you only need to provide a value for the partition key. For a composite primary key, you must provide values for both the partition key and the sort key.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table containing the requested item.\n')
    condition_expression: typing.Optional[str] = pydantic.Field(None, description='A condition that must be satisfied in order for a conditional DeleteItem to succeed. Default: - No condition expression\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attribute names\n')
    expression_attribute_values: typing.Optional[typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef]] = pydantic.Field(None, description='One or more values that can be substituted in an expression. Default: - No expression attribute values\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n')
    return_item_collection_metrics: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoItemCollectionMetrics] = pydantic.Field(None, description='Determines whether item collection metrics are returned. If set to SIZE, the response includes statistics about item collections, if any, that were modified during the operation are returned in the response. If set to NONE (the default), no statistics are returned. Default: DynamoItemCollectionMetrics.NONE\n')
    return_values: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoReturnValues] = pydantic.Field(None, description='Use ReturnValues if you want to get the item attributes as they appeared before they were deleted. Default: DynamoReturnValues.NONE\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_table: dynamodb.Table\n\n    tasks.DynamoDeleteItem(self, "DeleteItem",\n        key={"MessageId": tasks.DynamoAttributeValue.from_string("message-007")},\n        table=my_table,\n        result_path=sfn.JsonPath.DISCARD\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'key', 'table', 'condition_expression', 'expression_attribute_names', 'expression_attribute_values', 'return_consumed_capacity', 'return_item_collection_metrics', 'return_values']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoDeleteItemProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoDeleteItemPropsDefConfig] = pydantic.Field(None)


class DynamoDeleteItemPropsDefConfig(pydantic.BaseModel):
    table_config: typing.Optional[models._interface_methods.AwsDynamodbITableDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoGetItemProps
class DynamoGetItemPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    key: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='Primary key of the item to retrieve. For the primary key, you must provide all of the attributes. For example, with a simple primary key, you only need to provide a value for the partition key. For a composite primary key, you must provide values for both the partition key and the sort key.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table containing the requested item.\n')
    consistent_read: typing.Optional[bool] = pydantic.Field(None, description='Determines the read consistency model: If set to true, then the operation uses strongly consistent reads; otherwise, the operation uses eventually consistent reads. Default: false\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attributes\n')
    projection_expression: typing.Optional[typing.Sequence[models.aws_stepfunctions_tasks.DynamoProjectionExpressionDef]] = pydantic.Field(None, description='An array of DynamoProjectionExpression that identifies one or more attributes to retrieve from the table. These attributes can include scalars, sets, or elements of a JSON document. Default: - No projection expression\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_table: dynamodb.Table\n\n    tasks.DynamoGetItem(self, "Get Item",\n        key={"message_id": tasks.DynamoAttributeValue.from_string("message-007")},\n        table=my_table\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'key', 'table', 'consistent_read', 'expression_attribute_names', 'projection_expression', 'return_consumed_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoGetItemProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoGetItemPropsDefConfig] = pydantic.Field(None)


class DynamoGetItemPropsDefConfig(pydantic.BaseModel):
    table_config: typing.Optional[models._interface_methods.AwsDynamodbITableDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoPutItemProps
class DynamoPutItemPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    item: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='A map of attribute name/value pairs, one for each attribute. Only the primary key attributes are required; you can optionally provide other attribute name-value pairs for the item.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table where the item should be written .\n')
    condition_expression: typing.Optional[str] = pydantic.Field(None, description='A condition that must be satisfied in order for a conditional PutItem operation to succeed. Default: - No condition expression\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attribute names\n')
    expression_attribute_values: typing.Optional[typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef]] = pydantic.Field(None, description='One or more values that can be substituted in an expression. Default: - No expression attribute values\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n')
    return_item_collection_metrics: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoItemCollectionMetrics] = pydantic.Field(None, description='The item collection metrics to returned in the response. Default: DynamoItemCollectionMetrics.NONE\n')
    return_values: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoReturnValues] = pydantic.Field(None, description='Use ReturnValues if you want to get the item attributes as they appeared before they were updated with the PutItem request. Default: DynamoReturnValues.NONE\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_table: dynamodb.Table\n\n    tasks.DynamoPutItem(self, "PutItem",\n        item={\n            "MessageId": tasks.DynamoAttributeValue.from_string("message-007"),\n            "Text": tasks.DynamoAttributeValue.from_string(sfn.JsonPath.string_at("$.bar")),\n            "TotalCount": tasks.DynamoAttributeValue.from_number(10)\n        },\n        table=my_table\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'item', 'table', 'condition_expression', 'expression_attribute_names', 'expression_attribute_values', 'return_consumed_capacity', 'return_item_collection_metrics', 'return_values']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoPutItemProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoPutItemPropsDefConfig] = pydantic.Field(None)


class DynamoPutItemPropsDefConfig(pydantic.BaseModel):
    table_config: typing.Optional[models._interface_methods.AwsDynamodbITableDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoUpdateItemProps
class DynamoUpdateItemPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    key: typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef] = pydantic.Field(..., description='Primary key of the item to retrieve. For the primary key, you must provide all of the attributes. For example, with a simple primary key, you only need to provide a value for the partition key. For a composite primary key, you must provide values for both the partition key and the sort key.\n')
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The name of the table containing the requested item.\n')
    condition_expression: typing.Optional[str] = pydantic.Field(None, description='A condition that must be satisfied in order for a conditional DeleteItem to succeed. Default: - No condition expression\n')
    expression_attribute_names: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='One or more substitution tokens for attribute names in an expression. Default: - No expression attribute names\n')
    expression_attribute_values: typing.Optional[typing.Mapping[str, models.aws_stepfunctions_tasks.DynamoAttributeValueDef]] = pydantic.Field(None, description='One or more values that can be substituted in an expression. Default: - No expression attribute values\n')
    return_consumed_capacity: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity] = pydantic.Field(None, description='Determines the level of detail about provisioned throughput consumption that is returned in the response. Default: DynamoConsumedCapacity.NONE\n')
    return_item_collection_metrics: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoItemCollectionMetrics] = pydantic.Field(None, description='Determines whether item collection metrics are returned. If set to SIZE, the response includes statistics about item collections, if any, that were modified during the operation are returned in the response. If set to NONE (the default), no statistics are returned. Default: DynamoItemCollectionMetrics.NONE\n')
    return_values: typing.Optional[aws_cdk.aws_stepfunctions_tasks.DynamoReturnValues] = pydantic.Field(None, description='Use ReturnValues if you want to get the item attributes as they appeared before they were deleted. Default: DynamoReturnValues.NONE\n')
    update_expression: typing.Optional[str] = pydantic.Field(None, description='An expression that defines one or more attributes to be updated, the action to be performed on them, and new values for them. Default: - No update expression\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_table: dynamodb.Table\n\n    tasks.DynamoUpdateItem(self, "UpdateItem",\n        key={\n            "MessageId": tasks.DynamoAttributeValue.from_string("message-007")\n        },\n        table=my_table,\n        expression_attribute_values={\n            ":val": tasks.DynamoAttributeValue.number_from_string(sfn.JsonPath.string_at("$.Item.TotalCount.N")),\n            ":rand": tasks.DynamoAttributeValue.from_number(20)\n        },\n        update_expression="SET TotalCount = :val + :rand"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'key', 'table', 'condition_expression', 'expression_attribute_names', 'expression_attribute_values', 'return_consumed_capacity', 'return_item_collection_metrics', 'return_values', 'update_expression']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.DynamoUpdateItemProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DynamoUpdateItemPropsDefConfig] = pydantic.Field(None)


class DynamoUpdateItemPropsDefConfig(pydantic.BaseModel):
    table_config: typing.Optional[models._interface_methods.AwsDynamodbITableDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EcsEc2LaunchTargetOptions
class EcsEc2LaunchTargetOptionsDef(BaseStruct):
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='Placement constraints. Default: - None\n')
    placement_strategies: typing.Optional[typing.Sequence[models.aws_ecs.PlacementStrategyDef]] = pydantic.Field(None, description='Placement strategies. Default: - None\n\n:exampleMetadata: infused\n\nExample::\n\n    vpc = ec2.Vpc.from_lookup(self, "Vpc",\n        is_default=True\n    )\n\n    cluster = ecs.Cluster(self, "Ec2Cluster", vpc=vpc)\n    cluster.add_capacity("DefaultAutoScalingGroup",\n        instance_type=ec2.InstanceType("t2.micro"),\n        vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC)\n    )\n\n    task_definition = ecs.TaskDefinition(self, "TD",\n        compatibility=ecs.Compatibility.EC2\n    )\n\n    task_definition.add_container("TheContainer",\n        image=ecs.ContainerImage.from_registry("foo/bar"),\n        memory_limit_mi_b=256\n    )\n\n    run_task = tasks.EcsRunTask(self, "Run",\n        integration_pattern=sfn.IntegrationPattern.RUN_JOB,\n        cluster=cluster,\n        task_definition=task_definition,\n        launch_target=tasks.EcsEc2LaunchTarget(\n            placement_strategies=[\n                ecs.PlacementStrategy.spread_across_instances(),\n                ecs.PlacementStrategy.packed_by_cpu(),\n                ecs.PlacementStrategy.randomly()\n            ],\n            placement_constraints=[\n                ecs.PlacementConstraint.member_of("blieptuut")\n            ]\n        ),\n        propagated_tag_source=ecs.PropagatedTagSource.TASK_DEFINITION\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['placement_constraints', 'placement_strategies']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EcsEc2LaunchTargetOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EcsFargateLaunchTargetOptions
class EcsFargateLaunchTargetOptionsDef(BaseStruct):
    platform_version: aws_cdk.aws_ecs.FargatePlatformVersion = pydantic.Field(..., description='Refers to a specific runtime environment for Fargate task infrastructure. Fargate platform version is a combination of the kernel and container runtime versions.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    ecs_fargate_launch_target_options = stepfunctions_tasks.EcsFargateLaunchTargetOptions(\n        platform_version=ecs.FargatePlatformVersion.LATEST\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['platform_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EcsFargateLaunchTargetOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EcsLaunchTargetConfig
class EcsLaunchTargetConfigDef(BaseStruct):
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional parameters to pass to the base task. Default: - No additional parameters passed\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # parameters: Any\n\n    ecs_launch_target_config = stepfunctions_tasks.EcsLaunchTargetConfig(\n        parameters={\n            "parameters_key": parameters\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EcsLaunchTargetConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EcsRunTaskProps
class EcsRunTaskPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster: typing.Union[models.aws_ecs.ClusterDef] = pydantic.Field(..., description='The ECS cluster to run the task on.\n')
    launch_target: typing.Union[models.aws_stepfunctions_tasks.EcsEc2LaunchTargetDef, models.aws_stepfunctions_tasks.EcsFargateLaunchTargetDef] = pydantic.Field(..., description='An Amazon ECS launch type determines the type of infrastructure on which your tasks and services are hosted.\n')
    task_definition: models.aws_ecs.TaskDefinitionDef = pydantic.Field(..., description='[disable-awslint:ref-via-interface] Task Definition used for running tasks in the service. Note: this must be TaskDefinition, and not ITaskDefinition, as it requires properties that are not known for imported task definitions If you want to run a RunTask with an imported task definition, consider using CustomState\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description='Assign public IP addresses to each task. Default: false\n')
    container_overrides: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ContainerOverrideDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Container setting overrides. Specify the container to use and the overrides to apply. Default: - No overrides\n')
    propagated_tag_source: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition to the task. An error will be received if you specify the SERVICE option when running a task. Default: - No tags are propagated.\n')
    revision_number: typing.Union[int, float, None] = pydantic.Field(None, description="The revision number of ECS task definiton family. Default: - '$latest'\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Existing security groups to use for the tasks. Default: - A new security group is created\n')
    subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Subnets to place the task\'s ENIs. Default: - Public subnets if assignPublicIp is set. Private subnets otherwise.\n\n:exampleMetadata: infused\n\nExample::\n\n    vpc = ec2.Vpc.from_lookup(self, "Vpc",\n        is_default=True\n    )\n\n    cluster = ecs.Cluster(self, "Ec2Cluster", vpc=vpc)\n    cluster.add_capacity("DefaultAutoScalingGroup",\n        instance_type=ec2.InstanceType("t2.micro"),\n        vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC)\n    )\n\n    task_definition = ecs.TaskDefinition(self, "TD",\n        compatibility=ecs.Compatibility.EC2\n    )\n\n    task_definition.add_container("TheContainer",\n        image=ecs.ContainerImage.from_registry("foo/bar"),\n        memory_limit_mi_b=256\n    )\n\n    run_task = tasks.EcsRunTask(self, "Run",\n        integration_pattern=sfn.IntegrationPattern.RUN_JOB,\n        cluster=cluster,\n        task_definition=task_definition,\n        launch_target=tasks.EcsEc2LaunchTarget(\n            placement_strategies=[\n                ecs.PlacementStrategy.spread_across_instances(),\n                ecs.PlacementStrategy.packed_by_cpu(),\n                ecs.PlacementStrategy.randomly()\n            ],\n            placement_constraints=[\n                ecs.PlacementConstraint.member_of("blieptuut")\n            ]\n        ),\n        propagated_tag_source=ecs.PropagatedTagSource.TASK_DEFINITION\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster', 'launch_target', 'task_definition', 'assign_public_ip', 'container_overrides', 'propagated_tag_source', 'revision_number', 'security_groups', 'subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EcsRunTaskProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcsRunTaskPropsDefConfig] = pydantic.Field(None)


class EcsRunTaskPropsDefConfig(pydantic.BaseModel):
    cluster_config: typing.Optional[models._interface_methods.AwsEcsIClusterDefConfig] = pydantic.Field(None)
    task_definition_config: typing.Optional[models.aws_ecs.TaskDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EksCallProps
class EksCallPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster: typing.Union[models.aws_eks.ClusterDef, models.aws_eks.FargateClusterDef] = pydantic.Field(..., description='The EKS cluster.\n')
    http_method: aws_cdk.aws_stepfunctions_tasks.HttpMethods = pydantic.Field(..., description='HTTP method ("GET", "POST", "PUT", ...) part of HTTP request.\n')
    http_path: str = pydantic.Field(..., description='HTTP path of the Kubernetes REST API operation For example: /api/v1/namespaces/default/pods.\n')
    query_parameters: typing.Optional[typing.Mapping[str, typing.Sequence[str]]] = pydantic.Field(None, description='Query Parameters part of HTTP request. Default: - no query parameters\n')
    request_body: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Request body part of HTTP request. Default: - No request body\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_eks as eks\n\n\n    my_eks_cluster = eks.Cluster(self, "my sample cluster",\n        version=eks.KubernetesVersion.V1_18,\n        cluster_name="myEksCluster"\n    )\n\n    tasks.EksCall(self, "Call a EKS Endpoint",\n        cluster=my_eks_cluster,\n        http_method=tasks.HttpMethods.GET,\n        http_path="/api/v1/namespaces/default/pods"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster', 'http_method', 'http_path', 'query_parameters', 'request_body']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EksCallProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EksCallPropsDefConfig] = pydantic.Field(None)


class EksCallPropsDefConfig(pydantic.BaseModel):
    cluster_config: typing.Optional[models._interface_methods.AwsEksIClusterDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrAddStepProps
class EmrAddStepPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster_id: str = pydantic.Field(..., description='The ClusterId to add the Step to.\n')
    jar: str = pydantic.Field(..., description='A path to a JAR file run during the step.\n')
    name: str = pydantic.Field(..., description='The name of the Step.\n')
    action_on_failure: typing.Optional[aws_cdk.aws_stepfunctions_tasks.ActionOnFailure] = pydantic.Field(None, description='The action to take when the cluster step fails. Default: ActionOnFailure.CONTINUE\n')
    main_class: typing.Optional[str] = pydantic.Field(None, description='The name of the main class in the specified Java file. If not specified, the JAR file should specify a Main-Class in its manifest file. Default: - No mainClass\n')
    properties: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of Java properties that are set when the step runs. You can use these properties to pass key value pairs to your main function. Default: - No properties\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrAddStep(self, "Task",\n        cluster_id="ClusterId",\n        name="StepName",\n        jar="Jar",\n        action_on_failure=tasks.ActionOnFailure.CONTINUE\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster_id', 'jar', 'name', 'action_on_failure', 'main_class', 'properties']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrAddStepProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCancelStepProps
class EmrCancelStepPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    step_id: str = pydantic.Field(..., description='The StepId to cancel.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrCancelStep(self, "Task",\n        cluster_id="ClusterId",\n        step_id="StepId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster_id', 'step_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCancelStepProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterProps
class EmrContainersCreateVirtualClusterPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    eks_cluster: models.aws_stepfunctions_tasks.EksClusterInputDef = pydantic.Field(..., description='EKS Cluster or task input that contains the name of the cluster.\n')
    eks_namespace: typing.Optional[str] = pydantic.Field(None, description="The namespace of an EKS cluster. Default: - 'default'\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags assigned to the virtual cluster. Default: {}\n')
    virtual_cluster_name: typing.Optional[str] = pydantic.Field(None, description='Name of the virtual cluster that will be created. Default: - the name of the state machine execution that runs this task and state name\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrContainersCreateVirtualCluster(self, "Create a Virtual Cluster",\n        eks_cluster=tasks.EksClusterInput.from_task_input(sfn.TaskInput.from_text("clusterId")),\n        eks_namespace="specified-namespace"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'eks_cluster', 'eks_namespace', 'tags', 'virtual_cluster_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterProps
class EmrContainersDeleteVirtualClusterPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    virtual_cluster_id: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The ID of the virtual cluster that will be deleted.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrContainersDeleteVirtualCluster(self, "Delete a Virtual Cluster",\n        virtual_cluster_id=sfn.TaskInput.from_json_path_at("$.virtualCluster")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'virtual_cluster_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrContainersStartJobRunProps
class EmrContainersStartJobRunPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    job_driver: typing.Union[models.aws_stepfunctions_tasks.JobDriverDef, dict[str, typing.Any]] = pydantic.Field(..., description='The job driver for the job run.\n')
    release_label: models.aws_stepfunctions_tasks.ReleaseLabelDef = pydantic.Field(..., description='The Amazon EMR release version to use for the job run.\n')
    virtual_cluster: models.aws_stepfunctions_tasks.VirtualClusterInputDef = pydantic.Field(..., description='The ID of the virtual cluster where the job will be run.\n')
    application_config: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ApplicationConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The configurations for the application running in the job run. Maximum of 100 items Default: - No application config\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The execution role for the job run. If ``virtualClusterId`` is from a JSON input path, an execution role must be provided. If an execution role is provided, follow the documentation to update the role trust policy. Default: - Automatically generated only when the provided ``virtualClusterId`` is not an encoded JSON path\n')
    job_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job run. Default: - No job run name\n')
    monitoring: typing.Union[models.aws_stepfunctions_tasks.MonitoringDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for monitoring the job run. Default: - logging enabled and resources automatically generated if ``monitoring.logging`` is set to ``true``\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags assigned to job runs. Default: - None\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrContainersStartJobRun(self, "EMR Containers Start Job Run",\n        virtual_cluster=tasks.VirtualClusterInput.from_virtual_cluster_id("de92jdei2910fwedz"),\n        release_label=tasks.ReleaseLabel.EMR_6_2_0,\n        job_name="EMR-Containers-Job",\n        job_driver=tasks.JobDriver(\n            spark_submit_job_driver=tasks.SparkSubmitJobDriver(\n                entry_point=sfn.TaskInput.from_text("local:///usr/lib/spark/examples/src/main/python/pi.py")\n            )\n        ),\n        application_config=[tasks.ApplicationConfiguration(\n            classification=tasks.Classification.SPARK_DEFAULTS,\n            properties={\n                "spark.executor.instances": "1",\n                "spark.executor.memory": "512M"\n            }\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'job_driver', 'release_label', 'virtual_cluster', 'application_config', 'execution_role', 'job_name', 'monitoring', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrContainersStartJobRunProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ApplicationConfigProperty
class EmrCreateCluster_ApplicationConfigPropertyDef(BaseStruct):
    name: str = pydantic.Field(..., description='The name of the application.\n')
    additional_info: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='This option is for advanced users only. This is meta information about third-party applications that third-party vendors use for testing purposes. Default: No additionalInfo\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version of the application. Default: No version\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_Application.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    application_config_property = stepfunctions_tasks.EmrCreateCluster.ApplicationConfigProperty(\n        name="name",\n\n        # the properties below are optional\n        additional_info={\n            "additional_info_key": "additionalInfo"\n        },\n        args=["args"],\n        version="version"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'additional_info', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ApplicationConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.AutoScalingPolicyProperty
class EmrCreateCluster_AutoScalingPolicyPropertyDef(BaseStruct):
    constraints: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingConstraintsPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The upper and lower EC2 instance limits for an automatic scaling policy. Automatic scaling activity will not cause an instance group to grow above or below these limits.\n')
    rules: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingRulePropertyDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The scale-in and scale-out rules that comprise the automatic scaling policy.\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_AutoScalingPolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    auto_scaling_policy_property = stepfunctions_tasks.EmrCreateCluster.AutoScalingPolicyProperty(\n        constraints=stepfunctions_tasks.EmrCreateCluster.ScalingConstraintsProperty(\n            max_capacity=123,\n            min_capacity=123\n        ),\n        rules=[stepfunctions_tasks.EmrCreateCluster.ScalingRuleProperty(\n            action=stepfunctions_tasks.EmrCreateCluster.ScalingActionProperty(\n                simple_scaling_policy_configuration=stepfunctions_tasks.EmrCreateCluster.SimpleScalingPolicyConfigurationProperty(\n                    scaling_adjustment=123,\n\n                    # the properties below are optional\n                    adjustment_type=stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType.CHANGE_IN_CAPACITY,\n                    cool_down=123\n                ),\n\n                # the properties below are optional\n                market=stepfunctions_tasks.EmrCreateCluster.InstanceMarket.ON_DEMAND\n            ),\n            name="name",\n            trigger=stepfunctions_tasks.EmrCreateCluster.ScalingTriggerProperty(\n                cloud_watch_alarm_definition=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmDefinitionProperty(\n                    comparison_operator=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator.GREATER_THAN_OR_EQUAL,\n                    metric_name="metricName",\n                    period=cdk.Duration.minutes(30),\n\n                    # the properties below are optional\n                    dimensions=[stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty(\n                        key="key",\n                        value="value"\n                    )],\n                    evaluation_periods=123,\n                    namespace="namespace",\n                    statistic=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic.SAMPLE_COUNT,\n                    threshold=123,\n                    unit=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit.NONE\n                )\n            ),\n\n            # the properties below are optional\n            description="description"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['constraints', 'rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.AutoScalingPolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.BootstrapActionConfigProperty
class EmrCreateCluster_BootstrapActionConfigPropertyDef(BaseStruct):
    name: str = pydantic.Field(..., description='The name of the bootstrap action.\n')
    script_bootstrap_action: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScriptBootstrapActionConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The script run by the bootstrap action.\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_BootstrapActionConfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    bootstrap_action_config_property = stepfunctions_tasks.EmrCreateCluster.BootstrapActionConfigProperty(\n        name="name",\n        script_bootstrap_action=stepfunctions_tasks.EmrCreateCluster.ScriptBootstrapActionConfigProperty(\n            path="path",\n\n            # the properties below are optional\n            args=["args"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'script_bootstrap_action']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.BootstrapActionConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmDefinitionProperty
class EmrCreateCluster_CloudWatchAlarmDefinitionPropertyDef(BaseStruct):
    comparison_operator: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator = pydantic.Field(..., description='Determines how the metric specified by MetricName is compared to the value specified by Threshold.\n')
    metric_name: str = pydantic.Field(..., description='The name of the CloudWatch metric that is watched to determine an alarm condition.\n')
    period: models.DurationDef = pydantic.Field(..., description='The period, in seconds, over which the statistic is applied. EMR CloudWatch metrics are emitted every five minutes (300 seconds), so if an EMR CloudWatch metric is specified, specify 300.\n')
    dimensions: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_MetricDimensionPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A CloudWatch metric dimension. Default: - No dimensions\n')
    evaluation_periods: typing.Union[int, float, None] = pydantic.Field(None, description='The number of periods, in five-minute increments, during which the alarm condition must exist before the alarm triggers automatic scaling activity. Default: 1\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description="The namespace for the CloudWatch metric. Default: 'AWS/ElasticMapReduce'\n")
    statistic: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic] = pydantic.Field(None, description='The statistic to apply to the metric associated with the alarm. Default: CloudWatchAlarmStatistic.AVERAGE\n')
    threshold: typing.Union[int, float, None] = pydantic.Field(None, description='The value against which the specified statistic is compared. Default: - None\n')
    unit: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit] = pydantic.Field(None, description='The unit of measure associated with the CloudWatch metric being watched. The value specified for Unit must correspond to the units specified in the CloudWatch metric. Default: CloudWatchAlarmUnit.NONE\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_CloudWatchAlarmDefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    cloud_watch_alarm_definition_property = stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmDefinitionProperty(\n        comparison_operator=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator.GREATER_THAN_OR_EQUAL,\n        metric_name="metricName",\n        period=cdk.Duration.minutes(30),\n\n        # the properties below are optional\n        dimensions=[stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty(\n            key="key",\n            value="value"\n        )],\n        evaluation_periods=123,\n        namespace="namespace",\n        statistic=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic.SAMPLE_COUNT,\n        threshold=123,\n        unit=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit.NONE\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comparison_operator', 'metric_name', 'period', 'dimensions', 'evaluation_periods', 'namespace', 'statistic', 'threshold', 'unit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmDefinitionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrCreateCluster_CloudWatchAlarmDefinitionPropertyDefConfig] = pydantic.Field(None)


class EmrCreateCluster_CloudWatchAlarmDefinitionPropertyDefConfig(pydantic.BaseModel):
    period_config: typing.Optional[models.core.DurationDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty
class EmrCreateCluster_ConfigurationPropertyDef(BaseStruct):
    classification: typing.Optional[str] = pydantic.Field(None, description='The classification within a configuration. Default: No classification\n')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of additional configurations to apply within a configuration object. Default: No configurations\n')
    properties: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A set of properties specified within a configuration classification. Default: No properties\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_Configuration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # configuration_property_: stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty\n\n    configuration_property = stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty(\n        classification="classification",\n        configurations=[stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty(\n            classification="classification",\n            configurations=[configuration_property_],\n            properties={\n                "properties_key": "properties"\n            }\n        )],\n        properties={\n            "properties_key": "properties"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['classification', 'configurations', 'properties']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceConfigProperty
class EmrCreateCluster_EbsBlockDeviceConfigPropertyDef(BaseStruct):
    volume_specification: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_VolumeSpecificationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='EBS volume specifications such as volume type, IOPS, and size (GiB) that will be requested for the EBS volume attached to an EC2 instance in the cluster.\n')
    volumes_per_instance: typing.Union[int, float, None] = pydantic.Field(None, description='Number of EBS volumes with a specific volume configuration that will be associated with every instance in the instance group. Default: EMR selected default\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_EbsBlockDeviceConfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # size: cdk.Size\n\n    ebs_block_device_config_property = stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceConfigProperty(\n        volume_specification=stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty(\n            volume_size=size,\n            volume_type=stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType.GP2,\n\n            # the properties below are optional\n            iops=123\n        ),\n\n        # the properties below are optional\n        volumes_per_instance=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['volume_specification', 'volumes_per_instance']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EbsConfigurationProperty
class EmrCreateCluster_EbsConfigurationPropertyDef(BaseStruct):
    ebs_block_device_configs: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_EbsBlockDeviceConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of Amazon EBS volume specifications attached to a cluster instance. Default: - None\n')
    ebs_optimized: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether an Amazon EBS volume is EBS-optimized. Default: - EMR selected default\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_EbsConfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # size: cdk.Size\n\n    ebs_configuration_property = stepfunctions_tasks.EmrCreateCluster.EbsConfigurationProperty(\n        ebs_block_device_configs=[stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceConfigProperty(\n            volume_specification=stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty(\n                volume_size=size,\n                volume_type=stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType.GP2,\n\n                # the properties below are optional\n                iops=123\n            ),\n\n            # the properties below are optional\n            volumes_per_instance=123\n        )],\n        ebs_optimized=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['ebs_block_device_configs', 'ebs_optimized']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EbsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceFleetConfigProperty
class EmrCreateCluster_InstanceFleetConfigPropertyDef(BaseStruct):
    instance_fleet_type: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceRoleType = pydantic.Field(..., description='The node type that the instance fleet hosts. Valid values are MASTER,CORE,and TASK.\n')
    instance_type_configs: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceTypeConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The instance type configurations that define the EC2 instances in the instance fleet. Default: No instanceTpeConfigs\n')
    launch_specifications: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceFleetProvisioningSpecificationsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The launch specification for the instance fleet. Default: No launchSpecifications\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The friendly name of the instance fleet. Default: No name\n')
    target_on_demand_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The target capacity of On-Demand units for the instance fleet, which determines how many On-Demand instances to provision. Default: No targetOnDemandCapacity\n')
    target_spot_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The target capacity of Spot units for the instance fleet, which determines how many Spot instances to provision. Default: No targetSpotCapacity\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_InstanceFleetConfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # configuration_property_: stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty\n    # size: cdk.Size\n\n    instance_fleet_config_property = stepfunctions_tasks.EmrCreateCluster.InstanceFleetConfigProperty(\n        instance_fleet_type=stepfunctions_tasks.EmrCreateCluster.InstanceRoleType.MASTER,\n\n        # the properties below are optional\n        instance_type_configs=[stepfunctions_tasks.EmrCreateCluster.InstanceTypeConfigProperty(\n            instance_type="instanceType",\n\n            # the properties below are optional\n            bid_price="bidPrice",\n            bid_price_as_percentage_of_on_demand_price=123,\n            configurations=[stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty(\n                classification="classification",\n                configurations=[configuration_property_],\n                properties={\n                    "properties_key": "properties"\n                }\n            )],\n            ebs_configuration=stepfunctions_tasks.EmrCreateCluster.EbsConfigurationProperty(\n                ebs_block_device_configs=[stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceConfigProperty(\n                    volume_specification=stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty(\n                        volume_size=size,\n                        volume_type=stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType.GP2,\n\n                        # the properties below are optional\n                        iops=123\n                    ),\n\n                    # the properties below are optional\n                    volumes_per_instance=123\n                )],\n                ebs_optimized=False\n            ),\n            weighted_capacity=123\n        )],\n        launch_specifications=stepfunctions_tasks.EmrCreateCluster.InstanceFleetProvisioningSpecificationsProperty(\n            spot_specification=stepfunctions_tasks.EmrCreateCluster.SpotProvisioningSpecificationProperty(\n                timeout_action=stepfunctions_tasks.EmrCreateCluster.SpotTimeoutAction.SWITCH_TO_ON_DEMAND,\n                timeout_duration_minutes=123,\n\n                # the properties below are optional\n                allocation_strategy=stepfunctions_tasks.EmrCreateCluster.SpotAllocationStrategy.CAPACITY_OPTIMIZED,\n                block_duration_minutes=123\n            )\n        ),\n        name="name",\n        target_on_demand_capacity=123,\n        target_spot_capacity=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_fleet_type', 'instance_type_configs', 'launch_specifications', 'name', 'target_on_demand_capacity', 'target_spot_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceFleetConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceFleetProvisioningSpecificationsProperty
class EmrCreateCluster_InstanceFleetProvisioningSpecificationsPropertyDef(BaseStruct):
    spot_specification: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_SpotProvisioningSpecificationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The launch specification for Spot instances in the fleet, which determines the defined duration and provisioning timeout behavior.\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_InstanceFleetProvisioningSpecifications.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    instance_fleet_provisioning_specifications_property = stepfunctions_tasks.EmrCreateCluster.InstanceFleetProvisioningSpecificationsProperty(\n        spot_specification=stepfunctions_tasks.EmrCreateCluster.SpotProvisioningSpecificationProperty(\n            timeout_action=stepfunctions_tasks.EmrCreateCluster.SpotTimeoutAction.SWITCH_TO_ON_DEMAND,\n            timeout_duration_minutes=123,\n\n            # the properties below are optional\n            allocation_strategy=stepfunctions_tasks.EmrCreateCluster.SpotAllocationStrategy.CAPACITY_OPTIMIZED,\n            block_duration_minutes=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['spot_specification']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceFleetProvisioningSpecificationsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceGroupConfigProperty
class EmrCreateCluster_InstanceGroupConfigPropertyDef(BaseStruct):
    instance_count: typing.Union[int, float] = pydantic.Field(..., description='Target number of instances for the instance group.\n')
    instance_role: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceRoleType = pydantic.Field(..., description='The role of the instance group in the cluster.\n')
    instance_type: str = pydantic.Field(..., description='The EC2 instance type for all instances in the instance group.\n')
    auto_scaling_policy: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_AutoScalingPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An automatic scaling policy for a core instance group or task instance group in an Amazon EMR cluster. Default: - None\n')
    bid_price: typing.Optional[str] = pydantic.Field(None, description='The bid price for each EC2 Spot instance type as defined by InstanceType. Expressed in USD. Default: - None\n')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of configurations supplied for an EMR cluster instance group. Default: - None\n')
    ebs_configuration: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_EbsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='EBS configurations that will be attached to each EC2 instance in the instance group. Default: - None\n')
    market: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceMarket] = pydantic.Field(None, description='Market type of the EC2 instances used to create a cluster node. Default: - EMR selected default\n')
    name: typing.Optional[str] = pydantic.Field(None, description='Friendly name given to the instance group. Default: - None\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_InstanceGroupConfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # configuration_property_: stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty\n    # size: cdk.Size\n\n    instance_group_config_property = stepfunctions_tasks.EmrCreateCluster.InstanceGroupConfigProperty(\n        instance_count=123,\n        instance_role=stepfunctions_tasks.EmrCreateCluster.InstanceRoleType.MASTER,\n        instance_type="instanceType",\n\n        # the properties below are optional\n        auto_scaling_policy=stepfunctions_tasks.EmrCreateCluster.AutoScalingPolicyProperty(\n            constraints=stepfunctions_tasks.EmrCreateCluster.ScalingConstraintsProperty(\n                max_capacity=123,\n                min_capacity=123\n            ),\n            rules=[stepfunctions_tasks.EmrCreateCluster.ScalingRuleProperty(\n                action=stepfunctions_tasks.EmrCreateCluster.ScalingActionProperty(\n                    simple_scaling_policy_configuration=stepfunctions_tasks.EmrCreateCluster.SimpleScalingPolicyConfigurationProperty(\n                        scaling_adjustment=123,\n\n                        # the properties below are optional\n                        adjustment_type=stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType.CHANGE_IN_CAPACITY,\n                        cool_down=123\n                    ),\n\n                    # the properties below are optional\n                    market=stepfunctions_tasks.EmrCreateCluster.InstanceMarket.ON_DEMAND\n                ),\n                name="name",\n                trigger=stepfunctions_tasks.EmrCreateCluster.ScalingTriggerProperty(\n                    cloud_watch_alarm_definition=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmDefinitionProperty(\n                        comparison_operator=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator.GREATER_THAN_OR_EQUAL,\n                        metric_name="metricName",\n                        period=cdk.Duration.minutes(30),\n\n                        # the properties below are optional\n                        dimensions=[stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty(\n                            key="key",\n                            value="value"\n                        )],\n                        evaluation_periods=123,\n                        namespace="namespace",\n                        statistic=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic.SAMPLE_COUNT,\n                        threshold=123,\n                        unit=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit.NONE\n                    )\n                ),\n\n                # the properties below are optional\n                description="description"\n            )]\n        ),\n        bid_price="bidPrice",\n        configurations=[stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty(\n            classification="classification",\n            configurations=[configuration_property_],\n            properties={\n                "properties_key": "properties"\n            }\n        )],\n        ebs_configuration=stepfunctions_tasks.EmrCreateCluster.EbsConfigurationProperty(\n            ebs_block_device_configs=[stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceConfigProperty(\n                volume_specification=stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty(\n                    volume_size=size,\n                    volume_type=stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType.GP2,\n\n                    # the properties below are optional\n                    iops=123\n                ),\n\n                # the properties below are optional\n                volumes_per_instance=123\n            )],\n            ebs_optimized=False\n        ),\n        market=stepfunctions_tasks.EmrCreateCluster.InstanceMarket.ON_DEMAND,\n        name="name"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_count', 'instance_role', 'instance_type', 'auto_scaling_policy', 'bid_price', 'configurations', 'ebs_configuration', 'market', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceGroupConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstancesConfigProperty
class EmrCreateCluster_InstancesConfigPropertyDef(BaseStruct):
    additional_master_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of additional Amazon EC2 security group IDs for the master node. Default: - None\n')
    additional_slave_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of additional Amazon EC2 security group IDs for the core and task nodes. Default: - None\n')
    ec2_key_name: typing.Optional[str] = pydantic.Field(None, description='The name of the EC2 key pair that can be used to ssh to the master node as the user called "hadoop.". Default: - None\n')
    ec2_subnet_id: typing.Optional[str] = pydantic.Field(None, description='Applies to clusters that use the uniform instance group configuration. To launch the cluster in Amazon Virtual Private Cloud (Amazon VPC), set this parameter to the identifier of the Amazon VPC subnet where you want the cluster to launch. Default: EMR selected default\n')
    ec2_subnet_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Applies to clusters that use the instance fleet configuration. When multiple EC2 subnet IDs are specified, Amazon EMR evaluates them and launches instances in the optimal subnet. Default: EMR selected default\n')
    emr_managed_master_security_group: typing.Optional[str] = pydantic.Field(None, description='The identifier of the Amazon EC2 security group for the master node. Default: - None\n')
    emr_managed_slave_security_group: typing.Optional[str] = pydantic.Field(None, description='The identifier of the Amazon EC2 security group for the core and task nodes. Default: - None\n')
    hadoop_version: typing.Optional[str] = pydantic.Field(None, description='Applies only to Amazon EMR release versions earlier than 4.0. The Hadoop version for the cluster. Default: - 0.18 if the AmiVersion parameter is not set. If AmiVersion is set, the version of Hadoop for that AMI version is used.\n')
    instance_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of EC2 instances in the cluster. Default: 0\n')
    instance_fleets: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceFleetConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Describes the EC2 instances and instance configurations for clusters that use the instance fleet configuration. The instance fleet configuration is available only in Amazon EMR versions 4.8.0 and later, excluding 5.0.x versions. Default: - None\n')
    instance_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstanceGroupConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configuration for the instance groups in a cluster. Default: - None\n')
    master_instance_type: typing.Optional[str] = pydantic.Field(None, description='The EC2 instance type of the master node. Default: - None\n')
    placement: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_PlacementTypePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Availability Zone in which the cluster runs. Default: - EMR selected default\n')
    service_access_security_group: typing.Optional[str] = pydantic.Field(None, description='The identifier of the Amazon EC2 security group for the Amazon EMR service to access clusters in VPC private subnets. Default: - None\n')
    slave_instance_type: typing.Optional[str] = pydantic.Field(None, description='The EC2 instance type of the core and task nodes. Default: - None\n')
    termination_protected: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to lock the cluster to prevent the Amazon EC2 instances from being terminated by API call, user intervention, or in the event of a job-flow error. Default: false\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_JobFlowInstancesConfig.html\n:exampleMetadata: infused\n\nExample::\n\n    cluster_role = iam.Role(self, "ClusterRole",\n        assumed_by=iam.ServicePrincipal("ec2.amazonaws.com")\n    )\n\n    service_role = iam.Role(self, "ServiceRole",\n        assumed_by=iam.ServicePrincipal("elasticmapreduce.amazonaws.com")\n    )\n\n    auto_scaling_role = iam.Role(self, "AutoScalingRole",\n        assumed_by=iam.ServicePrincipal("elasticmapreduce.amazonaws.com")\n    )\n\n    auto_scaling_role.assume_role_policy.add_statements(\n        iam.PolicyStatement(\n            effect=iam.Effect.ALLOW,\n            principals=[\n                iam.ServicePrincipal("application-autoscaling.amazonaws.com")\n            ],\n            actions=["sts:AssumeRole"\n            ]\n        ))\n\n    tasks.EmrCreateCluster(self, "Create Cluster",\n        instances=tasks.EmrCreateCluster.InstancesConfigProperty(),\n        cluster_role=cluster_role,\n        name=sfn.TaskInput.from_json_path_at("$.ClusterName").value,\n        service_role=service_role,\n        auto_scaling_role=auto_scaling_role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['additional_master_security_groups', 'additional_slave_security_groups', 'ec2_key_name', 'ec2_subnet_id', 'ec2_subnet_ids', 'emr_managed_master_security_group', 'emr_managed_slave_security_group', 'hadoop_version', 'instance_count', 'instance_fleets', 'instance_groups', 'master_instance_type', 'placement', 'service_access_security_group', 'slave_instance_type', 'termination_protected']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstancesConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceTypeConfigProperty
class EmrCreateCluster_InstanceTypeConfigPropertyDef(BaseStruct):
    instance_type: str = pydantic.Field(..., description='An EC2 instance type.\n')
    bid_price: typing.Optional[str] = pydantic.Field(None, description='The bid price for each EC2 Spot instance type as defined by InstanceType. Expressed in USD. Default: - None\n')
    bid_price_as_percentage_of_on_demand_price: typing.Union[int, float, None] = pydantic.Field(None, description='The bid price, as a percentage of On-Demand price. Default: - None\n')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A configuration classification that applies when provisioning cluster instances, which can include configurations for applications and software that run on the cluster. Default: - None\n')
    ebs_configuration: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_EbsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration of Amazon Elastic Block Storage (EBS) attached to each instance as defined by InstanceType. Default: - None\n')
    weighted_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The number of units that a provisioned instance of this type provides toward fulfilling the target capacities defined in the InstanceFleetConfig. Default: - None\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_InstanceTypeConfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # configuration_property_: stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty\n    # size: cdk.Size\n\n    instance_type_config_property = stepfunctions_tasks.EmrCreateCluster.InstanceTypeConfigProperty(\n        instance_type="instanceType",\n\n        # the properties below are optional\n        bid_price="bidPrice",\n        bid_price_as_percentage_of_on_demand_price=123,\n        configurations=[stepfunctions_tasks.EmrCreateCluster.ConfigurationProperty(\n            classification="classification",\n            configurations=[configuration_property_],\n            properties={\n                "properties_key": "properties"\n            }\n        )],\n        ebs_configuration=stepfunctions_tasks.EmrCreateCluster.EbsConfigurationProperty(\n            ebs_block_device_configs=[stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceConfigProperty(\n                volume_specification=stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty(\n                    volume_size=size,\n                    volume_type=stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType.GP2,\n\n                    # the properties below are optional\n                    iops=123\n                ),\n\n                # the properties below are optional\n                volumes_per_instance=123\n            )],\n            ebs_optimized=False\n        ),\n        weighted_capacity=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_type', 'bid_price', 'bid_price_as_percentage_of_on_demand_price', 'configurations', 'ebs_configuration', 'weighted_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceTypeConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.KerberosAttributesProperty
class EmrCreateCluster_KerberosAttributesPropertyDef(BaseStruct):
    realm: str = pydantic.Field(..., description='The name of the Kerberos realm to which all nodes in a cluster belong. For example, EC2.INTERNAL.\n')
    ad_domain_join_password: typing.Optional[str] = pydantic.Field(None, description='The Active Directory password for ADDomainJoinUser. Default: No adDomainJoinPassword\n')
    ad_domain_join_user: typing.Optional[str] = pydantic.Field(None, description='Required only when establishing a cross-realm trust with an Active Directory domain. A user with sufficient privileges to join resources to the domain. Default: No adDomainJoinUser\n')
    cross_realm_trust_principal_password: typing.Optional[str] = pydantic.Field(None, description='Required only when establishing a cross-realm trust with a KDC in a different realm. The cross-realm principal password, which must be identical across realms. Default: No crossRealmTrustPrincipalPassword\n')
    kdc_admin_password: typing.Optional[str] = pydantic.Field(None, description='The password used within the cluster for the kadmin service on the cluster-dedicated KDC, which maintains Kerberos principals, password policies, and keytabs for the cluster. Default: No kdcAdminPassword\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_KerberosAttributes.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    kerberos_attributes_property = stepfunctions_tasks.EmrCreateCluster.KerberosAttributesProperty(\n        realm="realm",\n\n        # the properties below are optional\n        ad_domain_join_password="adDomainJoinPassword",\n        ad_domain_join_user="adDomainJoinUser",\n        cross_realm_trust_principal_password="crossRealmTrustPrincipalPassword",\n        kdc_admin_password="kdcAdminPassword"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['realm', 'ad_domain_join_password', 'ad_domain_join_user', 'cross_realm_trust_principal_password', 'kdc_admin_password']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.KerberosAttributesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty
class EmrCreateCluster_MetricDimensionPropertyDef(BaseStruct):
    key: str = pydantic.Field(..., description='The dimension name.\n')
    value: str = pydantic.Field(..., description='The dimension value.\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_MetricDimension.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    metric_dimension_property = stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty(\n        key="key",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.PlacementTypeProperty
class EmrCreateCluster_PlacementTypePropertyDef(BaseStruct):
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The Amazon EC2 Availability Zone for the cluster. AvailabilityZone is used for uniform instance groups, while AvailabilityZones (plural) is used for instance fleets. Default: - EMR selected default\n')
    availability_zones: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='When multiple Availability Zones are specified, Amazon EMR evaluates them and launches instances in the optimal Availability Zone. AvailabilityZones is used for instance fleets, while AvailabilityZone (singular) is used for uniform instance groups. Default: - EMR selected default\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_PlacementType.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    placement_type_property = stepfunctions_tasks.EmrCreateCluster.PlacementTypeProperty(\n        availability_zone="availabilityZone",\n        availability_zones=["availabilityZones"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['availability_zone', 'availability_zones']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.PlacementTypeProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingActionProperty
class EmrCreateCluster_ScalingActionPropertyDef(BaseStruct):
    simple_scaling_policy_configuration: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_SimpleScalingPolicyConfigurationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The type of adjustment the automatic scaling activity makes when triggered, and the periodicity of the adjustment.\n')
    market: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceMarket] = pydantic.Field(None, description='Not available for instance groups. Instance groups use the market type specified for the group. Default: - EMR selected default\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_ScalingAction.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    scaling_action_property = stepfunctions_tasks.EmrCreateCluster.ScalingActionProperty(\n        simple_scaling_policy_configuration=stepfunctions_tasks.EmrCreateCluster.SimpleScalingPolicyConfigurationProperty(\n            scaling_adjustment=123,\n\n            # the properties below are optional\n            adjustment_type=stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType.CHANGE_IN_CAPACITY,\n            cool_down=123\n        ),\n\n        # the properties below are optional\n        market=stepfunctions_tasks.EmrCreateCluster.InstanceMarket.ON_DEMAND\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['simple_scaling_policy_configuration', 'market']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingActionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingConstraintsProperty
class EmrCreateCluster_ScalingConstraintsPropertyDef(BaseStruct):
    max_capacity: typing.Union[int, float] = pydantic.Field(..., description='The upper boundary of EC2 instances in an instance group beyond which scaling activities are not allowed to grow. Scale-out activities will not add instances beyond this boundary.\n')
    min_capacity: typing.Union[int, float] = pydantic.Field(..., description='The lower boundary of EC2 instances in an instance group below which scaling activities are not allowed to shrink. Scale-in activities will not terminate instances below this boundary.\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_ScalingConstraints.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    scaling_constraints_property = stepfunctions_tasks.EmrCreateCluster.ScalingConstraintsProperty(\n        max_capacity=123,\n        min_capacity=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_capacity', 'min_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingConstraintsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingRuleProperty
class EmrCreateCluster_ScalingRulePropertyDef(BaseStruct):
    action: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingActionPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The conditions that trigger an automatic scaling activity.\n')
    name: str = pydantic.Field(..., description='The name used to identify an automatic scaling rule. Rule names must be unique within a scaling policy.\n')
    trigger: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ScalingTriggerPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The CloudWatch alarm definition that determines when automatic scaling activity is triggered.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A friendly, more verbose description of the automatic scaling rule. Default: - None\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_ScalingRule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    scaling_rule_property = stepfunctions_tasks.EmrCreateCluster.ScalingRuleProperty(\n        action=stepfunctions_tasks.EmrCreateCluster.ScalingActionProperty(\n            simple_scaling_policy_configuration=stepfunctions_tasks.EmrCreateCluster.SimpleScalingPolicyConfigurationProperty(\n                scaling_adjustment=123,\n\n                # the properties below are optional\n                adjustment_type=stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType.CHANGE_IN_CAPACITY,\n                cool_down=123\n            ),\n\n            # the properties below are optional\n            market=stepfunctions_tasks.EmrCreateCluster.InstanceMarket.ON_DEMAND\n        ),\n        name="name",\n        trigger=stepfunctions_tasks.EmrCreateCluster.ScalingTriggerProperty(\n            cloud_watch_alarm_definition=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmDefinitionProperty(\n                comparison_operator=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator.GREATER_THAN_OR_EQUAL,\n                metric_name="metricName",\n                period=cdk.Duration.minutes(30),\n\n                # the properties below are optional\n                dimensions=[stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty(\n                    key="key",\n                    value="value"\n                )],\n                evaluation_periods=123,\n                namespace="namespace",\n                statistic=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic.SAMPLE_COUNT,\n                threshold=123,\n                unit=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit.NONE\n            )\n        ),\n\n        # the properties below are optional\n        description="description"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'name', 'trigger', 'description']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingTriggerProperty
class EmrCreateCluster_ScalingTriggerPropertyDef(BaseStruct):
    cloud_watch_alarm_definition: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_CloudWatchAlarmDefinitionPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The definition of a CloudWatch metric alarm. When the defined alarm conditions are met along with other trigger parameters, scaling activity begins.\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_ScalingTrigger.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    scaling_trigger_property = stepfunctions_tasks.EmrCreateCluster.ScalingTriggerProperty(\n        cloud_watch_alarm_definition=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmDefinitionProperty(\n            comparison_operator=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator.GREATER_THAN_OR_EQUAL,\n            metric_name="metricName",\n            period=cdk.Duration.minutes(30),\n\n            # the properties below are optional\n            dimensions=[stepfunctions_tasks.EmrCreateCluster.MetricDimensionProperty(\n                key="key",\n                value="value"\n            )],\n            evaluation_periods=123,\n            namespace="namespace",\n            statistic=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic.SAMPLE_COUNT,\n            threshold=123,\n            unit=stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit.NONE\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_watch_alarm_definition']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingTriggerProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScriptBootstrapActionConfigProperty
class EmrCreateCluster_ScriptBootstrapActionConfigPropertyDef(BaseStruct):
    path: str = pydantic.Field(..., description='Location of the script to run during a bootstrap action. Can be either a location in Amazon S3 or on a local file system.\n')
    _init_params: typing.ClassVar[list[str]] = ['path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScriptBootstrapActionConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SimpleScalingPolicyConfigurationProperty
class EmrCreateCluster_SimpleScalingPolicyConfigurationPropertyDef(BaseStruct):
    scaling_adjustment: typing.Union[int, float] = pydantic.Field(..., description="The amount by which to scale in or scale out, based on the specified AdjustmentType. A positive value adds to the instance group's EC2 instance count while a negative number removes instances. If AdjustmentType is set to EXACT_CAPACITY, the number should only be a positive integer.\n")
    adjustment_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType] = pydantic.Field(None, description='The way in which EC2 instances are added (if ScalingAdjustment is a positive number) or terminated (if ScalingAdjustment is a negative number) each time the scaling activity is triggered. Default: - None\n')
    cool_down: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time, in seconds, after a scaling activity completes before any further trigger-related scaling activities can start. Default: 0\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_SimpleScalingPolicyConfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    simple_scaling_policy_configuration_property = stepfunctions_tasks.EmrCreateCluster.SimpleScalingPolicyConfigurationProperty(\n        scaling_adjustment=123,\n\n        # the properties below are optional\n        adjustment_type=stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType.CHANGE_IN_CAPACITY,\n        cool_down=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['scaling_adjustment', 'adjustment_type', 'cool_down']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SimpleScalingPolicyConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotProvisioningSpecificationProperty
class EmrCreateCluster_SpotProvisioningSpecificationPropertyDef(BaseStruct):
    timeout_action: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotTimeoutAction = pydantic.Field(..., description='The action to take when TargetSpotCapacity has not been fulfilled when the TimeoutDurationMinutes has expired.\n')
    timeout_duration_minutes: typing.Union[int, float] = pydantic.Field(..., description='The spot provisioning timeout period in minutes.\n')
    allocation_strategy: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotAllocationStrategy] = pydantic.Field(None, description='Specifies the strategy to use in launching Spot Instance fleets. Default: - No allocation strategy, i.e. spot instance type will be chosen based on current price only\n')
    block_duration_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='The defined duration for Spot instances (also known as Spot blocks) in minutes. Default: - No blockDurationMinutes\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_SpotProvisioningSpecification.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    spot_provisioning_specification_property = stepfunctions_tasks.EmrCreateCluster.SpotProvisioningSpecificationProperty(\n        timeout_action=stepfunctions_tasks.EmrCreateCluster.SpotTimeoutAction.SWITCH_TO_ON_DEMAND,\n        timeout_duration_minutes=123,\n\n        # the properties below are optional\n        allocation_strategy=stepfunctions_tasks.EmrCreateCluster.SpotAllocationStrategy.CAPACITY_OPTIMIZED,\n        block_duration_minutes=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['timeout_action', 'timeout_duration_minutes', 'allocation_strategy', 'block_duration_minutes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotProvisioningSpecificationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty
class EmrCreateCluster_VolumeSpecificationPropertyDef(BaseStruct):
    volume_size: models.SizeDef = pydantic.Field(..., description='The volume size. If the volume type is EBS-optimized, the minimum value is 10GiB. Maximum size is 1TiB\n')
    volume_type: aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType = pydantic.Field(..., description='The volume type. Volume types supported are gp2, io1, standard.\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the volume supports. Default: - EMR selected default\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_VolumeSpecification.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # size: cdk.Size\n\n    volume_specification_property = stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty(\n        volume_size=size,\n        volume_type=stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType.GP2,\n\n        # the properties below are optional\n        iops=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['volume_size', 'volume_type', 'iops']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.VolumeSpecificationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EmrCreateCluster_VolumeSpecificationPropertyDefConfig] = pydantic.Field(None)


class EmrCreateCluster_VolumeSpecificationPropertyDefConfig(pydantic.BaseModel):
    volume_size_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateClusterProps
class EmrCreateClusterPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    instances: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_InstancesConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='A specification of the number and type of Amazon EC2 instances.\n')
    name: str = pydantic.Field(..., description='The Name of the Cluster.\n')
    additional_info: typing.Optional[str] = pydantic.Field(None, description='A JSON string for selecting additional features. Default: - None\n')
    applications: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ApplicationConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A case-insensitive list of applications for Amazon EMR to install and configure when launching the cluster. Default: - EMR selected default\n')
    auto_scaling_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role for automatic scaling policies. Default: - A role will be created.\n')
    bootstrap_actions: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_BootstrapActionConfigPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of bootstrap actions to run before Hadoop starts on the cluster nodes. Default: - None\n')
    cluster_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Also called instance profile and EC2 role. An IAM role for an EMR cluster. The EC2 instances of the cluster assume this role. This attribute has been renamed from jobFlowRole to clusterRole to align with other ERM/StepFunction integration parameters. Default: - - A Role will be created\n')
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of configurations supplied for the EMR cluster you are creating. Default: - None\n')
    custom_ami_id: typing.Optional[str] = pydantic.Field(None, description='The ID of a custom Amazon EBS-backed Linux AMI. Default: - None\n')
    ebs_root_volume_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the EBS root device volume of the Linux AMI that is used for each EC2 instance. Default: - EMR selected default\n')
    kerberos_attributes: typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_KerberosAttributesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Attributes for Kerberos configuration when Kerberos authentication is enabled using a security configuration. Default: - None\n')
    log_uri: typing.Optional[str] = pydantic.Field(None, description='The location in Amazon S3 to write the log files of the job flow. Default: - None\n')
    release_label: typing.Optional[str] = pydantic.Field(None, description='The Amazon EMR release label, which determines the version of open-source application packages installed on the cluster. Default: - EMR selected default\n')
    scale_down_behavior: typing.Optional[aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EmrClusterScaleDownBehavior] = pydantic.Field(None, description='Specifies the way that individual Amazon EC2 instances terminate when an automatic scale-in activity occurs or an instance group is resized. Default: - EMR selected default\n')
    security_configuration: typing.Optional[str] = pydantic.Field(None, description='The name of a security configuration to apply to the cluster. Default: - None\n')
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that will be assumed by the Amazon EMR service to access AWS resources on your behalf. Default: - A role will be created that Amazon EMR service can assume.\n')
    step_concurrency_level: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the step concurrency level to allow multiple steps to run in parallel. Requires EMR release label 5.28.0 or above. Must be in range [1, 256]. Default: 1 - no step concurrency allowed\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of tags to associate with a cluster and propagate to Amazon EC2 instances. Default: - None\n')
    visible_to_all_users: typing.Optional[bool] = pydantic.Field(None, description='A value of true indicates that all IAM users in the AWS account can perform cluster actions if they have the proper IAM policy permissions. Default: true\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html\n:exampleMetadata: infused\n\nExample::\n\n    cluster_role = iam.Role(self, "ClusterRole",\n        assumed_by=iam.ServicePrincipal("ec2.amazonaws.com")\n    )\n\n    service_role = iam.Role(self, "ServiceRole",\n        assumed_by=iam.ServicePrincipal("elasticmapreduce.amazonaws.com")\n    )\n\n    auto_scaling_role = iam.Role(self, "AutoScalingRole",\n        assumed_by=iam.ServicePrincipal("elasticmapreduce.amazonaws.com")\n    )\n\n    auto_scaling_role.assume_role_policy.add_statements(\n        iam.PolicyStatement(\n            effect=iam.Effect.ALLOW,\n            principals=[\n                iam.ServicePrincipal("application-autoscaling.amazonaws.com")\n            ],\n            actions=["sts:AssumeRole"\n            ]\n        ))\n\n    tasks.EmrCreateCluster(self, "Create Cluster",\n        instances=tasks.EmrCreateCluster.InstancesConfigProperty(),\n        cluster_role=cluster_role,\n        name=sfn.TaskInput.from_json_path_at("$.ClusterName").value,\n        service_role=service_role,\n        auto_scaling_role=auto_scaling_role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'instances', 'name', 'additional_info', 'applications', 'auto_scaling_role', 'bootstrap_actions', 'cluster_role', 'configurations', 'custom_ami_id', 'ebs_root_volume_size', 'kerberos_attributes', 'log_uri', 'release_label', 'scale_down_behavior', 'security_configuration', 'service_role', 'step_concurrency_level', 'tags', 'visible_to_all_users']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrCreateClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameProps
class EmrModifyInstanceFleetByNamePropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    instance_fleet_name: str = pydantic.Field(..., description='The InstanceFleetName to update.\n')
    target_on_demand_capacity: typing.Union[int, float] = pydantic.Field(..., description='The target capacity of On-Demand units for the instance fleet. Default: - None\n')
    target_spot_capacity: typing.Union[int, float] = pydantic.Field(..., description='The target capacity of Spot units for the instance fleet. Default: - None\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrModifyInstanceFleetByName(self, "Task",\n        cluster_id="ClusterId",\n        instance_fleet_name="InstanceFleetName",\n        target_on_demand_capacity=2,\n        target_spot_capacity=0\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster_id', 'instance_fleet_name', 'target_on_demand_capacity', 'target_spot_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName.InstanceGroupModifyConfigProperty
class EmrModifyInstanceGroupByName_InstanceGroupModifyConfigPropertyDef(BaseStruct):
    configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EmrCreateCluster_ConfigurationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of new or modified configurations to apply for an instance group. Default: - None\n')
    e_c2_instance_ids_to_terminate: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The EC2 InstanceIds to terminate. After you terminate the instances, the instance group will not return to its original requested size. Default: - None\n')
    instance_count: typing.Union[int, float, None] = pydantic.Field(None, description='Target size for the instance group. Default: - None\n')
    shrink_policy: typing.Union[models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName_ShrinkPolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Policy for customizing shrink operations. Default: - None\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_InstanceGroupModifyConfig.html\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrModifyInstanceGroupByName(self, "Task",\n        cluster_id="ClusterId",\n        instance_group_name=sfn.JsonPath.string_at("$.InstanceGroupName"),\n        instance_group=tasks.EmrModifyInstanceGroupByName.InstanceGroupModifyConfigProperty(\n            instance_count=1\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['configurations', 'e_c2_instance_ids_to_terminate', 'instance_count', 'shrink_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName.InstanceGroupModifyConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName.InstanceResizePolicyProperty
class EmrModifyInstanceGroupByName_InstanceResizePolicyPropertyDef(BaseStruct):
    instances_to_protect: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Specific list of instances to be protected when shrinking an instance group. Default: - No instances will be protected when shrinking an instance group\n')
    instances_to_terminate: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Specific list of instances to be terminated when shrinking an instance group. Default: - No instances will be terminated when shrinking an instance group.\n')
    instance_termination_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Decommissioning timeout override for the specific list of instances to be terminated. Default: cdk.Duration.seconds\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_InstanceResizePolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    instance_resize_policy_property = stepfunctions_tasks.EmrModifyInstanceGroupByName.InstanceResizePolicyProperty(\n        instances_to_protect=["instancesToProtect"],\n        instances_to_terminate=["instancesToTerminate"],\n        instance_termination_timeout=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instances_to_protect', 'instances_to_terminate', 'instance_termination_timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName.InstanceResizePolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName.ShrinkPolicyProperty
class EmrModifyInstanceGroupByName_ShrinkPolicyPropertyDef(BaseStruct):
    decommission_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The desired timeout for decommissioning an instance. Overrides the default YARN decommissioning timeout. Default: - EMR selected default\n')
    instance_resize_policy: typing.Union[models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName_InstanceResizePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Custom policy for requesting termination protection or termination of specific instances when shrinking an instance group. Default: - None\n\n:see: https://docs.aws.amazon.com/emr/latest/APIReference/API_ShrinkPolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    shrink_policy_property = stepfunctions_tasks.EmrModifyInstanceGroupByName.ShrinkPolicyProperty(\n        decommission_timeout=cdk.Duration.minutes(30),\n        instance_resize_policy=stepfunctions_tasks.EmrModifyInstanceGroupByName.InstanceResizePolicyProperty(\n            instances_to_protect=["instancesToProtect"],\n            instances_to_terminate=["instancesToTerminate"],\n            instance_termination_timeout=cdk.Duration.minutes(30)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['decommission_timeout', 'instance_resize_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName.ShrinkPolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameProps
class EmrModifyInstanceGroupByNamePropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    instance_group: typing.Union[models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByName_InstanceGroupModifyConfigPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='The JSON that you want to provide to your ModifyInstanceGroup call as input. This uses the same syntax as the ModifyInstanceGroups API.\n')
    instance_group_name: str = pydantic.Field(..., description='The InstanceGroupName to update.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrModifyInstanceGroupByName(self, "Task",\n        cluster_id="ClusterId",\n        instance_group_name=sfn.JsonPath.string_at("$.InstanceGroupName"),\n        instance_group=tasks.EmrModifyInstanceGroupByName.InstanceGroupModifyConfigProperty(\n            instance_count=1\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster_id', 'instance_group', 'instance_group_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionProps
class EmrSetClusterTerminationProtectionPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster_id: str = pydantic.Field(..., description='The ClusterId to update.\n')
    termination_protected: bool = pydantic.Field(..., description='Termination protection indicator.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrSetClusterTerminationProtection(self, "Task",\n        cluster_id="ClusterId",\n        termination_protected=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster_id', 'termination_protected']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrTerminateClusterProps
class EmrTerminateClusterPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    cluster_id: str = pydantic.Field(..., description='The ClusterId to terminate.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrTerminateCluster(self, "Task",\n        cluster_id="ClusterId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'cluster_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EmrTerminateClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EncryptionConfiguration
class EncryptionConfigurationDef(BaseStruct):
    encryption_option: aws_cdk.aws_stepfunctions_tasks.EncryptionOption = pydantic.Field(..., description='Type of S3 server-side encryption enabled. Default: EncryptionOption.S3_MANAGED\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS key ARN or ID. Default: - No KMS key for Encryption Option SSE_S3 and default master key for Encryption Option SSE_KMS and CSE_KMS\n\n:see: https://docs.aws.amazon.com/athena/latest/APIReference/API_EncryptionConfiguration.html\n:exampleMetadata: infused\n\nExample::\n\n    start_query_execution_job = tasks.AthenaStartQueryExecution(self, "Start Athena Query",\n        query_string=sfn.JsonPath.string_at("$.queryString"),\n        query_execution_context=tasks.QueryExecutionContext(\n            database_name="mydatabase"\n        ),\n        result_configuration=tasks.ResultConfiguration(\n            encryption_configuration=tasks.EncryptionConfiguration(\n                encryption_option=tasks.EncryptionOption.S3_MANAGED\n            ),\n            output_location=s3.Location(\n                bucket_name="query-results-bucket",\n                object_key="folder"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['encryption_option', 'encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EncryptionConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EvaluateExpressionProps
class EvaluateExpressionPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    expression: str = pydantic.Field(..., description="The expression to evaluate. The expression may contain state paths. Example value: ``'$.a + $.b'``\n")
    runtime: typing.Optional[models.aws_lambda.RuntimeDef] = pydantic.Field(None, description='The runtime language to use to evaluate the expression. Default: lambda.Runtime.NODEJS_16_X\n\n:exampleMetadata: infused\n\nExample::\n\n    convert_to_seconds = tasks.EvaluateExpression(self, "Convert to seconds",\n        expression="$.waitMilliseconds / 1000",\n        result_path="$.waitSeconds"\n    )\n\n    create_message = tasks.EvaluateExpression(self, "Create message",\n        # Note: this is a string inside a string.\n        expression="`Now waiting ${$.waitSeconds} seconds...`",\n        runtime=lambda_.Runtime.NODEJS_16_X,\n        result_path="$.message"\n    )\n\n    publish_message = tasks.SnsPublish(self, "Publish message",\n        topic=sns.Topic(self, "cool-topic"),\n        message=sfn.TaskInput.from_json_path_at("$.message"),\n        result_path="$.sns"\n    )\n\n    wait = sfn.Wait(self, "Wait",\n        time=sfn.WaitTime.seconds_path("$.waitSeconds")\n    )\n\n    sfn.StateMachine(self, "StateMachine",\n        definition=convert_to_seconds.next(create_message).next(publish_message).next(wait)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'expression', 'runtime']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EvaluateExpressionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EventBridgePutEventsEntry
class EventBridgePutEventsEntryDef(BaseStruct):
    detail: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The event body. Can either be provided as an object or as a JSON-serialized string\n')
    detail_type: str = pydantic.Field(..., description='Used along with the source field to help identify the fields and values expected in the detail field. For example, events by CloudTrail have detail type "AWS API Call via CloudTrail"\n')
    source: str = pydantic.Field(..., description='The service or application that caused this event to be generated. Example value: ``com.example.service``\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus the entry will be sent to. Default: - event is sent to account\'s default event bus\n\n:see: https://docs.aws.amazon.com/eventbridge/latest/APIReference/API_PutEventsRequestEntry.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events as events\n    from aws_cdk import aws_stepfunctions as stepfunctions\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # event_bus: events.EventBus\n    # task_input: stepfunctions.TaskInput\n\n    event_bridge_put_events_entry = stepfunctions_tasks.EventBridgePutEventsEntry(\n        detail=task_input,\n        detail_type="detailType",\n        source="source",\n\n        # the properties below are optional\n        event_bus=event_bus\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['detail', 'detail_type', 'source', 'event_bus']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EventBridgePutEventsEntry'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EventBridgePutEventsProps
class EventBridgePutEventsPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    entries: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.EventBridgePutEventsEntryDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The entries that will be sent. Minimum number of entries is 1 and maximum is 10, unless `PutEvents API limit <https://docs.aws.amazon.com/eventbridge/latest/APIReference/API_PutEvents.html#API_PutEvents_RequestSyntax>`_ has changed.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_events as events\n\n\n    my_event_bus = events.EventBus(self, "EventBus",\n        event_bus_name="MyEventBus1"\n    )\n\n    tasks.EventBridgePutEvents(self, "Send an event to EventBridge",\n        entries=[tasks.EventBridgePutEventsEntry(\n            detail=sfn.TaskInput.from_object({\n                "Message": "Hello from Step Functions!"\n            }),\n            event_bus=my_event_bus,\n            detail_type="MessageFromStepFunctions",\n            source="step.functions"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'entries']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.EventBridgePutEventsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.GlueDataBrewStartJobRunProps
class GlueDataBrewStartJobRunPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    name: str = pydantic.Field(..., description='Glue DataBrew Job to run.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.GlueDataBrewStartJobRun(self, "Task",\n        name="databrew-job"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.GlueDataBrewStartJobRunProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.GlueStartJobRunProps
class GlueStartJobRunPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    glue_job_name: str = pydantic.Field(..., description='Glue job name.\n')
    arguments: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='The job arguments specifically for this run. For this job run, they replace the default arguments set in the job definition itself. Default: - Default arguments set in the job definition\n')
    notify_delay_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='After a job run starts, the number of minutes to wait before sending a job run delay notification. Must be at least 1 minute. Default: - Default delay set in the job definition\n')
    security_configuration: typing.Optional[str] = pydantic.Field(None, description='The name of the SecurityConfiguration structure to be used with this job run. This must match the Glue API Default: - Default configuration set in the job definition\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.GlueStartJobRun(self, "Task",\n        glue_job_name="my-glue-job",\n        arguments=sfn.TaskInput.from_object({\n            "key": "value"\n        }),\n        task_timeout=sfn.Timeout.duration(Duration.minutes(30)),\n        notify_delay_after=Duration.minutes(5)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'glue_job_name', 'arguments', 'notify_delay_after', 'security_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.GlueStartJobRunProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.JobDependency
class JobDependencyDef(BaseStruct):
    job_id: typing.Optional[str] = pydantic.Field(None, description='The job ID of the AWS Batch job associated with this dependency. Default: - No jobId\n')
    type: typing.Optional[str] = pydantic.Field(None, description='The type of the job dependency. Default: - No type\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    job_dependency = stepfunctions_tasks.JobDependency(\n        job_id="jobId",\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['job_id', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.JobDependency'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.JobDriver
class JobDriverDef(BaseStruct):
    spark_submit_job_driver: typing.Union[models.aws_stepfunctions_tasks.SparkSubmitJobDriverDef, dict[str, typing.Any]] = pydantic.Field(..., description='The job driver parameters specified for spark submit.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrContainersStartJobRun(self, "EMR Containers Start Job Run",\n        virtual_cluster=tasks.VirtualClusterInput.from_virtual_cluster_id("de92jdei2910fwedz"),\n        release_label=tasks.ReleaseLabel.EMR_6_2_0,\n        job_name="EMR-Containers-Job",\n        job_driver=tasks.JobDriver(\n            spark_submit_job_driver=tasks.SparkSubmitJobDriver(\n                entry_point=sfn.TaskInput.from_text("local:///usr/lib/spark/examples/src/main/python/pi.py")\n            )\n        ),\n        application_config=[tasks.ApplicationConfiguration(\n            classification=tasks.Classification.SPARK_DEFAULTS,\n            properties={\n                "spark.executor.instances": "1",\n                "spark.executor.memory": "512M"\n            }\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['spark_submit_job_driver']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.JobDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.LambdaInvokeProps
class LambdaInvokePropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    lambda_function: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='Lambda function to invoke.\n')
    client_context: typing.Optional[str] = pydantic.Field(None, description='Up to 3583 bytes of base64-encoded data about the invoking client to pass to the function. Default: - No context\n')
    invocation_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.LambdaInvocationType] = pydantic.Field(None, description='Invocation type of the Lambda function. Default: InvocationType.REQUEST_RESPONSE\n')
    payload: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description="The JSON that will be supplied as input to the Lambda function. Default: - The state input (JSON path '$')\n")
    payload_response_only: typing.Optional[bool] = pydantic.Field(None, description='Invoke the Lambda in a way that only returns the payload response without additional metadata. The ``payloadResponseOnly`` property cannot be used if ``integrationPattern``, ``invocationType``, ``clientContext``, or ``qualifier`` are specified. It always uses the REQUEST_RESPONSE behavior. Default: false\n')
    qualifier: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Version or alias to invoke a published version of the function. You only need to supply this if you want the version of the Lambda Function to depend on data in the state machine state. If not, you can pass the appropriate Alias or Version object directly as the ``lambdaFunction`` argument. Default: - Version or alias inherent to the ``lambdaFunction`` object.\n')
    retry_on_service_exceptions: typing.Optional[bool] = pydantic.Field(None, description='Whether to retry on Lambda service exceptions. This handles ``Lambda.ServiceException``, ``Lambda.AWSLambdaException`` and ``Lambda.SdkClientException`` with an interval of 2 seconds, a back-off rate of 2 and 6 maximum attempts. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    # fn: lambda.Function\n\n    tasks.LambdaInvoke(self, "Invoke with empty object as payload",\n        lambda_function=fn,\n        payload=sfn.TaskInput.from_object({})\n    )\n\n    # use the output of fn as input\n    tasks.LambdaInvoke(self, "Invoke with payload field in the state input",\n        lambda_function=fn,\n        payload=sfn.TaskInput.from_json_path_at("$.Payload")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'lambda_function', 'client_context', 'invocation_type', 'payload', 'payload_response_only', 'qualifier', 'retry_on_service_exceptions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.LambdaInvokeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[LambdaInvokePropsDefConfig] = pydantic.Field(None)


class LambdaInvokePropsDefConfig(pydantic.BaseModel):
    lambda_function_config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.LaunchTargetBindOptions
class LaunchTargetBindOptionsDef(BaseStruct):
    task_definition: typing.Union[models.aws_ecs.Ec2TaskDefinitionDef, models.aws_ecs.ExternalTaskDefinitionDef, models.aws_ecs.FargateTaskDefinitionDef, models.aws_ecs.TaskDefinitionDef] = pydantic.Field(..., description='Task definition to run Docker containers in Amazon ECS.\n')
    cluster: typing.Optional[typing.Union[models.aws_ecs.ClusterDef]] = pydantic.Field(None, description='A regional grouping of one or more container instances on which you can run tasks and services. Default: - No cluster\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n\n    launch_target_bind_options = stepfunctions_tasks.LaunchTargetBindOptions(\n        task_definition=task_definition,\n\n        # the properties below are optional\n        cluster=cluster\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['task_definition', 'cluster']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.LaunchTargetBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[LaunchTargetBindOptionsDefConfig] = pydantic.Field(None)


class LaunchTargetBindOptionsDefConfig(pydantic.BaseModel):
    task_definition_config: typing.Optional[models._interface_methods.AwsEcsITaskDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.MessageAttribute
class MessageAttributeDef(BaseStruct):
    value: typing.Any = pydantic.Field(..., description='The value of the attribute.\n')
    data_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.MessageAttributeDataType] = pydantic.Field(None, description='The data type for the attribute. Default: determined by type inspection if possible, fallback is String\n\n:see: https://docs.aws.amazon.com/sns/latest/dg/sns-message-attributes.html\n:exampleMetadata: infused\n\nExample::\n\n    topic = sns.Topic(self, "Topic")\n\n    # Use a field from the execution data as message.\n    task1 = tasks.SnsPublish(self, "Publish1",\n        topic=topic,\n        integration_pattern=sfn.IntegrationPattern.REQUEST_RESPONSE,\n        message=sfn.TaskInput.from_data_at("$.state.message"),\n        message_attributes={\n            "place": tasks.MessageAttribute(\n                value=sfn.JsonPath.string_at("$.place")\n            ),\n            "pic": tasks.MessageAttribute(\n                # BINARY must be explicitly set\n                data_type=tasks.MessageAttributeDataType.BINARY,\n                value=sfn.JsonPath.string_at("$.pic")\n            ),\n            "people": tasks.MessageAttribute(\n                value=4\n            ),\n            "handles": tasks.MessageAttribute(\n                value=["@kslater", "@jjf", null, "@mfanning"]\n            )\n        }\n    )\n\n    # Combine a field from the execution data with\n    # a literal object.\n    task2 = tasks.SnsPublish(self, "Publish2",\n        topic=topic,\n        message=sfn.TaskInput.from_object({\n            "field1": "somedata",\n            "field2": sfn.JsonPath.string_at("$.field2")\n        })\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['value', 'data_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.MessageAttribute'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.MetricDefinition
class MetricDefinitionDef(BaseStruct):
    name: str = pydantic.Field(..., description='Name of the metric.\n')
    regex: str = pydantic.Field(..., description='Regular expression that searches the output of a training job and gets the value of the metric.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    metric_definition = stepfunctions_tasks.MetricDefinition(\n        name="name",\n        regex="regex"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'regex']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.MetricDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ModelClientOptions
class ModelClientOptionsDef(BaseStruct):
    invocations_max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of retries when invocation requests are failing. Default: 0\n')
    invocations_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout duration for an invocation request. Default: Duration.minutes(1)\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTransformJob(self, "Batch Inference",\n        transform_job_name="MyTransformJob",\n        model_name="MyModelName",\n        model_client_options=tasks.ModelClientOptions(\n            invocations_max_retries=3,  # default is 0\n            invocations_timeout=Duration.minutes(5)\n        ),\n        transform_input=tasks.TransformInput(\n            transform_data_source=tasks.TransformDataSource(\n                s3_data_source=tasks.TransformS3DataSource(\n                    s3_uri="s3://inputbucket/train",\n                    s3_data_type=tasks.S3DataType.S3_PREFIX\n                )\n            )\n        ),\n        transform_output=tasks.TransformOutput(\n            s3_output_path="s3://outputbucket/TransformJobOutputPath"\n        ),\n        transform_resources=tasks.TransformResources(\n            instance_count=1,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M4, ec2.InstanceSize.XLARGE)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['invocations_max_retries', 'invocations_timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ModelClientOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.Monitoring
class MonitoringDef(BaseStruct):
    log_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='Amazon S3 Bucket for monitoring log publishing. You can configure your jobs to send log information to Amazon S3. Default: - if ``logging`` is manually set to ``true`` and a ``logBucket`` is not provided, a ``logBucket`` will be automatically generated`.\n')
    logging: typing.Optional[bool] = pydantic.Field(None, description='Enable logging for this job. If set to true, will automatically create a Cloudwatch Log Group and S3 bucket. This will be set to ``true`` implicitly if values are provided for ``logGroup`` or ``logBucket``. Default: true - true if values are provided for ``logGroup`` or ``logBucket``, false otherwise\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='A log group for CloudWatch monitoring. You can configure your jobs to send log information to CloudWatch Logs. Default: - if ``logging`` is manually set to ``true`` and a ``logGroup`` is not provided, a ``logGroup`` will be automatically generated`.\n')
    log_stream_name_prefix: typing.Optional[str] = pydantic.Field(None, description='A log stream name prefix for Cloudwatch monitoring. Default: - Log streams created in this log group have no default prefix\n')
    persistent_app_ui: typing.Optional[bool] = pydantic.Field(None, description='Monitoring configurations for the persistent application UI. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrContainersStartJobRun(self, "EMR Containers Start Job Run",\n        virtual_cluster=tasks.VirtualClusterInput.from_virtual_cluster_id("de92jdei2910fwedz"),\n        release_label=tasks.ReleaseLabel.EMR_6_2_0,\n        job_driver=tasks.JobDriver(\n            spark_submit_job_driver=tasks.SparkSubmitJobDriver(\n                entry_point=sfn.TaskInput.from_text("local:///usr/lib/spark/examples/src/main/python/pi.py"),\n                spark_submit_parameters="--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1"\n            )\n        ),\n        monitoring=tasks.Monitoring(\n            logging=True\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_bucket', 'logging', 'log_group', 'log_stream_name_prefix', 'persistent_app_ui']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.Monitoring'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.OutputDataConfig
class OutputDataConfigDef(BaseStruct):
    s3_output_location: models.aws_stepfunctions_tasks.S3LocationDef = pydantic.Field(..., description='Identifies the S3 path where you want Amazon SageMaker to store the model artifacts.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='Optional KMS encryption key that Amazon SageMaker uses to encrypt the model artifacts at rest using Amazon S3 server-side encryption. Default: - Amazon SageMaker uses the default KMS key for Amazon S3 for your role\'s account\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTrainingJob(self, "TrainSagemaker",\n        training_job_name=sfn.JsonPath.string_at("$.JobName"),\n        algorithm_specification=tasks.AlgorithmSpecification(\n            algorithm_name="BlazingText",\n            training_input_mode=tasks.InputMode.FILE\n        ),\n        input_data_config=[tasks.Channel(\n            channel_name="train",\n            data_source=tasks.DataSource(\n                s3_data_source=tasks.S3DataSource(\n                    s3_data_type=tasks.S3DataType.S3_PREFIX,\n                    s3_location=tasks.S3Location.from_json_expression("$.S3Bucket")\n                )\n            )\n        )],\n        output_data_config=tasks.OutputDataConfig(\n            s3_output_location=tasks.S3Location.from_bucket(s3.Bucket.from_bucket_name(self, "Bucket", "mybucket"), "myoutputpath")\n        ),\n        resource_config=tasks.ResourceConfig(\n            instance_count=1,\n            instance_type=ec2.InstanceType(sfn.JsonPath.string_at("$.InstanceType")),\n            volume_size=Size.gibibytes(50)\n        ),  # optional: default is 1 instance of EC2 `M4.XLarge` with `10GB` volume\n        stopping_condition=tasks.StoppingCondition(\n            max_runtime=Duration.hours(2)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_output_location', 'encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.OutputDataConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ProductionVariant
class ProductionVariantDef(BaseStruct):
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='The ML compute instance type.\n')
    model_name: str = pydantic.Field(..., description='The name of the model that you want to host. This is the name that you specified when creating the model.\n')
    variant_name: str = pydantic.Field(..., description='The name of the production variant.\n')
    accelerator_type: typing.Optional[models.aws_stepfunctions_tasks.AcceleratorTypeDef] = pydantic.Field(None, description='The size of the Elastic Inference (EI) instance to use for the production variant. Default: - None\n')
    initial_instance_count: typing.Union[int, float, None] = pydantic.Field(None, description='Number of instances to launch initially. Default: - 1\n')
    initial_variant_weight: typing.Union[int, float, None] = pydantic.Field(None, description='Determines initial traffic distribution among all of the models that you specify in the endpoint configuration. Default: - 1.0\n\n:see: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProductionVariant.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # accelerator_type: stepfunctions_tasks.AcceleratorType\n    # instance_type: ec2.InstanceType\n\n    production_variant = stepfunctions_tasks.ProductionVariant(\n        instance_type=instance_type,\n        model_name="modelName",\n        variant_name="variantName",\n\n        # the properties below are optional\n        accelerator_type=accelerator_type,\n        initial_instance_count=123,\n        initial_variant_weight=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_type', 'model_name', 'variant_name', 'accelerator_type', 'initial_instance_count', 'initial_variant_weight']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ProductionVariant'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ProductionVariantDefConfig] = pydantic.Field(None)


class ProductionVariantDefConfig(pydantic.BaseModel):
    instance_type_config: typing.Optional[models.aws_ec2.InstanceTypeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.QueryExecutionContext
class QueryExecutionContextDef(BaseStruct):
    catalog_name: typing.Optional[str] = pydantic.Field(None, description='Name of catalog used in query execution. Default: - No catalog\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='Name of database used in query execution. Default: - No database\n\n:see: https://docs.aws.amazon.com/athena/latest/APIReference/API_QueryExecutionContext.html\n:exampleMetadata: infused\n\nExample::\n\n    start_query_execution_job = tasks.AthenaStartQueryExecution(self, "Start Athena Query",\n        query_string=sfn.JsonPath.string_at("$.queryString"),\n        query_execution_context=tasks.QueryExecutionContext(\n            database_name="mydatabase"\n        ),\n        result_configuration=tasks.ResultConfiguration(\n            encryption_configuration=tasks.EncryptionConfiguration(\n                encryption_option=tasks.EncryptionOption.S3_MANAGED\n            ),\n            output_location=s3.Location(\n                bucket_name="query-results-bucket",\n                object_key="folder"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['catalog_name', 'database_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.QueryExecutionContext'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ResourceConfig
class ResourceConfigDef(BaseStruct):
    instance_count: typing.Union[int, float] = pydantic.Field(..., description='The number of ML compute instances to use. Default: 1 instance.\n')
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='ML compute instance type. To provide an instance type from the task input, supply an instance type in the following way where the value in the task input is an EC2 instance type prepended with "ml.":: new ec2.InstanceType(sfn.JsonPath.stringAt(\'$.path.to.instanceType\')); Default: ec2.InstanceType(ec2.InstanceClass.M4, ec2.InstanceType.XLARGE)\n')
    volume_size: models.SizeDef = pydantic.Field(..., description='Size of the ML storage volume that you want to provision. Default: 10 GB EBS volume.\n')
    volume_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS key that Amazon SageMaker uses to encrypt data on the storage volume attached to the ML compute instance(s) that run the training job. Default: - Amazon SageMaker uses the default KMS key for Amazon S3 for your role\'s account\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTrainingJob(self, "TrainSagemaker",\n        training_job_name=sfn.JsonPath.string_at("$.JobName"),\n        algorithm_specification=tasks.AlgorithmSpecification(\n            algorithm_name="BlazingText",\n            training_input_mode=tasks.InputMode.FILE\n        ),\n        input_data_config=[tasks.Channel(\n            channel_name="train",\n            data_source=tasks.DataSource(\n                s3_data_source=tasks.S3DataSource(\n                    s3_data_type=tasks.S3DataType.S3_PREFIX,\n                    s3_location=tasks.S3Location.from_json_expression("$.S3Bucket")\n                )\n            )\n        )],\n        output_data_config=tasks.OutputDataConfig(\n            s3_output_location=tasks.S3Location.from_bucket(s3.Bucket.from_bucket_name(self, "Bucket", "mybucket"), "myoutputpath")\n        ),\n        resource_config=tasks.ResourceConfig(\n            instance_count=1,\n            instance_type=ec2.InstanceType(sfn.JsonPath.string_at("$.InstanceType")),\n            volume_size=Size.gibibytes(50)\n        ),  # optional: default is 1 instance of EC2 `M4.XLarge` with `10GB` volume\n        stopping_condition=tasks.StoppingCondition(\n            max_runtime=Duration.hours(2)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_count', 'instance_type', 'volume_size', 'volume_encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ResourceConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ResourceConfigDefConfig] = pydantic.Field(None)


class ResourceConfigDefConfig(pydantic.BaseModel):
    instance_type_config: typing.Optional[models.aws_ec2.InstanceTypeDefConfig] = pydantic.Field(None)
    volume_size_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ResultConfiguration
class ResultConfigurationDef(BaseStruct):
    encryption_configuration: typing.Union[models.aws_stepfunctions_tasks.EncryptionConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Encryption option used if enabled in S3. Default: - SSE_S3 encrpytion is enabled with default encryption key\n')
    output_location: typing.Union[models.aws_s3.LocationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='S3 path of query results. Example value: ``s3://query-results-bucket/folder/`` Default: - Query Result Location set in Athena settings for this workgroup\n\n:see: https://docs.aws.amazon.com/athena/latest/APIReference/API_ResultConfiguration.html\n:exampleMetadata: infused\n\nExample::\n\n    start_query_execution_job = tasks.AthenaStartQueryExecution(self, "Start Athena Query",\n        query_string=sfn.JsonPath.string_at("$.queryString"),\n        query_execution_context=tasks.QueryExecutionContext(\n            database_name="mydatabase"\n        ),\n        result_configuration=tasks.ResultConfiguration(\n            encryption_configuration=tasks.EncryptionConfiguration(\n                encryption_option=tasks.EncryptionOption.S3_MANAGED\n            ),\n            output_location=s3.Location(\n                bucket_name="query-results-bucket",\n                object_key="folder"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['encryption_configuration', 'output_location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ResultConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.S3DataSource
class S3DataSourceDef(BaseStruct):
    s3_location: models.aws_stepfunctions_tasks.S3LocationDef = pydantic.Field(..., description='S3 Uri.\n')
    attribute_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='List of one or more attribute names to use that are found in a specified augmented manifest file. Default: - No attribute names\n')
    s3_data_distribution_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.S3DataDistributionType] = pydantic.Field(None, description='S3 Data Distribution Type. Default: - None\n')
    s3_data_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.S3DataType] = pydantic.Field(None, description='S3 Data Type. Default: S3_PREFIX\n\n:see: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTrainingJob(self, "TrainSagemaker",\n        training_job_name=sfn.JsonPath.string_at("$.JobName"),\n        algorithm_specification=tasks.AlgorithmSpecification(\n            algorithm_name="BlazingText",\n            training_input_mode=tasks.InputMode.FILE\n        ),\n        input_data_config=[tasks.Channel(\n            channel_name="train",\n            data_source=tasks.DataSource(\n                s3_data_source=tasks.S3DataSource(\n                    s3_data_type=tasks.S3DataType.S3_PREFIX,\n                    s3_location=tasks.S3Location.from_json_expression("$.S3Bucket")\n                )\n            )\n        )],\n        output_data_config=tasks.OutputDataConfig(\n            s3_output_location=tasks.S3Location.from_bucket(s3.Bucket.from_bucket_name(self, "Bucket", "mybucket"), "myoutputpath")\n        ),\n        resource_config=tasks.ResourceConfig(\n            instance_count=1,\n            instance_type=ec2.InstanceType(sfn.JsonPath.string_at("$.InstanceType")),\n            volume_size=Size.gibibytes(50)\n        ),  # optional: default is 1 instance of EC2 `M4.XLarge` with `10GB` volume\n        stopping_condition=tasks.StoppingCondition(\n            max_runtime=Duration.hours(2)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_location', 'attribute_names', 's3_data_distribution_type', 's3_data_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.S3DataSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.S3LocationBindOptions
class S3LocationBindOptionsDef(BaseStruct):
    for_reading: typing.Optional[bool] = pydantic.Field(None, description='Allow reading from the S3 Location. Default: false\n')
    for_writing: typing.Optional[bool] = pydantic.Field(None, description='Allow writing to the S3 Location. Default: false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    s3_location_bind_options = stepfunctions_tasks.S3LocationBindOptions(\n        for_reading=False,\n        for_writing=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['for_reading', 'for_writing']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.S3LocationBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.S3LocationConfig
class S3LocationConfigDef(BaseStruct):
    uri: str = pydantic.Field(..., description='Uniquely identifies the resource in Amazon S3.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    s3_location_config = stepfunctions_tasks.S3LocationConfig(\n        uri="uri"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['uri']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.S3LocationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigProps
class SageMakerCreateEndpointConfigPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    endpoint_config_name: str = pydantic.Field(..., description='The name of the endpoint configuration.\n')
    production_variants: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ProductionVariantDef, dict[str, typing.Any]]] = pydantic.Field(..., description='An list of ProductionVariant objects, one for each model that you want to host at this endpoint. Identifies a model that you want to host and the resources to deploy for hosting it. If you are deploying multiple models, tell Amazon SageMaker how to distribute traffic among the models by specifying variant weights.\n')
    kms_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='AWS Key Management Service key that Amazon SageMaker uses to encrypt data on the storage volume attached to the ML compute instance that hosts the endpoint. Default: - None\n')
    tags: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Tags to be applied to the endpoint configuration. Default: - No tags\n\n:see: https://docs.aws.amazon.com/step-functions/latest/dg/connect-sagemaker.html\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateEndpointConfig(self, "SagemakerEndpointConfig",\n        endpoint_config_name="MyEndpointConfig",\n        production_variants=[tasks.ProductionVariant(\n            initial_instance_count=2,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M5, ec2.InstanceSize.XLARGE),\n            model_name="MyModel",\n            variant_name="awesome-variant"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'endpoint_config_name', 'production_variants', 'kms_key', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpointProps
class SageMakerCreateEndpointPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    endpoint_config_name: str = pydantic.Field(..., description='The name of an endpoint configuration.\n')
    endpoint_name: str = pydantic.Field(..., description='The name of the endpoint. The name must be unique within an AWS Region in your AWS account.\n')
    tags: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Tags to be applied to the endpoint. Default: - No tags\n\n:see: https://docs.aws.amazon.com/step-functions/latest/dg/connect-sagemaker.html\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateEndpoint(self, "SagemakerEndpoint",\n        endpoint_name=sfn.JsonPath.string_at("$.EndpointName"),\n        endpoint_config_name=sfn.JsonPath.string_at("$.EndpointConfigName")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'endpoint_config_name', 'endpoint_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateEndpointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateModelProps
class SageMakerCreateModelPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    model_name: str = pydantic.Field(..., description='The name of the new model.\n')
    primary_container: typing.Union[models.aws_stepfunctions_tasks.ContainerDefinitionDef] = pydantic.Field(..., description='The definition of the primary docker image containing inference code, associated artifacts, and custom environment map that the inference code uses when the model is deployed for predictions.\n')
    containers: typing.Optional[typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ContainerDefinitionDef]]] = pydantic.Field(None, description='Specifies the containers in the inference pipeline. Default: - None\n')
    enable_network_isolation: typing.Optional[bool] = pydantic.Field(None, description='Isolates the model container. No inbound or outbound network calls can be made to or from the model container. Default: false\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An execution role that you can pass in a CreateModel API request. Default: - a role will be created.\n')
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets of the VPC to which the hosted model is connected (Note this parameter is only used when VPC is provided). Default: - Private Subnets are selected\n')
    tags: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='Tags to be applied to the model. Default: - No tags\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC that is accessible by the hosted model. Default: - None\n\n:see: https://docs.aws.amazon.com/step-functions/latest/dg/connect-sagemaker.html\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateModel(self, "Sagemaker",\n        model_name="MyModel",\n        primary_container=tasks.ContainerDefinition(\n            image=tasks.DockerImage.from_json_expression(sfn.JsonPath.string_at("$.Model.imageName")),\n            mode=tasks.Mode.SINGLE_MODEL,\n            model_s3_location=tasks.S3Location.from_json_expression("$.TrainingJob.ModelArtifacts.S3ModelArtifacts")\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'model_name', 'primary_container', 'containers', 'enable_network_isolation', 'role', 'subnet_selection', 'tags', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateModelProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SageMakerCreateModelPropsDefConfig] = pydantic.Field(None)


class SageMakerCreateModelPropsDefConfig(pydantic.BaseModel):
    primary_container_config: typing.Optional[models._interface_methods.AwsStepfunctionsTasksIContainerDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTrainingJobProps
class SageMakerCreateTrainingJobPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    algorithm_specification: typing.Union[models.aws_stepfunctions_tasks.AlgorithmSpecificationDef, dict[str, typing.Any]] = pydantic.Field(..., description='Identifies the training algorithm to use.\n')
    input_data_config: typing.Sequence[typing.Union[models.aws_stepfunctions_tasks.ChannelDef, dict[str, typing.Any]]] = pydantic.Field(..., description='Describes the various datasets (e.g. train, validation, test) and the Amazon S3 location where stored.\n')
    output_data_config: typing.Union[models.aws_stepfunctions_tasks.OutputDataConfigDef, dict[str, typing.Any]] = pydantic.Field(..., description='Identifies the Amazon S3 location where you want Amazon SageMaker to save the results of model training.\n')
    training_job_name: str = pydantic.Field(..., description='Training Job Name.\n')
    enable_network_isolation: typing.Optional[bool] = pydantic.Field(None, description='Isolates the training container. No inbound or outbound network calls can be made to or from the training container. Default: false\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set in the Docker container. Default: - No environment variables\n')
    hyperparameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Algorithm-specific parameters that influence the quality of the model. Set hyperparameters before you start the learning process. For a list of hyperparameters provided by Amazon SageMaker Default: - No hyperparameters\n')
    resource_config: typing.Union[models.aws_stepfunctions_tasks.ResourceConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the resources, ML compute instances, and ML storage volumes to deploy for model training. Default: - 1 instance of EC2 ``M4.XLarge`` with ``10GB`` volume\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role for the Training Job. The role must be granted all necessary permissions for the SageMaker training job to be able to operate. See https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createtrainingjob-perms Default: - a role will be created.\n')
    stopping_condition: typing.Union[models.aws_stepfunctions_tasks.StoppingConditionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Sets a time limit for training. Default: - max runtime of 1 hour\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Tags to be applied to the train job. Default: - No tags\n')
    vpc_config: typing.Union[models.aws_stepfunctions_tasks.VpcConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the VPC that you want your training job to connect to. Default: - No VPC\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTrainingJob(self, "TrainSagemaker",\n        training_job_name=sfn.JsonPath.string_at("$.JobName"),\n        algorithm_specification=tasks.AlgorithmSpecification(\n            algorithm_name="BlazingText",\n            training_input_mode=tasks.InputMode.FILE\n        ),\n        input_data_config=[tasks.Channel(\n            channel_name="train",\n            data_source=tasks.DataSource(\n                s3_data_source=tasks.S3DataSource(\n                    s3_data_type=tasks.S3DataType.S3_PREFIX,\n                    s3_location=tasks.S3Location.from_json_expression("$.S3Bucket")\n                )\n            )\n        )],\n        output_data_config=tasks.OutputDataConfig(\n            s3_output_location=tasks.S3Location.from_bucket(s3.Bucket.from_bucket_name(self, "Bucket", "mybucket"), "myoutputpath")\n        ),\n        resource_config=tasks.ResourceConfig(\n            instance_count=1,\n            instance_type=ec2.InstanceType(sfn.JsonPath.string_at("$.InstanceType")),\n            volume_size=Size.gibibytes(50)\n        ),  # optional: default is 1 instance of EC2 `M4.XLarge` with `10GB` volume\n        stopping_condition=tasks.StoppingCondition(\n            max_runtime=Duration.hours(2)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'algorithm_specification', 'input_data_config', 'output_data_config', 'training_job_name', 'enable_network_isolation', 'environment', 'hyperparameters', 'resource_config', 'role', 'stopping_condition', 'tags', 'vpc_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTrainingJobProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTransformJobProps
class SageMakerCreateTransformJobPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    model_name: str = pydantic.Field(..., description='Name of the model that you want to use for the transform job.\n')
    transform_input: typing.Union[models.aws_stepfunctions_tasks.TransformInputDef, dict[str, typing.Any]] = pydantic.Field(..., description='Dataset to be transformed and the Amazon S3 location where it is stored.\n')
    transform_job_name: str = pydantic.Field(..., description='Transform Job Name.\n')
    transform_output: typing.Union[models.aws_stepfunctions_tasks.TransformOutputDef, dict[str, typing.Any]] = pydantic.Field(..., description='S3 location where you want Amazon SageMaker to save the results from the transform job.\n')
    batch_strategy: typing.Optional[aws_cdk.aws_stepfunctions_tasks.BatchStrategy] = pydantic.Field(None, description='Number of records to include in a mini-batch for an HTTP inference request. Default: - No batch strategy\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Environment variables to set in the Docker container. Default: - No environment variables\n')
    max_concurrent_transforms: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of parallel requests that can be sent to each instance in a transform job. Default: - Amazon SageMaker checks the optional execution-parameters to determine the settings for your chosen algorithm. If the execution-parameters endpoint is not enabled, the default value is 1.\n')
    max_payload: typing.Optional[models.SizeDef] = pydantic.Field(None, description='Maximum allowed size of the payload, in MB. Default: 6\n')
    model_client_options: typing.Union[models.aws_stepfunctions_tasks.ModelClientOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configures the timeout and maximum number of retries for processing a transform job invocation. Default: - 0 retries and 60 seconds of timeout\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role for the Transform Job. Default: - A role is created with ``AmazonSageMakerFullAccess`` managed policy\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Tags to be applied to the train job. Default: - No tags\n')
    transform_resources: typing.Union[models.aws_stepfunctions_tasks.TransformResourcesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='ML compute instances for the transform job. Default: - 1 instance of type M4.XLarge\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTransformJob(self, "Batch Inference",\n        transform_job_name="MyTransformJob",\n        model_name="MyModelName",\n        model_client_options=tasks.ModelClientOptions(\n            invocations_max_retries=3,  # default is 0\n            invocations_timeout=Duration.minutes(5)\n        ),\n        transform_input=tasks.TransformInput(\n            transform_data_source=tasks.TransformDataSource(\n                s3_data_source=tasks.TransformS3DataSource(\n                    s3_uri="s3://inputbucket/train",\n                    s3_data_type=tasks.S3DataType.S3_PREFIX\n                )\n            )\n        ),\n        transform_output=tasks.TransformOutput(\n            s3_output_path="s3://outputbucket/TransformJobOutputPath"\n        ),\n        transform_resources=tasks.TransformResources(\n            instance_count=1,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M4, ec2.InstanceSize.XLARGE)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'model_name', 'transform_input', 'transform_job_name', 'transform_output', 'batch_strategy', 'environment', 'max_concurrent_transforms', 'max_payload', 'model_client_options', 'role', 'tags', 'transform_resources']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerCreateTransformJobProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SageMakerUpdateEndpointProps
class SageMakerUpdateEndpointPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    endpoint_config_name: str = pydantic.Field(..., description='The name of the new endpoint configuration.\n')
    endpoint_name: str = pydantic.Field(..., description='The name of the endpoint whose configuration you want to update.\n\n:see: https://docs.aws.amazon.com/step-functions/latest/dg/connect-sagemaker.html\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerUpdateEndpoint(self, "SagemakerEndpoint",\n        endpoint_name=sfn.JsonPath.string_at("$.Endpoint.Name"),\n        endpoint_config_name=sfn.JsonPath.string_at("$.Endpoint.EndpointConfig")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'endpoint_config_name', 'endpoint_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SageMakerUpdateEndpointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ShuffleConfig
class ShuffleConfigDef(BaseStruct):
    seed: typing.Union[int, float] = pydantic.Field(..., description='Determines the shuffling order.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    shuffle_config = stepfunctions_tasks.ShuffleConfig(\n        seed=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['seed']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.ShuffleConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SnsPublishProps
class SnsPublishPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    message: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The message you want to send. With the exception of SMS, messages must be UTF-8 encoded strings and at most 256 KB in size. For SMS, each message can contain up to 140 characters.\n')
    topic: typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef] = pydantic.Field(..., description='The SNS topic that the task will publish to.\n')
    message_attributes: typing.Optional[typing.Mapping[str, typing.Union[models.aws_stepfunctions_tasks.MessageAttributeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Add message attributes when publishing. These attributes carry additional metadata about the message and may be used for subscription filters. Default: {}\n')
    message_per_subscription_type: typing.Optional[bool] = pydantic.Field(None, description='Send different messages for each transport protocol. For example, you might want to send a shorter message to SMS subscribers and a more verbose message to email and SQS subscribers. Your message must be a JSON object with a top-level JSON key of "default" with a value that is a string You can define other top-level keys that define the message you want to send to a specific transport protocol (i.e. "sqs", "email", "http", etc) Default: false\n')
    subject: typing.Optional[str] = pydantic.Field(None, description='Used as the "Subject" line when the message is delivered to email endpoints. This field will also be included, if present, in the standard JSON messages delivered to other endpoints. Default: - No subject\n\n:exampleMetadata: infused\n\nExample::\n\n    convert_to_seconds = tasks.EvaluateExpression(self, "Convert to seconds",\n        expression="$.waitMilliseconds / 1000",\n        result_path="$.waitSeconds"\n    )\n\n    create_message = tasks.EvaluateExpression(self, "Create message",\n        # Note: this is a string inside a string.\n        expression="`Now waiting ${$.waitSeconds} seconds...`",\n        runtime=lambda_.Runtime.NODEJS_16_X,\n        result_path="$.message"\n    )\n\n    publish_message = tasks.SnsPublish(self, "Publish message",\n        topic=sns.Topic(self, "cool-topic"),\n        message=sfn.TaskInput.from_json_path_at("$.message"),\n        result_path="$.sns"\n    )\n\n    wait = sfn.Wait(self, "Wait",\n        time=sfn.WaitTime.seconds_path("$.waitSeconds")\n    )\n\n    sfn.StateMachine(self, "StateMachine",\n        definition=convert_to_seconds.next(create_message).next(publish_message).next(wait)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'message', 'topic', 'message_attributes', 'message_per_subscription_type', 'subject']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SnsPublishProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SnsPublishPropsDefConfig] = pydantic.Field(None)


class SnsPublishPropsDefConfig(pydantic.BaseModel):
    topic_config: typing.Optional[models._interface_methods.AwsSnsITopicDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SparkSubmitJobDriver
class SparkSubmitJobDriverDef(BaseStruct):
    entry_point: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The entry point of job application. Length Constraints: Minimum length of 1. Maximum length of 256.\n')
    entry_point_arguments: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description='The arguments for a job application in a task input object containing an array of strings. Length Constraints: Minimum length of 1. Maximum length of 10280. Default: - No arguments defined\n')
    spark_submit_parameters: typing.Optional[str] = pydantic.Field(None, description='The Spark submit parameters that are used for job runs. Length Constraints: Minimum length of 1. Maximum length of 102400. Default: - No spark submit parameters\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.EmrContainersStartJobRun(self, "EMR Containers Start Job Run",\n        virtual_cluster=tasks.VirtualClusterInput.from_virtual_cluster_id("de92jdei2910fwedz"),\n        release_label=tasks.ReleaseLabel.EMR_6_2_0,\n        job_name="EMR-Containers-Job",\n        job_driver=tasks.JobDriver(\n            spark_submit_job_driver=tasks.SparkSubmitJobDriver(\n                entry_point=sfn.TaskInput.from_text("local:///usr/lib/spark/examples/src/main/python/pi.py")\n            )\n        ),\n        application_config=[tasks.ApplicationConfiguration(\n            classification=tasks.Classification.SPARK_DEFAULTS,\n            properties={\n                "spark.executor.instances": "1",\n                "spark.executor.memory": "512M"\n            }\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['entry_point', 'entry_point_arguments', 'spark_submit_parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SparkSubmitJobDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SqsSendMessageProps
class SqsSendMessagePropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    message_body: models.aws_stepfunctions.TaskInputDef = pydantic.Field(..., description='The text message to send to the queue.\n')
    queue: typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef] = pydantic.Field(..., description='The SQS queue that messages will be sent to.\n')
    delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The length of time, for which to delay a message. Messages that you send to the queue remain invisible to consumers for the duration of the delay period. The maximum allowed delay is 15 minutes. Default: - delay set on the queue. If a delay is not set on the queue, messages are sent immediately (0 seconds).\n')
    message_deduplication_id: typing.Optional[str] = pydantic.Field(None, description="The token used for deduplication of sent messages. Any messages sent with the same deduplication ID are accepted successfully, but aren't delivered during the 5-minute deduplication interval. Default: - None\n")
    message_group_id: typing.Optional[str] = pydantic.Field(None, description='The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Messages in different message groups might be processed out of order. Default: - None\n\n:exampleMetadata: infused\n\nExample::\n\n    queue = sqs.Queue(self, "Queue")\n\n    # Use a field from the execution data as message.\n    task1 = tasks.SqsSendMessage(self, "Send1",\n        queue=queue,\n        message_body=sfn.TaskInput.from_json_path_at("$.message")\n    )\n\n    # Combine a field from the execution data with\n    # a literal object.\n    task2 = tasks.SqsSendMessage(self, "Send2",\n        queue=queue,\n        message_body=sfn.TaskInput.from_object({\n            "field1": "somedata",\n            "field2": sfn.JsonPath.string_at("$.field2")\n        })\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'message_body', 'queue', 'delay', 'message_deduplication_id', 'message_group_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.SqsSendMessageProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqsSendMessagePropsDefConfig] = pydantic.Field(None)


class SqsSendMessagePropsDefConfig(pydantic.BaseModel):
    queue_config: typing.Optional[models._interface_methods.AwsSqsIQueueDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.StepFunctionsInvokeActivityProps
class StepFunctionsInvokeActivityPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    activity: typing.Union[models.aws_stepfunctions.ActivityDef] = pydantic.Field(..., description='Step Functions Activity to invoke.\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Parameters pass a collection of key-value pairs, either static values or JSONPath expressions that select from the input. Default: No parameters\n\n:exampleMetadata: infused\n\nExample::\n\n    submit_job_activity = sfn.Activity(self, "SubmitJob")\n\n    tasks.StepFunctionsInvokeActivity(self, "Submit Job",\n        activity=submit_job_activity\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'activity', 'parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.StepFunctionsInvokeActivityProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StepFunctionsInvokeActivityPropsDefConfig] = pydantic.Field(None)


class StepFunctionsInvokeActivityPropsDefConfig(pydantic.BaseModel):
    activity_config: typing.Optional[models._interface_methods.AwsStepfunctionsIActivityDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.StepFunctionsStartExecutionProps
class StepFunctionsStartExecutionPropsDef(BaseStruct):
    comment: typing.Optional[str] = pydantic.Field(None, description='An optional description for this state. Default: - No comment\n')
    credentials: typing.Union[models.aws_stepfunctions.CredentialsDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Credentials for an IAM Role that the State Machine assumes for executing the task. This enables cross-account resource invocations. Default: - None (Task is executed using the State Machine's execution role)\n")
    heartbeat: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the heartbeat. Default: - None\n')
    heartbeat_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the heartbeat. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    input_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select part of the state to be the input to this state. May also be the special value JsonPath.DISCARD, which will cause the effective input to be the empty object {}. Default: - The entire task input (JSON path '$')\n")
    integration_pattern: typing.Optional[aws_cdk.aws_stepfunctions.IntegrationPattern] = pydantic.Field(None, description='AWS Step Functions integrates with services directly in the Amazon States Language. You can control these AWS services using service integration patterns Default: - ``IntegrationPattern.REQUEST_RESPONSE`` for most tasks. ``IntegrationPattern.RUN_JOB`` for the following exceptions: ``BatchSubmitJob``, ``EmrAddStep``, ``EmrCreateCluster``, ``EmrTerminationCluster``, and ``EmrContainersStartJobRun``.\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to select select a portion of the state output to pass to the next state. May also be the special value JsonPath.DISCARD, which will cause the effective output to be the empty object {}. Default: - The entire JSON node determined by the state input, the task result, and resultPath is passed to the next state (JSON path '$')\n")
    result_path: typing.Optional[str] = pydantic.Field(None, description="JSONPath expression to indicate where to inject the state's output. May also be the special value JsonPath.DISCARD, which will cause the state's input to become its output. Default: - Replaces the entire input with the result (JSON path '$')\n")
    result_selector: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="The JSON that will replace the state's raw result and become the effective result before ResultPath is applied. You can use ResultSelector to create a payload with values that are static or selected from the state's raw result. Default: - None\n")
    task_timeout: typing.Optional[models.aws_stepfunctions.TimeoutDef] = pydantic.Field(None, description='Timeout for the task. [disable-awslint:duration-prop-type] is needed because all props interface in aws-stepfunctions-tasks extend this interface Default: - None\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) Timeout for the task. Default: - None\n')
    state_machine: typing.Union[models.aws_stepfunctions.StateMachineDef] = pydantic.Field(..., description='The Step Functions state machine to start the execution on.\n')
    associate_with_parent: typing.Optional[bool] = pydantic.Field(None, description='Pass the execution ID from the context object to the execution input. This allows the Step Functions UI to link child executions from parent executions, making it easier to trace execution flow across state machines. If you set this property to ``true``, the ``input`` property must be an object (provided by ``sfn.TaskInput.fromObject``) or omitted entirely. Default: - false\n')
    input: typing.Optional[models.aws_stepfunctions.TaskInputDef] = pydantic.Field(None, description="The JSON input for the execution, same as that of StartExecution. Default: - The state input (JSON path '$')\n")
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the execution, same as that of StartExecution. Default: - None\n\n:exampleMetadata: infused\n\nExample::\n\n    # Define a state machine with one Pass state\n    child = sfn.StateMachine(self, "ChildStateMachine",\n        definition=sfn.Chain.start(sfn.Pass(self, "PassState"))\n    )\n\n    # Include the state machine in a Task state with callback pattern\n    task = tasks.StepFunctionsStartExecution(self, "ChildTask",\n        state_machine=child,\n        integration_pattern=sfn.IntegrationPattern.WAIT_FOR_TASK_TOKEN,\n        input=sfn.TaskInput.from_object({\n            "token": sfn.JsonPath.task_token,\n            "foo": "bar"\n        }),\n        name="MyExecutionName"\n    )\n\n    # Define a second state machine with the Task state above\n    sfn.StateMachine(self, "ParentStateMachine",\n        definition=task\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comment', 'credentials', 'heartbeat', 'heartbeat_timeout', 'input_path', 'integration_pattern', 'output_path', 'result_path', 'result_selector', 'task_timeout', 'timeout', 'state_machine', 'associate_with_parent', 'input', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.StepFunctionsStartExecutionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StepFunctionsStartExecutionPropsDefConfig] = pydantic.Field(None)


class StepFunctionsStartExecutionPropsDefConfig(pydantic.BaseModel):
    state_machine_config: typing.Optional[models._interface_methods.AwsStepfunctionsIStateMachineDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.StoppingCondition
class StoppingConditionDef(BaseStruct):
    max_runtime: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum length of time, in seconds, that the training or compilation job can run. Default: - 1 hour\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTrainingJob(self, "TrainSagemaker",\n        training_job_name=sfn.JsonPath.string_at("$.JobName"),\n        algorithm_specification=tasks.AlgorithmSpecification(\n            algorithm_name="BlazingText",\n            training_input_mode=tasks.InputMode.FILE\n        ),\n        input_data_config=[tasks.Channel(\n            channel_name="train",\n            data_source=tasks.DataSource(\n                s3_data_source=tasks.S3DataSource(\n                    s3_data_type=tasks.S3DataType.S3_PREFIX,\n                    s3_location=tasks.S3Location.from_json_expression("$.S3Bucket")\n                )\n            )\n        )],\n        output_data_config=tasks.OutputDataConfig(\n            s3_output_location=tasks.S3Location.from_bucket(s3.Bucket.from_bucket_name(self, "Bucket", "mybucket"), "myoutputpath")\n        ),\n        resource_config=tasks.ResourceConfig(\n            instance_count=1,\n            instance_type=ec2.InstanceType(sfn.JsonPath.string_at("$.InstanceType")),\n            volume_size=Size.gibibytes(50)\n        ),  # optional: default is 1 instance of EC2 `M4.XLarge` with `10GB` volume\n        stopping_condition=tasks.StoppingCondition(\n            max_runtime=Duration.hours(2)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_runtime']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.StoppingCondition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.TaskEnvironmentVariable
class TaskEnvironmentVariableDef(BaseStruct):
    name: str = pydantic.Field(..., description="Name for the environment variable. Use ``JsonPath`` class's static methods to specify name from a JSON path.\n")
    value: str = pydantic.Field(..., description='Value of the environment variable. Use ``JsonPath`` class\'s static methods to specify value from a JSON path.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    task_environment_variable = stepfunctions_tasks.TaskEnvironmentVariable(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.TaskEnvironmentVariable'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.TransformDataSource
class TransformDataSourceDef(BaseStruct):
    s3_data_source: typing.Union[models.aws_stepfunctions_tasks.TransformS3DataSourceDef, dict[str, typing.Any]] = pydantic.Field(..., description='S3 location of the input data.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTransformJob(self, "Batch Inference",\n        transform_job_name="MyTransformJob",\n        model_name="MyModelName",\n        model_client_options=tasks.ModelClientOptions(\n            invocations_max_retries=3,  # default is 0\n            invocations_timeout=Duration.minutes(5)\n        ),\n        transform_input=tasks.TransformInput(\n            transform_data_source=tasks.TransformDataSource(\n                s3_data_source=tasks.TransformS3DataSource(\n                    s3_uri="s3://inputbucket/train",\n                    s3_data_type=tasks.S3DataType.S3_PREFIX\n                )\n            )\n        ),\n        transform_output=tasks.TransformOutput(\n            s3_output_path="s3://outputbucket/TransformJobOutputPath"\n        ),\n        transform_resources=tasks.TransformResources(\n            instance_count=1,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M4, ec2.InstanceSize.XLARGE)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_data_source']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.TransformDataSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.TransformInput
class TransformInputDef(BaseStruct):
    transform_data_source: typing.Union[models.aws_stepfunctions_tasks.TransformDataSourceDef, dict[str, typing.Any]] = pydantic.Field(..., description='S3 location of the channel data.\n')
    compression_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.CompressionType] = pydantic.Field(None, description='The compression type of the transform data. Default: NONE\n')
    content_type: typing.Optional[str] = pydantic.Field(None, description='Multipurpose internet mail extension (MIME) type of the data. Default: - None\n')
    split_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.SplitType] = pydantic.Field(None, description='Method to use to split the transform job\'s data files into smaller batches. Default: NONE\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTransformJob(self, "Batch Inference",\n        transform_job_name="MyTransformJob",\n        model_name="MyModelName",\n        model_client_options=tasks.ModelClientOptions(\n            invocations_max_retries=3,  # default is 0\n            invocations_timeout=Duration.minutes(5)\n        ),\n        transform_input=tasks.TransformInput(\n            transform_data_source=tasks.TransformDataSource(\n                s3_data_source=tasks.TransformS3DataSource(\n                    s3_uri="s3://inputbucket/train",\n                    s3_data_type=tasks.S3DataType.S3_PREFIX\n                )\n            )\n        ),\n        transform_output=tasks.TransformOutput(\n            s3_output_path="s3://outputbucket/TransformJobOutputPath"\n        ),\n        transform_resources=tasks.TransformResources(\n            instance_count=1,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M4, ec2.InstanceSize.XLARGE)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['transform_data_source', 'compression_type', 'content_type', 'split_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.TransformInput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.TransformOutput
class TransformOutputDef(BaseStruct):
    s3_output_path: str = pydantic.Field(..., description='S3 path where you want Amazon SageMaker to store the results of the transform job.\n')
    accept: typing.Optional[str] = pydantic.Field(None, description='MIME type used to specify the output data. Default: - None\n')
    assemble_with: typing.Optional[aws_cdk.aws_stepfunctions_tasks.AssembleWith] = pydantic.Field(None, description='Defines how to assemble the results of the transform job as a single S3 object. Default: - None\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='AWS KMS key that Amazon SageMaker uses to encrypt the model artifacts at rest using Amazon S3 server-side encryption. Default: - default KMS key for Amazon S3 for your role\'s account.\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTransformJob(self, "Batch Inference",\n        transform_job_name="MyTransformJob",\n        model_name="MyModelName",\n        model_client_options=tasks.ModelClientOptions(\n            invocations_max_retries=3,  # default is 0\n            invocations_timeout=Duration.minutes(5)\n        ),\n        transform_input=tasks.TransformInput(\n            transform_data_source=tasks.TransformDataSource(\n                s3_data_source=tasks.TransformS3DataSource(\n                    s3_uri="s3://inputbucket/train",\n                    s3_data_type=tasks.S3DataType.S3_PREFIX\n                )\n            )\n        ),\n        transform_output=tasks.TransformOutput(\n            s3_output_path="s3://outputbucket/TransformJobOutputPath"\n        ),\n        transform_resources=tasks.TransformResources(\n            instance_count=1,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M4, ec2.InstanceSize.XLARGE)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_output_path', 'accept', 'assemble_with', 'encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.TransformOutput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.TransformResources
class TransformResourcesDef(BaseStruct):
    instance_count: typing.Union[int, float] = pydantic.Field(..., description='Number of ML compute instances to use in the transform job.\n')
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='ML compute instance type for the transform job.\n')
    volume_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='AWS KMS key that Amazon SageMaker uses to encrypt data on the storage volume attached to the ML compute instance(s). Default: - None\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTransformJob(self, "Batch Inference",\n        transform_job_name="MyTransformJob",\n        model_name="MyModelName",\n        model_client_options=tasks.ModelClientOptions(\n            invocations_max_retries=3,  # default is 0\n            invocations_timeout=Duration.minutes(5)\n        ),\n        transform_input=tasks.TransformInput(\n            transform_data_source=tasks.TransformDataSource(\n                s3_data_source=tasks.TransformS3DataSource(\n                    s3_uri="s3://inputbucket/train",\n                    s3_data_type=tasks.S3DataType.S3_PREFIX\n                )\n            )\n        ),\n        transform_output=tasks.TransformOutput(\n            s3_output_path="s3://outputbucket/TransformJobOutputPath"\n        ),\n        transform_resources=tasks.TransformResources(\n            instance_count=1,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M4, ec2.InstanceSize.XLARGE)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_count', 'instance_type', 'volume_encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.TransformResources'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[TransformResourcesDefConfig] = pydantic.Field(None)


class TransformResourcesDefConfig(pydantic.BaseModel):
    instance_type_config: typing.Optional[models.aws_ec2.InstanceTypeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.TransformS3DataSource
class TransformS3DataSourceDef(BaseStruct):
    s3_uri: str = pydantic.Field(..., description='Identifies either a key name prefix or a manifest.\n')
    s3_data_type: typing.Optional[aws_cdk.aws_stepfunctions_tasks.S3DataType] = pydantic.Field(None, description='S3 Data Type. Default: \'S3Prefix\'\n\n:exampleMetadata: infused\n\nExample::\n\n    tasks.SageMakerCreateTransformJob(self, "Batch Inference",\n        transform_job_name="MyTransformJob",\n        model_name="MyModelName",\n        model_client_options=tasks.ModelClientOptions(\n            invocations_max_retries=3,  # default is 0\n            invocations_timeout=Duration.minutes(5)\n        ),\n        transform_input=tasks.TransformInput(\n            transform_data_source=tasks.TransformDataSource(\n                s3_data_source=tasks.TransformS3DataSource(\n                    s3_uri="s3://inputbucket/train",\n                    s3_data_type=tasks.S3DataType.S3_PREFIX\n                )\n            )\n        ),\n        transform_output=tasks.TransformOutput(\n            s3_output_path="s3://outputbucket/TransformJobOutputPath"\n        ),\n        transform_resources=tasks.TransformResources(\n            instance_count=1,\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.M4, ec2.InstanceSize.XLARGE)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_uri', 's3_data_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.TransformS3DataSource'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_stepfunctions_tasks.VpcConfig
class VpcConfigDef(BaseStruct):
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='VPC.\n')
    subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='VPC subnets. Default: - Private Subnets are selected\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_stepfunctions_tasks as stepfunctions_tasks\n\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # vpc: ec2.Vpc\n\n    vpc_config = stepfunctions_tasks.VpcConfig(\n        vpc=vpc,\n\n        # the properties below are optional\n        subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_stepfunctions_tasks.VpcConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[VpcConfigDefConfig] = pydantic.Field(None)


class VpcConfigDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ActionOnFailure
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AssembleWith
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.AuthType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.BatchStrategy
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.CompressionType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoConsumedCapacity
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoItemCollectionMetrics
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.DynamoReturnValues
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmComparisonOperator
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmStatistic
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.CloudWatchAlarmUnit
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EbsBlockDeviceVolumeType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.EmrClusterScaleDownBehavior
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceMarket
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.InstanceRoleType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.ScalingAdjustmentType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotAllocationStrategy
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EmrCreateCluster.SpotTimeoutAction
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.EncryptionOption
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.HttpMethod
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.HttpMethods
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.InputMode
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.LambdaInvocationType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.MessageAttributeDataType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.Mode
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.RecordWrapperType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.S3DataDistributionType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.S3DataType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.SplitType
# skipping emum

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.IContainerDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.IEcsLaunchTarget
#  skipping Interface

#  autogenerated from aws_cdk.aws_stepfunctions_tasks.ISageMakerTask
#  skipping Interface

import models

class ModuleModel(pydantic.BaseModel):
    AcceleratorClass: typing.Optional[dict[str, AcceleratorClassDef]] = pydantic.Field(None)
    AcceleratorType: typing.Optional[dict[str, AcceleratorTypeDef]] = pydantic.Field(None)
    Classification: typing.Optional[dict[str, ClassificationDef]] = pydantic.Field(None)
    ContainerDefinition: typing.Optional[dict[str, ContainerDefinitionDef]] = pydantic.Field(None)
    DockerImage: typing.Optional[dict[str, DockerImageDef]] = pydantic.Field(None)
    DynamoAttributeValue: typing.Optional[dict[str, DynamoAttributeValueDef]] = pydantic.Field(None)
    DynamoProjectionExpression: typing.Optional[dict[str, DynamoProjectionExpressionDef]] = pydantic.Field(None)
    EcsEc2LaunchTarget: typing.Optional[dict[str, EcsEc2LaunchTargetDef]] = pydantic.Field(None)
    EcsFargateLaunchTarget: typing.Optional[dict[str, EcsFargateLaunchTargetDef]] = pydantic.Field(None)
    EksClusterInput: typing.Optional[dict[str, EksClusterInputDef]] = pydantic.Field(None)
    ReleaseLabel: typing.Optional[dict[str, ReleaseLabelDef]] = pydantic.Field(None)
    S3Location: typing.Optional[dict[str, S3LocationDef]] = pydantic.Field(None)
    VirtualClusterInput: typing.Optional[dict[str, VirtualClusterInputDef]] = pydantic.Field(None)
    AthenaGetQueryExecution: typing.Optional[dict[str, AthenaGetQueryExecutionDef]] = pydantic.Field(None)
    AthenaGetQueryResults: typing.Optional[dict[str, AthenaGetQueryResultsDef]] = pydantic.Field(None)
    AthenaStartQueryExecution: typing.Optional[dict[str, AthenaStartQueryExecutionDef]] = pydantic.Field(None)
    AthenaStopQueryExecution: typing.Optional[dict[str, AthenaStopQueryExecutionDef]] = pydantic.Field(None)
    BatchSubmitJob: typing.Optional[dict[str, BatchSubmitJobDef]] = pydantic.Field(None)
    CallApiGatewayHttpApiEndpoint: typing.Optional[dict[str, CallApiGatewayHttpApiEndpointDef]] = pydantic.Field(None)
    CallApiGatewayRestApiEndpoint: typing.Optional[dict[str, CallApiGatewayRestApiEndpointDef]] = pydantic.Field(None)
    CallAwsService: typing.Optional[dict[str, CallAwsServiceDef]] = pydantic.Field(None)
    CodeBuildStartBuild: typing.Optional[dict[str, CodeBuildStartBuildDef]] = pydantic.Field(None)
    DynamoDeleteItem: typing.Optional[dict[str, DynamoDeleteItemDef]] = pydantic.Field(None)
    DynamoGetItem: typing.Optional[dict[str, DynamoGetItemDef]] = pydantic.Field(None)
    DynamoPutItem: typing.Optional[dict[str, DynamoPutItemDef]] = pydantic.Field(None)
    DynamoUpdateItem: typing.Optional[dict[str, DynamoUpdateItemDef]] = pydantic.Field(None)
    EcsRunTask: typing.Optional[dict[str, EcsRunTaskDef]] = pydantic.Field(None)
    EksCall: typing.Optional[dict[str, EksCallDef]] = pydantic.Field(None)
    EmrAddStep: typing.Optional[dict[str, EmrAddStepDef]] = pydantic.Field(None)
    EmrCancelStep: typing.Optional[dict[str, EmrCancelStepDef]] = pydantic.Field(None)
    EmrContainersCreateVirtualCluster: typing.Optional[dict[str, EmrContainersCreateVirtualClusterDef]] = pydantic.Field(None)
    EmrContainersDeleteVirtualCluster: typing.Optional[dict[str, EmrContainersDeleteVirtualClusterDef]] = pydantic.Field(None)
    EmrContainersStartJobRun: typing.Optional[dict[str, EmrContainersStartJobRunDef]] = pydantic.Field(None)
    EmrCreateCluster: typing.Optional[dict[str, EmrCreateClusterDef]] = pydantic.Field(None)
    EmrModifyInstanceFleetByName: typing.Optional[dict[str, EmrModifyInstanceFleetByNameDef]] = pydantic.Field(None)
    EmrModifyInstanceGroupByName: typing.Optional[dict[str, EmrModifyInstanceGroupByNameDef]] = pydantic.Field(None)
    EmrSetClusterTerminationProtection: typing.Optional[dict[str, EmrSetClusterTerminationProtectionDef]] = pydantic.Field(None)
    EmrTerminateCluster: typing.Optional[dict[str, EmrTerminateClusterDef]] = pydantic.Field(None)
    EvaluateExpression: typing.Optional[dict[str, EvaluateExpressionDef]] = pydantic.Field(None)
    EventBridgePutEvents: typing.Optional[dict[str, EventBridgePutEventsDef]] = pydantic.Field(None)
    GlueDataBrewStartJobRun: typing.Optional[dict[str, GlueDataBrewStartJobRunDef]] = pydantic.Field(None)
    GlueStartJobRun: typing.Optional[dict[str, GlueStartJobRunDef]] = pydantic.Field(None)
    LambdaInvoke: typing.Optional[dict[str, LambdaInvokeDef]] = pydantic.Field(None)
    SageMakerCreateEndpoint: typing.Optional[dict[str, SageMakerCreateEndpointDef]] = pydantic.Field(None)
    SageMakerCreateEndpointConfig: typing.Optional[dict[str, SageMakerCreateEndpointConfigDef]] = pydantic.Field(None)
    SageMakerCreateModel: typing.Optional[dict[str, SageMakerCreateModelDef]] = pydantic.Field(None)
    SageMakerCreateTrainingJob: typing.Optional[dict[str, SageMakerCreateTrainingJobDef]] = pydantic.Field(None)
    SageMakerCreateTransformJob: typing.Optional[dict[str, SageMakerCreateTransformJobDef]] = pydantic.Field(None)
    SageMakerUpdateEndpoint: typing.Optional[dict[str, SageMakerUpdateEndpointDef]] = pydantic.Field(None)
    SnsPublish: typing.Optional[dict[str, SnsPublishDef]] = pydantic.Field(None)
    SqsSendMessage: typing.Optional[dict[str, SqsSendMessageDef]] = pydantic.Field(None)
    StepFunctionsInvokeActivity: typing.Optional[dict[str, StepFunctionsInvokeActivityDef]] = pydantic.Field(None)
    StepFunctionsStartExecution: typing.Optional[dict[str, StepFunctionsStartExecutionDef]] = pydantic.Field(None)
    AlgorithmSpecification: typing.Optional[dict[str, AlgorithmSpecificationDef]] = pydantic.Field(None)
    ApplicationConfiguration: typing.Optional[dict[str, ApplicationConfigurationDef]] = pydantic.Field(None)
    AthenaGetQueryExecutionProps: typing.Optional[dict[str, AthenaGetQueryExecutionPropsDef]] = pydantic.Field(None)
    AthenaGetQueryResultsProps: typing.Optional[dict[str, AthenaGetQueryResultsPropsDef]] = pydantic.Field(None)
    AthenaStartQueryExecutionProps: typing.Optional[dict[str, AthenaStartQueryExecutionPropsDef]] = pydantic.Field(None)
    AthenaStopQueryExecutionProps: typing.Optional[dict[str, AthenaStopQueryExecutionPropsDef]] = pydantic.Field(None)
    BatchContainerOverrides: typing.Optional[dict[str, BatchContainerOverridesDef]] = pydantic.Field(None)
    BatchJobDependency: typing.Optional[dict[str, BatchJobDependencyDef]] = pydantic.Field(None)
    BatchSubmitJobProps: typing.Optional[dict[str, BatchSubmitJobPropsDef]] = pydantic.Field(None)
    CallApiGatewayEndpointBaseProps: typing.Optional[dict[str, CallApiGatewayEndpointBasePropsDef]] = pydantic.Field(None)
    CallApiGatewayHttpApiEndpointProps: typing.Optional[dict[str, CallApiGatewayHttpApiEndpointPropsDef]] = pydantic.Field(None)
    CallApiGatewayRestApiEndpointProps: typing.Optional[dict[str, CallApiGatewayRestApiEndpointPropsDef]] = pydantic.Field(None)
    CallAwsServiceProps: typing.Optional[dict[str, CallAwsServicePropsDef]] = pydantic.Field(None)
    Channel: typing.Optional[dict[str, ChannelDef]] = pydantic.Field(None)
    CodeBuildStartBuildProps: typing.Optional[dict[str, CodeBuildStartBuildPropsDef]] = pydantic.Field(None)
    CommonEcsRunTaskProps: typing.Optional[dict[str, CommonEcsRunTaskPropsDef]] = pydantic.Field(None)
    ContainerDefinitionConfig: typing.Optional[dict[str, ContainerDefinitionConfigDef]] = pydantic.Field(None)
    ContainerDefinitionOptions: typing.Optional[dict[str, ContainerDefinitionOptionsDef]] = pydantic.Field(None)
    ContainerOverride: typing.Optional[dict[str, ContainerOverrideDef]] = pydantic.Field(None)
    ContainerOverrides: typing.Optional[dict[str, ContainerOverridesDef]] = pydantic.Field(None)
    DataSource: typing.Optional[dict[str, DataSourceDef]] = pydantic.Field(None)
    DockerImageConfig: typing.Optional[dict[str, DockerImageConfigDef]] = pydantic.Field(None)
    DynamoDeleteItemProps: typing.Optional[dict[str, DynamoDeleteItemPropsDef]] = pydantic.Field(None)
    DynamoGetItemProps: typing.Optional[dict[str, DynamoGetItemPropsDef]] = pydantic.Field(None)
    DynamoPutItemProps: typing.Optional[dict[str, DynamoPutItemPropsDef]] = pydantic.Field(None)
    DynamoUpdateItemProps: typing.Optional[dict[str, DynamoUpdateItemPropsDef]] = pydantic.Field(None)
    EcsEc2LaunchTargetOptions: typing.Optional[dict[str, EcsEc2LaunchTargetOptionsDef]] = pydantic.Field(None)
    EcsFargateLaunchTargetOptions: typing.Optional[dict[str, EcsFargateLaunchTargetOptionsDef]] = pydantic.Field(None)
    EcsLaunchTargetConfig: typing.Optional[dict[str, EcsLaunchTargetConfigDef]] = pydantic.Field(None)
    EcsRunTaskProps: typing.Optional[dict[str, EcsRunTaskPropsDef]] = pydantic.Field(None)
    EksCallProps: typing.Optional[dict[str, EksCallPropsDef]] = pydantic.Field(None)
    EmrAddStepProps: typing.Optional[dict[str, EmrAddStepPropsDef]] = pydantic.Field(None)
    EmrCancelStepProps: typing.Optional[dict[str, EmrCancelStepPropsDef]] = pydantic.Field(None)
    EmrContainersCreateVirtualClusterProps: typing.Optional[dict[str, EmrContainersCreateVirtualClusterPropsDef]] = pydantic.Field(None)
    EmrContainersDeleteVirtualClusterProps: typing.Optional[dict[str, EmrContainersDeleteVirtualClusterPropsDef]] = pydantic.Field(None)
    EmrContainersStartJobRunProps: typing.Optional[dict[str, EmrContainersStartJobRunPropsDef]] = pydantic.Field(None)
    EmrCreateCluster_ApplicationConfigProperty: typing.Optional[dict[str, EmrCreateCluster_ApplicationConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_AutoScalingPolicyProperty: typing.Optional[dict[str, EmrCreateCluster_AutoScalingPolicyPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_BootstrapActionConfigProperty: typing.Optional[dict[str, EmrCreateCluster_BootstrapActionConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_CloudWatchAlarmDefinitionProperty: typing.Optional[dict[str, EmrCreateCluster_CloudWatchAlarmDefinitionPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_ConfigurationProperty: typing.Optional[dict[str, EmrCreateCluster_ConfigurationPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_EbsBlockDeviceConfigProperty: typing.Optional[dict[str, EmrCreateCluster_EbsBlockDeviceConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_EbsConfigurationProperty: typing.Optional[dict[str, EmrCreateCluster_EbsConfigurationPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_InstanceFleetConfigProperty: typing.Optional[dict[str, EmrCreateCluster_InstanceFleetConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_InstanceFleetProvisioningSpecificationsProperty: typing.Optional[dict[str, EmrCreateCluster_InstanceFleetProvisioningSpecificationsPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_InstanceGroupConfigProperty: typing.Optional[dict[str, EmrCreateCluster_InstanceGroupConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_InstancesConfigProperty: typing.Optional[dict[str, EmrCreateCluster_InstancesConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_InstanceTypeConfigProperty: typing.Optional[dict[str, EmrCreateCluster_InstanceTypeConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_KerberosAttributesProperty: typing.Optional[dict[str, EmrCreateCluster_KerberosAttributesPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_MetricDimensionProperty: typing.Optional[dict[str, EmrCreateCluster_MetricDimensionPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_PlacementTypeProperty: typing.Optional[dict[str, EmrCreateCluster_PlacementTypePropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_ScalingActionProperty: typing.Optional[dict[str, EmrCreateCluster_ScalingActionPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_ScalingConstraintsProperty: typing.Optional[dict[str, EmrCreateCluster_ScalingConstraintsPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_ScalingRuleProperty: typing.Optional[dict[str, EmrCreateCluster_ScalingRulePropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_ScalingTriggerProperty: typing.Optional[dict[str, EmrCreateCluster_ScalingTriggerPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_ScriptBootstrapActionConfigProperty: typing.Optional[dict[str, EmrCreateCluster_ScriptBootstrapActionConfigPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_SimpleScalingPolicyConfigurationProperty: typing.Optional[dict[str, EmrCreateCluster_SimpleScalingPolicyConfigurationPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_SpotProvisioningSpecificationProperty: typing.Optional[dict[str, EmrCreateCluster_SpotProvisioningSpecificationPropertyDef]] = pydantic.Field(None)
    EmrCreateCluster_VolumeSpecificationProperty: typing.Optional[dict[str, EmrCreateCluster_VolumeSpecificationPropertyDef]] = pydantic.Field(None)
    EmrCreateClusterProps: typing.Optional[dict[str, EmrCreateClusterPropsDef]] = pydantic.Field(None)
    EmrModifyInstanceFleetByNameProps: typing.Optional[dict[str, EmrModifyInstanceFleetByNamePropsDef]] = pydantic.Field(None)
    EmrModifyInstanceGroupByName_InstanceGroupModifyConfigProperty: typing.Optional[dict[str, EmrModifyInstanceGroupByName_InstanceGroupModifyConfigPropertyDef]] = pydantic.Field(None)
    EmrModifyInstanceGroupByName_InstanceResizePolicyProperty: typing.Optional[dict[str, EmrModifyInstanceGroupByName_InstanceResizePolicyPropertyDef]] = pydantic.Field(None)
    EmrModifyInstanceGroupByName_ShrinkPolicyProperty: typing.Optional[dict[str, EmrModifyInstanceGroupByName_ShrinkPolicyPropertyDef]] = pydantic.Field(None)
    EmrModifyInstanceGroupByNameProps: typing.Optional[dict[str, EmrModifyInstanceGroupByNamePropsDef]] = pydantic.Field(None)
    EmrSetClusterTerminationProtectionProps: typing.Optional[dict[str, EmrSetClusterTerminationProtectionPropsDef]] = pydantic.Field(None)
    EmrTerminateClusterProps: typing.Optional[dict[str, EmrTerminateClusterPropsDef]] = pydantic.Field(None)
    EncryptionConfiguration: typing.Optional[dict[str, EncryptionConfigurationDef]] = pydantic.Field(None)
    EvaluateExpressionProps: typing.Optional[dict[str, EvaluateExpressionPropsDef]] = pydantic.Field(None)
    EventBridgePutEventsEntry: typing.Optional[dict[str, EventBridgePutEventsEntryDef]] = pydantic.Field(None)
    EventBridgePutEventsProps: typing.Optional[dict[str, EventBridgePutEventsPropsDef]] = pydantic.Field(None)
    GlueDataBrewStartJobRunProps: typing.Optional[dict[str, GlueDataBrewStartJobRunPropsDef]] = pydantic.Field(None)
    GlueStartJobRunProps: typing.Optional[dict[str, GlueStartJobRunPropsDef]] = pydantic.Field(None)
    JobDependency: typing.Optional[dict[str, JobDependencyDef]] = pydantic.Field(None)
    JobDriver: typing.Optional[dict[str, JobDriverDef]] = pydantic.Field(None)
    LambdaInvokeProps: typing.Optional[dict[str, LambdaInvokePropsDef]] = pydantic.Field(None)
    LaunchTargetBindOptions: typing.Optional[dict[str, LaunchTargetBindOptionsDef]] = pydantic.Field(None)
    MessageAttribute: typing.Optional[dict[str, MessageAttributeDef]] = pydantic.Field(None)
    MetricDefinition: typing.Optional[dict[str, MetricDefinitionDef]] = pydantic.Field(None)
    ModelClientOptions: typing.Optional[dict[str, ModelClientOptionsDef]] = pydantic.Field(None)
    Monitoring: typing.Optional[dict[str, MonitoringDef]] = pydantic.Field(None)
    OutputDataConfig: typing.Optional[dict[str, OutputDataConfigDef]] = pydantic.Field(None)
    ProductionVariant: typing.Optional[dict[str, ProductionVariantDef]] = pydantic.Field(None)
    QueryExecutionContext: typing.Optional[dict[str, QueryExecutionContextDef]] = pydantic.Field(None)
    ResourceConfig: typing.Optional[dict[str, ResourceConfigDef]] = pydantic.Field(None)
    ResultConfiguration: typing.Optional[dict[str, ResultConfigurationDef]] = pydantic.Field(None)
    S3DataSource: typing.Optional[dict[str, S3DataSourceDef]] = pydantic.Field(None)
    S3LocationBindOptions: typing.Optional[dict[str, S3LocationBindOptionsDef]] = pydantic.Field(None)
    S3LocationConfig: typing.Optional[dict[str, S3LocationConfigDef]] = pydantic.Field(None)
    SageMakerCreateEndpointConfigProps: typing.Optional[dict[str, SageMakerCreateEndpointConfigPropsDef]] = pydantic.Field(None)
    SageMakerCreateEndpointProps: typing.Optional[dict[str, SageMakerCreateEndpointPropsDef]] = pydantic.Field(None)
    SageMakerCreateModelProps: typing.Optional[dict[str, SageMakerCreateModelPropsDef]] = pydantic.Field(None)
    SageMakerCreateTrainingJobProps: typing.Optional[dict[str, SageMakerCreateTrainingJobPropsDef]] = pydantic.Field(None)
    SageMakerCreateTransformJobProps: typing.Optional[dict[str, SageMakerCreateTransformJobPropsDef]] = pydantic.Field(None)
    SageMakerUpdateEndpointProps: typing.Optional[dict[str, SageMakerUpdateEndpointPropsDef]] = pydantic.Field(None)
    ShuffleConfig: typing.Optional[dict[str, ShuffleConfigDef]] = pydantic.Field(None)
    SnsPublishProps: typing.Optional[dict[str, SnsPublishPropsDef]] = pydantic.Field(None)
    SparkSubmitJobDriver: typing.Optional[dict[str, SparkSubmitJobDriverDef]] = pydantic.Field(None)
    SqsSendMessageProps: typing.Optional[dict[str, SqsSendMessagePropsDef]] = pydantic.Field(None)
    StepFunctionsInvokeActivityProps: typing.Optional[dict[str, StepFunctionsInvokeActivityPropsDef]] = pydantic.Field(None)
    StepFunctionsStartExecutionProps: typing.Optional[dict[str, StepFunctionsStartExecutionPropsDef]] = pydantic.Field(None)
    StoppingCondition: typing.Optional[dict[str, StoppingConditionDef]] = pydantic.Field(None)
    TaskEnvironmentVariable: typing.Optional[dict[str, TaskEnvironmentVariableDef]] = pydantic.Field(None)
    TransformDataSource: typing.Optional[dict[str, TransformDataSourceDef]] = pydantic.Field(None)
    TransformInput: typing.Optional[dict[str, TransformInputDef]] = pydantic.Field(None)
    TransformOutput: typing.Optional[dict[str, TransformOutputDef]] = pydantic.Field(None)
    TransformResources: typing.Optional[dict[str, TransformResourcesDef]] = pydantic.Field(None)
    TransformS3DataSource: typing.Optional[dict[str, TransformS3DataSourceDef]] = pydantic.Field(None)
    VpcConfig: typing.Optional[dict[str, VpcConfigDef]] = pydantic.Field(None)
    ...
