from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams

#  autogenerated from aws_cdk.aws_codepipeline_actions.Action
class ActionDef(BaseClass):
    action_name: str = pydantic.Field(..., description='')
    artifact_bounds: typing.Union[models.aws_codepipeline.ActionArtifactBoundsDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    category: aws_cdk.aws_codepipeline.ActionCategory = pydantic.Field(..., description='The category of the action. The category defines which action type the owner (the entity that performs the action) performs.\n')
    provider: str = pydantic.Field(..., description='The service provider that the action calls.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='The account the Action is supposed to live in. For Actions backed by resources, this is inferred from the Stack ``resource`` is part of. However, some Actions, like the CloudFormation ones, are not backed by any resource, and they still might want to be cross-account. In general, a concrete Action class should specify either ``resource``, or ``account`` - but not both.\n')
    inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='')
    outputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='')
    owner: typing.Optional[str] = pydantic.Field(None, description='')
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    resource: typing.Optional[models.AnyResource] = pydantic.Field(None, description='The optional resource that is backing this Action. This is used for automatically handling Actions backed by resources from a different account and/or region.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The order in which AWS CodePipeline runs this action. For more information, see the AWS CodePipeline User Guide. https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html#action-requirements\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description='The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names\n')
    version: typing.Optional[str] = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'artifact_bounds', 'category', 'provider', 'account', 'inputs', 'outputs', 'owner', 'region', 'resource', 'role', 'run_order', 'variables_namespace', 'version']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.Action'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ActionDefConfig] = pydantic.Field(None)


class ActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[ActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class ActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class ActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.AlexaSkillDeployAction
class AlexaSkillDeployActionDef(BaseClass):
    client_id: str = pydantic.Field(..., description='The client id of the developer console token.')
    client_secret: models.SecretValueDef = pydantic.Field(..., description='The client secret of the developer console token.\n')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source artifact containing the voice model and skill manifest.\n')
    refresh_token: models.SecretValueDef = pydantic.Field(..., description='The refresh token of the developer console token.\n')
    skill_id: str = pydantic.Field(..., description='The Alexa skill id.\n')
    parameter_overrides_artifact: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='An optional artifact containing overrides for the skill manifest.\n')
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['client_id', 'client_secret', 'input', 'refresh_token', 'skill_id', 'parameter_overrides_artifact', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.AlexaSkillDeployAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AlexaSkillDeployActionDefConfig] = pydantic.Field(None)


class AlexaSkillDeployActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AlexaSkillDeployActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[AlexaSkillDeployActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class AlexaSkillDeployActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class AlexaSkillDeployActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.BaseJenkinsProvider
class BaseJenkinsProviderDef(BaseClass):
    version: typing.Optional[str] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.BaseJenkinsProvider'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.CacheControl
class CacheControlDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_string', 'max_age', 'must_revalidate', 'no_cache', 'no_transform', 'proxy_revalidate', 's_max_age', 'set_private', 'set_public']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CacheControl'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CacheControlDefConfig] = pydantic.Field(None)


class CacheControlDefConfig(pydantic.BaseModel):
    from_string: typing.Optional[list[CacheControlDefFromStringParams]] = pydantic.Field(None, description='Allows you to create an arbitrary cache control directive, in case our support is missing a method for a particular directive.')
    max_age: typing.Optional[list[CacheControlDefMaxAgeParams]] = pydantic.Field(None, description="The 'max-age' cache control directive.")
    must_revalidate: typing.Optional[list[CacheControlDefMustRevalidateParams]] = pydantic.Field(None, description="The 'must-revalidate' cache control directive.")
    no_cache: typing.Optional[list[CacheControlDefNoCacheParams]] = pydantic.Field(None, description="The 'no-cache' cache control directive.")
    no_transform: typing.Optional[list[CacheControlDefNoTransformParams]] = pydantic.Field(None, description="The 'no-transform' cache control directive.")
    proxy_revalidate: typing.Optional[list[CacheControlDefProxyRevalidateParams]] = pydantic.Field(None, description="The 'proxy-revalidate' cache control directive.")
    s_max_age: typing.Optional[list[CacheControlDefSMaxAgeParams]] = pydantic.Field(None, description="The 's-max-age' cache control directive.")
    set_private: typing.Optional[list[CacheControlDefSetPrivateParams]] = pydantic.Field(None, description="The 'private' cache control directive.")
    set_public: typing.Optional[list[CacheControlDefSetPublicParams]] = pydantic.Field(None, description="The 'public' cache control directive.")

class CacheControlDefFromStringParams(pydantic.BaseModel):
    s: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefMaxAgeParams(pydantic.BaseModel):
    t: models.DurationDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefMustRevalidateParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefNoCacheParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefNoTransformParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefProxyRevalidateParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefSMaxAgeParams(pydantic.BaseModel):
    t: models.DurationDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefSetPrivateParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...

class CacheControlDefSetPublicParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_codepipeline_actions.CacheControlDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationCreateReplaceChangeSetAction
class CloudFormationCreateReplaceChangeSetActionDef(BaseClass):
    admin_permissions: bool = pydantic.Field(..., description="Whether to grant full permissions to CloudFormation while deploying this template. Setting this to ``true`` affects the defaults for ``role`` and ``capabilities``, if you don't specify any alternatives. The default role that will be created for you will have full (i.e., ``*``) permissions on all resources, and the deployment will have named IAM capabilities (i.e., able to create all IAM resources). This is a shorthand that you can use if you fully trust the templates that are deployed in this pipeline. If you want more fine-grained permissions, use ``addToRolePolicy`` and ``capabilities`` to control what the CloudFormation deployment is allowed to do.")
    change_set_name: str = pydantic.Field(..., description='Name of the change set to create or update.\n')
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    template_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description="Input artifact with the ChangeSet's CloudFormation template.\n")
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Acknowledge certain changes made as part of deployment. For stacks that contain certain resources, explicit acknowledgement is required that AWS CloudFormation might create or update those resources. For example, you must specify ``ANONYMOUS_IAM`` or ``NAMED_IAM`` if your stack template contains AWS Identity and Access Management (IAM) resources. For more information, see the link below. Default: None, unless ``adminPermissions`` is true\n')
    deployment_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role to assume when deploying changes. If not specified, a fresh role is created. The role is created with zero permissions unless ``adminPermissions`` is true, in which case the role will have full permissions. Default: A fresh role with full or no permissions (depending on the value of ``adminPermissions``).\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The list of additional input Artifacts for this Action. This is especially useful when used in conjunction with the ``parameterOverrides`` property. For example, if you have: parameterOverrides: { \'Param1\': action1.outputArtifact.bucketName, \'Param2\': action2.outputArtifact.objectKey, } , if the output Artifacts of ``action1`` and ``action2`` were not used to set either the ``templateConfiguration`` or the ``templatePath`` properties, you need to make sure to include them in the ``extraInputs`` - otherwise, you\'ll get an "unrecognized Artifact" error during your Pipeline\'s execution.\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    parameter_overrides: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional template parameters. Template parameters specified here take precedence over template parameters found in the artifact specified by the ``templateConfiguration`` property. We recommend that you use the template configuration file to specify most of your parameter values. Use parameter overrides to specify only dynamic parameter values (values that are unknown until you run the pipeline). All parameter names must be present in the stack template. Note: the entire object cannot be more than 1kB. Default: No overrides\n')
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    template_configuration: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description='Input artifact to use for template parameters values and stack policy. The template configuration file should contain a JSON object that should look like this: ``{ "Parameters": {...}, "Tags": {...}, "StackPolicy": {... }}``. For more information, see `AWS CloudFormation Artifacts <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-cfn-artifacts.html>`_. Note that if you include sensitive information, such as passwords, restrict access to this file. Default: No template configuration based on input artifacts\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['admin_permissions', 'change_set_name', 'stack_name', 'template_path', 'account', 'cfn_capabilities', 'deployment_role', 'extra_inputs', 'output', 'output_file_name', 'parameter_overrides', 'region', 'template_configuration', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['add_to_deployment_role_policy', 'bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationCreateReplaceChangeSetAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationCreateReplaceChangeSetActionDefConfig] = pydantic.Field(None)


class CloudFormationCreateReplaceChangeSetActionDefConfig(pydantic.BaseModel):
    add_to_deployment_role_policy: typing.Optional[list[CloudFormationCreateReplaceChangeSetActionDefAddToDeploymentRolePolicyParams]] = pydantic.Field(None, description='Add statement to the service role assumed by CloudFormation while executing this action.')
    bind: typing.Optional[list[CloudFormationCreateReplaceChangeSetActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CloudFormationCreateReplaceChangeSetActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')
    deployment_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class CloudFormationCreateReplaceChangeSetActionDefAddToDeploymentRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class CloudFormationCreateReplaceChangeSetActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CloudFormationCreateReplaceChangeSetActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationCreateUpdateStackAction
class CloudFormationCreateUpdateStackActionDef(BaseClass):
    admin_permissions: bool = pydantic.Field(..., description="Whether to grant full permissions to CloudFormation while deploying this template. Setting this to ``true`` affects the defaults for ``role`` and ``capabilities``, if you don't specify any alternatives. The default role that will be created for you will have full (i.e., ``*``) permissions on all resources, and the deployment will have named IAM capabilities (i.e., able to create all IAM resources). This is a shorthand that you can use if you fully trust the templates that are deployed in this pipeline. If you want more fine-grained permissions, use ``addToRolePolicy`` and ``capabilities`` to control what the CloudFormation deployment is allowed to do.")
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    template_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='Input artifact with the CloudFormation template to deploy.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Acknowledge certain changes made as part of deployment. For stacks that contain certain resources, explicit acknowledgement is required that AWS CloudFormation might create or update those resources. For example, you must specify ``ANONYMOUS_IAM`` or ``NAMED_IAM`` if your stack template contains AWS Identity and Access Management (IAM) resources. For more information, see the link below. Default: None, unless ``adminPermissions`` is true\n')
    deployment_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role to assume when deploying changes. If not specified, a fresh role is created. The role is created with zero permissions unless ``adminPermissions`` is true, in which case the role will have full permissions. Default: A fresh role with full or no permissions (depending on the value of ``adminPermissions``).\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The list of additional input Artifacts for this Action. This is especially useful when used in conjunction with the ``parameterOverrides`` property. For example, if you have: parameterOverrides: { \'Param1\': action1.outputArtifact.bucketName, \'Param2\': action2.outputArtifact.objectKey, } , if the output Artifacts of ``action1`` and ``action2`` were not used to set either the ``templateConfiguration`` or the ``templatePath`` properties, you need to make sure to include them in the ``extraInputs`` - otherwise, you\'ll get an "unrecognized Artifact" error during your Pipeline\'s execution.\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    parameter_overrides: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional template parameters. Template parameters specified here take precedence over template parameters found in the artifact specified by the ``templateConfiguration`` property. We recommend that you use the template configuration file to specify most of your parameter values. Use parameter overrides to specify only dynamic parameter values (values that are unknown until you run the pipeline). All parameter names must be present in the stack template. Note: the entire object cannot be more than 1kB. Default: No overrides\n')
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    replace_on_failure: typing.Optional[bool] = pydantic.Field(None, description="Replace the stack if it's in a failed state. If this is set to true and the stack is in a failed state (one of ROLLBACK_COMPLETE, ROLLBACK_FAILED, CREATE_FAILED, DELETE_FAILED, or UPDATE_ROLLBACK_FAILED), AWS CloudFormation deletes the stack and then creates a new stack. If this is not set to true and the stack is in a failed state, the deployment fails. Default: false\n")
    template_configuration: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description='Input artifact to use for template parameters values and stack policy. The template configuration file should contain a JSON object that should look like this: ``{ "Parameters": {...}, "Tags": {...}, "StackPolicy": {... }}``. For more information, see `AWS CloudFormation Artifacts <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-cfn-artifacts.html>`_. Note that if you include sensitive information, such as passwords, restrict access to this file. Default: No template configuration based on input artifacts\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['admin_permissions', 'stack_name', 'template_path', 'account', 'cfn_capabilities', 'deployment_role', 'extra_inputs', 'output', 'output_file_name', 'parameter_overrides', 'region', 'replace_on_failure', 'template_configuration', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['add_to_deployment_role_policy', 'bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationCreateUpdateStackAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationCreateUpdateStackActionDefConfig] = pydantic.Field(None)


class CloudFormationCreateUpdateStackActionDefConfig(pydantic.BaseModel):
    add_to_deployment_role_policy: typing.Optional[list[CloudFormationCreateUpdateStackActionDefAddToDeploymentRolePolicyParams]] = pydantic.Field(None, description='Add statement to the service role assumed by CloudFormation while executing this action.')
    bind: typing.Optional[list[CloudFormationCreateUpdateStackActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CloudFormationCreateUpdateStackActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')
    deployment_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class CloudFormationCreateUpdateStackActionDefAddToDeploymentRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class CloudFormationCreateUpdateStackActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CloudFormationCreateUpdateStackActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationDeleteStackAction
class CloudFormationDeleteStackActionDef(BaseClass):
    admin_permissions: bool = pydantic.Field(..., description="Whether to grant full permissions to CloudFormation while deploying this template. Setting this to ``true`` affects the defaults for ``role`` and ``capabilities``, if you don't specify any alternatives. The default role that will be created for you will have full (i.e., ``*``) permissions on all resources, and the deployment will have named IAM capabilities (i.e., able to create all IAM resources). This is a shorthand that you can use if you fully trust the templates that are deployed in this pipeline. If you want more fine-grained permissions, use ``addToRolePolicy`` and ``capabilities`` to control what the CloudFormation deployment is allowed to do.")
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Acknowledge certain changes made as part of deployment. For stacks that contain certain resources, explicit acknowledgement is required that AWS CloudFormation might create or update those resources. For example, you must specify ``ANONYMOUS_IAM`` or ``NAMED_IAM`` if your stack template contains AWS Identity and Access Management (IAM) resources. For more information, see the link below. Default: None, unless ``adminPermissions`` is true\n')
    deployment_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role to assume when deploying changes. If not specified, a fresh role is created. The role is created with zero permissions unless ``adminPermissions`` is true, in which case the role will have full permissions. Default: A fresh role with full or no permissions (depending on the value of ``adminPermissions``).\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The list of additional input Artifacts for this Action. This is especially useful when used in conjunction with the ``parameterOverrides`` property. For example, if you have: parameterOverrides: { \'Param1\': action1.outputArtifact.bucketName, \'Param2\': action2.outputArtifact.objectKey, } , if the output Artifacts of ``action1`` and ``action2`` were not used to set either the ``templateConfiguration`` or the ``templatePath`` properties, you need to make sure to include them in the ``extraInputs`` - otherwise, you\'ll get an "unrecognized Artifact" error during your Pipeline\'s execution.\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    parameter_overrides: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional template parameters. Template parameters specified here take precedence over template parameters found in the artifact specified by the ``templateConfiguration`` property. We recommend that you use the template configuration file to specify most of your parameter values. Use parameter overrides to specify only dynamic parameter values (values that are unknown until you run the pipeline). All parameter names must be present in the stack template. Note: the entire object cannot be more than 1kB. Default: No overrides\n')
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    template_configuration: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description='Input artifact to use for template parameters values and stack policy. The template configuration file should contain a JSON object that should look like this: ``{ "Parameters": {...}, "Tags": {...}, "StackPolicy": {... }}``. For more information, see `AWS CloudFormation Artifacts <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-cfn-artifacts.html>`_. Note that if you include sensitive information, such as passwords, restrict access to this file. Default: No template configuration based on input artifacts\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['admin_permissions', 'stack_name', 'account', 'cfn_capabilities', 'deployment_role', 'extra_inputs', 'output', 'output_file_name', 'parameter_overrides', 'region', 'template_configuration', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['add_to_deployment_role_policy', 'bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationDeleteStackAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationDeleteStackActionDefConfig] = pydantic.Field(None)


class CloudFormationDeleteStackActionDefConfig(pydantic.BaseModel):
    add_to_deployment_role_policy: typing.Optional[list[CloudFormationDeleteStackActionDefAddToDeploymentRolePolicyParams]] = pydantic.Field(None, description='Add statement to the service role assumed by CloudFormation while executing this action.')
    bind: typing.Optional[list[CloudFormationDeleteStackActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CloudFormationDeleteStackActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')
    deployment_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class CloudFormationDeleteStackActionDefAddToDeploymentRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class CloudFormationDeleteStackActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CloudFormationDeleteStackActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackInstancesAction
class CloudFormationDeployStackInstancesActionDef(BaseClass):
    stack_instances: models.aws_codepipeline_actions.StackInstancesDef = pydantic.Field(..., description='Specify where to create or update Stack Instances. You can specify either AWS Accounts Ids or AWS Organizations Organizational Units.')
    stack_set_name: str = pydantic.Field(..., description='The name of the StackSet we are adding instances to.\n')
    parameter_overrides: typing.Optional[models.aws_codepipeline_actions.StackSetParametersDef] = pydantic.Field(None, description='Parameter values that only apply to the current Stack Instances. These parameters are shared between all instances added by this action. Default: - no parameters will be overridden\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    failure_tolerance_percentage: typing.Union[int, float, None] = pydantic.Field(None, description="The percentage of accounts per Region for which this stack operation can fail before AWS CloudFormation stops the operation in that Region. If the operation is stopped in a Region, AWS CloudFormation doesn't attempt the operation in subsequent Regions. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. Default: 0%\n")
    max_account_concurrency_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage of accounts in which to perform this operation at one time. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. If rounding down would result in zero, AWS CloudFormation sets the number as one instead. Although you use this setting to specify the maximum, for large deployments the actual number of accounts acted upon concurrently may be lower due to service throttling. Default: 1%\n')
    stack_set_region: typing.Optional[str] = pydantic.Field(None, description="The AWS Region the StackSet is in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps.crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: - same region as the Pipeline\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['stack_instances', 'stack_set_name', 'parameter_overrides', 'role', 'failure_tolerance_percentage', 'max_account_concurrency_percentage', 'stack_set_region', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackInstancesAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationDeployStackInstancesActionDefConfig] = pydantic.Field(None)


class CloudFormationDeployStackInstancesActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CloudFormationDeployStackInstancesActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CloudFormationDeployStackInstancesActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class CloudFormationDeployStackInstancesActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CloudFormationDeployStackInstancesActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackSetAction
class CloudFormationDeployStackSetActionDef(BaseClass):
    stack_set_name: str = pydantic.Field(..., description='The name to associate with the stack set. This name must be unique in the Region where it is created. The name may only contain alphanumeric and hyphen characters. It must begin with an alphabetic character and be 128 characters or fewer.')
    template: models.aws_codepipeline_actions.StackSetTemplateDef = pydantic.Field(..., description='The location of the template that defines the resources in the stack set. This must point to a template with a maximum size of 460,800 bytes. Enter the path to the source artifact name and template file.\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Indicates that the template can create and update resources, depending on the types of resources in the template. You must use this property if you have IAM resources in your stack template or you create a stack directly from a template containing macros. Default: - the StackSet will have no IAM capabilities\n')
    deployment_model: typing.Optional[models.aws_codepipeline_actions.StackSetDeploymentModelDef] = pydantic.Field(None, description='Determines how IAM roles are created and managed. The choices are: - Self Managed: you create IAM roles with the required permissions in the administration account and all target accounts. - Service Managed: only available if the account and target accounts are part of an AWS Organization. The necessary roles will be created for you. If you want to deploy to all accounts that are a member of AWS Organizations Organizational Units (OUs), you must select Service Managed permissions. Note: This parameter can only be changed when no stack instances exist in the stack set. Default: StackSetDeploymentModel.selfManaged()\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the stack set. You can use this to describe the stack set’s purpose or other relevant information. Default: - no description\n')
    parameters: typing.Optional[models.aws_codepipeline_actions.StackSetParametersDef] = pydantic.Field(None, description='The template parameters for your stack set. These parameters are shared between all instances of the stack set. Default: - no parameters will be used\n')
    stack_instances: typing.Optional[models.aws_codepipeline_actions.StackInstancesDef] = pydantic.Field(None, description="Specify where to create or update Stack Instances. You can specify either AWS Accounts Ids or AWS Organizations Organizational Units. Default: - don't create or update any Stack Instances\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    failure_tolerance_percentage: typing.Union[int, float, None] = pydantic.Field(None, description="The percentage of accounts per Region for which this stack operation can fail before AWS CloudFormation stops the operation in that Region. If the operation is stopped in a Region, AWS CloudFormation doesn't attempt the operation in subsequent Regions. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. Default: 0%\n")
    max_account_concurrency_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage of accounts in which to perform this operation at one time. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. If rounding down would result in zero, AWS CloudFormation sets the number as one instead. Although you use this setting to specify the maximum, for large deployments the actual number of accounts acted upon concurrently may be lower due to service throttling. Default: 1%\n')
    stack_set_region: typing.Optional[str] = pydantic.Field(None, description="The AWS Region the StackSet is in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps.crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: - same region as the Pipeline\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['stack_set_name', 'template', 'cfn_capabilities', 'deployment_model', 'description', 'parameters', 'stack_instances', 'role', 'failure_tolerance_percentage', 'max_account_concurrency_percentage', 'stack_set_region', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackSetAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationDeployStackSetActionDefConfig] = pydantic.Field(None)


class CloudFormationDeployStackSetActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CloudFormationDeployStackSetActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CloudFormationDeployStackSetActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class CloudFormationDeployStackSetActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CloudFormationDeployStackSetActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationExecuteChangeSetAction
class CloudFormationExecuteChangeSetActionDef(BaseClass):
    change_set_name: str = pydantic.Field(..., description='Name of the change set to execute.')
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['change_set_name', 'stack_name', 'account', 'output', 'output_file_name', 'region', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationExecuteChangeSetAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationExecuteChangeSetActionDefConfig] = pydantic.Field(None)


class CloudFormationExecuteChangeSetActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CloudFormationExecuteChangeSetActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CloudFormationExecuteChangeSetActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class CloudFormationExecuteChangeSetActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CloudFormationExecuteChangeSetActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeBuildAction
class CodeBuildActionDef(BaseClass):
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source to use as input for this action.')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description="The action's Project.\n")
    check_secrets_in_plain_text_env_variables: typing.Optional[bool] = pydantic.Field(None, description='Whether to check for the presence of any secrets in the environment variables of the default type, BuildEnvironmentVariableType.PLAINTEXT. Since using a secret for the value of that kind of variable would result in it being displayed in plain text in the AWS Console, the construct will throw an exception if it detects a secret was passed there. Pass this property as false if you want to skip this validation, and keep using a secret in a plain text environment variable. Default: true\n')
    combine_batch_build_artifacts: typing.Optional[bool] = pydantic.Field(None, description='Combine the build artifacts for a batch builds. Enabling this will combine the build artifacts into the same location for batch builds. If ``executeBatchBuild`` is not set to ``true``, this property is ignored. Default: false\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables to pass to the CodeBuild project when this action executes. If a variable with the same name was set both on the project level, and here, this value will take precedence. Default: - No additional environment variables are specified.\n')
    execute_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build. Enabling this will enable batch builds on the CodeBuild project. Default: false\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The list of additional input Artifacts for this action. The directories the additional inputs will be available at are available during the project's build in the CODEBUILD_SRC_DIR_ environment variables. The project's build always starts in the directory with the primary input artifact checked out, the one pointed to by the ``input`` property. For more information, see https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html .\n")
    outputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The list of output Artifacts for this action. **Note**: if you specify more than one output Artifact here, you cannot use the primary 'artifacts' section of the buildspec; you have to use the 'secondary-artifacts' section instead. See https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html for details. Default: the action will not have any outputs\n")
    type: typing.Optional[aws_cdk.aws_codepipeline_actions.CodeBuildActionType] = pydantic.Field(None, description='The type of the action that determines its CodePipeline Category - Build, or Test. Default: CodeBuildActionType.BUILD\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['input', 'project', 'check_secrets_in_plain_text_env_variables', 'combine_batch_build_artifacts', 'environment_variables', 'execute_batch_build', 'extra_inputs', 'outputs', 'type', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change', 'variable']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeBuildAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeBuildActionDefConfig] = pydantic.Field(None)


class CodeBuildActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CodeBuildActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CodeBuildActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')
    variable: typing.Optional[list[CodeBuildActionDefVariableParams]] = pydantic.Field(None, description="Reference a CodePipeline variable defined by the CodeBuild project this action points to.\nVariables in CodeBuild actions are defined using the 'exported-variables' subsection of the 'env'\nsection of the buildspec.")

class CodeBuildActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CodeBuildActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class CodeBuildActionDefVariableParams(pydantic.BaseModel):
    variable_name: str = pydantic.Field(..., description="the name of the variable to reference. A variable by this name must be present in the 'exported-variables' section of the buildspec\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec-ref-syntax\n")
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeCommitSourceAction
class CodeCommitSourceActionDef(BaseClass):
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    repository: typing.Union[models.aws_codecommit.RepositoryDef] = pydantic.Field(..., description='The CodeCommit repository.\n')
    branch: typing.Optional[str] = pydantic.Field(None, description="Default: 'master'\n")
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='Whether the output should be the contents of the repository (which is the default), or a link that allows CodeBuild to clone the repository before building. **Note**: if this option is true, then only CodeBuild actions can use the resulting ``output``. Default: false\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role to be used by on commit event rule. Used only when trigger value is CodeCommitTrigger.EVENTS. Default: a new role will be created.\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.CodeCommitTrigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Default: CodeCommitTrigger.EVENTS\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['output', 'repository', 'branch', 'code_build_clone_output', 'event_role', 'trigger', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeCommitSourceAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeCommitSourceActionDefConfig] = pydantic.Field(None)


class CodeCommitSourceActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CodeCommitSourceActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CodeCommitSourceActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class CodeCommitSourceActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CodeCommitSourceActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeDeployEcsDeployAction
class CodeDeployEcsDeployActionDef(BaseClass):
    deployment_group: typing.Union[models.aws_codedeploy.EcsDeploymentGroupDef] = pydantic.Field(..., description='The CodeDeploy ECS Deployment Group to deploy to.')
    app_spec_template_file: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description="The name of the CodeDeploy AppSpec file. During deployment, a new task definition will be registered with ECS, and the new task definition ID will be inserted into the CodeDeploy AppSpec file. The AppSpec file contents will be provided to CodeDeploy for the deployment. Use this property if you want to use a different name for this file than the default 'appspec.yaml'. If you use this property, you don't need to specify the ``appSpecTemplateInput`` property. Default: - one of this property, or ``appSpecTemplateInput``, is required\n")
    app_spec_template_input: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="The artifact containing the CodeDeploy AppSpec file. During deployment, a new task definition will be registered with ECS, and the new task definition ID will be inserted into the CodeDeploy AppSpec file. The AppSpec file contents will be provided to CodeDeploy for the deployment. If you use this property, it's assumed the file is called 'appspec.yaml'. If your AppSpec file uses a different filename, leave this property empty, and use the ``appSpecTemplateFile`` property instead. Default: - one of this property, or ``appSpecTemplateFile``, is required\n")
    container_image_inputs: typing.Optional[typing.Sequence[typing.Union[models.aws_codepipeline_actions.CodeDeployEcsContainerImageInputDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configuration for dynamically updated images in the task definition. Provide pairs of an image details input artifact and a placeholder string that will be used to dynamically update the ECS task definition template file prior to deployment. A maximum of 4 images can be given.\n')
    task_definition_template_file: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description="The name of the ECS task definition template file. During deployment, the task definition template file contents will be registered with ECS. Use this property if you want to use a different name for this file than the default 'taskdef.json'. If you use this property, you don't need to specify the ``taskDefinitionTemplateInput`` property. Default: - one of this property, or ``taskDefinitionTemplateInput``, is required\n")
    task_definition_template_input: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="The artifact containing the ECS task definition template file. During deployment, the task definition template file contents will be registered with ECS. If you use this property, it's assumed the file is called 'taskdef.json'. If your task definition template uses a different filename, leave this property empty, and use the ``taskDefinitionTemplateFile`` property instead. Default: - one of this property, or ``taskDefinitionTemplateFile``, is required\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['deployment_group', 'app_spec_template_file', 'app_spec_template_input', 'container_image_inputs', 'task_definition_template_file', 'task_definition_template_input', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeDeployEcsDeployAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeDeployEcsDeployActionDefConfig] = pydantic.Field(None)


class CodeDeployEcsDeployActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CodeDeployEcsDeployActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CodeDeployEcsDeployActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class CodeDeployEcsDeployActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CodeDeployEcsDeployActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeDeployServerDeployAction
class CodeDeployServerDeployActionDef(BaseClass):
    deployment_group: typing.Union[models.aws_codedeploy.ServerDeploymentGroupDef] = pydantic.Field(..., description='The CodeDeploy server Deployment Group to deploy to.')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source to use as input for deployment.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['deployment_group', 'input', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeDeployServerDeployAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeDeployServerDeployActionDefConfig] = pydantic.Field(None)


class CodeDeployServerDeployActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CodeDeployServerDeployActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CodeDeployServerDeployActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class CodeDeployServerDeployActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CodeDeployServerDeployActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeStarConnectionsSourceAction
class CodeStarConnectionsSourceActionDef(BaseClass):
    connection_arn: str = pydantic.Field(..., description='The ARN of the CodeStar Connection created in the AWS console that has permissions to access this GitHub or BitBucket repository.')
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The output artifact that this action produces. Can be used as input for further pipeline actions.\n')
    owner: str = pydantic.Field(..., description='The owning user or organization of the repository.\n')
    repo: str = pydantic.Field(..., description='The name of the repository.\n')
    branch: typing.Optional[str] = pydantic.Field(None, description="The branch to build. Default: 'master'\n")
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='Whether the output should be the contents of the repository (which is the default), or a link that allows CodeBuild to clone the repository before building. **Note**: if this option is true, then only CodeBuild actions can use the resulting ``output``. Default: false\n')
    trigger_on_push: typing.Optional[bool] = pydantic.Field(None, description='Controls automatically starting your pipeline when a new commit is made on the configured repository and branch. If unspecified, the default value is true, and the field does not display by default. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['connection_arn', 'output', 'owner', 'repo', 'branch', 'code_build_clone_output', 'trigger_on_push', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeStarConnectionsSourceAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeStarConnectionsSourceActionDefConfig] = pydantic.Field(None)


class CodeStarConnectionsSourceActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[CodeStarConnectionsSourceActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[CodeStarConnectionsSourceActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class CodeStarConnectionsSourceActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class CodeStarConnectionsSourceActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.EcrSourceAction
class EcrSourceActionDef(BaseClass):
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='The repository that will be watched for changes.\n')
    image_tag: typing.Optional[str] = pydantic.Field(None, description="The image tag that will be checked for changes. It is not possible to trigger on changes to more than one tag. Default: 'latest'\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['output', 'repository', 'image_tag', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.EcrSourceAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcrSourceActionDefConfig] = pydantic.Field(None)


class EcrSourceActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[EcrSourceActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[EcrSourceActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class EcrSourceActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class EcrSourceActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.EcsDeployAction
class EcsDeployActionDef(BaseClass):
    service: typing.Union[models.aws_ecs.BaseServiceDef, models.aws_ecs.Ec2ServiceDef, models.aws_ecs.ExternalServiceDef, models.aws_ecs.FargateServiceDef] = pydantic.Field(..., description='The ECS Service to deploy.')
    deployment_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Timeout for the ECS deployment in minutes. Value must be between 1-60. Default: - 60 minutes\n')
    image_file: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description="The name of the JSON image definitions file to use for deployments. The JSON file is a list of objects, each with 2 keys: ``name`` is the name of the container in the Task Definition, and ``imageUri`` is the Docker image URI you want to update your service with. Use this property if you want to use a different name for this file than the default 'imagedefinitions.json'. If you use this property, you don't need to specify the ``input`` property. Default: - one of this property, or ``input``, is required\n")
    input: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="The input artifact that contains the JSON image definitions file to use for deployments. The JSON file is a list of objects, each with 2 keys: ``name`` is the name of the container in the Task Definition, and ``imageUri`` is the Docker image URI you want to update your service with. If you use this property, it's assumed the file is called 'imagedefinitions.json'. If your build uses a different file, leave this property empty, and use the ``imageFile`` property instead. Default: - one of this property, or ``imageFile``, is required\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['service', 'deployment_timeout', 'image_file', 'input', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.EcsDeployAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcsDeployActionDefConfig] = pydantic.Field(None)


class EcsDeployActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[EcsDeployActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[EcsDeployActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class EcsDeployActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class EcsDeployActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.ElasticBeanstalkDeployAction
class ElasticBeanstalkDeployActionDef(BaseClass):
    application_name: str = pydantic.Field(..., description='The name of the AWS Elastic Beanstalk application to deploy.')
    environment_name: str = pydantic.Field(..., description='The name of the AWS Elastic Beanstalk environment to deploy to.\n')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source to use as input for deployment.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['application_name', 'environment_name', 'input', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.ElasticBeanstalkDeployAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ElasticBeanstalkDeployActionDefConfig] = pydantic.Field(None)


class ElasticBeanstalkDeployActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ElasticBeanstalkDeployActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[ElasticBeanstalkDeployActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class ElasticBeanstalkDeployActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class ElasticBeanstalkDeployActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.GitHubSourceAction
class GitHubSourceActionDef(BaseClass):
    oauth_token: models.SecretValueDef = pydantic.Field(..., description="A GitHub OAuth token to use for authentication. It is recommended to use a Secrets Manager ``Secret`` to obtain the token: const oauth = cdk.SecretValue.secretsManager('my-github-token'); new GitHubSourceAction(this, 'GitHubAction', { oauthToken: oauth, ... }); If you rotate the value in the Secret, you must also change at least one property of the CodePipeline to force CloudFormation to re-read the secret. The GitHub Personal Access Token should have these scopes: - **repo** - to read the repository - **admin:repo_hook** - if you plan to use webhooks (true by default)")
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    owner: str = pydantic.Field(..., description='The GitHub account/user that owns the repo.\n')
    repo: str = pydantic.Field(..., description='The name of the repo, without the username.\n')
    branch: typing.Optional[str] = pydantic.Field(None, description='The branch to use. Default: "master"\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.GitHubTrigger] = pydantic.Field(None, description='How AWS CodePipeline should be triggered. With the default value "WEBHOOK", a webhook is created in GitHub that triggers the action With "POLL", CodePipeline periodically checks the source for changes With "None", the action is not triggered through changes in the source To use ``WEBHOOK``, your GitHub Personal Access Token should have **admin:repo_hook** scope (in addition to the regular **repo** scope). Default: GitHubTrigger.WEBHOOK\n')
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['oauth_token', 'output', 'owner', 'repo', 'branch', 'trigger', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.GitHubSourceAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[GitHubSourceActionDefConfig] = pydantic.Field(None)


class GitHubSourceActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[GitHubSourceActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[GitHubSourceActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class GitHubSourceActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class GitHubSourceActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.JenkinsAction
class JenkinsActionDef(BaseClass):
    jenkins_provider: typing.Union[models.aws_codepipeline_actions.BaseJenkinsProviderDef, models.aws_codepipeline_actions.JenkinsProviderDef] = pydantic.Field(..., description='The Jenkins Provider for this Action.')
    project_name: str = pydantic.Field(..., description='The name of the project (sometimes also called job, or task) on your Jenkins installation that will be invoked by this Action.\n')
    type: aws_cdk.aws_codepipeline_actions.JenkinsActionType = pydantic.Field(..., description='The type of the Action - Build, or Test.\n')
    inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The source to use as input for this build.\n')
    outputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='')
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['jenkins_provider', 'project_name', 'type', 'inputs', 'outputs', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.JenkinsAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[JenkinsActionDefConfig] = pydantic.Field(None)


class JenkinsActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[JenkinsActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[JenkinsActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class JenkinsActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class JenkinsActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.LambdaInvokeAction
class LambdaInvokeActionDef(BaseClass):
    lambda_: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='The lambda function to invoke.')
    inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The optional input Artifacts of the Action. A Lambda Action can have up to 5 inputs. The inputs will appear in the event passed to the Lambda, under the ``'CodePipeline.job'.data.inputArtifacts`` path. Default: the Action will not have any inputs\n")
    outputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The optional names of the output Artifacts of the Action. A Lambda Action can have up to 5 outputs. The outputs will appear in the event passed to the Lambda, under the ``'CodePipeline.job'.data.outputArtifacts`` path. It is the responsibility of the Lambda to upload ZIP files with the Artifact contents to the provided locations. Default: the Action will not have any outputs\n")
    user_parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='A set of key-value pairs that will be accessible to the invoked Lambda inside the event that the Pipeline will call it with. Only one of ``userParameters`` or ``userParametersString`` can be specified. Default: - no user parameters will be passed\n')
    user_parameters_string: typing.Optional[str] = pydantic.Field(None, description='The string representation of the user parameters that will be accessible to the invoked Lambda inside the event that the Pipeline will call it with. Only one of ``userParametersString`` or ``userParameters`` can be specified. Default: - no user parameters will be passed\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['lambda_', 'inputs', 'outputs', 'user_parameters', 'user_parameters_string', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change', 'variable']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.LambdaInvokeAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[LambdaInvokeActionDefConfig] = pydantic.Field(None)


class LambdaInvokeActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[LambdaInvokeActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[LambdaInvokeActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')
    variable: typing.Optional[list[LambdaInvokeActionDefVariableParams]] = pydantic.Field(None, description="Reference a CodePipeline variable defined by the Lambda function this action points to.\nVariables in Lambda invoke actions are defined by calling the PutJobSuccessResult CodePipeline API call\nwith the 'outputVariables' property filled.")

class LambdaInvokeActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class LambdaInvokeActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class LambdaInvokeActionDefVariableParams(pydantic.BaseModel):
    variable_name: str = pydantic.Field(..., description="the name of the variable to reference. A variable by this name must be present in the 'outputVariables' section of the PutJobSuccessResult request that the Lambda function calls when the action is invoked\n\n:see: https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobSuccessResult.html\n")
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.ManualApprovalAction
class ManualApprovalActionDef(BaseClass):
    additional_information: typing.Optional[str] = pydantic.Field(None, description='Any additional information that you want to include in the notification email message.')
    external_entity_link: typing.Optional[str] = pydantic.Field(None, description='URL you want to provide to the reviewer as part of the approval request. Default: - the approval request will not have an external link\n')
    notification_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='Optional SNS topic to send notifications to when an approval is pending.\n')
    notify_emails: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of email addresses to subscribe to notifications when this Action is pending approval. If this has been provided, but not ``notificationTopic``, a new Topic will be created.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['additional_information', 'external_entity_link', 'notification_topic', 'notify_emails', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'grant_manual_approval', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.ManualApprovalAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ManualApprovalActionDefConfig] = pydantic.Field(None)


class ManualApprovalActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ManualApprovalActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    grant_manual_approval: typing.Optional[list[ManualApprovalActionDefGrantManualApprovalParams]] = pydantic.Field(None, description='grant the provided principal the permissions to approve or reject this manual approval action.\nFor more info see:\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-iam-permissions.html')
    on_state_change: typing.Optional[list[ManualApprovalActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class ManualApprovalActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class ManualApprovalActionDefGrantManualApprovalParams(pydantic.BaseModel):
    grantable: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the grantable to attach the permissions to.')
    ...

class ManualApprovalActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.S3DeployAction
class S3DeployActionDef(BaseClass):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The Amazon S3 bucket that is the deploy target.')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The input Artifact to deploy to Amazon S3.\n')
    access_control: typing.Optional[aws_cdk.aws_s3.BucketAccessControl] = pydantic.Field(None, description='The specified canned ACL to objects deployed to Amazon S3. This overwrites any existing ACL that was applied to the object. Default: - the original object ACL\n')
    cache_control: typing.Optional[typing.Sequence[models.aws_codepipeline_actions.CacheControlDef]] = pydantic.Field(None, description='The caching behavior for requests/responses for objects in the bucket. The final cache control property will be the result of joining all of the provided array elements with a comma (plus a space after the comma). Default: - none, decided by the HTTP client\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS encryption key for the host bucket. The encryptionKey parameter encrypts uploaded artifacts with the provided AWS KMS key. Default: - none\n')
    extract: typing.Optional[bool] = pydantic.Field(None, description='Should the deploy action extract the artifact before deploying to Amazon S3. Default: true\n')
    object_key: typing.Optional[str] = pydantic.Field(None, description='The key of the target object. This is required if extract is false.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'input', 'access_control', 'cache_control', 'encryption_key', 'extract', 'object_key', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.S3DeployAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[S3DeployActionDefConfig] = pydantic.Field(None)


class S3DeployActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[S3DeployActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[S3DeployActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class S3DeployActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class S3DeployActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.S3SourceAction
class S3SourceActionDef(BaseClass):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The Amazon S3 bucket that stores the source code. If you import an encrypted bucket in your stack, please specify the encryption key at import time by using ``Bucket.fromBucketAttributes()`` method.')
    bucket_key: str = pydantic.Field(..., description='The key within the S3 bucket that stores the source code.\n')
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.S3Trigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Note that if this is S3Trigger.EVENTS, you need to make sure to include the source Bucket in a CloudTrail Trail, as otherwise the CloudWatch Events will not be emitted. Default: S3Trigger.POLL\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_key', 'output', 'trigger', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.S3SourceAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[S3SourceActionDefConfig] = pydantic.Field(None)


class S3SourceActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[S3SourceActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[S3SourceActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class S3SourceActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class S3SourceActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.ServiceCatalogDeployActionBeta1
class ServiceCatalogDeployActionBeta1Def(BaseClass):
    product_id: str = pydantic.Field(..., description='The identifier of the product in the Service Catalog. This product must already exist.')
    product_version_name: str = pydantic.Field(..., description='The name of the version of the Service Catalog product to be deployed.\n')
    template_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='The path to the cloudformation artifact.\n')
    product_version_description: typing.Optional[str] = pydantic.Field(None, description="The optional description of this version of the Service Catalog product. Default: ''\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['product_id', 'product_version_name', 'template_path', 'product_version_description', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.ServiceCatalogDeployActionBeta1'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ServiceCatalogDeployActionBeta1DefConfig] = pydantic.Field(None)


class ServiceCatalogDeployActionBeta1DefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ServiceCatalogDeployActionBeta1DefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[ServiceCatalogDeployActionBeta1DefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class ServiceCatalogDeployActionBeta1DefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class ServiceCatalogDeployActionBeta1DefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.StackInstances
class StackInstancesDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_artifact_path', 'in_accounts', 'in_organizational_units']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.StackInstances'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StackInstancesDefConfig] = pydantic.Field(None)


class StackInstancesDefConfig(pydantic.BaseModel):
    from_artifact_path: typing.Optional[list[StackInstancesDefFromArtifactPathParams]] = pydantic.Field(None, description='Create stack instances in a set of accounts or organizational units taken from the pipeline artifacts, and a set of regions  The file must be a JSON file containing a list of strings.\nFor example::\n\n   [\n     "111111111111",\n     "222222222222",\n     "333333333333"\n   ]\n\nStack Instances will be created in every combination of region and account, or region and\nOrganizational Units (OUs).\n\nIf this is set of Organizational Units, you must have selected ``StackSetDeploymentModel.organizations()``\nas deployment model.')
    in_accounts: typing.Optional[list[StackInstancesDefInAccountsParams]] = pydantic.Field(None, description="Create stack instances in a set of accounts and regions passed as literal lists.\nStack Instances will be created in every combination of region and account.\n.. epigraph::\n\n   NOTE: ``StackInstances.inAccounts()`` and ``StackInstances.inOrganizationalUnits()``\n   have exactly the same behavior, and you can use them interchangeably if you want.\n   The only difference between them is that your code clearly indicates what entity\n   it's working with.")
    in_organizational_units: typing.Optional[list[StackInstancesDefInOrganizationalUnitsParams]] = pydantic.Field(None, description="Create stack instances in all accounts in a set of Organizational Units (OUs) and regions passed as literal lists.\nIf you want to deploy to Organization Units, you must choose have created the StackSet\nwith ``deploymentModel: DeploymentModel.organizations()``.\n\nStack Instances will be created in every combination of region and account.\n.. epigraph::\n\n   NOTE: ``StackInstances.inAccounts()`` and ``StackInstances.inOrganizationalUnits()``\n   have exactly the same behavior, and you can use them interchangeably if you want.\n   The only difference between them is that your code clearly indicates what entity\n   it's working with.")

class StackInstancesDefFromArtifactPathParams(pydantic.BaseModel):
    artifact_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='-\n')
    regions: typing.Sequence[str] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackInstancesDefConfig]] = pydantic.Field(None)
    ...

class StackInstancesDefInAccountsParams(pydantic.BaseModel):
    accounts: typing.Sequence[str] = pydantic.Field(..., description='-\n')
    regions: typing.Sequence[str] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackInstancesDefConfig]] = pydantic.Field(None)
    ...

class StackInstancesDefInOrganizationalUnitsParams(pydantic.BaseModel):
    ous: typing.Sequence[str] = pydantic.Field(..., description='-\n')
    regions: typing.Sequence[str] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackInstancesDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.StackSetDeploymentModel
class StackSetDeploymentModelDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['organizations', 'self_managed']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.StackSetDeploymentModel'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StackSetDeploymentModelDefConfig] = pydantic.Field(None)


class StackSetDeploymentModelDefConfig(pydantic.BaseModel):
    organizations: typing.Optional[list[StackSetDeploymentModelDefOrganizationsParams]] = pydantic.Field(None, description='Deploy to AWS Organizations accounts.\nAWS CloudFormation StackSets automatically creates the IAM roles required\nto deploy to accounts managed by AWS Organizations. This requires an\naccount to be a member of an Organization.\n\nUsing this deployment model, you can specify either AWS Account Ids or\nOrganization Unit Ids in the ``stackInstances`` parameter.')
    self_managed: typing.Optional[list[StackSetDeploymentModelDefSelfManagedParams]] = pydantic.Field(None, description='Deploy to AWS Accounts not managed by AWS Organizations.\nYou are responsible for creating Execution Roles in every account you will\nbe deploying to in advance to create the actual stack instances. Unless you\nspecify overrides, StackSets expects the execution roles you create to have\nthe default name ``AWSCloudFormationStackSetExecutionRole``. See the `Grant\nself-managed\npermissions <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html>`_\nsection of the CloudFormation documentation.\n\nThe CDK will automatically create the central Administration Role in the\nPipeline account which will be used to assume the Execution Role in each of\nthe target accounts.\n\nIf you wish to use a pre-created Administration Role, use ``Role.fromRoleName()``\nor ``Role.fromRoleArn()`` to import it, and pass it to this function::\n\n   existing_admin_role = iam.Role.from_role_name(self, "AdminRole", "AWSCloudFormationStackSetAdministrationRole")\n\n   deployment_model = codepipeline_actions.StackSetDeploymentModel.self_managed(\n       # Use an existing Role. Leave this out to create a new Role.\n       administration_role=existing_admin_role\n   )\n\nUsing this deployment model, you can only specify AWS Account Ids in the\n``stackInstances`` parameter.')

class StackSetDeploymentModelDefOrganizationsParams(pydantic.BaseModel):
    auto_deployment: typing.Optional[aws_cdk.aws_codepipeline_actions.StackSetOrganizationsAutoDeployment] = pydantic.Field(None, description='Automatically deploy to new accounts added to Organizational Units. Whether AWS CloudFormation StackSets automatically deploys to AWS Organizations accounts that are added to a target organization or organizational unit (OU). Default: Disabled')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackSetDeploymentModelDefConfig]] = pydantic.Field(None)
    ...

class StackSetDeploymentModelDefSelfManagedParams(pydantic.BaseModel):
    administration_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role in the administrator account used to assume execution roles in the target accounts. You must create this role before using the StackSet action. The role needs to be assumable by CloudFormation, and it needs to be able to ``sts:AssumeRole`` each of the execution roles (whose names are specified in the ``executionRoleName`` parameter) in each of the target accounts. If you do not specify the role, we assume you have created a role named ``AWSCloudFormationStackSetAdministrationRole``. Default: - Assume an existing role named ``AWSCloudFormationStackSetAdministrationRole`` in the same account as the pipeline.\n')
    execution_role_name: typing.Optional[str] = pydantic.Field(None, description='The name of the IAM role in the target accounts used to perform stack set operations. You must create these roles in each of the target accounts before using the StackSet action. The roles need to be assumable by by the ``administrationRole``, and need to have the permissions necessary to successfully create and modify the resources that the subsequent CloudFormation deployments need. Administrator permissions would be commonly granted to these, but if you can scope the permissions down frome there you would be safer. Default: AWSCloudFormationStackSetExecutionRole\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\n')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackSetDeploymentModelDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.StackSetParameters
class StackSetParametersDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_artifact_path', 'from_literal']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.StackSetParameters'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StackSetParametersDefConfig] = pydantic.Field(None)


class StackSetParametersDefConfig(pydantic.BaseModel):
    from_artifact_path: typing.Optional[list[StackSetParametersDefFromArtifactPathParams]] = pydantic.Field(None, description='Read the parameters from a JSON file from one of the pipeline\'s artifacts.\nThe file needs to contain a list of ``{ ParameterKey, ParameterValue, UsePreviousValue }`` objects, like\nthis::\n\n   [\n       {\n           "ParameterKey": "BucketName",\n           "ParameterValue": "my-bucket"\n       },\n       {\n           "ParameterKey": "Asset1",\n           "ParameterValue": "true"\n       },\n       {\n           "ParameterKey": "Asset2",\n           "UsePreviousValue": true\n       }\n   ]\n\nYou must specify all template parameters. Parameters you don\'t specify will revert\nto their ``Default`` values as specified in the template.\n\nFor of parameters you want to retain their existing values\nwithout specifying what those values are, set ``UsePreviousValue: true``.\nUse of this feature is discouraged. CDK is for\nspecifying desired-state infrastructure, and use of this feature makes the\nparameter values unmanaged.')
    from_literal: typing.Optional[list[StackSetParametersDefFromLiteralParams]] = pydantic.Field(None, description="A list of template parameters for your stack set.\nYou must specify all template parameters. Parameters you don't specify will revert\nto their ``Default`` values as specified in the template.\n\nSpecify the names of parameters you want to retain their existing values,\nwithout specifying what those values are, in an array in the second\nargument to this function. Use of this feature is discouraged. CDK is for\nspecifying desired-state infrastructure, and use of this feature makes the\nparameter values unmanaged.")

class StackSetParametersDefFromArtifactPathParams(pydantic.BaseModel):
    artifact_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackSetParametersDefConfig]] = pydantic.Field(None)
    ...

class StackSetParametersDefFromLiteralParams(pydantic.BaseModel):
    parameters: typing.Mapping[str, str] = pydantic.Field(..., description='-\n')
    use_previous_values: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='-\n\nExample::\n\n    parameters = codepipeline_actions.StackSetParameters.from_literal({\n        "BucketName": "my-bucket",\n        "Asset1": "true"\n    })\n')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackSetParametersDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.StackSetTemplate
class StackSetTemplateDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_artifact_path']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.StackSetTemplate'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StackSetTemplateDefConfig] = pydantic.Field(None)


class StackSetTemplateDefConfig(pydantic.BaseModel):
    from_artifact_path: typing.Optional[list[StackSetTemplateDefFromArtifactPathParams]] = pydantic.Field(None, description='Use a file in an artifact as Stack Template.')

class StackSetTemplateDefFromArtifactPathParams(pydantic.BaseModel):
    artifact_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StackSetTemplateDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.StateMachineInput
class StateMachineInputDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['file_path', 'literal']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.StateMachineInput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StateMachineInputDefConfig] = pydantic.Field(None)


class StateMachineInputDefConfig(pydantic.BaseModel):
    file_path: typing.Optional[list[StateMachineInputDefFilePathParams]] = pydantic.Field(None, description='When the input type is FilePath, input artifact and filepath must be specified.')
    literal: typing.Optional[list[StateMachineInputDefLiteralParams]] = pydantic.Field(None, description='When the input type is Literal, input value is passed directly to the state machine input.')

class StateMachineInputDefFilePathParams(pydantic.BaseModel):
    input_file: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StateMachineInputDefConfig]] = pydantic.Field(None)
    ...

class StateMachineInputDefLiteralParams(pydantic.BaseModel):
    object: typing.Mapping[typing.Any, typing.Any] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_codepipeline_actions.StateMachineInputDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.StepFunctionInvokeAction
class StepFunctionInvokeActionDef(BaseClass):
    state_machine: typing.Union[models.aws_stepfunctions.StateMachineDef] = pydantic.Field(..., description='The state machine to invoke.')
    execution_name_prefix: typing.Optional[str] = pydantic.Field(None, description='Prefix (optional). By default, the action execution ID is used as the state machine execution name. If a prefix is provided, it is prepended to the action execution ID with a hyphen and together used as the state machine execution name. Default: - action execution ID\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The optional output Artifact of the Action. Default: the Action will not have any outputs\n')
    state_machine_input: typing.Optional[models.aws_codepipeline_actions.StateMachineInputDef] = pydantic.Field(None, description='Represents the input to the StateMachine. This includes input artifact, input type and the statemachine input. Default: - none\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set")
    _init_params: typing.ClassVar[list[str]] = ['state_machine', 'execution_name_prefix', 'output', 'state_machine_input', 'role', 'action_name', 'run_order', 'variables_namespace']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'on_state_change']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.StepFunctionInvokeAction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StepFunctionInvokeActionDefConfig] = pydantic.Field(None)


class StepFunctionInvokeActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[StepFunctionInvokeActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[StepFunctionInvokeActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')

class StepFunctionInvokeActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')
    ...

class StepFunctionInvokeActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.JenkinsProvider
class JenkinsProviderDef(BaseConstruct):
    provider_name: str = pydantic.Field(..., description='The name of the Jenkins provider that you set in the AWS CodePipeline plugin configuration of your Jenkins project.\n')
    server_url: str = pydantic.Field(..., description='The base URL of your Jenkins server.\n')
    for_build: typing.Optional[bool] = pydantic.Field(None, description='Whether to immediately register a Jenkins Provider for the build category. The Provider will always be registered if you create a ``JenkinsAction``. Default: false\n')
    for_test: typing.Optional[bool] = pydantic.Field(None, description='Whether to immediately register a Jenkins Provider for the test category. The Provider will always be registered if you create a ``JenkinsTestAction``. Default: false\n')
    version: typing.Optional[str] = pydantic.Field(None, description="The version of your provider. Default: '1'")
    _init_params: typing.ClassVar[list[str]] = ['provider_name', 'server_url', 'for_build', 'for_test', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_jenkins_provider_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.JenkinsProvider'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_jenkins_provider_attributes']
    ...


    from_jenkins_provider_attributes: typing.Optional[JenkinsProviderDefFromJenkinsProviderAttributesParams] = pydantic.Field(None, description='Import a Jenkins provider registered either outside the CDK, or in a different CDK Stack.')

class JenkinsProviderDefFromJenkinsProviderAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='the parent Construct for the new provider.\n')
    id: str = pydantic.Field(..., description='the identifier of the new provider Construct.\n')
    provider_name: str = pydantic.Field(..., description='The name of the Jenkins provider that you set in the AWS CodePipeline plugin configuration of your Jenkins project.\n')
    server_url: str = pydantic.Field(..., description='The base URL of your Jenkins server.\n')
    version: typing.Optional[str] = pydantic.Field(None, description="The version of your provider. Default: '1'\n")
    ...


#  autogenerated from aws_cdk.aws_codepipeline_actions.AlexaSkillDeployActionProps
class AlexaSkillDeployActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    client_id: str = pydantic.Field(..., description='The client id of the developer console token.\n')
    client_secret: models.SecretValueDef = pydantic.Field(..., description='The client secret of the developer console token.\n')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source artifact containing the voice model and skill manifest.\n')
    refresh_token: models.SecretValueDef = pydantic.Field(..., description='The refresh token of the developer console token.\n')
    skill_id: str = pydantic.Field(..., description='The Alexa skill id.\n')
    parameter_overrides_artifact: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='An optional artifact containing overrides for the skill manifest.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Read the secrets from ParameterStore\n    client_id = SecretValue.secrets_manager("AlexaClientId")\n    client_secret = SecretValue.secrets_manager("AlexaClientSecret")\n    refresh_token = SecretValue.secrets_manager("AlexaRefreshToken")\n\n    # Add deploy action\n    source_output = codepipeline.Artifact()\n    codepipeline_actions.AlexaSkillDeployAction(\n        action_name="DeploySkill",\n        run_order=1,\n        input=source_output,\n        client_id=client_id.to_string(),\n        client_secret=client_secret,\n        refresh_token=refresh_token,\n        skill_id="amzn1.ask.skill.12345678-1234-1234-1234-123456789012"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'client_id', 'client_secret', 'input', 'refresh_token', 'skill_id', 'parameter_overrides_artifact']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.AlexaSkillDeployActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AlexaSkillDeployActionPropsDefConfig] = pydantic.Field(None)


class AlexaSkillDeployActionPropsDefConfig(pydantic.BaseModel):
    client_secret_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)
    input_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)
    refresh_token_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationCreateReplaceChangeSetActionProps
class CloudFormationCreateReplaceChangeSetActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    admin_permissions: bool = pydantic.Field(..., description="Whether to grant full permissions to CloudFormation while deploying this template. Setting this to ``true`` affects the defaults for ``role`` and ``capabilities``, if you don't specify any alternatives. The default role that will be created for you will have full (i.e., ``*``) permissions on all resources, and the deployment will have named IAM capabilities (i.e., able to create all IAM resources). This is a shorthand that you can use if you fully trust the templates that are deployed in this pipeline. If you want more fine-grained permissions, use ``addToRolePolicy`` and ``capabilities`` to control what the CloudFormation deployment is allowed to do.\n")
    change_set_name: str = pydantic.Field(..., description='Name of the change set to create or update.\n')
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    template_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description="Input artifact with the ChangeSet's CloudFormation template.\n")
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Acknowledge certain changes made as part of deployment. For stacks that contain certain resources, explicit acknowledgement is required that AWS CloudFormation might create or update those resources. For example, you must specify ``ANONYMOUS_IAM`` or ``NAMED_IAM`` if your stack template contains AWS Identity and Access Management (IAM) resources. For more information, see the link below. Default: None, unless ``adminPermissions`` is true\n')
    deployment_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role to assume when deploying changes. If not specified, a fresh role is created. The role is created with zero permissions unless ``adminPermissions`` is true, in which case the role will have full permissions. Default: A fresh role with full or no permissions (depending on the value of ``adminPermissions``).\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The list of additional input Artifacts for this Action. This is especially useful when used in conjunction with the ``parameterOverrides`` property. For example, if you have: parameterOverrides: { \'Param1\': action1.outputArtifact.bucketName, \'Param2\': action2.outputArtifact.objectKey, } , if the output Artifacts of ``action1`` and ``action2`` were not used to set either the ``templateConfiguration`` or the ``templatePath`` properties, you need to make sure to include them in the ``extraInputs`` - otherwise, you\'ll get an "unrecognized Artifact" error during your Pipeline\'s execution.\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    parameter_overrides: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional template parameters. Template parameters specified here take precedence over template parameters found in the artifact specified by the ``templateConfiguration`` property. We recommend that you use the template configuration file to specify most of your parameter values. Use parameter overrides to specify only dynamic parameter values (values that are unknown until you run the pipeline). All parameter names must be present in the stack template. Note: the entire object cannot be more than 1kB. Default: No overrides\n')
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    template_configuration: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description='Input artifact to use for template parameters values and stack policy. The template configuration file should contain a JSON object that should look like this: ``{ "Parameters": {...}, "Tags": {...}, "StackPolicy": {... }}``. For more information, see `AWS CloudFormation Artifacts <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-cfn-artifacts.html>`_. Note that if you include sensitive information, such as passwords, restrict access to this file. Default: No template configuration based on input artifacts\n\n:exampleMetadata: lit=aws-codepipeline-actions/test/integ.cfn-template-from-repo.lit.ts infused\n\nExample::\n\n    # Source stage: read from repository\n    repo = codecommit.Repository(stack, "TemplateRepo",\n        repository_name="template-repo"\n    )\n    source_output = codepipeline.Artifact("SourceArtifact")\n    source = cpactions.CodeCommitSourceAction(\n        action_name="Source",\n        repository=repo,\n        output=source_output,\n        trigger=cpactions.CodeCommitTrigger.POLL\n    )\n    source_stage = {\n        "stage_name": "Source",\n        "actions": [source]\n    }\n\n    # Deployment stage: create and deploy changeset with manual approval\n    stack_name = "OurStack"\n    change_set_name = "StagedChangeSet"\n\n    prod_stage = {\n        "stage_name": "Deploy",\n        "actions": [\n            cpactions.CloudFormationCreateReplaceChangeSetAction(\n                action_name="PrepareChanges",\n                stack_name=stack_name,\n                change_set_name=change_set_name,\n                admin_permissions=True,\n                template_path=source_output.at_path("template.yaml"),\n                run_order=1\n            ),\n            cpactions.ManualApprovalAction(\n                action_name="ApproveChanges",\n                run_order=2\n            ),\n            cpactions.CloudFormationExecuteChangeSetAction(\n                action_name="ExecuteChanges",\n                stack_name=stack_name,\n                change_set_name=change_set_name,\n                run_order=3\n            )\n        ]\n    }\n\n    codepipeline.Pipeline(stack, "Pipeline",\n        stages=[source_stage, prod_stage\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'admin_permissions', 'change_set_name', 'stack_name', 'template_path', 'account', 'cfn_capabilities', 'deployment_role', 'extra_inputs', 'output', 'output_file_name', 'parameter_overrides', 'region', 'template_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationCreateReplaceChangeSetActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationCreateReplaceChangeSetActionPropsDefConfig] = pydantic.Field(None)


class CloudFormationCreateReplaceChangeSetActionPropsDefConfig(pydantic.BaseModel):
    template_path_config: typing.Optional[models.aws_codepipeline.ArtifactPathDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationCreateUpdateStackActionProps
class CloudFormationCreateUpdateStackActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    admin_permissions: bool = pydantic.Field(..., description="Whether to grant full permissions to CloudFormation while deploying this template. Setting this to ``true`` affects the defaults for ``role`` and ``capabilities``, if you don't specify any alternatives. The default role that will be created for you will have full (i.e., ``*``) permissions on all resources, and the deployment will have named IAM capabilities (i.e., able to create all IAM resources). This is a shorthand that you can use if you fully trust the templates that are deployed in this pipeline. If you want more fine-grained permissions, use ``addToRolePolicy`` and ``capabilities`` to control what the CloudFormation deployment is allowed to do.\n")
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    template_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='Input artifact with the CloudFormation template to deploy.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Acknowledge certain changes made as part of deployment. For stacks that contain certain resources, explicit acknowledgement is required that AWS CloudFormation might create or update those resources. For example, you must specify ``ANONYMOUS_IAM`` or ``NAMED_IAM`` if your stack template contains AWS Identity and Access Management (IAM) resources. For more information, see the link below. Default: None, unless ``adminPermissions`` is true\n')
    deployment_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role to assume when deploying changes. If not specified, a fresh role is created. The role is created with zero permissions unless ``adminPermissions`` is true, in which case the role will have full permissions. Default: A fresh role with full or no permissions (depending on the value of ``adminPermissions``).\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The list of additional input Artifacts for this Action. This is especially useful when used in conjunction with the ``parameterOverrides`` property. For example, if you have: parameterOverrides: { \'Param1\': action1.outputArtifact.bucketName, \'Param2\': action2.outputArtifact.objectKey, } , if the output Artifacts of ``action1`` and ``action2`` were not used to set either the ``templateConfiguration`` or the ``templatePath`` properties, you need to make sure to include them in the ``extraInputs`` - otherwise, you\'ll get an "unrecognized Artifact" error during your Pipeline\'s execution.\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    parameter_overrides: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional template parameters. Template parameters specified here take precedence over template parameters found in the artifact specified by the ``templateConfiguration`` property. We recommend that you use the template configuration file to specify most of your parameter values. Use parameter overrides to specify only dynamic parameter values (values that are unknown until you run the pipeline). All parameter names must be present in the stack template. Note: the entire object cannot be more than 1kB. Default: No overrides\n')
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    replace_on_failure: typing.Optional[bool] = pydantic.Field(None, description="Replace the stack if it's in a failed state. If this is set to true and the stack is in a failed state (one of ROLLBACK_COMPLETE, ROLLBACK_FAILED, CREATE_FAILED, DELETE_FAILED, or UPDATE_ROLLBACK_FAILED), AWS CloudFormation deletes the stack and then creates a new stack. If this is not set to true and the stack is in a failed state, the deployment fails. Default: false\n")
    template_configuration: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description='Input artifact to use for template parameters values and stack policy. The template configuration file should contain a JSON object that should look like this: ``{ "Parameters": {...}, "Tags": {...}, "StackPolicy": {... }}``. For more information, see `AWS CloudFormation Artifacts <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-cfn-artifacts.html>`_. Note that if you include sensitive information, such as passwords, restrict access to this file. Default: No template configuration based on input artifacts\n\n:exampleMetadata: infused\n\nExample::\n\n    from aws_cdk import PhysicalName\n\n    # in stack for account 123456789012...\n    # other_account_stack: Stack\n\n    action_role = iam.Role(other_account_stack, "ActionRole",\n        assumed_by=iam.AccountPrincipal("123456789012"),\n        # the role has to have a physical name set\n        role_name=PhysicalName.GENERATE_IF_NEEDED\n    )\n\n    # in the pipeline stack...\n    source_output = codepipeline.Artifact()\n    codepipeline_actions.CloudFormationCreateUpdateStackAction(\n        action_name="CloudFormationCreateUpdate",\n        stack_name="MyStackName",\n        admin_permissions=True,\n        template_path=source_output.at_path("template.yaml"),\n        role=action_role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'admin_permissions', 'stack_name', 'template_path', 'account', 'cfn_capabilities', 'deployment_role', 'extra_inputs', 'output', 'output_file_name', 'parameter_overrides', 'region', 'replace_on_failure', 'template_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationCreateUpdateStackActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationCreateUpdateStackActionPropsDefConfig] = pydantic.Field(None)


class CloudFormationCreateUpdateStackActionPropsDefConfig(pydantic.BaseModel):
    template_path_config: typing.Optional[models.aws_codepipeline.ArtifactPathDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationDeleteStackActionProps
class CloudFormationDeleteStackActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    admin_permissions: bool = pydantic.Field(..., description="Whether to grant full permissions to CloudFormation while deploying this template. Setting this to ``true`` affects the defaults for ``role`` and ``capabilities``, if you don't specify any alternatives. The default role that will be created for you will have full (i.e., ``*``) permissions on all resources, and the deployment will have named IAM capabilities (i.e., able to create all IAM resources). This is a shorthand that you can use if you fully trust the templates that are deployed in this pipeline. If you want more fine-grained permissions, use ``addToRolePolicy`` and ``capabilities`` to control what the CloudFormation deployment is allowed to do.\n")
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Acknowledge certain changes made as part of deployment. For stacks that contain certain resources, explicit acknowledgement is required that AWS CloudFormation might create or update those resources. For example, you must specify ``ANONYMOUS_IAM`` or ``NAMED_IAM`` if your stack template contains AWS Identity and Access Management (IAM) resources. For more information, see the link below. Default: None, unless ``adminPermissions`` is true\n')
    deployment_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role to assume when deploying changes. If not specified, a fresh role is created. The role is created with zero permissions unless ``adminPermissions`` is true, in which case the role will have full permissions. Default: A fresh role with full or no permissions (depending on the value of ``adminPermissions``).\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The list of additional input Artifacts for this Action. This is especially useful when used in conjunction with the ``parameterOverrides`` property. For example, if you have: parameterOverrides: { \'Param1\': action1.outputArtifact.bucketName, \'Param2\': action2.outputArtifact.objectKey, } , if the output Artifacts of ``action1`` and ``action2`` were not used to set either the ``templateConfiguration`` or the ``templatePath`` properties, you need to make sure to include them in the ``extraInputs`` - otherwise, you\'ll get an "unrecognized Artifact" error during your Pipeline\'s execution.\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    parameter_overrides: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Additional template parameters. Template parameters specified here take precedence over template parameters found in the artifact specified by the ``templateConfiguration`` property. We recommend that you use the template configuration file to specify most of your parameter values. Use parameter overrides to specify only dynamic parameter values (values that are unknown until you run the pipeline). All parameter names must be present in the stack template. Note: the entire object cannot be more than 1kB. Default: No overrides\n')
    region: typing.Optional[str] = pydantic.Field(None, description="The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n")
    template_configuration: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description='Input artifact to use for template parameters values and stack policy. The template configuration file should contain a JSON object that should look like this: ``{ "Parameters": {...}, "Tags": {...}, "StackPolicy": {... }}``. For more information, see `AWS CloudFormation Artifacts <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-cfn-artifacts.html>`_. Note that if you include sensitive information, such as passwords, restrict access to this file. Default: No template configuration based on input artifacts\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_codepipeline as codepipeline\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n    from aws_cdk import aws_iam as iam\n\n    # artifact: codepipeline.Artifact\n    # artifact_path: codepipeline.ArtifactPath\n    # parameter_overrides: Any\n    # role: iam.Role\n\n    cloud_formation_delete_stack_action_props = codepipeline_actions.CloudFormationDeleteStackActionProps(\n        action_name="actionName",\n        admin_permissions=False,\n        stack_name="stackName",\n\n        # the properties below are optional\n        account="account",\n        cfn_capabilities=[cdk.CfnCapabilities.NONE],\n        deployment_role=role,\n        extra_inputs=[artifact],\n        output=artifact,\n        output_file_name="outputFileName",\n        parameter_overrides={\n            "parameter_overrides_key": parameter_overrides\n        },\n        region="region",\n        role=role,\n        run_order=123,\n        template_configuration=artifact_path,\n        variables_namespace="variablesNamespace"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'admin_permissions', 'stack_name', 'account', 'cfn_capabilities', 'deployment_role', 'extra_inputs', 'output', 'output_file_name', 'parameter_overrides', 'region', 'template_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationDeleteStackActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackInstancesActionProps
class CloudFormationDeployStackInstancesActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    failure_tolerance_percentage: typing.Union[int, float, None] = pydantic.Field(None, description="The percentage of accounts per Region for which this stack operation can fail before AWS CloudFormation stops the operation in that Region. If the operation is stopped in a Region, AWS CloudFormation doesn't attempt the operation in subsequent Regions. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. Default: 0%\n")
    max_account_concurrency_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage of accounts in which to perform this operation at one time. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. If rounding down would result in zero, AWS CloudFormation sets the number as one instead. Although you use this setting to specify the maximum, for large deployments the actual number of accounts acted upon concurrently may be lower due to service throttling. Default: 1%\n')
    stack_set_region: typing.Optional[str] = pydantic.Field(None, description="The AWS Region the StackSet is in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps.crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: - same region as the Pipeline\n")
    stack_instances: models.aws_codepipeline_actions.StackInstancesDef = pydantic.Field(..., description='Specify where to create or update Stack Instances. You can specify either AWS Accounts Ids or AWS Organizations Organizational Units.\n')
    stack_set_name: str = pydantic.Field(..., description='The name of the StackSet we are adding instances to.\n')
    parameter_overrides: typing.Optional[models.aws_codepipeline_actions.StackSetParametersDef] = pydantic.Field(None, description='Parameter values that only apply to the current Stack Instances. These parameters are shared between all instances added by this action. Default: - no parameters will be overridden\n\n:exampleMetadata: infused\n\nExample::\n\n    # pipeline: codepipeline.Pipeline\n    # source_output: codepipeline.Artifact\n\n\n    pipeline.add_stage(\n        stage_name="DeployStackSets",\n        actions=[\n            # First, update the StackSet itself with the newest template\n            codepipeline_actions.CloudFormationDeployStackSetAction(\n                action_name="UpdateStackSet",\n                run_order=1,\n                stack_set_name="MyStackSet",\n                template=codepipeline_actions.StackSetTemplate.from_artifact_path(source_output.at_path("template.yaml")),\n\n                # Change this to \'StackSetDeploymentModel.organizations()\' if you want to deploy to OUs\n                deployment_model=codepipeline_actions.StackSetDeploymentModel.self_managed(),\n                # This deploys to a set of accounts\n                stack_instances=codepipeline_actions.StackInstances.in_accounts(["111111111111"], ["us-east-1", "eu-west-1"])\n            ),\n\n            # Afterwards, update/create additional instances in other accounts\n            codepipeline_actions.CloudFormationDeployStackInstancesAction(\n                action_name="AddMoreInstances",\n                run_order=2,\n                stack_set_name="MyStackSet",\n                stack_instances=codepipeline_actions.StackInstances.in_accounts(["222222222222", "333333333333"], ["us-east-1", "eu-west-1"])\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'failure_tolerance_percentage', 'max_account_concurrency_percentage', 'stack_set_region', 'stack_instances', 'stack_set_name', 'parameter_overrides']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackInstancesActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationDeployStackInstancesActionPropsDefConfig] = pydantic.Field(None)


class CloudFormationDeployStackInstancesActionPropsDefConfig(pydantic.BaseModel):
    stack_instances_config: typing.Optional[models.aws_codepipeline_actions.StackInstancesDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackSetActionProps
class CloudFormationDeployStackSetActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    failure_tolerance_percentage: typing.Union[int, float, None] = pydantic.Field(None, description="The percentage of accounts per Region for which this stack operation can fail before AWS CloudFormation stops the operation in that Region. If the operation is stopped in a Region, AWS CloudFormation doesn't attempt the operation in subsequent Regions. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. Default: 0%\n")
    max_account_concurrency_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage of accounts in which to perform this operation at one time. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. If rounding down would result in zero, AWS CloudFormation sets the number as one instead. Although you use this setting to specify the maximum, for large deployments the actual number of accounts acted upon concurrently may be lower due to service throttling. Default: 1%\n')
    stack_set_region: typing.Optional[str] = pydantic.Field(None, description="The AWS Region the StackSet is in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps.crossRegionReplicationBuckets`` property. If you don't, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: - same region as the Pipeline\n")
    stack_set_name: str = pydantic.Field(..., description='The name to associate with the stack set. This name must be unique in the Region where it is created. The name may only contain alphanumeric and hyphen characters. It must begin with an alphabetic character and be 128 characters or fewer.\n')
    template: models.aws_codepipeline_actions.StackSetTemplateDef = pydantic.Field(..., description='The location of the template that defines the resources in the stack set. This must point to a template with a maximum size of 460,800 bytes. Enter the path to the source artifact name and template file.\n')
    cfn_capabilities: typing.Optional[typing.Sequence[aws_cdk.CfnCapabilities]] = pydantic.Field(None, description='Indicates that the template can create and update resources, depending on the types of resources in the template. You must use this property if you have IAM resources in your stack template or you create a stack directly from a template containing macros. Default: - the StackSet will have no IAM capabilities\n')
    deployment_model: typing.Optional[models.aws_codepipeline_actions.StackSetDeploymentModelDef] = pydantic.Field(None, description='Determines how IAM roles are created and managed. The choices are: - Self Managed: you create IAM roles with the required permissions in the administration account and all target accounts. - Service Managed: only available if the account and target accounts are part of an AWS Organization. The necessary roles will be created for you. If you want to deploy to all accounts that are a member of AWS Organizations Organizational Units (OUs), you must select Service Managed permissions. Note: This parameter can only be changed when no stack instances exist in the stack set. Default: StackSetDeploymentModel.selfManaged()\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the stack set. You can use this to describe the stack set’s purpose or other relevant information. Default: - no description\n')
    parameters: typing.Optional[models.aws_codepipeline_actions.StackSetParametersDef] = pydantic.Field(None, description='The template parameters for your stack set. These parameters are shared between all instances of the stack set. Default: - no parameters will be used\n')
    stack_instances: typing.Optional[models.aws_codepipeline_actions.StackInstancesDef] = pydantic.Field(None, description='Specify where to create or update Stack Instances. You can specify either AWS Accounts Ids or AWS Organizations Organizational Units. Default: - don\'t create or update any Stack Instances\n\n:exampleMetadata: infused\n\nExample::\n\n    # pipeline: codepipeline.Pipeline\n    # source_output: codepipeline.Artifact\n\n\n    pipeline.add_stage(\n        stage_name="DeployStackSets",\n        actions=[\n            # First, update the StackSet itself with the newest template\n            codepipeline_actions.CloudFormationDeployStackSetAction(\n                action_name="UpdateStackSet",\n                run_order=1,\n                stack_set_name="MyStackSet",\n                template=codepipeline_actions.StackSetTemplate.from_artifact_path(source_output.at_path("template.yaml")),\n\n                # Change this to \'StackSetDeploymentModel.organizations()\' if you want to deploy to OUs\n                deployment_model=codepipeline_actions.StackSetDeploymentModel.self_managed(),\n                # This deploys to a set of accounts\n                stack_instances=codepipeline_actions.StackInstances.in_accounts(["111111111111"], ["us-east-1", "eu-west-1"])\n            ),\n\n            # Afterwards, update/create additional instances in other accounts\n            codepipeline_actions.CloudFormationDeployStackInstancesAction(\n                action_name="AddMoreInstances",\n                run_order=2,\n                stack_set_name="MyStackSet",\n                stack_instances=codepipeline_actions.StackInstances.in_accounts(["222222222222", "333333333333"], ["us-east-1", "eu-west-1"])\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'failure_tolerance_percentage', 'max_account_concurrency_percentage', 'stack_set_region', 'stack_set_name', 'template', 'cfn_capabilities', 'deployment_model', 'description', 'parameters', 'stack_instances']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationDeployStackSetActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CloudFormationDeployStackSetActionPropsDefConfig] = pydantic.Field(None)


class CloudFormationDeployStackSetActionPropsDefConfig(pydantic.BaseModel):
    template_config: typing.Optional[models.aws_codepipeline_actions.StackSetTemplateDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CloudFormationExecuteChangeSetActionProps
class CloudFormationExecuteChangeSetActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    change_set_name: str = pydantic.Field(..., description='Name of the change set to execute.\n')
    stack_name: str = pydantic.Field(..., description='The name of the stack to apply this action to.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account this Action is supposed to operate in. **Note**: if you specify the ``role`` property, this is ignored - the action will operate in the same region the passed role does. Default: - action resides in the same account as the pipeline\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The name of the output artifact to generate. Only applied if ``outputFileName`` is set as well. Default: Automatically generated artifact name.\n')
    output_file_name: typing.Optional[str] = pydantic.Field(None, description="A name for the filename in the output artifact to store the AWS CloudFormation call's result. The file will contain the result of the call to AWS CloudFormation (for example the call to UpdateStack or CreateChangeSet). AWS CodePipeline adds the file to the output artifact after performing the specified action. Default: No output artifact generated\n")
    region: typing.Optional[str] = pydantic.Field(None, description='The AWS region the given Action resides in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps#crossRegionReplicationBuckets`` property. If you don\'t, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: the Action resides in the same region as the Pipeline\n\n:exampleMetadata: lit=aws-codepipeline-actions/test/integ.cfn-template-from-repo.lit.ts infused\n\nExample::\n\n    # Source stage: read from repository\n    repo = codecommit.Repository(stack, "TemplateRepo",\n        repository_name="template-repo"\n    )\n    source_output = codepipeline.Artifact("SourceArtifact")\n    source = cpactions.CodeCommitSourceAction(\n        action_name="Source",\n        repository=repo,\n        output=source_output,\n        trigger=cpactions.CodeCommitTrigger.POLL\n    )\n    source_stage = {\n        "stage_name": "Source",\n        "actions": [source]\n    }\n\n    # Deployment stage: create and deploy changeset with manual approval\n    stack_name = "OurStack"\n    change_set_name = "StagedChangeSet"\n\n    prod_stage = {\n        "stage_name": "Deploy",\n        "actions": [\n            cpactions.CloudFormationCreateReplaceChangeSetAction(\n                action_name="PrepareChanges",\n                stack_name=stack_name,\n                change_set_name=change_set_name,\n                admin_permissions=True,\n                template_path=source_output.at_path("template.yaml"),\n                run_order=1\n            ),\n            cpactions.ManualApprovalAction(\n                action_name="ApproveChanges",\n                run_order=2\n            ),\n            cpactions.CloudFormationExecuteChangeSetAction(\n                action_name="ExecuteChanges",\n                stack_name=stack_name,\n                change_set_name=change_set_name,\n                run_order=3\n            )\n        ]\n    }\n\n    codepipeline.Pipeline(stack, "Pipeline",\n        stages=[source_stage, prod_stage\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'change_set_name', 'stack_name', 'account', 'output', 'output_file_name', 'region']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CloudFormationExecuteChangeSetActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeBuildActionProps
class CodeBuildActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source to use as input for this action.\n')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description="The action's Project.\n")
    check_secrets_in_plain_text_env_variables: typing.Optional[bool] = pydantic.Field(None, description='Whether to check for the presence of any secrets in the environment variables of the default type, BuildEnvironmentVariableType.PLAINTEXT. Since using a secret for the value of that kind of variable would result in it being displayed in plain text in the AWS Console, the construct will throw an exception if it detects a secret was passed there. Pass this property as false if you want to skip this validation, and keep using a secret in a plain text environment variable. Default: true\n')
    combine_batch_build_artifacts: typing.Optional[bool] = pydantic.Field(None, description='Combine the build artifacts for a batch builds. Enabling this will combine the build artifacts into the same location for batch builds. If ``executeBatchBuild`` is not set to ``true``, this property is ignored. Default: false\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables to pass to the CodeBuild project when this action executes. If a variable with the same name was set both on the project level, and here, this value will take precedence. Default: - No additional environment variables are specified.\n')
    execute_batch_build: typing.Optional[bool] = pydantic.Field(None, description='Trigger a batch build. Enabling this will enable batch builds on the CodeBuild project. Default: false\n')
    extra_inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The list of additional input Artifacts for this action. The directories the additional inputs will be available at are available during the project's build in the CODEBUILD_SRC_DIR_ environment variables. The project's build always starts in the directory with the primary input artifact checked out, the one pointed to by the ``input`` property. For more information, see https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html .\n")
    outputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The list of output Artifacts for this action. **Note**: if you specify more than one output Artifact here, you cannot use the primary 'artifacts' section of the buildspec; you have to use the 'secondary-artifacts' section instead. See https://docs.aws.amazon.com/codebuild/latest/userguide/sample-multi-in-out.html for details. Default: the action will not have any outputs\n")
    type: typing.Optional[aws_cdk.aws_codepipeline_actions.CodeBuildActionType] = pydantic.Field(None, description='The type of the action that determines its CodePipeline Category - Build, or Test. Default: CodeBuildActionType.BUILD\n\n:exampleMetadata: infused\n\nExample::\n\n    # Create a Cloudfront Web Distribution\n    import aws_cdk.aws_cloudfront as cloudfront\n    # distribution: cloudfront.Distribution\n\n\n    # Create the build project that will invalidate the cache\n    invalidate_build_project = codebuild.PipelineProject(self, "InvalidateProject",\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2",\n            "phases": {\n                "build": {\n                    "commands": ["aws cloudfront create-invalidation --distribution-id ${CLOUDFRONT_ID} --paths "/*""\n                    ]\n                }\n            }\n        }),\n        environment_variables={\n            "CLOUDFRONT_ID": codebuild.BuildEnvironmentVariable(value=distribution.distribution_id)\n        }\n    )\n\n    # Add Cloudfront invalidation permissions to the project\n    distribution_arn = f"arn:aws:cloudfront::{this.account}:distribution/{distribution.distributionId}"\n    invalidate_build_project.add_to_role_policy(iam.PolicyStatement(\n        resources=[distribution_arn],\n        actions=["cloudfront:CreateInvalidation"\n        ]\n    ))\n\n    # Create the pipeline (here only the S3 deploy and Invalidate cache build)\n    deploy_bucket = s3.Bucket(self, "DeployBucket")\n    deploy_input = codepipeline.Artifact()\n    codepipeline.Pipeline(self, "Pipeline",\n        stages=[codepipeline.StageProps(\n            stage_name="Deploy",\n            actions=[\n                codepipeline_actions.S3DeployAction(\n                    action_name="S3Deploy",\n                    bucket=deploy_bucket,\n                    input=deploy_input,\n                    run_order=1\n                ),\n                codepipeline_actions.CodeBuildAction(\n                    action_name="InvalidateCache",\n                    project=invalidate_build_project,\n                    input=deploy_input,\n                    run_order=2\n                )\n            ]\n        )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'input', 'project', 'check_secrets_in_plain_text_env_variables', 'combine_batch_build_artifacts', 'environment_variables', 'execute_batch_build', 'extra_inputs', 'outputs', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeBuildActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeBuildActionPropsDefConfig] = pydantic.Field(None)


class CodeBuildActionPropsDefConfig(pydantic.BaseModel):
    input_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)
    project_config: typing.Optional[models._interface_methods.AwsCodebuildIProjectDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeCommitSourceActionProps
class CodeCommitSourceActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    repository: typing.Union[models.aws_codecommit.RepositoryDef] = pydantic.Field(..., description='The CodeCommit repository.\n')
    branch: typing.Optional[str] = pydantic.Field(None, description="Default: 'master'\n")
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='Whether the output should be the contents of the repository (which is the default), or a link that allows CodeBuild to clone the repository before building. **Note**: if this option is true, then only CodeBuild actions can use the resulting ``output``. Default: false\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role to be used by on commit event rule. Used only when trigger value is CodeCommitTrigger.EVENTS. Default: a new role will be created.\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.CodeCommitTrigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Default: CodeCommitTrigger.EVENTS\n\n:exampleMetadata: lit=aws-codepipeline-actions/test/integ.cfn-template-from-repo.lit.ts infused\n\nExample::\n\n    # Source stage: read from repository\n    repo = codecommit.Repository(stack, "TemplateRepo",\n        repository_name="template-repo"\n    )\n    source_output = codepipeline.Artifact("SourceArtifact")\n    source = cpactions.CodeCommitSourceAction(\n        action_name="Source",\n        repository=repo,\n        output=source_output,\n        trigger=cpactions.CodeCommitTrigger.POLL\n    )\n    source_stage = {\n        "stage_name": "Source",\n        "actions": [source]\n    }\n\n    # Deployment stage: create and deploy changeset with manual approval\n    stack_name = "OurStack"\n    change_set_name = "StagedChangeSet"\n\n    prod_stage = {\n        "stage_name": "Deploy",\n        "actions": [\n            cpactions.CloudFormationCreateReplaceChangeSetAction(\n                action_name="PrepareChanges",\n                stack_name=stack_name,\n                change_set_name=change_set_name,\n                admin_permissions=True,\n                template_path=source_output.at_path("template.yaml"),\n                run_order=1\n            ),\n            cpactions.ManualApprovalAction(\n                action_name="ApproveChanges",\n                run_order=2\n            ),\n            cpactions.CloudFormationExecuteChangeSetAction(\n                action_name="ExecuteChanges",\n                stack_name=stack_name,\n                change_set_name=change_set_name,\n                run_order=3\n            )\n        ]\n    }\n\n    codepipeline.Pipeline(stack, "Pipeline",\n        stages=[source_stage, prod_stage\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'output', 'repository', 'branch', 'code_build_clone_output', 'event_role', 'trigger']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeCommitSourceActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeCommitSourceActionPropsDefConfig] = pydantic.Field(None)


class CodeCommitSourceActionPropsDefConfig(pydantic.BaseModel):
    output_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)
    repository_config: typing.Optional[models._interface_methods.AwsCodecommitIRepositoryDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeCommitSourceVariables
class CodeCommitSourceVariablesDef(BaseStruct):
    author_date: str = pydantic.Field(..., description='The date the currently last commit on the tracked branch was authored, in ISO-8601 format.\n')
    branch_name: str = pydantic.Field(..., description='The name of the branch this action tracks.\n')
    commit_id: str = pydantic.Field(..., description='The SHA1 hash of the currently last commit on the tracked branch.\n')
    commit_message: str = pydantic.Field(..., description='The message of the currently last commit on the tracked branch.\n')
    committer_date: str = pydantic.Field(..., description='The date the currently last commit on the tracked branch was committed, in ISO-8601 format.\n')
    repository_name: str = pydantic.Field(..., description='The name of the repository this action points to.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    code_commit_source_variables = codepipeline_actions.CodeCommitSourceVariables(\n        author_date="authorDate",\n        branch_name="branchName",\n        commit_id="commitId",\n        commit_message="commitMessage",\n        committer_date="committerDate",\n        repository_name="repositoryName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['author_date', 'branch_name', 'commit_id', 'commit_message', 'committer_date', 'repository_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeCommitSourceVariables'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeDeployEcsContainerImageInput
class CodeDeployEcsContainerImageInputDef(BaseStruct):
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The artifact that contains an ``imageDetails.json`` file with the image URI. The artifact\'s ``imageDetails.json`` file must be a JSON file containing an ``ImageURI`` property. For example: ``{ "ImageURI": "ACCOUNTID.dkr.ecr.us-west-2.amazonaws.com/dk-image-repo@sha256:example3" }``\n')
    task_definition_placeholder: typing.Optional[str] = pydantic.Field(None, description='The placeholder string in the ECS task definition template file that will be replaced with the image URI. The placeholder string must be surrounded by angle brackets in the template file. For example, if the task definition template file contains a placeholder like ``"image": "<PLACEHOLDER>"``, then the ``taskDefinitionPlaceholder`` value should be ``PLACEHOLDER``. Default: IMAGE\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline as codepipeline\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    # artifact: codepipeline.Artifact\n\n    code_deploy_ecs_container_image_input = codepipeline_actions.CodeDeployEcsContainerImageInput(\n        input=artifact,\n\n        # the properties below are optional\n        task_definition_placeholder="taskDefinitionPlaceholder"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['input', 'task_definition_placeholder']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeDeployEcsContainerImageInput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeDeployEcsContainerImageInputDefConfig] = pydantic.Field(None)


class CodeDeployEcsContainerImageInputDefConfig(pydantic.BaseModel):
    input_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeDeployEcsDeployActionProps
class CodeDeployEcsDeployActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    deployment_group: typing.Union[models.aws_codedeploy.EcsDeploymentGroupDef] = pydantic.Field(..., description='The CodeDeploy ECS Deployment Group to deploy to.\n')
    app_spec_template_file: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description="The name of the CodeDeploy AppSpec file. During deployment, a new task definition will be registered with ECS, and the new task definition ID will be inserted into the CodeDeploy AppSpec file. The AppSpec file contents will be provided to CodeDeploy for the deployment. Use this property if you want to use a different name for this file than the default 'appspec.yaml'. If you use this property, you don't need to specify the ``appSpecTemplateInput`` property. Default: - one of this property, or ``appSpecTemplateInput``, is required\n")
    app_spec_template_input: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="The artifact containing the CodeDeploy AppSpec file. During deployment, a new task definition will be registered with ECS, and the new task definition ID will be inserted into the CodeDeploy AppSpec file. The AppSpec file contents will be provided to CodeDeploy for the deployment. If you use this property, it's assumed the file is called 'appspec.yaml'. If your AppSpec file uses a different filename, leave this property empty, and use the ``appSpecTemplateFile`` property instead. Default: - one of this property, or ``appSpecTemplateFile``, is required\n")
    container_image_inputs: typing.Optional[typing.Sequence[typing.Union[models.aws_codepipeline_actions.CodeDeployEcsContainerImageInputDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configuration for dynamically updated images in the task definition. Provide pairs of an image details input artifact and a placeholder string that will be used to dynamically update the ECS task definition template file prior to deployment. A maximum of 4 images can be given.\n')
    task_definition_template_file: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description="The name of the ECS task definition template file. During deployment, the task definition template file contents will be registered with ECS. Use this property if you want to use a different name for this file than the default 'taskdef.json'. If you use this property, you don't need to specify the ``taskDefinitionTemplateInput`` property. Default: - one of this property, or ``taskDefinitionTemplateInput``, is required\n")
    task_definition_template_input: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The artifact containing the ECS task definition template file. During deployment, the task definition template file contents will be registered with ECS. If you use this property, it\'s assumed the file is called \'taskdef.json\'. If your task definition template uses a different filename, leave this property empty, and use the ``taskDefinitionTemplateFile`` property instead. Default: - one of this property, or ``taskDefinitionTemplateFile``, is required\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codedeploy as codedeploy\n    from aws_cdk import aws_codepipeline as codepipeline\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n    from aws_cdk import aws_iam as iam\n\n    # artifact: codepipeline.Artifact\n    # artifact_path: codepipeline.ArtifactPath\n    # ecs_deployment_group: codedeploy.EcsDeploymentGroup\n    # role: iam.Role\n\n    code_deploy_ecs_deploy_action_props = codepipeline_actions.CodeDeployEcsDeployActionProps(\n        action_name="actionName",\n        deployment_group=ecs_deployment_group,\n\n        # the properties below are optional\n        app_spec_template_file=artifact_path,\n        app_spec_template_input=artifact,\n        container_image_inputs=[codepipeline_actions.CodeDeployEcsContainerImageInput(\n            input=artifact,\n\n            # the properties below are optional\n            task_definition_placeholder="taskDefinitionPlaceholder"\n        )],\n        role=role,\n        run_order=123,\n        task_definition_template_file=artifact_path,\n        task_definition_template_input=artifact,\n        variables_namespace="variablesNamespace"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'deployment_group', 'app_spec_template_file', 'app_spec_template_input', 'container_image_inputs', 'task_definition_template_file', 'task_definition_template_input']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeDeployEcsDeployActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeDeployEcsDeployActionPropsDefConfig] = pydantic.Field(None)


class CodeDeployEcsDeployActionPropsDefConfig(pydantic.BaseModel):
    deployment_group_config: typing.Optional[models._interface_methods.AwsCodedeployIEcsDeploymentGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeDeployServerDeployActionProps
class CodeDeployServerDeployActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    deployment_group: typing.Union[models.aws_codedeploy.ServerDeploymentGroupDef] = pydantic.Field(..., description='The CodeDeploy server Deployment Group to deploy to.\n')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source to use as input for deployment.\n\n:exampleMetadata: infused\n\nExample::\n\n    # deployment_group: codedeploy.ServerDeploymentGroup\n    pipeline = codepipeline.Pipeline(self, "MyPipeline",\n        pipeline_name="MyPipeline"\n    )\n\n    # add the source and build Stages to the Pipeline...\n    build_output = codepipeline.Artifact()\n    deploy_action = codepipeline_actions.CodeDeployServerDeployAction(\n        action_name="CodeDeploy",\n        input=build_output,\n        deployment_group=deployment_group\n    )\n    pipeline.add_stage(\n        stage_name="Deploy",\n        actions=[deploy_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'deployment_group', 'input']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeDeployServerDeployActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeDeployServerDeployActionPropsDefConfig] = pydantic.Field(None)


class CodeDeployServerDeployActionPropsDefConfig(pydantic.BaseModel):
    deployment_group_config: typing.Optional[models._interface_methods.AwsCodedeployIServerDeploymentGroupDefConfig] = pydantic.Field(None)
    input_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeStarConnectionsSourceActionProps
class CodeStarConnectionsSourceActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    connection_arn: str = pydantic.Field(..., description='The ARN of the CodeStar Connection created in the AWS console that has permissions to access this GitHub or BitBucket repository.\n')
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The output artifact that this action produces. Can be used as input for further pipeline actions.\n')
    owner: str = pydantic.Field(..., description='The owning user or organization of the repository.\n')
    repo: str = pydantic.Field(..., description='The name of the repository.\n')
    branch: typing.Optional[str] = pydantic.Field(None, description="The branch to build. Default: 'master'\n")
    code_build_clone_output: typing.Optional[bool] = pydantic.Field(None, description='Whether the output should be the contents of the repository (which is the default), or a link that allows CodeBuild to clone the repository before building. **Note**: if this option is true, then only CodeBuild actions can use the resulting ``output``. Default: false\n')
    trigger_on_push: typing.Optional[bool] = pydantic.Field(None, description='Controls automatically starting your pipeline when a new commit is made on the configured repository and branch. If unspecified, the default value is true, and the field does not display by default. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    source_output = codepipeline.Artifact()\n    source_action = codepipeline_actions.CodeStarConnectionsSourceAction(\n        action_name="BitBucket_Source",\n        owner="aws",\n        repo="aws-cdk",\n        output=source_output,\n        connection_arn="arn:aws:codestar-connections:us-east-1:123456789012:connection/12345678-abcd-12ab-34cdef5678gh"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'connection_arn', 'output', 'owner', 'repo', 'branch', 'code_build_clone_output', 'trigger_on_push']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeStarConnectionsSourceActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CodeStarConnectionsSourceActionPropsDefConfig] = pydantic.Field(None)


class CodeStarConnectionsSourceActionPropsDefConfig(pydantic.BaseModel):
    output_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeStarSourceVariables
class CodeStarSourceVariablesDef(BaseStruct):
    author_date: str = pydantic.Field(..., description='The date the currently last commit on the tracked branch was authored, in ISO-8601 format.\n')
    branch_name: str = pydantic.Field(..., description='The name of the branch this action tracks.\n')
    commit_id: str = pydantic.Field(..., description='The SHA1 hash of the currently last commit on the tracked branch.\n')
    commit_message: str = pydantic.Field(..., description='The message of the currently last commit on the tracked branch.\n')
    connection_arn: str = pydantic.Field(..., description='The connection ARN this source uses.\n')
    full_repository_name: str = pydantic.Field(..., description='The name of the repository this action points to.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    code_star_source_variables = codepipeline_actions.CodeStarSourceVariables(\n        author_date="authorDate",\n        branch_name="branchName",\n        commit_id="commitId",\n        commit_message="commitMessage",\n        connection_arn="connectionArn",\n        full_repository_name="fullRepositoryName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['author_date', 'branch_name', 'commit_id', 'commit_message', 'connection_arn', 'full_repository_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CodeStarSourceVariables'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.CommonCloudFormationStackSetOptions
class CommonCloudFormationStackSetOptionsDef(BaseStruct):
    failure_tolerance_percentage: typing.Union[int, float, None] = pydantic.Field(None, description="The percentage of accounts per Region for which this stack operation can fail before AWS CloudFormation stops the operation in that Region. If the operation is stopped in a Region, AWS CloudFormation doesn't attempt the operation in subsequent Regions. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. Default: 0%\n")
    max_account_concurrency_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage of accounts in which to perform this operation at one time. When calculating the number of accounts based on the specified percentage, AWS CloudFormation rounds down to the next whole number. If rounding down would result in zero, AWS CloudFormation sets the number as one instead. Although you use this setting to specify the maximum, for large deployments the actual number of accounts acted upon concurrently may be lower due to service throttling. Default: 1%\n')
    stack_set_region: typing.Optional[str] = pydantic.Field(None, description='The AWS Region the StackSet is in. Note that a cross-region Pipeline requires replication buckets to function correctly. You can provide their names with the ``PipelineProps.crossRegionReplicationBuckets`` property. If you don\'t, the CodePipeline Construct will create new Stacks in your CDK app containing those buckets, that you will need to ``cdk deploy`` before deploying the main, Pipeline-containing Stack. Default: - same region as the Pipeline\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    common_cloud_formation_stack_set_options = codepipeline_actions.CommonCloudFormationStackSetOptions(\n        failure_tolerance_percentage=123,\n        max_account_concurrency_percentage=123,\n        stack_set_region="stackSetRegion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['failure_tolerance_percentage', 'max_account_concurrency_percentage', 'stack_set_region']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.CommonCloudFormationStackSetOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.EcrSourceActionProps
class EcrSourceActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='The repository that will be watched for changes.\n')
    image_tag: typing.Optional[str] = pydantic.Field(None, description='The image tag that will be checked for changes. It is not possible to trigger on changes to more than one tag. Default: \'latest\'\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_ecr as ecr\n\n    # ecr_repository: ecr.Repository\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    source_output = codepipeline.Artifact()\n    source_action = codepipeline_actions.EcrSourceAction(\n        action_name="ECR",\n        repository=ecr_repository,\n        image_tag="some-tag",  # optional, default: \'latest\'\n        output=source_output\n    )\n    pipeline.add_stage(\n        stage_name="Source",\n        actions=[source_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'output', 'repository', 'image_tag']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.EcrSourceActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcrSourceActionPropsDefConfig] = pydantic.Field(None)


class EcrSourceActionPropsDefConfig(pydantic.BaseModel):
    output_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)
    repository_config: typing.Optional[models._interface_methods.AwsEcrIRepositoryDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.EcrSourceVariables
class EcrSourceVariablesDef(BaseStruct):
    image_digest: str = pydantic.Field(..., description="The digest of the current image, in the form ':'.\n")
    image_tag: str = pydantic.Field(..., description='The Docker tag of the current image.\n')
    image_uri: str = pydantic.Field(..., description='The full ECR Docker URI of the current image.\n')
    registry_id: str = pydantic.Field(..., description='The identifier of the registry. In ECR, this is usually the ID of the AWS account owning it.\n')
    repository_name: str = pydantic.Field(..., description='The physical name of the repository that this action tracks.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    ecr_source_variables = codepipeline_actions.EcrSourceVariables(\n        image_digest="imageDigest",\n        image_tag="imageTag",\n        image_uri="imageUri",\n        registry_id="registryId",\n        repository_name="repositoryName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image_digest', 'image_tag', 'image_uri', 'registry_id', 'repository_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.EcrSourceVariables'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.EcsDeployActionProps
class EcsDeployActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    service: typing.Union[models.aws_ecs.BaseServiceDef, models.aws_ecs.Ec2ServiceDef, models.aws_ecs.ExternalServiceDef, models.aws_ecs.FargateServiceDef] = pydantic.Field(..., description='The ECS Service to deploy.\n')
    deployment_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Timeout for the ECS deployment in minutes. Value must be between 1-60. Default: - 60 minutes\n')
    image_file: typing.Optional[models.aws_codepipeline.ArtifactPathDef] = pydantic.Field(None, description="The name of the JSON image definitions file to use for deployments. The JSON file is a list of objects, each with 2 keys: ``name`` is the name of the container in the Task Definition, and ``imageUri`` is the Docker image URI you want to update your service with. Use this property if you want to use a different name for this file than the default 'imagedefinitions.json'. If you use this property, you don't need to specify the ``input`` property. Default: - one of this property, or ``input``, is required\n")
    input: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The input artifact that contains the JSON image definitions file to use for deployments. The JSON file is a list of objects, each with 2 keys: ``name`` is the name of the container in the Task Definition, and ``imageUri`` is the Docker image URI you want to update your service with. If you use this property, it\'s assumed the file is called \'imagedefinitions.json\'. If your build uses a different file, leave this property empty, and use the ``imageFile`` property instead. Default: - one of this property, or ``imageFile``, is required\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_ecs as ecs\n\n    # service: ecs.FargateService\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    build_output = codepipeline.Artifact()\n    deploy_stage = pipeline.add_stage(\n        stage_name="Deploy",\n        actions=[\n            codepipeline_actions.EcsDeployAction(\n                action_name="DeployAction",\n                service=service,\n                # if your file is called imagedefinitions.json,\n                # use the `input` property,\n                # and leave out the `imageFile` property\n                input=build_output,\n                # if your file name is _not_ imagedefinitions.json,\n                # use the `imageFile` property,\n                # and leave out the `input` property\n                image_file=build_output.at_path("imageDef.json"),\n                deployment_timeout=Duration.minutes(60)\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'service', 'deployment_timeout', 'image_file', 'input']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.EcsDeployActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[EcsDeployActionPropsDefConfig] = pydantic.Field(None)


class EcsDeployActionPropsDefConfig(pydantic.BaseModel):
    service_config: typing.Optional[models._interface_methods.AwsEcsIBaseServiceDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.ElasticBeanstalkDeployActionProps
class ElasticBeanstalkDeployActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    application_name: str = pydantic.Field(..., description='The name of the AWS Elastic Beanstalk application to deploy.\n')
    environment_name: str = pydantic.Field(..., description='The name of the AWS Elastic Beanstalk environment to deploy to.\n')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The source to use as input for deployment.\n\n:exampleMetadata: infused\n\nExample::\n\n    source_output = codepipeline.Artifact()\n    target_bucket = s3.Bucket(self, "MyBucket")\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    deploy_action = codepipeline_actions.ElasticBeanstalkDeployAction(\n        action_name="ElasticBeanstalkDeploy",\n        input=source_output,\n        environment_name="envName",\n        application_name="appName"\n    )\n\n    deploy_stage = pipeline.add_stage(\n        stage_name="Deploy",\n        actions=[deploy_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'application_name', 'environment_name', 'input']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.ElasticBeanstalkDeployActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ElasticBeanstalkDeployActionPropsDefConfig] = pydantic.Field(None)


class ElasticBeanstalkDeployActionPropsDefConfig(pydantic.BaseModel):
    input_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.GitHubSourceActionProps
class GitHubSourceActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    oauth_token: models.SecretValueDef = pydantic.Field(..., description="A GitHub OAuth token to use for authentication. It is recommended to use a Secrets Manager ``Secret`` to obtain the token: const oauth = cdk.SecretValue.secretsManager('my-github-token'); new GitHubSourceAction(this, 'GitHubAction', { oauthToken: oauth, ... }); If you rotate the value in the Secret, you must also change at least one property of the CodePipeline to force CloudFormation to re-read the secret. The GitHub Personal Access Token should have these scopes: - **repo** - to read the repository - **admin:repo_hook** - if you plan to use webhooks (true by default)\n")
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    owner: str = pydantic.Field(..., description='The GitHub account/user that owns the repo.\n')
    repo: str = pydantic.Field(..., description='The name of the repo, without the username.\n')
    branch: typing.Optional[str] = pydantic.Field(None, description='The branch to use. Default: "master"\n')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.GitHubTrigger] = pydantic.Field(None, description='How AWS CodePipeline should be triggered. With the default value "WEBHOOK", a webhook is created in GitHub that triggers the action With "POLL", CodePipeline periodically checks the source for changes With "None", the action is not triggered through changes in the source To use ``WEBHOOK``, your GitHub Personal Access Token should have **admin:repo_hook** scope (in addition to the regular **repo** scope). Default: GitHubTrigger.WEBHOOK\n\n:exampleMetadata: infused\n\nExample::\n\n    # Read the secret from Secrets Manager\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    source_output = codepipeline.Artifact()\n    source_action = codepipeline_actions.GitHubSourceAction(\n        action_name="GitHub_Source",\n        owner="awslabs",\n        repo="aws-cdk",\n        oauth_token=SecretValue.secrets_manager("my-github-token"),\n        output=source_output,\n        branch="develop"\n    )\n    pipeline.add_stage(\n        stage_name="Source",\n        actions=[source_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'oauth_token', 'output', 'owner', 'repo', 'branch', 'trigger']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.GitHubSourceActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[GitHubSourceActionPropsDefConfig] = pydantic.Field(None)


class GitHubSourceActionPropsDefConfig(pydantic.BaseModel):
    oauth_token_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)
    output_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.GitHubSourceVariables
class GitHubSourceVariablesDef(BaseStruct):
    author_date: str = pydantic.Field(..., description='The date the currently last commit on the tracked branch was authored, in ISO-8601 format.\n')
    branch_name: str = pydantic.Field(..., description='The name of the branch this action tracks.\n')
    commit_id: str = pydantic.Field(..., description='The SHA1 hash of the currently last commit on the tracked branch.\n')
    commit_message: str = pydantic.Field(..., description='The message of the currently last commit on the tracked branch.\n')
    committer_date: str = pydantic.Field(..., description='The date the currently last commit on the tracked branch was committed, in ISO-8601 format.\n')
    commit_url: str = pydantic.Field(..., description='The GitHub API URL of the currently last commit on the tracked branch.\n')
    repository_name: str = pydantic.Field(..., description='The name of the repository this action points to.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    git_hub_source_variables = codepipeline_actions.GitHubSourceVariables(\n        author_date="authorDate",\n        branch_name="branchName",\n        commit_id="commitId",\n        commit_message="commitMessage",\n        committer_date="committerDate",\n        commit_url="commitUrl",\n        repository_name="repositoryName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['author_date', 'branch_name', 'commit_id', 'commit_message', 'committer_date', 'commit_url', 'repository_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.GitHubSourceVariables'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.JenkinsActionProps
class JenkinsActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    jenkins_provider: typing.Union[models.aws_codepipeline_actions.BaseJenkinsProviderDef, models.aws_codepipeline_actions.JenkinsProviderDef] = pydantic.Field(..., description='The Jenkins Provider for this Action.\n')
    project_name: str = pydantic.Field(..., description='The name of the project (sometimes also called job, or task) on your Jenkins installation that will be invoked by this Action.\n')
    type: aws_cdk.aws_codepipeline_actions.JenkinsActionType = pydantic.Field(..., description='The type of the Action - Build, or Test.\n')
    inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='The source to use as input for this build.\n')
    outputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'jenkins_provider', 'project_name', 'type', 'inputs', 'outputs']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.JenkinsActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.JenkinsProviderAttributes
class JenkinsProviderAttributesDef(BaseStruct):
    provider_name: str = pydantic.Field(..., description='The name of the Jenkins provider that you set in the AWS CodePipeline plugin configuration of your Jenkins project.\n')
    server_url: str = pydantic.Field(..., description='The base URL of your Jenkins server.\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version of your provider. Default: \'1\'\n\n:exampleMetadata: infused\n\nExample::\n\n    jenkins_provider = codepipeline_actions.JenkinsProvider.from_jenkins_provider_attributes(self, "JenkinsProvider",\n        provider_name="MyJenkinsProvider",\n        server_url="http://my-jenkins.com:8080",\n        version="2"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['provider_name', 'server_url', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.JenkinsProviderAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.JenkinsProviderProps
class JenkinsProviderPropsDef(BaseStruct):
    provider_name: str = pydantic.Field(..., description='The name of the Jenkins provider that you set in the AWS CodePipeline plugin configuration of your Jenkins project.')
    server_url: str = pydantic.Field(..., description='The base URL of your Jenkins server.\n')
    for_build: typing.Optional[bool] = pydantic.Field(None, description='Whether to immediately register a Jenkins Provider for the build category. The Provider will always be registered if you create a ``JenkinsAction``. Default: false\n')
    for_test: typing.Optional[bool] = pydantic.Field(None, description='Whether to immediately register a Jenkins Provider for the test category. The Provider will always be registered if you create a ``JenkinsTestAction``. Default: false\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version of your provider. Default: \'1\'\n\n:exampleMetadata: infused\n\nExample::\n\n    jenkins_provider = codepipeline_actions.JenkinsProvider(self, "JenkinsProvider",\n        provider_name="MyJenkinsProvider",\n        server_url="http://my-jenkins.com:8080",\n        version="2"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['provider_name', 'server_url', 'for_build', 'for_test', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.JenkinsProviderProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.LambdaInvokeActionProps
class LambdaInvokeActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    lambda_: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='The lambda function to invoke.\n')
    inputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The optional input Artifacts of the Action. A Lambda Action can have up to 5 inputs. The inputs will appear in the event passed to the Lambda, under the ``'CodePipeline.job'.data.inputArtifacts`` path. Default: the Action will not have any inputs\n")
    outputs: typing.Optional[typing.Sequence[models.aws_codepipeline.ArtifactDef]] = pydantic.Field(None, description="The optional names of the output Artifacts of the Action. A Lambda Action can have up to 5 outputs. The outputs will appear in the event passed to the Lambda, under the ``'CodePipeline.job'.data.outputArtifacts`` path. It is the responsibility of the Lambda to upload ZIP files with the Artifact contents to the provided locations. Default: the Action will not have any outputs\n")
    user_parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='A set of key-value pairs that will be accessible to the invoked Lambda inside the event that the Pipeline will call it with. Only one of ``userParameters`` or ``userParametersString`` can be specified. Default: - no user parameters will be passed\n')
    user_parameters_string: typing.Optional[str] = pydantic.Field(None, description='The string representation of the user parameters that will be accessible to the invoked Lambda inside the event that the Pipeline will call it with. Only one of ``userParametersString`` or ``userParameters`` can be specified. Default: - no user parameters will be passed\n\n:exampleMetadata: infused\n\nExample::\n\n    # fn: lambda.Function\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    lambda_action = codepipeline_actions.LambdaInvokeAction(\n        action_name="Lambda",\n        lambda_=fn\n    )\n    pipeline.add_stage(\n        stage_name="Lambda",\n        actions=[lambda_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'lambda_', 'inputs', 'outputs', 'user_parameters', 'user_parameters_string']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.LambdaInvokeActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[LambdaInvokeActionPropsDefConfig] = pydantic.Field(None)


class LambdaInvokeActionPropsDefConfig(pydantic.BaseModel):
    lambda__config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.ManualApprovalActionProps
class ManualApprovalActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    additional_information: typing.Optional[str] = pydantic.Field(None, description='Any additional information that you want to include in the notification email message.\n')
    external_entity_link: typing.Optional[str] = pydantic.Field(None, description='URL you want to provide to the reviewer as part of the approval request. Default: - the approval request will not have an external link\n')
    notification_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='Optional SNS topic to send notifications to when an approval is pending.\n')
    notify_emails: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of email addresses to subscribe to notifications when this Action is pending approval. If this has been provided, but not ``notificationTopic``, a new Topic will be created.\n\n:exampleMetadata: infused\n\nExample::\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    approve_stage = pipeline.add_stage(stage_name="Approve")\n    manual_approval_action = codepipeline_actions.ManualApprovalAction(\n        action_name="Approve"\n    )\n    approve_stage.add_action(manual_approval_action)\n\n    role = iam.Role.from_role_arn(self, "Admin", Arn.format(ArnComponents(service="iam", resource="role", resource_name="Admin"), self))\n    manual_approval_action.grant_manual_approval(role)\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'additional_information', 'external_entity_link', 'notification_topic', 'notify_emails']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.ManualApprovalActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.OrganizationsDeploymentProps
class OrganizationsDeploymentPropsDef(BaseStruct):
    auto_deployment: typing.Optional[aws_cdk.aws_codepipeline_actions.StackSetOrganizationsAutoDeployment] = pydantic.Field(None, description='Automatically deploy to new accounts added to Organizational Units. Whether AWS CloudFormation StackSets automatically deploys to AWS Organizations accounts that are added to a target organization or organizational unit (OU). Default: Disabled\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    organizations_deployment_props = codepipeline_actions.OrganizationsDeploymentProps(\n        auto_deployment=codepipeline_actions.StackSetOrganizationsAutoDeployment.ENABLED\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_deployment']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.OrganizationsDeploymentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.S3DeployActionProps
class S3DeployActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The Amazon S3 bucket that is the deploy target.\n')
    input: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='The input Artifact to deploy to Amazon S3.\n')
    access_control: typing.Optional[aws_cdk.aws_s3.BucketAccessControl] = pydantic.Field(None, description='The specified canned ACL to objects deployed to Amazon S3. This overwrites any existing ACL that was applied to the object. Default: - the original object ACL\n')
    cache_control: typing.Optional[typing.Sequence[models.aws_codepipeline_actions.CacheControlDef]] = pydantic.Field(None, description='The caching behavior for requests/responses for objects in the bucket. The final cache control property will be the result of joining all of the provided array elements with a comma (plus a space after the comma). Default: - none, decided by the HTTP client\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS encryption key for the host bucket. The encryptionKey parameter encrypts uploaded artifacts with the provided AWS KMS key. Default: - none\n')
    extract: typing.Optional[bool] = pydantic.Field(None, description='Should the deploy action extract the artifact before deploying to Amazon S3. Default: true\n')
    object_key: typing.Optional[str] = pydantic.Field(None, description='The key of the target object. This is required if extract is false.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    source_output = codepipeline.Artifact()\n    target_bucket = s3.Bucket(self, "MyBucket")\n    key = kms.Key(stack, "EnvVarEncryptKey",\n        description="sample key"\n    )\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    deploy_action = codepipeline_actions.S3DeployAction(\n        action_name="S3Deploy",\n        bucket=target_bucket,\n        input=source_output,\n        encryption_key=key\n    )\n    deploy_stage = pipeline.add_stage(\n        stage_name="Deploy",\n        actions=[deploy_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'bucket', 'input', 'access_control', 'cache_control', 'encryption_key', 'extract', 'object_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.S3DeployActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[S3DeployActionPropsDefConfig] = pydantic.Field(None)


class S3DeployActionPropsDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)
    input_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.S3SourceActionProps
class S3SourceActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The Amazon S3 bucket that stores the source code. If you import an encrypted bucket in your stack, please specify the encryption key at import time by using ``Bucket.fromBucketAttributes()`` method.\n')
    bucket_key: str = pydantic.Field(..., description='The key within the S3 bucket that stores the source code.\n')
    output: models.aws_codepipeline.ArtifactDef = pydantic.Field(..., description='')
    trigger: typing.Optional[aws_cdk.aws_codepipeline_actions.S3Trigger] = pydantic.Field(None, description='How should CodePipeline detect source changes for this Action. Note that if this is S3Trigger.EVENTS, you need to make sure to include the source Bucket in a CloudTrail Trail, as otherwise the CloudWatch Events will not be emitted. Default: S3Trigger.POLL\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_cloudtrail as cloudtrail\n\n    # source_bucket: s3.Bucket\n\n    source_output = codepipeline.Artifact()\n    key = "some/key.zip"\n    trail = cloudtrail.Trail(self, "CloudTrail")\n    trail.add_s3_event_selector([cloudtrail.S3EventSelector(\n        bucket=source_bucket,\n        object_prefix=key\n    )],\n        read_write_type=cloudtrail.ReadWriteType.WRITE_ONLY\n    )\n    source_action = codepipeline_actions.S3SourceAction(\n        action_name="S3Source",\n        bucket_key=key,\n        bucket=source_bucket,\n        output=source_output,\n        trigger=codepipeline_actions.S3Trigger.EVENTS\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'bucket', 'bucket_key', 'output', 'trigger']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.S3SourceActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[S3SourceActionPropsDefConfig] = pydantic.Field(None)


class S3SourceActionPropsDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)
    output_config: typing.Optional[models.aws_codepipeline.ArtifactDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.S3SourceVariables
class S3SourceVariablesDef(BaseStruct):
    e_tag: str = pydantic.Field(..., description='The e-tag of the S3 version of the object that triggered the build.\n')
    version_id: str = pydantic.Field(..., description='The identifier of the S3 version of the object that triggered the build.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_codepipeline_actions as codepipeline_actions\n\n    s3_source_variables = codepipeline_actions.S3SourceVariables(\n        e_tag="eTag",\n        version_id="versionId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['e_tag', 'version_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.S3SourceVariables'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.SelfManagedDeploymentProps
class SelfManagedDeploymentPropsDef(BaseStruct):
    administration_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role in the administrator account used to assume execution roles in the target accounts. You must create this role before using the StackSet action. The role needs to be assumable by CloudFormation, and it needs to be able to ``sts:AssumeRole`` each of the execution roles (whose names are specified in the ``executionRoleName`` parameter) in each of the target accounts. If you do not specify the role, we assume you have created a role named ``AWSCloudFormationStackSetAdministrationRole``. Default: - Assume an existing role named ``AWSCloudFormationStackSetAdministrationRole`` in the same account as the pipeline.\n')
    execution_role_name: typing.Optional[str] = pydantic.Field(None, description='The name of the IAM role in the target accounts used to perform stack set operations. You must create these roles in each of the target accounts before using the StackSet action. The roles need to be assumable by by the ``administrationRole``, and need to have the permissions necessary to successfully create and modify the resources that the subsequent CloudFormation deployments need. Administrator permissions would be commonly granted to these, but if you can scope the permissions down frome there you would be safer. Default: AWSCloudFormationStackSetExecutionRole\n\n:exampleMetadata: infused\n\nExample::\n\n    existing_admin_role = iam.Role.from_role_name(self, "AdminRole", "AWSCloudFormationStackSetAdministrationRole")\n\n    deployment_model = codepipeline_actions.StackSetDeploymentModel.self_managed(\n        # Use an existing Role. Leave this out to create a new Role.\n        administration_role=existing_admin_role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['administration_role', 'execution_role_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.SelfManagedDeploymentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_codepipeline_actions.ServiceCatalogDeployActionBeta1Props
class ServiceCatalogDeployActionBeta1PropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    product_id: str = pydantic.Field(..., description='The identifier of the product in the Service Catalog. This product must already exist.\n')
    product_version_name: str = pydantic.Field(..., description='The name of the version of the Service Catalog product to be deployed.\n')
    template_path: models.aws_codepipeline.ArtifactPathDef = pydantic.Field(..., description='The path to the cloudformation artifact.\n')
    product_version_description: typing.Optional[str] = pydantic.Field(None, description='The optional description of this version of the Service Catalog product. Default: \'\'\n\n:exampleMetadata: infused\n\nExample::\n\n    cdk_build_output = codepipeline.Artifact()\n    service_catalog_deploy_action = codepipeline_actions.ServiceCatalogDeployActionBeta1(\n        action_name="ServiceCatalogDeploy",\n        template_path=cdk_build_output.at_path("Sample.template.json"),\n        product_version_name="Version - " + Date.now.to_string,\n        product_version_description="This is a version from the pipeline with a new description.",\n        product_id="prod-XXXXXXXX"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'product_id', 'product_version_name', 'template_path', 'product_version_description']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.ServiceCatalogDeployActionBeta1Props'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ServiceCatalogDeployActionBeta1PropsDefConfig] = pydantic.Field(None)


class ServiceCatalogDeployActionBeta1PropsDefConfig(pydantic.BaseModel):
    template_path_config: typing.Optional[models.aws_codepipeline.ArtifactPathDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.StepFunctionsInvokeActionProps
class StepFunctionsInvokeActionPropsDef(BaseStruct):
    action_name: str = pydantic.Field(..., description='The physical, human-readable name of the Action. Note that Action names must be unique within a single Stage.\n')
    run_order: typing.Union[int, float, None] = pydantic.Field(None, description='The runOrder property for this Action. RunOrder determines the relative order in which multiple Actions in the same Stage execute. Default: 1\n')
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="The name of the namespace to use for variables emitted by this action. Default: - a name will be generated, based on the stage and action names, if any of the action's variables were referenced - otherwise, no namespace will be set\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description="The Role in which context's this Action will be executing in. The Pipeline's Role will assume this Role (the required permissions for that will be granted automatically) right before executing this Action. This Action will be passed into your ``IAction.bind`` method in the ``ActionBindOptions.role`` property. Default: a new Role will be generated\n")
    state_machine: typing.Union[models.aws_stepfunctions.StateMachineDef] = pydantic.Field(..., description='The state machine to invoke.\n')
    execution_name_prefix: typing.Optional[str] = pydantic.Field(None, description='Prefix (optional). By default, the action execution ID is used as the state machine execution name. If a prefix is provided, it is prepended to the action execution ID with a hyphen and together used as the state machine execution name. Default: - action execution ID\n')
    output: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description='The optional output Artifact of the Action. Default: the Action will not have any outputs\n')
    state_machine_input: typing.Optional[models.aws_codepipeline_actions.StateMachineInputDef] = pydantic.Field(None, description='Represents the input to the StateMachine. This includes input artifact, input type and the statemachine input. Default: - none\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_stepfunctions as stepfunctions\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    start_state = stepfunctions.Pass(self, "StartState")\n    simple_state_machine = stepfunctions.StateMachine(self, "SimpleStateMachine",\n        definition=start_state\n    )\n    step_function_action = codepipeline_actions.StepFunctionInvokeAction(\n        action_name="Invoke",\n        state_machine=simple_state_machine,\n        state_machine_input=codepipeline_actions.StateMachineInput.literal({"IsHelloWorldExample": True})\n    )\n    pipeline.add_stage(\n        stage_name="StepFunctions",\n        actions=[step_function_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action_name', 'run_order', 'variables_namespace', 'role', 'state_machine', 'execution_name_prefix', 'output', 'state_machine_input']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_codepipeline_actions.StepFunctionsInvokeActionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[StepFunctionsInvokeActionPropsDefConfig] = pydantic.Field(None)


class StepFunctionsInvokeActionPropsDefConfig(pydantic.BaseModel):
    state_machine_config: typing.Optional[models._interface_methods.AwsStepfunctionsIStateMachineDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeBuildActionType
# skipping emum

#  autogenerated from aws_cdk.aws_codepipeline_actions.CodeCommitTrigger
# skipping emum

#  autogenerated from aws_cdk.aws_codepipeline_actions.GitHubTrigger
# skipping emum

#  autogenerated from aws_cdk.aws_codepipeline_actions.JenkinsActionType
# skipping emum

#  autogenerated from aws_cdk.aws_codepipeline_actions.S3Trigger
# skipping emum

#  autogenerated from aws_cdk.aws_codepipeline_actions.StackSetOrganizationsAutoDeployment
# skipping emum

#  autogenerated from aws_cdk.aws_codepipeline_actions.IJenkinsProvider
#  skipping Interface

import models

class ModuleModel(pydantic.BaseModel):
    Action: typing.Optional[dict[str, ActionDef]] = pydantic.Field(None)
    AlexaSkillDeployAction: typing.Optional[dict[str, AlexaSkillDeployActionDef]] = pydantic.Field(None)
    BaseJenkinsProvider: typing.Optional[dict[str, BaseJenkinsProviderDef]] = pydantic.Field(None)
    CacheControl: typing.Optional[dict[str, CacheControlDef]] = pydantic.Field(None)
    CloudFormationCreateReplaceChangeSetAction: typing.Optional[dict[str, CloudFormationCreateReplaceChangeSetActionDef]] = pydantic.Field(None)
    CloudFormationCreateUpdateStackAction: typing.Optional[dict[str, CloudFormationCreateUpdateStackActionDef]] = pydantic.Field(None)
    CloudFormationDeleteStackAction: typing.Optional[dict[str, CloudFormationDeleteStackActionDef]] = pydantic.Field(None)
    CloudFormationDeployStackInstancesAction: typing.Optional[dict[str, CloudFormationDeployStackInstancesActionDef]] = pydantic.Field(None)
    CloudFormationDeployStackSetAction: typing.Optional[dict[str, CloudFormationDeployStackSetActionDef]] = pydantic.Field(None)
    CloudFormationExecuteChangeSetAction: typing.Optional[dict[str, CloudFormationExecuteChangeSetActionDef]] = pydantic.Field(None)
    CodeBuildAction: typing.Optional[dict[str, CodeBuildActionDef]] = pydantic.Field(None)
    CodeCommitSourceAction: typing.Optional[dict[str, CodeCommitSourceActionDef]] = pydantic.Field(None)
    CodeDeployEcsDeployAction: typing.Optional[dict[str, CodeDeployEcsDeployActionDef]] = pydantic.Field(None)
    CodeDeployServerDeployAction: typing.Optional[dict[str, CodeDeployServerDeployActionDef]] = pydantic.Field(None)
    CodeStarConnectionsSourceAction: typing.Optional[dict[str, CodeStarConnectionsSourceActionDef]] = pydantic.Field(None)
    EcrSourceAction: typing.Optional[dict[str, EcrSourceActionDef]] = pydantic.Field(None)
    EcsDeployAction: typing.Optional[dict[str, EcsDeployActionDef]] = pydantic.Field(None)
    ElasticBeanstalkDeployAction: typing.Optional[dict[str, ElasticBeanstalkDeployActionDef]] = pydantic.Field(None)
    GitHubSourceAction: typing.Optional[dict[str, GitHubSourceActionDef]] = pydantic.Field(None)
    JenkinsAction: typing.Optional[dict[str, JenkinsActionDef]] = pydantic.Field(None)
    LambdaInvokeAction: typing.Optional[dict[str, LambdaInvokeActionDef]] = pydantic.Field(None)
    ManualApprovalAction: typing.Optional[dict[str, ManualApprovalActionDef]] = pydantic.Field(None)
    S3DeployAction: typing.Optional[dict[str, S3DeployActionDef]] = pydantic.Field(None)
    S3SourceAction: typing.Optional[dict[str, S3SourceActionDef]] = pydantic.Field(None)
    ServiceCatalogDeployActionBeta1: typing.Optional[dict[str, ServiceCatalogDeployActionBeta1Def]] = pydantic.Field(None)
    StackInstances: typing.Optional[dict[str, StackInstancesDef]] = pydantic.Field(None)
    StackSetDeploymentModel: typing.Optional[dict[str, StackSetDeploymentModelDef]] = pydantic.Field(None)
    StackSetParameters: typing.Optional[dict[str, StackSetParametersDef]] = pydantic.Field(None)
    StackSetTemplate: typing.Optional[dict[str, StackSetTemplateDef]] = pydantic.Field(None)
    StateMachineInput: typing.Optional[dict[str, StateMachineInputDef]] = pydantic.Field(None)
    StepFunctionInvokeAction: typing.Optional[dict[str, StepFunctionInvokeActionDef]] = pydantic.Field(None)
    JenkinsProvider: typing.Optional[dict[str, JenkinsProviderDef]] = pydantic.Field(None)
    AlexaSkillDeployActionProps: typing.Optional[dict[str, AlexaSkillDeployActionPropsDef]] = pydantic.Field(None)
    CloudFormationCreateReplaceChangeSetActionProps: typing.Optional[dict[str, CloudFormationCreateReplaceChangeSetActionPropsDef]] = pydantic.Field(None)
    CloudFormationCreateUpdateStackActionProps: typing.Optional[dict[str, CloudFormationCreateUpdateStackActionPropsDef]] = pydantic.Field(None)
    CloudFormationDeleteStackActionProps: typing.Optional[dict[str, CloudFormationDeleteStackActionPropsDef]] = pydantic.Field(None)
    CloudFormationDeployStackInstancesActionProps: typing.Optional[dict[str, CloudFormationDeployStackInstancesActionPropsDef]] = pydantic.Field(None)
    CloudFormationDeployStackSetActionProps: typing.Optional[dict[str, CloudFormationDeployStackSetActionPropsDef]] = pydantic.Field(None)
    CloudFormationExecuteChangeSetActionProps: typing.Optional[dict[str, CloudFormationExecuteChangeSetActionPropsDef]] = pydantic.Field(None)
    CodeBuildActionProps: typing.Optional[dict[str, CodeBuildActionPropsDef]] = pydantic.Field(None)
    CodeCommitSourceActionProps: typing.Optional[dict[str, CodeCommitSourceActionPropsDef]] = pydantic.Field(None)
    CodeCommitSourceVariables: typing.Optional[dict[str, CodeCommitSourceVariablesDef]] = pydantic.Field(None)
    CodeDeployEcsContainerImageInput: typing.Optional[dict[str, CodeDeployEcsContainerImageInputDef]] = pydantic.Field(None)
    CodeDeployEcsDeployActionProps: typing.Optional[dict[str, CodeDeployEcsDeployActionPropsDef]] = pydantic.Field(None)
    CodeDeployServerDeployActionProps: typing.Optional[dict[str, CodeDeployServerDeployActionPropsDef]] = pydantic.Field(None)
    CodeStarConnectionsSourceActionProps: typing.Optional[dict[str, CodeStarConnectionsSourceActionPropsDef]] = pydantic.Field(None)
    CodeStarSourceVariables: typing.Optional[dict[str, CodeStarSourceVariablesDef]] = pydantic.Field(None)
    CommonCloudFormationStackSetOptions: typing.Optional[dict[str, CommonCloudFormationStackSetOptionsDef]] = pydantic.Field(None)
    EcrSourceActionProps: typing.Optional[dict[str, EcrSourceActionPropsDef]] = pydantic.Field(None)
    EcrSourceVariables: typing.Optional[dict[str, EcrSourceVariablesDef]] = pydantic.Field(None)
    EcsDeployActionProps: typing.Optional[dict[str, EcsDeployActionPropsDef]] = pydantic.Field(None)
    ElasticBeanstalkDeployActionProps: typing.Optional[dict[str, ElasticBeanstalkDeployActionPropsDef]] = pydantic.Field(None)
    GitHubSourceActionProps: typing.Optional[dict[str, GitHubSourceActionPropsDef]] = pydantic.Field(None)
    GitHubSourceVariables: typing.Optional[dict[str, GitHubSourceVariablesDef]] = pydantic.Field(None)
    JenkinsActionProps: typing.Optional[dict[str, JenkinsActionPropsDef]] = pydantic.Field(None)
    JenkinsProviderAttributes: typing.Optional[dict[str, JenkinsProviderAttributesDef]] = pydantic.Field(None)
    JenkinsProviderProps: typing.Optional[dict[str, JenkinsProviderPropsDef]] = pydantic.Field(None)
    LambdaInvokeActionProps: typing.Optional[dict[str, LambdaInvokeActionPropsDef]] = pydantic.Field(None)
    ManualApprovalActionProps: typing.Optional[dict[str, ManualApprovalActionPropsDef]] = pydantic.Field(None)
    OrganizationsDeploymentProps: typing.Optional[dict[str, OrganizationsDeploymentPropsDef]] = pydantic.Field(None)
    S3DeployActionProps: typing.Optional[dict[str, S3DeployActionPropsDef]] = pydantic.Field(None)
    S3SourceActionProps: typing.Optional[dict[str, S3SourceActionPropsDef]] = pydantic.Field(None)
    S3SourceVariables: typing.Optional[dict[str, S3SourceVariablesDef]] = pydantic.Field(None)
    SelfManagedDeploymentProps: typing.Optional[dict[str, SelfManagedDeploymentPropsDef]] = pydantic.Field(None)
    ServiceCatalogDeployActionBeta1Props: typing.Optional[dict[str, ServiceCatalogDeployActionBeta1PropsDef]] = pydantic.Field(None)
    StepFunctionsInvokeActionProps: typing.Optional[dict[str, StepFunctionsInvokeActionPropsDef]] = pydantic.Field(None)
    ...
