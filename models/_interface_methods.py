from __future__ import annotations
import pydantic
import aws_cdk
import typing
import datetime



class CoreIAnyProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[list[CoreIAnyProducerDefProduceParams]] = pydantic.Field(None, description='Produce the value.')


class CoreIAnyProducerDefProduceParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIAspectDefConfig(pydantic.BaseModel):
    visit: typing.Optional[list[CoreIAspectDefVisitParams]] = pydantic.Field(None, description='All aspects can visit an IConstruct.')


class CoreIAspectDefVisitParams(pydantic.BaseModel):
    node: models.AnyResource = pydantic.Field(..., description='-')
#  aws-cdk-lib.IAsset skipped


class CoreIBoundStackSynthesizerDefConfig(pydantic.BaseModel):
    add_docker_image_asset: typing.Optional[list[CoreIBoundStackSynthesizerDefAddDockerImageAssetParams]] = pydantic.Field(None, description='Register a Docker Image Asset.\nReturns the parameters that can be used to refer to the asset inside the template.')
    add_file_asset: typing.Optional[list[CoreIBoundStackSynthesizerDefAddFileAssetParams]] = pydantic.Field(None, description='Register a File Asset.\nReturns the parameters that can be used to refer to the asset inside the template.')
    bind: typing.Optional[list[CoreIBoundStackSynthesizerDefBindParams]] = pydantic.Field(None, description='Bind to the stack this environment is going to be used on.\nMust be called before any of the other methods are called, and can only be called once.')
    synthesize: typing.Optional[list[CoreIBoundStackSynthesizerDefSynthesizeParams]] = pydantic.Field(None, description='Synthesize the associated stack to the session.')


class CoreIBoundStackSynthesizerDefAddDockerImageAssetParams(pydantic.BaseModel):
    source_hash: str = pydantic.Field(..., description='The hash of the contents of the docker build context. This hash is used throughout the system to identify this image and avoid duplicate work in case the source did not change. NOTE: this means that if you wish to update your docker image, you must make a modification to the source (e.g. add some metadata to your Dockerfile).\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    directory_name: typing.Optional[str] = pydantic.Field(None, description='The directory where the Dockerfile is stored, must be relative to the cloud assembly root. Default: - Exactly one of ``directoryName`` and ``executable`` is required\n')
    docker_build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Only allowed when ``directoryName`` is specified. Default: - no build args are passed\n')
    docker_build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets to pass to the ``docker build`` command. Since Docker build secrets are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Only allowed when ``directoryName`` is specified. Default: - no build secrets are passed\n')
    docker_build_target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Only allowed when ``directoryName`` is specified. Default: - no target\n')
    docker_cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    docker_cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    docker_file: typing.Optional[str] = pydantic.Field(None, description='Path to the Dockerfile (relative to the directory). Only allowed when ``directoryName`` is specified. Default: - no file\n')
    docker_outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no build args are passed\n')
    executable: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An external command that will produce the packaged asset. The command should produce the name of a local Docker image on ``stdout``. Default: - Exactly one of ``directoryName`` and ``executable`` is required\n')
    network_mode: typing.Optional[str] = pydantic.Field(None, description='Networking mode for the RUN commands during build. *Requires Docker Engine API v1.25+*. Specify this property to build images on a specific networking mode. Default: - no networking mode specified\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Specify this property to build images on a specific platform. Default: - no platform specified (the current machine architecture will be used)')

class CoreIBoundStackSynthesizerDefAddFileAssetParams(pydantic.BaseModel):
    source_hash: str = pydantic.Field(..., description="A hash on the content source. This hash is used to uniquely identify this asset throughout the system. If this value doesn't change, the asset will not be rebuilt or republished.\n")
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    executable: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An external command that will produce the packaged asset. The command should produce the location of a ZIP file on ``stdout``. Default: - Exactly one of ``fileName`` and ``executable`` is required\n')
    file_name: typing.Optional[str] = pydantic.Field(None, description='The path, relative to the root of the cloud assembly, in which this asset source resides. This can be a path to a file or a directory, depending on the packaging type. Default: - Exactly one of ``fileName`` and ``executable`` is required\n')
    packaging: typing.Optional[aws_cdk.FileAssetPackaging] = pydantic.Field(None, description='Which type of packaging to perform. Default: - Required if ``fileName`` is specified.')

class CoreIBoundStackSynthesizerDefBindParams(pydantic.BaseModel):
    stack: models.StackDef = pydantic.Field(..., description='-')

class CoreIBoundStackSynthesizerDefSynthesizeParams(pydantic.BaseModel):
    session: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreICfnConditionExpressionDefConfig(pydantic.BaseModel):
    resolve: typing.Optional[list[CoreICfnConditionExpressionDefResolveParams]] = pydantic.Field(None, description="Produce the Token's value at resolution time.")


class CoreICfnConditionExpressionDefResolveParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')
#  aws-cdk-lib.ICfnResourceOptions skipped


class CoreICfnRuleConditionExpressionDefConfig(pydantic.BaseModel):
    resolve: typing.Optional[list[CoreICfnRuleConditionExpressionDefResolveParams]] = pydantic.Field(None, description="Produce the Token's value at resolution time.")


class CoreICfnRuleConditionExpressionDefResolveParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIFragmentConcatenatorDefConfig(pydantic.BaseModel):
    join: typing.Optional[list[CoreIFragmentConcatenatorDefJoinParams]] = pydantic.Field(None, description='Join the fragment on the left and on the right.')


class CoreIFragmentConcatenatorDefJoinParams(pydantic.BaseModel):
    left: typing.Any = pydantic.Field(..., description='-\n')
    right: typing.Any = pydantic.Field(..., description='-')

class CoreIInspectableDefConfig(pydantic.BaseModel):
    inspect: typing.Optional[list[CoreIInspectableDefInspectParams]] = pydantic.Field(None, description='Examines construct.')


class CoreIInspectableDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')

class CoreIListProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[list[CoreIListProducerDefProduceParams]] = pydantic.Field(None, description='Produce the list value.')


class CoreIListProducerDefProduceParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreILocalBundlingDefConfig(pydantic.BaseModel):
    try_bundle: typing.Optional[list[CoreILocalBundlingDefTryBundleParams]] = pydantic.Field(None, description='This method is called before attempting docker bundling to allow the bundler to be executed locally.\nIf the local bundler exists, and bundling\nwas performed locally, return ``true``. Otherwise, return ``false``.')


class CoreILocalBundlingDefTryBundleParams(pydantic.BaseModel):
    output_dir: str = pydantic.Field(..., description='the directory where the bundled asset should be output.\n')
    image: models.DockerImageDef = pydantic.Field(..., description='The Docker image where the command will run.\n')
    bundling_file_access: typing.Optional[aws_cdk.BundlingFileAccess] = pydantic.Field(None, description='The access mechanism used to make source files available to the bundling container and to return the bundling output back to the host. Default: - BundlingFileAccess.BIND_MOUNT\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command to run in the Docker container. Example value: ``['npm', 'install']`` Default: - run the command defined in the image\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The entrypoint to run in the Docker container. Example value: ``['/bin/sh', '-c']`` Default: - run the entrypoint defined in the image\n")
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the Docker container. Default: - no environment variables.\n')
    local: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='Local bundling provider. The provider implements a method ``tryBundle()`` which should return ``true`` if local bundling was performed. If ``false`` is returned, docker bundling will be done. Default: - bundling will only be performed in a Docker container\n')
    network: typing.Optional[str] = pydantic.Field(None, description='Docker `Networking options <https://docs.docker.com/engine/reference/commandline/run/#connect-a-container-to-a-network---network>`_. Default: - no networking options\n')
    output_type: typing.Optional[aws_cdk.BundlingOutput] = pydantic.Field(None, description='The type of output that this bundling operation is producing. Default: BundlingOutput.AUTO_DISCOVER\n')
    security_opt: typing.Optional[str] = pydantic.Field(None, description='`Security configuration <https://docs.docker.com/engine/reference/run/#security-configuration>`_ when running the docker container. Default: - no security options\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use when running the Docker container. user | user:group | uid | uid:gid | user:gid | uid:group Default: - uid:gid of the current user or 1000:1000 on Windows\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.DockerVolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional Docker volumes to mount. Default: - no additional volumes are mounted\n')
    volumes_from: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Where to mount the specified volumes from. Default: - no containers are specified to mount volumes from\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Working directory inside the Docker container. Default: /asset-input')

class CoreINumberProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[list[CoreINumberProducerDefProduceParams]] = pydantic.Field(None, description='Produce the number value.')


class CoreINumberProducerDefProduceParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')
#  aws-cdk-lib.IPolicyValidationContextBeta1 skipped


class CoreIPolicyValidationPluginBeta1DefConfig(pydantic.BaseModel):
    validate_: typing.Optional[list[CoreIPolicyValidationPluginBeta1DefValidateParams]] = pydantic.Field(None, description='The method that will be called by the CDK framework to perform validations.\nThis is where the plugin will evaluate the CloudFormation\ntemplates for compliance and report and violations', alias='validate')


class CoreIPolicyValidationPluginBeta1DefValidateParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIPostProcessorDefConfig(pydantic.BaseModel):
    post_process: typing.Optional[list[CoreIPostProcessorDefPostProcessParams]] = pydantic.Field(None, description='Process the completely resolved value, after full recursion/resolution has happened.')


class CoreIPostProcessorDefPostProcessParams(pydantic.BaseModel):
    input: typing.Any = pydantic.Field(..., description='-\n')
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIResolvableDefConfig(pydantic.BaseModel):
    resolve: typing.Optional[list[CoreIResolvableDefResolveParams]] = pydantic.Field(None, description="Produce the Token's value at resolution time.")


class CoreIResolvableDefResolveParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIResolveContextDefConfig(pydantic.BaseModel):
    register_post_processor: typing.Optional[list[CoreIResolveContextDefRegisterPostProcessorParams]] = pydantic.Field(None, description='Use this postprocessor after the entire token structure has been resolved.')
    resolve: typing.Optional[list[CoreIResolveContextDefResolveParams]] = pydantic.Field(None, description='Resolve an inner object.')


class CoreIResolveContextDefRegisterPostProcessorParams(pydantic.BaseModel):
    post_processor: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIResolveContextDefResolveParams(pydantic.BaseModel):
    x: typing.Any = pydantic.Field(..., description='-\n')
    allow_intrinsic_keys: typing.Optional[bool] = pydantic.Field(None, description="Change the 'allowIntrinsicKeys' option. Default: - Unchanged\n")
    remove_empty: typing.Optional[bool] = pydantic.Field(None, description='Whether to remove undefined elements from arrays and objects when resolving. Default: - Unchanged')

class CoreIResourceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[CoreIResourceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class CoreIResourceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class CoreIReusableStackSynthesizerDefConfig(pydantic.BaseModel):
    add_docker_image_asset: typing.Optional[list[CoreIReusableStackSynthesizerDefAddDockerImageAssetParams]] = pydantic.Field(None, description='Register a Docker Image Asset.\nReturns the parameters that can be used to refer to the asset inside the template.')
    add_file_asset: typing.Optional[list[CoreIReusableStackSynthesizerDefAddFileAssetParams]] = pydantic.Field(None, description='Register a File Asset.\nReturns the parameters that can be used to refer to the asset inside the template.')
    bind: typing.Optional[list[CoreIReusableStackSynthesizerDefBindParams]] = pydantic.Field(None, description='Bind to the stack this environment is going to be used on.\nMust be called before any of the other methods are called, and can only be called once.')
    reusable_bind: typing.Optional[list[CoreIReusableStackSynthesizerDefReusableBindParams]] = pydantic.Field(None, description='Produce a bound Stack Synthesizer for the given stack.\nThis method may be called more than once on the same object.')
    synthesize: typing.Optional[list[CoreIReusableStackSynthesizerDefSynthesizeParams]] = pydantic.Field(None, description='Synthesize the associated stack to the session.')


class CoreIReusableStackSynthesizerDefAddDockerImageAssetParams(pydantic.BaseModel):
    source_hash: str = pydantic.Field(..., description='The hash of the contents of the docker build context. This hash is used throughout the system to identify this image and avoid duplicate work in case the source did not change. NOTE: this means that if you wish to update your docker image, you must make a modification to the source (e.g. add some metadata to your Dockerfile).\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    directory_name: typing.Optional[str] = pydantic.Field(None, description='The directory where the Dockerfile is stored, must be relative to the cloud assembly root. Default: - Exactly one of ``directoryName`` and ``executable`` is required\n')
    docker_build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Only allowed when ``directoryName`` is specified. Default: - no build args are passed\n')
    docker_build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets to pass to the ``docker build`` command. Since Docker build secrets are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Only allowed when ``directoryName`` is specified. Default: - no build secrets are passed\n')
    docker_build_target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Only allowed when ``directoryName`` is specified. Default: - no target\n')
    docker_cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    docker_cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    docker_file: typing.Optional[str] = pydantic.Field(None, description='Path to the Dockerfile (relative to the directory). Only allowed when ``directoryName`` is specified. Default: - no file\n')
    docker_outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no build args are passed\n')
    executable: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An external command that will produce the packaged asset. The command should produce the name of a local Docker image on ``stdout``. Default: - Exactly one of ``directoryName`` and ``executable`` is required\n')
    network_mode: typing.Optional[str] = pydantic.Field(None, description='Networking mode for the RUN commands during build. *Requires Docker Engine API v1.25+*. Specify this property to build images on a specific networking mode. Default: - no networking mode specified\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Specify this property to build images on a specific platform. Default: - no platform specified (the current machine architecture will be used)')

class CoreIReusableStackSynthesizerDefAddFileAssetParams(pydantic.BaseModel):
    source_hash: str = pydantic.Field(..., description="A hash on the content source. This hash is used to uniquely identify this asset throughout the system. If this value doesn't change, the asset will not be rebuilt or republished.\n")
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    executable: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An external command that will produce the packaged asset. The command should produce the location of a ZIP file on ``stdout``. Default: - Exactly one of ``fileName`` and ``executable`` is required\n')
    file_name: typing.Optional[str] = pydantic.Field(None, description='The path, relative to the root of the cloud assembly, in which this asset source resides. This can be a path to a file or a directory, depending on the packaging type. Default: - Exactly one of ``fileName`` and ``executable`` is required\n')
    packaging: typing.Optional[aws_cdk.FileAssetPackaging] = pydantic.Field(None, description='Which type of packaging to perform. Default: - Required if ``fileName`` is specified.')

class CoreIReusableStackSynthesizerDefBindParams(pydantic.BaseModel):
    stack: models.StackDef = pydantic.Field(..., description='-')

class CoreIReusableStackSynthesizerDefReusableBindParams(pydantic.BaseModel):
    stack: models.StackDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models._interface_methods.CoreIBoundStackSynthesizerDefConfig]] = pydantic.Field(None)

class CoreIReusableStackSynthesizerDefSynthesizeParams(pydantic.BaseModel):
    session: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIStableAnyProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[bool] = pydantic.Field(None, description='Produce the value.')


class CoreIStableListProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[bool] = pydantic.Field(None, description='Produce the list value.')


class CoreIStableNumberProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[bool] = pydantic.Field(None, description='Produce the number value.')


class CoreIStableStringProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[bool] = pydantic.Field(None, description='Produce the string value.')


class CoreIStackSynthesizerDefConfig(pydantic.BaseModel):
    add_docker_image_asset: typing.Optional[list[CoreIStackSynthesizerDefAddDockerImageAssetParams]] = pydantic.Field(None, description='Register a Docker Image Asset.\nReturns the parameters that can be used to refer to the asset inside the template.')
    add_file_asset: typing.Optional[list[CoreIStackSynthesizerDefAddFileAssetParams]] = pydantic.Field(None, description='Register a File Asset.\nReturns the parameters that can be used to refer to the asset inside the template.')
    bind: typing.Optional[list[CoreIStackSynthesizerDefBindParams]] = pydantic.Field(None, description='Bind to the stack this environment is going to be used on.\nMust be called before any of the other methods are called, and can only be called once.')
    synthesize: typing.Optional[list[CoreIStackSynthesizerDefSynthesizeParams]] = pydantic.Field(None, description='Synthesize the associated stack to the session.')


class CoreIStackSynthesizerDefAddDockerImageAssetParams(pydantic.BaseModel):
    source_hash: str = pydantic.Field(..., description='The hash of the contents of the docker build context. This hash is used throughout the system to identify this image and avoid duplicate work in case the source did not change. NOTE: this means that if you wish to update your docker image, you must make a modification to the source (e.g. add some metadata to your Dockerfile).\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    directory_name: typing.Optional[str] = pydantic.Field(None, description='The directory where the Dockerfile is stored, must be relative to the cloud assembly root. Default: - Exactly one of ``directoryName`` and ``executable`` is required\n')
    docker_build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Only allowed when ``directoryName`` is specified. Default: - no build args are passed\n')
    docker_build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets to pass to the ``docker build`` command. Since Docker build secrets are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Only allowed when ``directoryName`` is specified. Default: - no build secrets are passed\n')
    docker_build_target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Only allowed when ``directoryName`` is specified. Default: - no target\n')
    docker_cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    docker_cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    docker_file: typing.Optional[str] = pydantic.Field(None, description='Path to the Dockerfile (relative to the directory). Only allowed when ``directoryName`` is specified. Default: - no file\n')
    docker_outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no build args are passed\n')
    executable: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An external command that will produce the packaged asset. The command should produce the name of a local Docker image on ``stdout``. Default: - Exactly one of ``directoryName`` and ``executable`` is required\n')
    network_mode: typing.Optional[str] = pydantic.Field(None, description='Networking mode for the RUN commands during build. *Requires Docker Engine API v1.25+*. Specify this property to build images on a specific networking mode. Default: - no networking mode specified\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Specify this property to build images on a specific platform. Default: - no platform specified (the current machine architecture will be used)')

class CoreIStackSynthesizerDefAddFileAssetParams(pydantic.BaseModel):
    source_hash: str = pydantic.Field(..., description="A hash on the content source. This hash is used to uniquely identify this asset throughout the system. If this value doesn't change, the asset will not be rebuilt or republished.\n")
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    executable: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An external command that will produce the packaged asset. The command should produce the location of a ZIP file on ``stdout``. Default: - Exactly one of ``fileName`` and ``executable`` is required\n')
    file_name: typing.Optional[str] = pydantic.Field(None, description='The path, relative to the root of the cloud assembly, in which this asset source resides. This can be a path to a file or a directory, depending on the packaging type. Default: - Exactly one of ``fileName`` and ``executable`` is required\n')
    packaging: typing.Optional[aws_cdk.FileAssetPackaging] = pydantic.Field(None, description='Which type of packaging to perform. Default: - Required if ``fileName`` is specified.')

class CoreIStackSynthesizerDefBindParams(pydantic.BaseModel):
    stack: models.StackDef = pydantic.Field(..., description='-')

class CoreIStackSynthesizerDefSynthesizeParams(pydantic.BaseModel):
    session: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreIStringProducerDefConfig(pydantic.BaseModel):
    produce: typing.Optional[list[CoreIStringProducerDefProduceParams]] = pydantic.Field(None, description='Produce the string value.')


class CoreIStringProducerDefProduceParams(pydantic.BaseModel):
    context: models.UnsupportedResource = pydantic.Field(..., description='-')
#  aws-cdk-lib.ISynthesisSession skipped

#  aws-cdk-lib.ITaggable skipped

#  aws-cdk-lib.ITaggableV2 skipped

#  aws-cdk-lib.ITemplateOptions skipped


class CoreITokenMapperDefConfig(pydantic.BaseModel):
    map_token: typing.Optional[list[CoreITokenMapperDefMapTokenParams]] = pydantic.Field(None, description='Replace a single token.')


class CoreITokenMapperDefMapTokenParams(pydantic.BaseModel):
    t: typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef] = pydantic.Field(..., description='-')

class CoreITokenResolverDefConfig(pydantic.BaseModel):
    resolve_list: typing.Optional[list[CoreITokenResolverDefResolveListParams]] = pydantic.Field(None, description='Resolve a tokenized list.')
    resolve_string: typing.Optional[list[CoreITokenResolverDefResolveStringParams]] = pydantic.Field(None, description='Resolve a string with at least one stringified token in it.\n(May use concatenation)')
    resolve_token: typing.Optional[list[CoreITokenResolverDefResolveTokenParams]] = pydantic.Field(None, description='Resolve a single token.')


class CoreITokenResolverDefResolveListParams(pydantic.BaseModel):
    l: typing.Sequence[str] = pydantic.Field(..., description='-\n')
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreITokenResolverDefResolveStringParams(pydantic.BaseModel):
    s: models.TokenizedStringFragmentsDef = pydantic.Field(..., description='-\n')
    context: models.UnsupportedResource = pydantic.Field(..., description='-')

class CoreITokenResolverDefResolveTokenParams(pydantic.BaseModel):
    t: typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef] = pydantic.Field(..., description='-\n')
    context: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    post_processor: models.UnsupportedResource = pydantic.Field(..., description='-')

class AwsAcmpcaICertificateAuthorityDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsAcmpcaICertificateAuthorityDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAcmpcaICertificateAuthorityDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApigatewayIAccessLogDestinationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsApigatewayIAccessLogDestinationDefBindParams]] = pydantic.Field(None, description='Binds this destination to the RestApi Stage.')


class AwsApigatewayIAccessLogDestinationDefBindParams(pydantic.BaseModel):
    stage: typing.Union[models.aws_apigateway.StageBaseDef, models.aws_apigateway.StageDef] = pydantic.Field(..., description='-')

class AwsApigatewayIApiKeyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsApigatewayIApiKeyDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApigatewayIApiKeyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_apigateway.IAuthorizer skipped


class AwsApigatewayIDomainNameDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsApigatewayIDomainNameDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApigatewayIDomainNameDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApigatewayIGatewayResponseDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsApigatewayIGatewayResponseDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApigatewayIGatewayResponseDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_apigateway.IModel skipped


class AwsApigatewayIRequestValidatorDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsApigatewayIRequestValidatorDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApigatewayIRequestValidatorDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApigatewayIResourceDefConfig(pydantic.BaseModel):
    add_cors_preflight: typing.Optional[list[AwsApigatewayIResourceDefAddCorsPreflightParams]] = pydantic.Field(None, description='Adds an OPTIONS method to this resource which responds to Cross-Origin Resource Sharing (CORS) preflight requests.\nCross-Origin Resource Sharing (CORS) is a mechanism that uses additional\nHTTP headers to tell browsers to give a web application running at one\norigin, access to selected resources from a different origin. A web\napplication executes a cross-origin HTTP request when it requests a\nresource that has a different origin (domain, protocol, or port) from its\nown.')
    add_method: typing.Optional[list[AwsApigatewayIResourceDefAddMethodParams]] = pydantic.Field(None, description='Defines a new method for this resource.')
    add_proxy: typing.Optional[list[AwsApigatewayIResourceDefAddProxyParams]] = pydantic.Field(None, description='Adds a greedy proxy resource ("{proxy+}") and an ANY method to this route.')
    add_resource: typing.Optional[list[AwsApigatewayIResourceDefAddResourceParams]] = pydantic.Field(None, description='Defines a new child resource where this resource is the parent.')
    apply_removal_policy: typing.Optional[list[AwsApigatewayIResourceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    get_resource: typing.Optional[list[AwsApigatewayIResourceDefGetResourceParams]] = pydantic.Field(None, description='Retrieves a child resource by path part.')
    resource_for_path: typing.Optional[list[AwsApigatewayIResourceDefResourceForPathParams]] = pydantic.Field(None, description='Gets or create all resources leading up to the specified path.\n- Path may only start with "/" if this method is called on the root resource.\n- All resources are created using default options.')


class AwsApigatewayIResourceDefAddCorsPreflightParams(pydantic.BaseModel):
    allow_origins: typing.Sequence[str] = pydantic.Field(..., description='Specifies the list of origins that are allowed to make requests to this resource. If you wish to allow all origins, specify ``Cors.ALL_ORIGINS`` or ``[ * ]``. Responses will include the ``Access-Control-Allow-Origin`` response header. If ``Cors.ALL_ORIGINS`` is specified, the ``Vary: Origin`` response header will also be included.\n')
    allow_credentials: typing.Optional[bool] = pydantic.Field(None, description='The Access-Control-Allow-Credentials response header tells browsers whether to expose the response to frontend JavaScript code when the request\'s credentials mode (Request.credentials) is "include". When a request\'s credentials mode (Request.credentials) is "include", browsers will only expose the response to frontend JavaScript code if the Access-Control-Allow-Credentials value is true. Credentials are cookies, authorization headers or TLS client certificates. Default: false\n')
    allow_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The Access-Control-Allow-Headers response header is used in response to a preflight request which includes the Access-Control-Request-Headers to indicate which HTTP headers can be used during the actual request. Default: Cors.DEFAULT_HEADERS\n')
    allow_methods: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The Access-Control-Allow-Methods response header specifies the method or methods allowed when accessing the resource in response to a preflight request. If ``ANY`` is specified, it will be expanded to ``Cors.ALL_METHODS``. Default: Cors.ALL_METHODS\n')
    disable_cache: typing.Optional[bool] = pydantic.Field(None, description='Sets Access-Control-Max-Age to -1, which means that caching is disabled. This option cannot be used with ``maxAge``. Default: - cache is enabled\n')
    expose_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The Access-Control-Expose-Headers response header indicates which headers can be exposed as part of the response by listing their names. If you want clients to be able to access other headers, you have to list them using the Access-Control-Expose-Headers header. Default: - only the 6 CORS-safelisted response headers are exposed: Cache-Control, Content-Language, Content-Type, Expires, Last-Modified, Pragma\n')
    max_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The Access-Control-Max-Age response header indicates how long the results of a preflight request (that is the information contained in the Access-Control-Allow-Methods and Access-Control-Allow-Headers headers) can be cached. To disable caching altogether use ``disableCache: true``. Default: - browser-specific (see reference)\n')
    status_code: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the response status code returned from the OPTIONS method. Default: 204\n')
    return_config: typing.Optional[list[models.aws_apigateway.MethodDefConfig]] = pydantic.Field(None)

class AwsApigatewayIResourceDefAddMethodParams(pydantic.BaseModel):
    http_method: str = pydantic.Field(..., description='The HTTP method.\n')
    target: typing.Optional[models.aws_apigateway.IntegrationDef] = pydantic.Field(None, description='The target backend integration for this method.\n')
    api_key_required: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the method requires clients to submit a valid API key. Default: false\n')
    authorization_scopes: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of authorization scopes configured on the method. The scopes are used with a COGNITO_USER_POOLS authorizer to authorize the method invocation. Default: - no authorization scopes\n')
    authorization_type: typing.Optional[aws_cdk.aws_apigateway.AuthorizationType] = pydantic.Field(None, description="Method authorization. If the value is set of ``Custom``, an ``authorizer`` must also be specified. If you're using one of the authorizers that are available via the ``Authorizer`` class, such as ``Authorizer#token()``, it is recommended that this option not be specified. The authorizer will take care of setting the correct authorization type. However, specifying an authorization type using this property that conflicts with what is expected by the ``Authorizer`` will result in an error. Default: - open access unless ``authorizer`` is specified\n")
    authorizer: typing.Optional[typing.Union[models.aws_apigateway.AuthorizerDef, models.aws_apigateway.CognitoUserPoolsAuthorizerDef, models.aws_apigateway.CognitoUserPoolsAuthorizerDef, models.aws_apigateway.RequestAuthorizerDef, models.aws_apigateway.RequestAuthorizerDef, models.aws_apigateway.TokenAuthorizerDef, models.aws_apigateway.TokenAuthorizerDef]] = pydantic.Field(None, description='If ``authorizationType`` is ``Custom``, this specifies the ID of the method authorizer resource. If specified, the value of ``authorizationType`` must be set to ``Custom``\n')
    method_responses: typing.Optional[typing.Sequence[typing.Union[models.aws_apigateway.MethodResponseDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The responses that can be sent to the client who calls the method. Default: None This property is not required, but if these are not supplied for a Lambda proxy integration, the Lambda function must return a value of the correct format, for the integration response to be correctly mapped to a response to the client.\n')
    operation_name: typing.Optional[str] = pydantic.Field(None, description='A friendly operation name for the method. For example, you can assign the OperationName of ListPets for the GET /pets method.\n')
    request_models: typing.Optional[typing.Mapping[str, typing.Union[models.aws_apigateway.ModelDef]]] = pydantic.Field(None, description="The models which describe data structure of request payload. When combined with ``requestValidator`` or ``requestValidatorOptions``, the service will validate the API request payload before it reaches the API's Integration (including proxies). Specify ``requestModels`` as key-value pairs, with a content type (e.g. ``'application/json'``) as the key and an API Gateway Model as the value.\n")
    request_parameters: typing.Optional[typing.Mapping[str, bool]] = pydantic.Field(None, description='The request parameters that API Gateway accepts. Specify request parameters as key-value pairs (string-to-Boolean mapping), with a source as the key and a Boolean as the value. The Boolean specifies whether a parameter is required. A source must match the format method.request.location.name, where the location is querystring, path, or header, and name is a valid, unique parameter name. Default: None\n')
    request_validator: typing.Optional[typing.Union[models.aws_apigateway.RequestValidatorDef]] = pydantic.Field(None, description='The ID of the associated request validator. Only one of ``requestValidator`` or ``requestValidatorOptions`` must be specified. Works together with ``requestModels`` or ``requestParameters`` to validate the request before it reaches integration like Lambda Proxy Integration. Default: - No default validator\n')
    request_validator_options: typing.Union[models.aws_apigateway.RequestValidatorOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Request validator options to create new validator Only one of ``requestValidator`` or ``requestValidatorOptions`` must be specified. Works together with ``requestModels`` or ``requestParameters`` to validate the request before it reaches integration like Lambda Proxy Integration. Default: - No default validator\n')
    return_config: typing.Optional[list[models.aws_apigateway.MethodDefConfig]] = pydantic.Field(None)

class AwsApigatewayIResourceDefAddProxyParams(pydantic.BaseModel):
    any_method: typing.Optional[bool] = pydantic.Field(None, description='Adds an "ANY" method to this resource. If set to ``false``, you will have to explicitly add methods to this resource after it\'s created. Default: true\n')
    default_cors_preflight_options: typing.Union[models.aws_apigateway.CorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Adds a CORS preflight OPTIONS method to this resource and all child resources. You can add CORS at the resource-level using ``addCorsPreflight``. Default: - CORS is disabled\n')
    default_integration: typing.Optional[models.aws_apigateway.IntegrationDef] = pydantic.Field(None, description='An integration to use as a default for all methods created within this API unless an integration is specified. Default: - Inherited from parent.\n')
    default_method_options: typing.Union[models.aws_apigateway.MethodOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Method options to use as a default for all methods created within this API unless custom options are specified. Default: - Inherited from parent.')
    return_config: typing.Optional[list[models.aws_apigateway.ProxyResourceDefConfig]] = pydantic.Field(None)

class AwsApigatewayIResourceDefAddResourceParams(pydantic.BaseModel):
    path_part: str = pydantic.Field(..., description='The path part for the child resource.\n')
    default_cors_preflight_options: typing.Union[models.aws_apigateway.CorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Adds a CORS preflight OPTIONS method to this resource and all child resources. You can add CORS at the resource-level using ``addCorsPreflight``. Default: - CORS is disabled\n')
    default_integration: typing.Optional[models.aws_apigateway.IntegrationDef] = pydantic.Field(None, description='An integration to use as a default for all methods created within this API unless an integration is specified. Default: - Inherited from parent.\n')
    default_method_options: typing.Union[models.aws_apigateway.MethodOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Method options to use as a default for all methods created within this API unless custom options are specified. Default: - Inherited from parent.\n')
    return_config: typing.Optional[list[models.aws_apigateway.ResourceDefConfig]] = pydantic.Field(None)

class AwsApigatewayIResourceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApigatewayIResourceDefGetResourceParams(pydantic.BaseModel):
    path_part: str = pydantic.Field(..., description='The path part of the child resource.\n')

class AwsApigatewayIResourceDefResourceForPathParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The relative path.\n')
    return_config: typing.Optional[list[models.aws_apigateway.ResourceDefConfig]] = pydantic.Field(None)

class AwsApigatewayIRestApiDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsApigatewayIRestApiDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    arn_for_execute_api: typing.Optional[list[AwsApigatewayIRestApiDefArnForExecuteApiParams]] = pydantic.Field(None, description='Gets the "execute-api" ARN.')


class AwsApigatewayIRestApiDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApigatewayIRestApiDefArnForExecuteApiParams(pydantic.BaseModel):
    method: typing.Optional[str] = pydantic.Field(None, description='The method (default ``*``).\n')
    path: typing.Optional[str] = pydantic.Field(None, description="The resource path. Must start with '/' (default ``*``)\n")
    stage: typing.Optional[str] = pydantic.Field(None, description='The stage (default ``*``).\n\n:default:\n\n"*" returns the execute API ARN for all methods/resources in\nthis API.\n')

class AwsApigatewayIStageDefConfig(pydantic.BaseModel):
    add_api_key: typing.Optional[list[AwsApigatewayIStageDefAddApiKeyParams]] = pydantic.Field(None, description='Add an ApiKey to this Stage.')
    apply_removal_policy: typing.Optional[list[AwsApigatewayIStageDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApigatewayIStageDefAddApiKeyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    api_key_name: typing.Optional[str] = pydantic.Field(None, description="A name for the API key. If you don't specify a name, AWS CloudFormation generates a unique physical ID and uses that ID for the API key name. Default: automically generated name\n")
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the purpose of the API key. Default: none\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value of the API key. Must be at least 20 characters long. Default: none\n')
    default_cors_preflight_options: typing.Union[models.aws_apigateway.CorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Adds a CORS preflight OPTIONS method to this resource and all child resources. You can add CORS at the resource-level using ``addCorsPreflight``. Default: - CORS is disabled\n')
    default_integration: typing.Optional[models.aws_apigateway.IntegrationDef] = pydantic.Field(None, description='An integration to use as a default for all methods created within this API unless an integration is specified. Default: - Inherited from parent.\n')
    default_method_options: typing.Union[models.aws_apigateway.MethodOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Method options to use as a default for all methods created within this API unless custom options are specified. Default: - Inherited from parent.')
    return_config: typing.Optional[list[models._interface_methods.AwsApigatewayIApiKeyDefConfig]] = pydantic.Field(None)

class AwsApigatewayIStageDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApigatewayIUsagePlanDefConfig(pydantic.BaseModel):
    add_api_key: typing.Optional[list[AwsApigatewayIUsagePlanDefAddApiKeyParams]] = pydantic.Field(None, description='Adds an ApiKey.')
    apply_removal_policy: typing.Optional[list[AwsApigatewayIUsagePlanDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApigatewayIUsagePlanDefAddApiKeyParams(pydantic.BaseModel):
    api_key: typing.Union[models.aws_apigateway.ApiKeyDef, models.aws_apigateway.RateLimitedApiKeyDef] = pydantic.Field(..., description='the api key to associate with this usage plan.\n')
    override_logical_id: typing.Optional[str] = pydantic.Field(None, description='Override the CloudFormation logical id of the AWS::ApiGateway::UsagePlanKey resource. Default: - autogenerated by the CDK')

class AwsApigatewayIUsagePlanDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApigatewayIVpcLinkDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsApigatewayIVpcLinkDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApigatewayIVpcLinkDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsApplicationautoscalingIScalableTargetDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsApplicationautoscalingIScalableTargetDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsApplicationautoscalingIScalableTargetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppmeshIGatewayRouteDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsAppmeshIGatewayRouteDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAppmeshIGatewayRouteDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppmeshIMeshDefConfig(pydantic.BaseModel):
    add_virtual_gateway: typing.Optional[list[AwsAppmeshIMeshDefAddVirtualGatewayParams]] = pydantic.Field(None, description='Creates a new VirtualGateway in this Mesh.\nNote that the Gateway is created in the same Stack that this Mesh belongs to,\nwhich might be different than the current stack.')
    add_virtual_node: typing.Optional[list[AwsAppmeshIMeshDefAddVirtualNodeParams]] = pydantic.Field(None, description='Creates a new VirtualNode in this Mesh.\nNote that the Node is created in the same Stack that this Mesh belongs to,\nwhich might be different than the current stack.')
    add_virtual_router: typing.Optional[list[AwsAppmeshIMeshDefAddVirtualRouterParams]] = pydantic.Field(None, description='Creates a new VirtualRouter in this Mesh.\nNote that the Router is created in the same Stack that this Mesh belongs to,\nwhich might be different than the current stack.')
    apply_removal_policy: typing.Optional[list[AwsAppmeshIMeshDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAppmeshIMeshDefAddVirtualGatewayParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    access_log: typing.Optional[models.aws_appmesh.AccessLogDef] = pydantic.Field(None, description='Access Logging Configuration for the VirtualGateway. Default: - no access logging\n')
    backend_defaults: typing.Union[models.aws_appmesh.BackendDefaultsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Default Configuration Virtual Node uses to communicate with Virtual Service. Default: - No Config\n')
    listeners: typing.Optional[typing.Sequence[models.aws_appmesh.VirtualGatewayListenerDef]] = pydantic.Field(None, description='Listeners for the VirtualGateway. Only one is supported. Default: - Single HTTP listener on port 8080\n')
    virtual_gateway_name: typing.Optional[str] = pydantic.Field(None, description='Name of the VirtualGateway. Default: - A name is automatically determined')
    return_config: typing.Optional[list[models.aws_appmesh.VirtualGatewayDefConfig]] = pydantic.Field(None)

class AwsAppmeshIMeshDefAddVirtualNodeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    access_log: typing.Optional[models.aws_appmesh.AccessLogDef] = pydantic.Field(None, description='Access Logging Configuration for the virtual node. Default: - No access logging\n')
    backend_defaults: typing.Union[models.aws_appmesh.BackendDefaultsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Default Configuration Virtual Node uses to communicate with Virtual Service. Default: - No Config\n')
    backends: typing.Optional[typing.Sequence[models.aws_appmesh.BackendDef]] = pydantic.Field(None, description='Virtual Services that this is node expected to send outbound traffic to. Default: - No backends\n')
    listeners: typing.Optional[typing.Sequence[models.aws_appmesh.VirtualNodeListenerDef]] = pydantic.Field(None, description='Initial listener for the virtual node. Default: - No listeners\n')
    service_discovery: typing.Optional[models.aws_appmesh.ServiceDiscoveryDef] = pydantic.Field(None, description='Defines how upstream clients will discover this VirtualNode. Default: - No Service Discovery\n')
    virtual_node_name: typing.Optional[str] = pydantic.Field(None, description='The name of the VirtualNode. Default: - A name is automatically determined')
    return_config: typing.Optional[list[models.aws_appmesh.VirtualNodeDefConfig]] = pydantic.Field(None)

class AwsAppmeshIMeshDefAddVirtualRouterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    listeners: typing.Optional[typing.Sequence[models.aws_appmesh.VirtualRouterListenerDef]] = pydantic.Field(None, description='Listener specification for the VirtualRouter. Default: - A listener on HTTP port 8080\n')
    virtual_router_name: typing.Optional[str] = pydantic.Field(None, description='The name of the VirtualRouter. Default: - A name is automatically determined')
    return_config: typing.Optional[list[models.aws_appmesh.VirtualRouterDefConfig]] = pydantic.Field(None)

class AwsAppmeshIMeshDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppmeshIRouteDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsAppmeshIRouteDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAppmeshIRouteDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppmeshIVirtualGatewayDefConfig(pydantic.BaseModel):
    add_gateway_route: typing.Optional[list[AwsAppmeshIVirtualGatewayDefAddGatewayRouteParams]] = pydantic.Field(None, description='Utility method to add a new GatewayRoute to the VirtualGateway.')
    apply_removal_policy: typing.Optional[list[AwsAppmeshIVirtualGatewayDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_stream_aggregated_resources: typing.Optional[list[AwsAppmeshIVirtualGatewayDefGrantStreamAggregatedResourcesParams]] = pydantic.Field(None, description='Grants the given entity ``appmesh:StreamAggregatedResources``.')


class AwsAppmeshIVirtualGatewayDefAddGatewayRouteParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    route_spec: models.aws_appmesh.GatewayRouteSpecDef = pydantic.Field(..., description='What protocol the route uses.\n')
    gateway_route_name: typing.Optional[str] = pydantic.Field(None, description='The name of the GatewayRoute. Default: - an automatically generated name')
    return_config: typing.Optional[list[models.aws_appmesh.GatewayRouteDefConfig]] = pydantic.Field(None)

class AwsAppmeshIVirtualGatewayDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppmeshIVirtualGatewayDefGrantStreamAggregatedResourcesParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsAppmeshIVirtualNodeDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsAppmeshIVirtualNodeDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_stream_aggregated_resources: typing.Optional[list[AwsAppmeshIVirtualNodeDefGrantStreamAggregatedResourcesParams]] = pydantic.Field(None, description='Grants the given entity ``appmesh:StreamAggregatedResources``.')


class AwsAppmeshIVirtualNodeDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppmeshIVirtualNodeDefGrantStreamAggregatedResourcesParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsAppmeshIVirtualRouterDefConfig(pydantic.BaseModel):
    add_route: typing.Optional[list[AwsAppmeshIVirtualRouterDefAddRouteParams]] = pydantic.Field(None, description='Add a single route to the router.')
    apply_removal_policy: typing.Optional[list[AwsAppmeshIVirtualRouterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAppmeshIVirtualRouterDefAddRouteParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    route_spec: models.aws_appmesh.RouteSpecDef = pydantic.Field(..., description='Protocol specific spec.\n')
    route_name: typing.Optional[str] = pydantic.Field(None, description='The name of the route. Default: - An automatically generated name')
    return_config: typing.Optional[list[models.aws_appmesh.RouteDefConfig]] = pydantic.Field(None)

class AwsAppmeshIVirtualRouterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppmeshIVirtualServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsAppmeshIVirtualServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAppmeshIVirtualServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppsyncIAppsyncFunctionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsAppsyncIAppsyncFunctionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAppsyncIAppsyncFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppsyncIGraphqlApiDefConfig(pydantic.BaseModel):
    add_dynamo_db_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddDynamoDbDataSourceParams]] = pydantic.Field(None, description='add a new DynamoDB data source to this API.')
    add_elasticsearch_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddElasticsearchDataSourceParams]] = pydantic.Field(None, description='(deprecated) add a new elasticsearch data source to this API.')
    add_event_bridge_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddEventBridgeDataSourceParams]] = pydantic.Field(None, description='Add an EventBridge data source to this api.')
    add_http_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddHttpDataSourceParams]] = pydantic.Field(None, description='add a new http data source to this API.')
    add_lambda_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddLambdaDataSourceParams]] = pydantic.Field(None, description='add a new Lambda data source to this API.')
    add_none_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddNoneDataSourceParams]] = pydantic.Field(None, description="add a new dummy data source to this API.\nUseful for pipeline resolvers\nand for backend changes that don't require a data source.")
    add_open_search_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddOpenSearchDataSourceParams]] = pydantic.Field(None, description='Add a new OpenSearch data source to this API.')
    add_rds_data_source: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddRdsDataSourceParams]] = pydantic.Field(None, description='add a new Rds data source to this API.')
    add_schema_dependency: typing.Optional[list[AwsAppsyncIGraphqlApiDefAddSchemaDependencyParams]] = pydantic.Field(None, description='Add schema dependency if not imported.')
    apply_removal_policy: typing.Optional[list[AwsAppsyncIGraphqlApiDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    create_resolver: typing.Optional[list[AwsAppsyncIGraphqlApiDefCreateResolverParams]] = pydantic.Field(None, description='creates a new resolver for this datasource and API using the given properties.')


class AwsAppsyncIGraphqlApiDefAddDynamoDbDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    table: typing.Union[models.aws_dynamodb.TableDef] = pydantic.Field(..., description='The DynamoDB table backing this data source.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id')
    return_config: typing.Optional[list[models.aws_appsync.DynamoDbDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddElasticsearchDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    domain: typing.Union[models.aws_elasticsearch.DomainDef] = pydantic.Field(..., description='The elasticsearch domain for this data source.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id\n\n:deprecated: - use ``addOpenSearchDataSource``\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_appsync.ElasticsearchDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddEventBridgeDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    event_bus: typing.Union[models.aws_events.EventBusDef] = pydantic.Field(..., description='The EventBridge EventBus on which to put events.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id')
    return_config: typing.Optional[list[models.aws_appsync.EventBridgeDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddHttpDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    endpoint: str = pydantic.Field(..., description='The http endpoint.\n')
    authorization_config: typing.Union[models.aws_appsync.AwsIamConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The authorization config in case the HTTP endpoint requires authorization. Default: - none\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id')
    return_config: typing.Optional[list[models.aws_appsync.HttpDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddLambdaDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    lambda_function: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='The Lambda function to call to interact with this data source.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id')
    return_config: typing.Optional[list[models.aws_appsync.LambdaDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddNoneDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id')
    return_config: typing.Optional[list[models.aws_appsync.NoneDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddOpenSearchDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    domain: typing.Union[models.aws_opensearchservice.DomainDef] = pydantic.Field(..., description='The OpenSearch domain for this data source.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id')
    return_config: typing.Optional[list[models.aws_appsync.OpenSearchDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddRdsDataSourceParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description="The data source's id.\n")
    serverless_cluster: typing.Union[models.aws_rds.ServerlessClusterDef, models.aws_rds.ServerlessClusterFromSnapshotDef] = pydantic.Field(..., description='The serverless cluster to interact with this data source.\n')
    secret_store: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret store that contains the username and password for the serverless cluster.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The optional name of the database to use within the cluster.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the data source. Default: - No description\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the data source, overrides the id given by cdk. Default: - generated by cdk given the id')
    return_config: typing.Optional[list[models.aws_appsync.RdsDataSourceDefConfig]] = pydantic.Field(None)

class AwsAppsyncIGraphqlApiDefAddSchemaDependencyParams(pydantic.BaseModel):
    construct_: models.CfnResourceDef = pydantic.Field(..., description='the dependee.', alias='construct')

class AwsAppsyncIGraphqlApiDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAppsyncIGraphqlApiDefCreateResolverParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    data_source: typing.Optional[models.aws_appsync.BaseDataSourceDef] = pydantic.Field(None, description='The data source this resolver is using. Default: - No datasource\n')
    field_name: str = pydantic.Field(..., description='name of the GraphQL field in the given type this resolver is attached to.\n')
    type_name: str = pydantic.Field(..., description='name of the GraphQL type this resolver is attached to.\n')
    caching_config: typing.Union[models.aws_appsync.CachingConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The caching configuration for this resolver. Default: - No caching configuration\n')
    code: typing.Optional[models.aws_appsync.CodeDef] = pydantic.Field(None, description='The function code. Default: - no code is used\n')
    max_batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of elements per batch, when using batch invoke. Default: - No max batch size\n')
    pipeline_config: typing.Optional[typing.Sequence[typing.Union[models.aws_appsync.AppsyncFunctionDef]]] = pydantic.Field(None, description='configuration of the pipeline resolver. Default: - no pipeline resolver configuration An empty array | undefined sets resolver to be of kind, unit\n')
    request_mapping_template: typing.Optional[models.aws_appsync.MappingTemplateDef] = pydantic.Field(None, description='The request mapping template for this resolver. Default: - No mapping template\n')
    response_mapping_template: typing.Optional[models.aws_appsync.MappingTemplateDef] = pydantic.Field(None, description='The response mapping template for this resolver. Default: - No mapping template\n')
    runtime: typing.Optional[models.aws_appsync.FunctionRuntimeDef] = pydantic.Field(None, description='The functions runtime. Default: - no function runtime, VTL mapping templates used')

class AwsAppsyncISchemaDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsAppsyncISchemaDefBindParams]] = pydantic.Field(None, description='Binds a schema string to a GraphQlApi.')


class AwsAppsyncISchemaDefBindParams(pydantic.BaseModel):
    api: typing.Union[models.aws_appsync.GraphqlApiBaseDef, models.aws_appsync.GraphqlApiDef] = pydantic.Field(..., description='the api to bind the schema to.\n')
#  aws-cdk-lib.aws_appsync.ISchemaConfig skipped


class AwsAutoscalingIAutoScalingGroupDefConfig(pydantic.BaseModel):
    add_lifecycle_hook: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefAddLifecycleHookParams]] = pydantic.Field(None, description='Send a message to either an SQS queue or SNS topic when instances launch or terminate.')
    add_user_data: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefAddUserDataParams]] = pydantic.Field(None, description="Add command to the startup script of fleet instances.\nThe command must be in the scripting language supported by the fleet's OS (i.e. Linux/Windows).\nDoes nothing for imported ASGs.")
    add_warm_pool: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefAddWarmPoolParams]] = pydantic.Field(None, description='Add a pool of pre-initialized EC2 instances that sits alongside an Auto Scaling group.')
    apply_removal_policy: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    scale_on_cpu_utilization: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefScaleOnCpuUtilizationParams]] = pydantic.Field(None, description='Scale out or in to achieve a target CPU utilization.')
    scale_on_incoming_bytes: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefScaleOnIncomingBytesParams]] = pydantic.Field(None, description='Scale out or in to achieve a target network ingress rate.')
    scale_on_metric: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefScaleOnMetricParams]] = pydantic.Field(None, description='Scale out or in, in response to a metric.')
    scale_on_outgoing_bytes: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefScaleOnOutgoingBytesParams]] = pydantic.Field(None, description='Scale out or in to achieve a target network egress rate.')
    scale_on_schedule: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefScaleOnScheduleParams]] = pydantic.Field(None, description='Scale out or in based on time.')
    scale_to_track_metric: typing.Optional[list[AwsAutoscalingIAutoScalingGroupDefScaleToTrackMetricParams]] = pydantic.Field(None, description='Scale out or in in order to keep a metric around a target value.')


class AwsAutoscalingIAutoScalingGroupDefAddLifecycleHookParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    lifecycle_transition: aws_cdk.aws_autoscaling.LifecycleTransition = pydantic.Field(..., description='The state of the Amazon EC2 instance to which you want to attach the lifecycle hook.\n')
    default_result: typing.Optional[aws_cdk.aws_autoscaling.DefaultResult] = pydantic.Field(None, description='The action the Auto Scaling group takes when the lifecycle hook timeout elapses or if an unexpected failure occurs. Default: Continue\n')
    heartbeat_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Maximum time between calls to RecordLifecycleActionHeartbeat for the hook. If the lifecycle hook times out, perform the action in DefaultResult. Default: - No heartbeat timeout.\n')
    lifecycle_hook_name: typing.Optional[str] = pydantic.Field(None, description='Name of the lifecycle hook. Default: - Automatically generated name.\n')
    notification_metadata: typing.Optional[str] = pydantic.Field(None, description='Additional data to pass to the lifecycle hook target. Default: - No metadata.\n')
    notification_target: typing.Optional[typing.Union[models.aws_autoscaling_hooktargets.FunctionHookDef, models.aws_autoscaling_hooktargets.QueueHookDef, models.aws_autoscaling_hooktargets.TopicHookDef]] = pydantic.Field(None, description='The target of the lifecycle hook. Default: - No target.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that allows publishing to the notification target. Default: - A role will be created if a target is provided. Otherwise, no role is created.')
    return_config: typing.Optional[list[models.aws_autoscaling.LifecycleHookDefConfig]] = pydantic.Field(None)

class AwsAutoscalingIAutoScalingGroupDefAddUserDataParams(pydantic.BaseModel):
    commands: list[str] = pydantic.Field(...)

class AwsAutoscalingIAutoScalingGroupDefAddWarmPoolParams(pydantic.BaseModel):
    max_group_prepared_capacity: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of instances that are allowed to be in the warm pool or in any state except Terminated for the Auto Scaling group. If the value is not specified, Amazon EC2 Auto Scaling launches and maintains the difference between the group's maximum capacity and its desired capacity. Default: - max size of the Auto Scaling group\n")
    min_size: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of instances to maintain in the warm pool. Default: 0\n')
    pool_state: typing.Optional[aws_cdk.aws_autoscaling.PoolState] = pydantic.Field(None, description='The instance state to transition to after the lifecycle actions are complete. Default: PoolState.STOPPED\n')
    reuse_on_scale_in: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether instances in the Auto Scaling group can be returned to the warm pool on scale in. If the value is not specified, instances in the Auto Scaling group will be terminated when the group scales in. Default: false')
    return_config: typing.Optional[list[models.aws_autoscaling.WarmPoolDefConfig]] = pydantic.Field(None)

class AwsAutoscalingIAutoScalingGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAutoscalingIAutoScalingGroupDefScaleOnCpuUtilizationParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target_utilization_percent: typing.Union[int, float] = pydantic.Field(..., description='Target average CPU utilization across the task.\n')
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scaling completes before another scaling activity can start. Default: - The default cooldown configured on the AutoScalingGroup.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the autoscaling group. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the group. Default: false\n")
    estimated_instance_warmup: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Estimated time until a newly launched instance can send metrics to CloudWatch. Default: - Same as the cooldown.')

class AwsAutoscalingIAutoScalingGroupDefScaleOnIncomingBytesParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target_bytes_per_second: typing.Union[int, float] = pydantic.Field(..., description='Target average bytes/seconds on each instance.\n')
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scaling completes before another scaling activity can start. Default: - The default cooldown configured on the AutoScalingGroup.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the autoscaling group. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the group. Default: false\n")
    estimated_instance_warmup: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Estimated time until a newly launched instance can send metrics to CloudWatch. Default: - Same as the cooldown.')

class AwsAutoscalingIAutoScalingGroupDefScaleOnMetricParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    metric: typing.Union[models.aws_cloudwatch.MathExpressionDef, models.aws_cloudwatch.MetricDef] = pydantic.Field(..., description='Metric to scale on.\n')
    scaling_steps: typing.Sequence[typing.Union[models.aws_autoscaling.ScalingIntervalDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The intervals for scaling. Maps a range of metric values to a particular scaling behavior.\n')
    adjustment_type: typing.Optional[aws_cdk.aws_autoscaling.AdjustmentType] = pydantic.Field(None, description="How the adjustment numbers inside 'intervals' are interpreted. Default: ChangeInCapacity\n")
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Grace period after scaling activity. Default: Default cooldown period on your AutoScalingGroup\n')
    estimated_instance_warmup: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Estimated time until a newly launched instance can send metrics to CloudWatch. Default: Same as the cooldown\n')
    evaluation_periods: typing.Union[int, float, None] = pydantic.Field(None, description='How many evaluation periods of the metric to wait before triggering a scaling action. Raising this value can be used to smooth out the metric, at the expense of slower response times. Default: 1\n')
    metric_aggregation_type: typing.Optional[aws_cdk.aws_autoscaling.MetricAggregationType] = pydantic.Field(None, description='Aggregation to apply to all data points over the evaluation periods. Only has meaning if ``evaluationPeriods != 1``. Default: - The statistic from the metric if applicable (MIN, MAX, AVERAGE), otherwise AVERAGE.\n')
    min_adjustment_magnitude: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum absolute number to adjust capacity with as result of percentage scaling. Only when using AdjustmentType = PercentChangeInCapacity, this number controls the minimum absolute effect size. Default: No minimum scaling effect')

class AwsAutoscalingIAutoScalingGroupDefScaleOnOutgoingBytesParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target_bytes_per_second: typing.Union[int, float] = pydantic.Field(..., description='Target average bytes/seconds on each instance.\n')
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scaling completes before another scaling activity can start. Default: - The default cooldown configured on the AutoScalingGroup.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the autoscaling group. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the group. Default: false\n")
    estimated_instance_warmup: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Estimated time until a newly launched instance can send metrics to CloudWatch. Default: - Same as the cooldown.')

class AwsAutoscalingIAutoScalingGroupDefScaleOnScheduleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    schedule: models.aws_autoscaling.ScheduleDef = pydantic.Field(..., description='When to perform this action. Supports cron expressions. For more information about cron expressions, see https://en.wikipedia.org/wiki/Cron.\n')
    desired_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new desired capacity. At the scheduled time, set the desired capacity to the given capacity. At least one of maxCapacity, minCapacity, or desiredCapacity must be supplied. Default: - No new desired capacity.\n')
    end_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action expires. Default: - The rule never expires.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new maximum capacity. At the scheduled time, set the maximum capacity to the given capacity. At least one of maxCapacity, minCapacity, or desiredCapacity must be supplied. Default: - No new maximum capacity.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new minimum capacity. At the scheduled time, set the minimum capacity to the given capacity. At least one of maxCapacity, minCapacity, or desiredCapacity must be supplied. Default: - No new minimum capacity.\n')
    start_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action becomes active. Default: - The rule is activate immediately.\n')
    time_zone: typing.Optional[str] = pydantic.Field(None, description='Specifies the time zone for a cron expression. If a time zone is not provided, UTC is used by default. Valid values are the canonical names of the IANA time zones, derived from the IANA Time Zone Database (such as Etc/GMT+9 or Pacific/Tahiti). For more information, see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. Default: - UTC')
    return_config: typing.Optional[list[models.aws_autoscaling.ScheduledActionDefConfig]] = pydantic.Field(None)

class AwsAutoscalingIAutoScalingGroupDefScaleToTrackMetricParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    metric: typing.Union[models.aws_cloudwatch.MathExpressionDef, models.aws_cloudwatch.MetricDef] = pydantic.Field(..., description="Metric to track. The metric must represent a utilization, so that if it's higher than the target value, your ASG should scale out, and if it's lower it should scale in.\n")
    target_value: typing.Union[int, float] = pydantic.Field(..., description='Value to keep the metric around.\n')
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scaling completes before another scaling activity can start. Default: - The default cooldown configured on the AutoScalingGroup.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the autoscaling group. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the group. Default: false\n")
    estimated_instance_warmup: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Estimated time until a newly launched instance can send metrics to CloudWatch. Default: - Same as the cooldown.')

class AwsAutoscalingILifecycleHookDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsAutoscalingILifecycleHookDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsAutoscalingILifecycleHookDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsAutoscalingILifecycleHookTargetDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsAutoscalingILifecycleHookTargetDefBindParams]] = pydantic.Field(None, description='Called when this object is used as the target of a lifecycle hook.')


class AwsAutoscalingILifecycleHookTargetDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    lifecycle_hook: models.aws_autoscaling.LifecycleHookDef = pydantic.Field(..., description='The lifecycle hook to attach to. [disable-awslint:ref-via-interface]\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to use when attaching to the lifecycle hook. [disable-awslint:ref-via-interface] Default: : a role is not created unless the target arn is specified')
    return_config: typing.Optional[list[models.aws_autoscaling.LifecycleHookTargetConfigDefConfig]] = pydantic.Field(None)

class AwsAutoscalingCommonIRandomGeneratorDefConfig(pydantic.BaseModel):
    next_boolean: typing.Optional[bool] = pydantic.Field(None, description='')
    next_int: typing.Optional[list[AwsAutoscalingCommonIRandomGeneratorDefNextIntParams]] = pydantic.Field(None, description='')


class AwsAutoscalingCommonIRandomGeneratorDefNextIntParams(pydantic.BaseModel):
    min: typing.Union[int, float] = pydantic.Field(..., description='-')
    max: typing.Union[int, float] = pydantic.Field(..., description='-')

class AwsBackupIBackupPlanDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsBackupIBackupPlanDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsBackupIBackupPlanDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsBackupIBackupVaultDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsBackupIBackupVaultDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsBackupIBackupVaultDefGrantParams]] = pydantic.Field(None, description='Grant the actions defined in actions to the given grantee on this backup vault.')


class AwsBackupIBackupVaultDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsBackupIBackupVaultDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCertificatemanagerICertificateDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCertificatemanagerICertificateDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    metric_days_to_expiry: typing.Optional[list[AwsCertificatemanagerICertificateDefMetricDaysToExpiryParams]] = pydantic.Field(None, description='Return the DaysToExpiry metric for this AWS Certificate Manager Certificate. By default, this is the minimum value over 1 day.\nThis metric is no longer emitted once the certificate has effectively\nexpired, so alarms configured on this metric should probably treat missing\ndata as "breaching".')


class AwsCertificatemanagerICertificateDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCertificatemanagerICertificateDefMetricDaysToExpiryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsChatbotISlackChannelConfigurationDefConfig(pydantic.BaseModel):
    add_to_role_policy: typing.Optional[list[AwsChatbotISlackChannelConfigurationDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role.')
    apply_removal_policy: typing.Optional[list[AwsChatbotISlackChannelConfigurationDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    bind_as_notification_rule_target: typing.Optional[list[AwsChatbotISlackChannelConfigurationDefBindAsNotificationRuleTargetParams]] = pydantic.Field(None, description='Returns a target configuration for notification rule.')
    metric: typing.Optional[list[AwsChatbotISlackChannelConfigurationDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this SlackChannelConfiguration.')


class AwsChatbotISlackChannelConfigurationDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsChatbotISlackChannelConfigurationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsChatbotISlackChannelConfigurationDefBindAsNotificationRuleTargetParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsChatbotISlackChannelConfigurationDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
#  aws-cdk-lib.aws_cloudfront.ICachePolicy skipped


class AwsCloudfrontIDistributionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCloudfrontIDistributionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsCloudfrontIDistributionDefGrantParams]] = pydantic.Field(None, description="Adds an IAM policy statement associated with this distribution to an IAM principal's policy.")
    grant_create_invalidation: typing.Optional[list[AwsCloudfrontIDistributionDefGrantCreateInvalidationParams]] = pydantic.Field(None, description='Grant to create invalidations for this bucket to an IAM principal (Role/Group/User).')


class AwsCloudfrontIDistributionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCloudfrontIDistributionDefGrantParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCloudfrontIDistributionDefGrantCreateInvalidationParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCloudfrontIFunctionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCloudfrontIFunctionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCloudfrontIFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCloudfrontIKeyGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCloudfrontIKeyGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCloudfrontIKeyGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCloudfrontIOriginDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsCloudfrontIOriginDefBindParams]] = pydantic.Field(None, description='The method called when a given Origin is added (for the first time) to a Distribution.')


class AwsCloudfrontIOriginDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    origin_id: str = pydantic.Field(..., description='The identifier of this Origin, as assigned by the Distribution this Origin has been used added to.')

class AwsCloudfrontIOriginAccessIdentityDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCloudfrontIOriginAccessIdentityDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCloudfrontIOriginAccessIdentityDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_cloudfront.IOriginRequestPolicy skipped


class AwsCloudfrontIPublicKeyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCloudfrontIPublicKeyDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCloudfrontIPublicKeyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_cloudfront.IResponseHeadersPolicy skipped


class AwsCloudwatchIAlarmDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCloudwatchIAlarmDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    render_alarm_rule: typing.Optional[bool] = pydantic.Field(None, description='serialized representation of Alarm Rule to be used when building the Composite Alarm resource.')


class AwsCloudwatchIAlarmDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCloudwatchIAlarmActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsCloudwatchIAlarmActionDefBindParams]] = pydantic.Field(None, description='Return the properties required to send alarm actions to this CloudWatch alarm.')


class AwsCloudwatchIAlarmActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='root Construct that allows creating new Constructs.\n')
    alarm: typing.Union[models.aws_cloudwatch.AlarmBaseDef, models.aws_cloudwatch.AlarmDef, models.aws_cloudwatch.CompositeAlarmDef] = pydantic.Field(..., description='CloudWatch alarm that the action will target.')

class AwsCloudwatchIAlarmRuleDefConfig(pydantic.BaseModel):
    render_alarm_rule: typing.Optional[bool] = pydantic.Field(None, description='serialized representation of Alarm Rule to be used when building the Composite Alarm resource.')

#  aws-cdk-lib.aws_cloudwatch.IMetric skipped


class AwsCloudwatchIWidgetDefConfig(pydantic.BaseModel):
    position: typing.Optional[list[AwsCloudwatchIWidgetDefPositionParams]] = pydantic.Field(None, description='Place the widget at a given position.')


class AwsCloudwatchIWidgetDefPositionParams(pydantic.BaseModel):
    x: typing.Union[int, float] = pydantic.Field(..., description='-\n')
    y: typing.Union[int, float] = pydantic.Field(..., description='-')

class AwsCodebuildIArtifactsDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsCodebuildIArtifactsDefBindParams]] = pydantic.Field(None, description='Callback when an Artifacts class is used in a CodeBuild Project.')


class AwsCodebuildIArtifactsDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='a root Construct that allows creating new Constructs.\n')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description='the Project this Artifacts is used in.')

class AwsCodebuildIBindableBuildImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsCodebuildIBindableBuildImageDefBindParams]] = pydantic.Field(None, description='Function that allows the build image access to the construct tree.')
    run_script_buildspec: typing.Optional[list[AwsCodebuildIBindableBuildImageDefRunScriptBuildspecParams]] = pydantic.Field(None, description='Make a buildspec to run the indicated script.')
    validate_: typing.Optional[list[AwsCodebuildIBindableBuildImageDefValidateParams]] = pydantic.Field(None, description='Allows the image a chance to validate whether the passed configuration is correct.', alias='validate')


class AwsCodebuildIBindableBuildImageDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description='-')

class AwsCodebuildIBindableBuildImageDefRunScriptBuildspecParams(pydantic.BaseModel):
    entrypoint: str = pydantic.Field(..., description='-')

class AwsCodebuildIBindableBuildImageDefValidateParams(pydantic.BaseModel):
    build_image: typing.Optional[typing.Union[models.aws_codebuild.LinuxArmBuildImageDef, models.aws_codebuild.LinuxBuildImageDef, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None, description='The image used for the builds. Default: LinuxBuildImage.STANDARD_1_0\n')
    certificate: typing.Union[models.aws_codebuild.BuildEnvironmentCertificateDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the PEM-encoded certificate for the build project. Default: - No external certificate is added to the project\n')
    compute_type: typing.Optional[aws_cdk.aws_codebuild.ComputeType] = pydantic.Field(None, description='The type of compute to use for this build. See the ``ComputeType`` enum for the possible values. Default: taken from ``#buildImage#defaultComputeType``\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables that your builds can use.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Indicates how the project builds Docker images. Specify true to enable running the Docker daemon inside a Docker container. This value must be set to true only if this build project will be used to build Docker images, and the specified build environment image is not one provided by AWS CodeBuild with Docker support. Otherwise, all associated builds that attempt to interact with the Docker daemon will fail. Default: false')

class AwsCodebuildIBuildImageDefConfig(pydantic.BaseModel):
    run_script_buildspec: typing.Optional[list[AwsCodebuildIBuildImageDefRunScriptBuildspecParams]] = pydantic.Field(None, description='Make a buildspec to run the indicated script.')
    validate_: typing.Optional[list[AwsCodebuildIBuildImageDefValidateParams]] = pydantic.Field(None, description='Allows the image a chance to validate whether the passed configuration is correct.', alias='validate')


class AwsCodebuildIBuildImageDefRunScriptBuildspecParams(pydantic.BaseModel):
    entrypoint: str = pydantic.Field(..., description='-')

class AwsCodebuildIBuildImageDefValidateParams(pydantic.BaseModel):
    build_image: typing.Optional[typing.Union[models.aws_codebuild.LinuxArmBuildImageDef, models.aws_codebuild.LinuxBuildImageDef, models.aws_codebuild.WindowsBuildImageDef]] = pydantic.Field(None, description='The image used for the builds. Default: LinuxBuildImage.STANDARD_1_0\n')
    certificate: typing.Union[models.aws_codebuild.BuildEnvironmentCertificateDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the PEM-encoded certificate for the build project. Default: - No external certificate is added to the project\n')
    compute_type: typing.Optional[aws_cdk.aws_codebuild.ComputeType] = pydantic.Field(None, description='The type of compute to use for this build. See the ``ComputeType`` enum for the possible values. Default: taken from ``#buildImage#defaultComputeType``\n')
    environment_variables: typing.Optional[typing.Mapping[str, typing.Union[models.aws_codebuild.BuildEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The environment variables that your builds can use.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Indicates how the project builds Docker images. Specify true to enable running the Docker daemon inside a Docker container. This value must be set to true only if this build project will be used to build Docker images, and the specified build environment image is not one provided by AWS CodeBuild with Docker support. Otherwise, all associated builds that attempt to interact with the Docker daemon will fail. Default: false')

class AwsCodebuildIFileSystemLocationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsCodebuildIFileSystemLocationDefBindParams]] = pydantic.Field(None, description='Called by the project when a file system is added so it can perform binding operations on this file system location.')


class AwsCodebuildIFileSystemLocationDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description='-')

class AwsCodebuildIProjectDefConfig(pydantic.BaseModel):
    add_to_role_policy: typing.Optional[list[AwsCodebuildIProjectDefAddToRolePolicyParams]] = pydantic.Field(None, description='')
    apply_removal_policy: typing.Optional[list[AwsCodebuildIProjectDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    bind_as_notification_rule_source: typing.Optional[list[AwsCodebuildIProjectDefBindAsNotificationRuleSourceParams]] = pydantic.Field(None, description='Returns a source configuration for notification rule.')
    enable_batch_builds: typing.Optional[bool] = pydantic.Field(None, description='Enable batch builds.\nReturns an object contining the batch service role if batch builds\ncould be enabled.')
    metric: typing.Optional[list[AwsCodebuildIProjectDefMetricParams]] = pydantic.Field(None, description='')
    metric_builds: typing.Optional[list[AwsCodebuildIProjectDefMetricBuildsParams]] = pydantic.Field(None, description='Measures the number of builds triggered.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    metric_duration: typing.Optional[list[AwsCodebuildIProjectDefMetricDurationParams]] = pydantic.Field(None, description='Measures the duration of all builds over time.\nUnits: Seconds\n\nValid CloudWatch statistics: Average (recommended), Maximum, Minimum')
    metric_failed_builds: typing.Optional[list[AwsCodebuildIProjectDefMetricFailedBuildsParams]] = pydantic.Field(None, description='Measures the number of builds that failed because of client error or because of a timeout.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    metric_succeeded_builds: typing.Optional[list[AwsCodebuildIProjectDefMetricSucceededBuildsParams]] = pydantic.Field(None, description='Measures the number of successful builds.\nUnits: Count\n\nValid CloudWatch statistics: Sum')
    notify_on: typing.Optional[list[AwsCodebuildIProjectDefNotifyOnParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule triggered when the project events emitted by you specified, it very similar to ``onEvent`` API.\nYou can also use the methods ``notifyOnBuildSucceeded`` and\n``notifyOnBuildFailed`` to define rules for these specific event emitted.')
    notify_on_build_failed: typing.Optional[list[AwsCodebuildIProjectDefNotifyOnBuildFailedParams]] = pydantic.Field(None, description='Defines a CodeStar notification rule which triggers when a build fails.')
    notify_on_build_succeeded: typing.Optional[list[AwsCodebuildIProjectDefNotifyOnBuildSucceededParams]] = pydantic.Field(None, description='Defines a CodeStar notification rule which triggers when a build completes successfully.')
    on_build_failed: typing.Optional[list[AwsCodebuildIProjectDefOnBuildFailedParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build fails.')
    on_build_started: typing.Optional[list[AwsCodebuildIProjectDefOnBuildStartedParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build starts.')
    on_build_succeeded: typing.Optional[list[AwsCodebuildIProjectDefOnBuildSucceededParams]] = pydantic.Field(None, description='Defines an event rule which triggers when a build completes successfully.')
    on_event: typing.Optional[list[AwsCodebuildIProjectDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule triggered when something happens with this project.')
    on_phase_change: typing.Optional[list[AwsCodebuildIProjectDefOnPhaseChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule that triggers upon phase change of this build project.')
    on_state_change: typing.Optional[list[AwsCodebuildIProjectDefOnStateChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule triggered when the build project state changes.\nYou can filter specific build status events using an event\npattern filter on the ``build-status`` detail field:\n\nconst rule = project.onStateChange(\'OnBuildStarted\', { target });\nrule.addEventPattern({\ndetail: {\n\'build-status\': [\n"IN_PROGRESS",\n"SUCCEEDED",\n"FAILED",\n"STOPPED"\n]\n}\n});\n\nYou can also use the methods ``onBuildFailed`` and ``onBuildSucceeded`` to define rules for\nthese specific state changes.\n\nTo access fields from the event in the event target input,\nuse the static fields on the ``StateChangeEvent`` class.')


class AwsCodebuildIProjectDefAddToRolePolicyParams(pydantic.BaseModel):
    policy_statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsCodebuildIProjectDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodebuildIProjectDefBindAsNotificationRuleSourceParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsCodebuildIProjectDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='The name of the metric.')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefMetricBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefMetricFailedBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefMetricSucceededBuildsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefNotifyOnParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The logical identifier of the CodeStar Notifications rule that will be created.\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='The target to register for the CodeStar Notifications destination.\n')
    events: typing.Sequence[aws_cdk.aws_codebuild.ProjectNotificationEvents] = pydantic.Field(..., description='A list of event types associated with this notification rule for CodeBuild Project. For a complete list of event types and IDs, see Notification concepts in the Developer Tools Console User Guide.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefNotifyOnBuildFailedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefNotifyOnBuildSucceededParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefOnBuildFailedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefOnBuildStartedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefOnBuildSucceededParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefOnPhaseChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIProjectDefOnStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n\n:see: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html\n')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodebuildIReportGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodebuildIReportGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_write: typing.Optional[list[AwsCodebuildIReportGroupDefGrantWriteParams]] = pydantic.Field(None, description='Grants the given entity permissions to write (that is, upload reports to) this report group.')


class AwsCodebuildIReportGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodebuildIReportGroupDefGrantWriteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCodebuildISourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsCodebuildISourceDefBindParams]] = pydantic.Field(None, description='')


class AwsCodebuildISourceDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    project: typing.Union[models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(..., description='-')

class AwsCodecommitIRepositoryDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodecommitIRepositoryDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    bind_as_notification_rule_source: typing.Optional[list[AwsCodecommitIRepositoryDefBindAsNotificationRuleSourceParams]] = pydantic.Field(None, description='Returns a source configuration for notification rule.')
    grant: typing.Optional[list[AwsCodecommitIRepositoryDefGrantParams]] = pydantic.Field(None, description='Grant the given principal identity permissions to perform the actions on this repository.')
    grant_pull: typing.Optional[list[AwsCodecommitIRepositoryDefGrantPullParams]] = pydantic.Field(None, description='Grant the given identity permissions to pull this repository.')
    grant_pull_push: typing.Optional[list[AwsCodecommitIRepositoryDefGrantPullPushParams]] = pydantic.Field(None, description='Grant the given identity permissions to pull and push this repository.')
    grant_read: typing.Optional[list[AwsCodecommitIRepositoryDefGrantReadParams]] = pydantic.Field(None, description='Grant the given identity permissions to read this repository.')
    notify_on: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule triggered when the project events specified by you are emitted. Similar to ``onEvent`` API.\nYou can also use the methods to define rules for the specific event emitted.\neg: ``notifyOnPullRequstCreated``.')
    notify_on_approval_rule_overridden: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnApprovalRuleOverriddenParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule which triggers when an approval rule is overridden.')
    notify_on_approval_status_changed: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnApprovalStatusChangedParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule which triggers when an approval status is changed.')
    notify_on_branch_or_tag_created: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnBranchOrTagCreatedParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule which triggers when a new branch or tag is created.')
    notify_on_branch_or_tag_deleted: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnBranchOrTagDeletedParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule which triggers when a branch or tag is deleted.')
    notify_on_pull_request_comment: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnPullRequestCommentParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule which triggers when a comment is made on a pull request.')
    notify_on_pull_request_created: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnPullRequestCreatedParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule which triggers when a pull request is created.')
    notify_on_pull_request_merged: typing.Optional[list[AwsCodecommitIRepositoryDefNotifyOnPullRequestMergedParams]] = pydantic.Field(None, description='Defines a CodeStar Notification rule which triggers when a pull request is merged.')
    on_comment_on_commit: typing.Optional[list[AwsCodecommitIRepositoryDefOnCommentOnCommitParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a comment is made on a commit.')
    on_comment_on_pull_request: typing.Optional[list[AwsCodecommitIRepositoryDefOnCommentOnPullRequestParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a comment is made on a pull request.')
    on_commit: typing.Optional[list[AwsCodecommitIRepositoryDefOnCommitParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a commit is pushed to a branch.')
    on_event: typing.Optional[list[AwsCodecommitIRepositoryDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers for repository events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')
    on_pull_request_state_change: typing.Optional[list[AwsCodecommitIRepositoryDefOnPullRequestStateChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a pull request state is changed.')
    on_reference_created: typing.Optional[list[AwsCodecommitIRepositoryDefOnReferenceCreatedParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a reference is created (i.e. a new branch/tag is created) to the repository.')
    on_reference_deleted: typing.Optional[list[AwsCodecommitIRepositoryDefOnReferenceDeletedParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a reference is delete (i.e. a branch/tag is deleted) from the repository.')
    on_reference_updated: typing.Optional[list[AwsCodecommitIRepositoryDefOnReferenceUpdatedParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a reference is updated (i.e. a commit is pushed to an existing or new branch) from the repository.')
    on_state_change: typing.Optional[list[AwsCodecommitIRepositoryDefOnStateChangeParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers when a "CodeCommit Repository State Change" event occurs.')


class AwsCodecommitIRepositoryDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodecommitIRepositoryDefBindAsNotificationRuleSourceParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsCodecommitIRepositoryDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefGrantPullParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefGrantPullPushParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    events: typing.Sequence[aws_cdk.aws_codecommit.RepositoryNotificationEvents] = pydantic.Field(..., description='A list of event types associated with this notification rule for CodeCommit repositories. For a complete list of event types and IDs, see Notification concepts in the Developer Tools Console User Guide.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnApprovalRuleOverriddenParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnApprovalStatusChangedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnBranchOrTagCreatedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnBranchOrTagDeletedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnPullRequestCommentParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnPullRequestCreatedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefNotifyOnPullRequestMergedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnCommentOnCommitParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnCommentOnPullRequestParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnCommitParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    branches: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The branch to monitor. Default: - All branches\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnPullRequestStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnReferenceCreatedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnReferenceDeletedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnReferenceUpdatedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodecommitIRepositoryDefOnStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
#  aws-cdk-lib.aws_codedeploy.IBaseDeploymentConfig skipped


class AwsCodedeployIEcsApplicationDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodedeployIEcsApplicationDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCodedeployIEcsApplicationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_codedeploy.IEcsDeploymentConfig skipped


class AwsCodedeployIEcsDeploymentGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodedeployIEcsDeploymentGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCodedeployIEcsDeploymentGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodedeployILambdaApplicationDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodedeployILambdaApplicationDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCodedeployILambdaApplicationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_codedeploy.ILambdaDeploymentConfig skipped


class AwsCodedeployILambdaDeploymentGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodedeployILambdaDeploymentGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCodedeployILambdaDeploymentGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodedeployIServerApplicationDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodedeployIServerApplicationDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCodedeployIServerApplicationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_codedeploy.IServerDeploymentConfig skipped


class AwsCodedeployIServerDeploymentGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodedeployIServerDeploymentGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCodedeployIServerDeploymentGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodeguruprofilerIProfilingGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodeguruprofilerIProfilingGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_publish: typing.Optional[list[AwsCodeguruprofilerIProfilingGroupDefGrantPublishParams]] = pydantic.Field(None, description='Grant access to publish profiling information to the Profiling Group to the given identity.\nThis will grant the following permissions:\n\n- codeguru-profiler:ConfigureAgent\n- codeguru-profiler:PostAgentProfile')
    grant_read: typing.Optional[list[AwsCodeguruprofilerIProfilingGroupDefGrantReadParams]] = pydantic.Field(None, description='Grant access to read profiling information from the Profiling Group to the given identity.\nThis will grant the following permissions:\n\n- codeguru-profiler:GetProfile\n- codeguru-profiler:DescribeProfilingGroup')


class AwsCodeguruprofilerIProfilingGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodeguruprofilerIProfilingGroupDefGrantPublishParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='Principal to grant publish rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCodeguruprofilerIProfilingGroupDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='Principal to grant read rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsCodepipelineIActionDefBindParams]] = pydantic.Field(None, description='The callback invoked when this Action is added to a Pipeline.')
    on_state_change: typing.Optional[list[AwsCodepipelineIActionDefOnStateChangeParams]] = pydantic.Field(None, description='Creates an Event that will be triggered whenever the state of this Action changes.')


class AwsCodepipelineIActionDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='the Construct tree scope the Action can use if it needs to create any resources.\n')
    stage: models.UnsupportedResource = pydantic.Field(..., description='the ``IStage`` this Action is being added to.\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='')
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='')

class AwsCodepipelineIActionDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='the name to use for the new Event.\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='the optional target for the Event.\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIPipelineDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCodepipelineIPipelineDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    bind_as_notification_rule_source: typing.Optional[list[AwsCodepipelineIPipelineDefBindAsNotificationRuleSourceParams]] = pydantic.Field(None, description='Returns a source configuration for notification rule.')
    notify_on: typing.Optional[list[AwsCodepipelineIPipelineDefNotifyOnParams]] = pydantic.Field(None, description='Defines a CodeStar notification rule triggered when the pipeline events emitted by you specified, it very similar to ``onEvent`` API.\nYou can also use the methods ``notifyOnExecutionStateChange``, ``notifyOnAnyStageStateChange``,\n``notifyOnAnyActionStateChange`` and ``notifyOnAnyManualApprovalStateChange``\nto define rules for these specific event emitted.')
    notify_on_any_action_state_change: typing.Optional[list[AwsCodepipelineIPipelineDefNotifyOnAnyActionStateChangeParams]] = pydantic.Field(None, description='Define an notification rule triggered by the set of the "Action execution" events emitted from this pipeline.')
    notify_on_any_manual_approval_state_change: typing.Optional[list[AwsCodepipelineIPipelineDefNotifyOnAnyManualApprovalStateChangeParams]] = pydantic.Field(None, description='Define an notification rule triggered by the set of the "Manual approval" events emitted from this pipeline.')
    notify_on_any_stage_state_change: typing.Optional[list[AwsCodepipelineIPipelineDefNotifyOnAnyStageStateChangeParams]] = pydantic.Field(None, description='Define an notification rule triggered by the set of the "Stage execution" events emitted from this pipeline.')
    notify_on_execution_state_change: typing.Optional[list[AwsCodepipelineIPipelineDefNotifyOnExecutionStateChangeParams]] = pydantic.Field(None, description='Define an notification rule triggered by the set of the "Pipeline execution" events emitted from this pipeline.')
    on_event: typing.Optional[list[AwsCodepipelineIPipelineDefOnEventParams]] = pydantic.Field(None, description='Define an event rule triggered by this CodePipeline.')
    on_state_change: typing.Optional[list[AwsCodepipelineIPipelineDefOnStateChangeParams]] = pydantic.Field(None, description='Define an event rule triggered by the "CodePipeline Pipeline Execution State Change" event emitted from this pipeline.')


class AwsCodepipelineIPipelineDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodepipelineIPipelineDefBindAsNotificationRuleSourceParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsCodepipelineIPipelineDefNotifyOnParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the CodeStar notification rule.\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='The target to register for the CodeStar Notifications destination.\n')
    events: typing.Sequence[aws_cdk.aws_codepipeline.PipelineNotificationEvents] = pydantic.Field(..., description='A list of event types associated with this notification rule for CodePipeline Pipeline. For a complete list of event types and IDs, see Notification concepts in the Developer Tools Console User Guide.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIPipelineDefNotifyOnAnyActionStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Identifier for this notification handler.\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='The target to register for the CodeStar Notifications destination.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n\n:see: https://docs.aws.amazon.com/dtconsole/latest/userguide/concepts.html#events-ref-pipeline\n')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIPipelineDefNotifyOnAnyManualApprovalStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Identifier for this notification handler.\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='The target to register for the CodeStar Notifications destination.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n\n:see: https://docs.aws.amazon.com/dtconsole/latest/userguide/concepts.html#events-ref-pipeline\n')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIPipelineDefNotifyOnAnyStageStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Identifier for this notification handler.\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='The target to register for the CodeStar Notifications destination.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n\n:see: https://docs.aws.amazon.com/dtconsole/latest/userguide/concepts.html#events-ref-pipeline\n')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIPipelineDefNotifyOnExecutionStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Identifier for this notification handler.\n')
    target: models.UnsupportedResource = pydantic.Field(..., description='The target to register for the CodeStar Notifications destination.\n')
    detail_type: typing.Optional[aws_cdk.aws_codestarnotifications.DetailType] = pydantic.Field(None, description='The level of detail to include in the notifications for this resource. BASIC will include only the contents of the event as it would appear in AWS CloudWatch. FULL will include any supplemental information provided by AWS CodeStar Notifications and/or the service for the resource for which the notification is created. Default: DetailType.FULL\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="The status of the notification rule. If the enabled is set to DISABLED, notifications aren't sent for the notification rule. Default: true\n")
    notification_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the notification rule. Notification rule names must be unique in your AWS account. Default: - generated from the ``id``\n\n:see: https://docs.aws.amazon.com/dtconsole/latest/userguide/concepts.html#events-ref-pipeline\n')
    return_config: typing.Optional[list[models._interface_methods.AwsCodestarnotificationsINotificationRuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIPipelineDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Identifier for this event handler.\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIPipelineDefOnStateChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Identifier for this event handler.\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsCodepipelineIStageDefConfig(pydantic.BaseModel):
    add_action: typing.Optional[list[AwsCodepipelineIStageDefAddActionParams]] = pydantic.Field(None, description='')
    on_state_change: typing.Optional[list[AwsCodepipelineIStageDefOnStateChangeParams]] = pydantic.Field(None, description='')


class AwsCodepipelineIStageDefAddActionParams(pydantic.BaseModel):
    action: typing.Union[models.aws_codepipeline.ActionDef, models.aws_codepipeline_actions.ActionDef, models.aws_codepipeline_actions.AlexaSkillDeployActionDef, models.aws_codepipeline_actions.CloudFormationCreateReplaceChangeSetActionDef, models.aws_codepipeline_actions.CloudFormationCreateUpdateStackActionDef, models.aws_codepipeline_actions.CloudFormationDeleteStackActionDef, models.aws_codepipeline_actions.CloudFormationDeployStackInstancesActionDef, models.aws_codepipeline_actions.CloudFormationDeployStackSetActionDef, models.aws_codepipeline_actions.CloudFormationExecuteChangeSetActionDef, models.aws_codepipeline_actions.CodeBuildActionDef, models.aws_codepipeline_actions.CodeCommitSourceActionDef, models.aws_codepipeline_actions.CodeDeployEcsDeployActionDef, models.aws_codepipeline_actions.CodeDeployServerDeployActionDef, models.aws_codepipeline_actions.CodeStarConnectionsSourceActionDef, models.aws_codepipeline_actions.EcrSourceActionDef, models.aws_codepipeline_actions.EcsDeployActionDef, models.aws_codepipeline_actions.ElasticBeanstalkDeployActionDef, models.aws_codepipeline_actions.GitHubSourceActionDef, models.aws_codepipeline_actions.JenkinsActionDef, models.aws_codepipeline_actions.LambdaInvokeActionDef, models.aws_codepipeline_actions.ManualApprovalActionDef, models.aws_codepipeline_actions.S3DeployActionDef, models.aws_codepipeline_actions.S3SourceActionDef, models.aws_codepipeline_actions.ServiceCatalogDeployActionBeta1Def, models.aws_codepipeline_actions.StepFunctionInvokeActionDef] = pydantic.Field(..., description='-')

class AwsCodepipelineIStageDefOnStateChangeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='-\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the rule is enabled. Default: true\n')
    event_bus: typing.Optional[typing.Union[models.aws_events.EventBusDef]] = pydantic.Field(None, description='The event bus to associate with this rule. Default: - The default event bus.\n')
    schedule: typing.Optional[models.aws_events.ScheduleDef] = pydantic.Field(None, description='The schedule or rate (frequency) that determines when EventBridge runs the rule. You must specify this property, the ``eventPattern`` property, or both. For more information, see Schedule Expression Syntax for Rules in the Amazon EventBridge User Guide. Default: - None.\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]]] = pydantic.Field(None, description='Targets to invoke when this rule matches an event. Input will be the full matched event. If you wish to specify custom target input, use ``addTarget(target[, inputOptions])``. Default: - No targets.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
#  aws-cdk-lib.aws_codepipeline_actions.IJenkinsProvider skipped


class AwsCodestarnotificationsINotificationRuleDefConfig(pydantic.BaseModel):
    add_target: typing.Optional[list[AwsCodestarnotificationsINotificationRuleDefAddTargetParams]] = pydantic.Field(None, description='Adds target to notification rule.')
    apply_removal_policy: typing.Optional[list[AwsCodestarnotificationsINotificationRuleDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCodestarnotificationsINotificationRuleDefAddTargetParams(pydantic.BaseModel):
    target: models.UnsupportedResource = pydantic.Field(..., description='The SNS topic or AWS Chatbot Slack target.\n')

class AwsCodestarnotificationsINotificationRuleDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCodestarnotificationsINotificationRuleSourceDefConfig(pydantic.BaseModel):
    bind_as_notification_rule_source: typing.Optional[list[AwsCodestarnotificationsINotificationRuleSourceDefBindAsNotificationRuleSourceParams]] = pydantic.Field(None, description='Returns a source configuration for notification rule.')


class AwsCodestarnotificationsINotificationRuleSourceDefBindAsNotificationRuleSourceParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsCodestarnotificationsINotificationRuleTargetDefConfig(pydantic.BaseModel):
    bind_as_notification_rule_target: typing.Optional[list[AwsCodestarnotificationsINotificationRuleTargetDefBindAsNotificationRuleTargetParams]] = pydantic.Field(None, description='Returns a target configuration for notification rule.')


class AwsCodestarnotificationsINotificationRuleTargetDefBindAsNotificationRuleTargetParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsCognitoICustomAttributeDefConfig(pydantic.BaseModel):
    bind: typing.Optional[bool] = pydantic.Field(None, description='Bind this custom attribute type to the values as expected by CloudFormation.')


class AwsCognitoIUserPoolDefConfig(pydantic.BaseModel):
    add_client: typing.Optional[list[AwsCognitoIUserPoolDefAddClientParams]] = pydantic.Field(None, description='Add a new app client to this user pool.')
    add_domain: typing.Optional[list[AwsCognitoIUserPoolDefAddDomainParams]] = pydantic.Field(None, description='Associate a domain to this user pool.')
    add_resource_server: typing.Optional[list[AwsCognitoIUserPoolDefAddResourceServerParams]] = pydantic.Field(None, description='Add a new resource server to this user pool.')
    apply_removal_policy: typing.Optional[list[AwsCognitoIUserPoolDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsCognitoIUserPoolDefGrantParams]] = pydantic.Field(None, description="Adds an IAM policy statement associated with this user pool to an IAM principal's policy.")
    register_identity_provider: typing.Optional[list[AwsCognitoIUserPoolDefRegisterIdentityProviderParams]] = pydantic.Field(None, description='Register an identity provider with this user pool.')


class AwsCognitoIUserPoolDefAddClientParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    access_token_validity: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Validity of the access token. Values between 5 minutes and 1 day are valid. The duration can not be longer than the refresh token validity. Default: Duration.minutes(60)\n')
    auth_flows: typing.Union[models.aws_cognito.AuthFlowDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The set of OAuth authentication flows to enable on the client. Default: - all auth flows disabled\n')
    auth_session_validity: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Cognito creates a session token for each API request in an authentication flow. AuthSessionValidity is the duration, in minutes, of that session token. see defaults in ``AuthSessionValidity``. Valid duration is from 3 to 15 minutes. Default: - Duration.minutes(3)\n')
    disable_o_auth: typing.Optional[bool] = pydantic.Field(None, description='Turns off all OAuth interactions for this client. Default: false\n')
    enable_token_revocation: typing.Optional[bool] = pydantic.Field(None, description='Enable token revocation for this client. Default: true for new user pool clients\n')
    generate_secret: typing.Optional[bool] = pydantic.Field(None, description='Whether to generate a client secret. Default: false\n')
    id_token_validity: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Validity of the ID token. Values between 5 minutes and 1 day are valid. The duration can not be longer than the refresh token validity. Default: Duration.minutes(60)\n')
    o_auth: typing.Union[models.aws_cognito.OAuthSettingsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='OAuth settings for this client to interact with the app. An error is thrown when this is specified and ``disableOAuth`` is set. Default: - see defaults in ``OAuthSettings``. meaningless if ``disableOAuth`` is set.\n')
    prevent_user_existence_errors: typing.Optional[bool] = pydantic.Field(None, description="Whether Cognito returns a UserNotFoundException exception when the user does not exist in the user pool (false), or whether it returns another type of error that doesn't reveal the user's absence. Default: false\n")
    read_attributes: typing.Optional[models.aws_cognito.ClientAttributesDef] = pydantic.Field(None, description='The set of attributes this client will be able to read. Default: - all standard and custom attributes\n')
    refresh_token_validity: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Validity of the refresh token. Values between 60 minutes and 10 years are valid. Default: Duration.days(30)\n')
    supported_identity_providers: typing.Optional[typing.Sequence[models.aws_cognito.UserPoolClientIdentityProviderDef]] = pydantic.Field(None, description='The list of identity providers that users should be able to use to sign in using this client. Default: - supports all identity providers that are registered with the user pool. If the user pool and/or identity providers are imported, either specify this option explicitly or ensure that the identity providers are registered with the user pool using the ``UserPool.registerIdentityProvider()`` API.\n')
    user_pool_client_name: typing.Optional[str] = pydantic.Field(None, description='Name of the application client. Default: - cloudformation generated name\n')
    write_attributes: typing.Optional[models.aws_cognito.ClientAttributesDef] = pydantic.Field(None, description='The set of attributes this client will be able to write. Default: - all standard and custom attributes\n\n:see: https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-client-apps.html\n')
    return_config: typing.Optional[list[models.aws_cognito.UserPoolClientDefConfig]] = pydantic.Field(None)

class AwsCognitoIUserPoolDefAddDomainParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    cognito_domain: typing.Union[models.aws_cognito.CognitoDomainOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Associate a cognito prefix domain with your user pool Either ``customDomain`` or ``cognitoDomain`` must be specified. Default: - not set if ``customDomain`` is specified, otherwise, throws an error.\n')
    custom_domain: typing.Union[models.aws_cognito.CustomDomainOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Associate a custom domain with your user pool Either ``customDomain`` or ``cognitoDomain`` must be specified. Default: - not set if ``cognitoDomain`` is specified, otherwise, throws an error.\n\n:see: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-assign-domain.html\n')
    return_config: typing.Optional[list[models.aws_cognito.UserPoolDomainDefConfig]] = pydantic.Field(None)

class AwsCognitoIUserPoolDefAddResourceServerParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    identifier: str = pydantic.Field(..., description='A unique resource server identifier for the resource server.\n')
    scopes: typing.Optional[typing.Sequence[models.aws_cognito.ResourceServerScopeDef]] = pydantic.Field(None, description='Oauth scopes. Default: - No scopes will be added\n')
    user_pool_resource_server_name: typing.Optional[str] = pydantic.Field(None, description='A friendly name for the resource server. Default: - same as ``identifier``\n\n:see: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-resource-servers.html\n')
    return_config: typing.Optional[list[models.aws_cognito.UserPoolResourceServerDefConfig]] = pydantic.Field(None)

class AwsCognitoIUserPoolDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCognitoIUserPoolDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsCognitoIUserPoolDefRegisterIdentityProviderParams(pydantic.BaseModel):
    provider: typing.Union[models.aws_cognito.UserPoolIdentityProviderAmazonDef, models.aws_cognito.UserPoolIdentityProviderAppleDef, models.aws_cognito.UserPoolIdentityProviderFacebookDef, models.aws_cognito.UserPoolIdentityProviderGoogleDef, models.aws_cognito.UserPoolIdentityProviderOidcDef, models.aws_cognito.UserPoolIdentityProviderSamlDef] = pydantic.Field(..., description='-')

class AwsCognitoIUserPoolClientDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCognitoIUserPoolClientDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCognitoIUserPoolClientDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCognitoIUserPoolDomainDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCognitoIUserPoolDomainDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCognitoIUserPoolDomainDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCognitoIUserPoolIdentityProviderDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCognitoIUserPoolIdentityProviderDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCognitoIUserPoolIdentityProviderDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsCognitoIUserPoolResourceServerDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsCognitoIUserPoolResourceServerDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsCognitoIUserPoolResourceServerDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsConfigIRuleDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsConfigIRuleDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    on_compliance_change: typing.Optional[list[AwsConfigIRuleDefOnComplianceChangeParams]] = pydantic.Field(None, description='Defines a EventBridge event rule which triggers for rule compliance events.')
    on_event: typing.Optional[list[AwsConfigIRuleDefOnEventParams]] = pydantic.Field(None, description='Defines an EventBridge event rule which triggers for rule events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')
    on_re_evaluation_status: typing.Optional[list[AwsConfigIRuleDefOnReEvaluationStatusParams]] = pydantic.Field(None, description='Defines a EventBridge event rule which triggers for rule re-evaluation status events.')


class AwsConfigIRuleDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsConfigIRuleDefOnComplianceChangeParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsConfigIRuleDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsConfigIRuleDefOnReEvaluationStatusParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsDocdbIClusterParameterGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsDocdbIClusterParameterGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsDocdbIClusterParameterGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsDocdbIDatabaseClusterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsDocdbIDatabaseClusterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsDocdbIDatabaseClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsDocdbIDatabaseInstanceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsDocdbIDatabaseInstanceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsDocdbIDatabaseInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsDynamodbIScalableTableAttributeDefConfig(pydantic.BaseModel):
    scale_on_schedule: typing.Optional[list[AwsDynamodbIScalableTableAttributeDefScaleOnScheduleParams]] = pydantic.Field(None, description='Add scheduled scaling for this scaling attribute.')
    scale_on_utilization: typing.Optional[list[AwsDynamodbIScalableTableAttributeDefScaleOnUtilizationParams]] = pydantic.Field(None, description='Scale out or in to keep utilization at a given level.')


class AwsDynamodbIScalableTableAttributeDefScaleOnScheduleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    schedule: models.aws_applicationautoscaling.ScheduleDef = pydantic.Field(..., description='When to perform this action.\n')
    end_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action expires. Default: The rule never expires.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new maximum capacity. During the scheduled time, the current capacity is above the maximum capacity, Application Auto Scaling scales in to the maximum capacity. At least one of maxCapacity and minCapacity must be supplied. Default: No new maximum capacity\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new minimum capacity. During the scheduled time, if the current capacity is below the minimum capacity, Application Auto Scaling scales out to the minimum capacity. At least one of maxCapacity and minCapacity must be supplied. Default: No new minimum capacity\n')
    start_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action becomes active. Default: The rule is activate immediately')

class AwsDynamodbIScalableTableAttributeDefScaleOnUtilizationParams(pydantic.BaseModel):
    target_utilization_percent: typing.Union[int, float] = pydantic.Field(..., description='Target utilization percentage for the attribute.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency')

class AwsDynamodbITableDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsDynamodbITableDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsDynamodbITableDefGrantParams]] = pydantic.Field(None, description="Adds an IAM policy statement associated with this table to an IAM principal's policy.\nIf ``encryptionKey`` is present, appropriate grants to the key needs to be added\nseparately using the ``table.encryptionKey.grant*`` methods.")
    grant_full_access: typing.Optional[list[AwsDynamodbITableDefGrantFullAccessParams]] = pydantic.Field(None, description='Permits all DynamoDB operations ("dynamodb:*") to an IAM principal.\nAppropriate grants will also be added to the customer-managed KMS key\nif one was configured.')
    grant_read_data: typing.Optional[list[AwsDynamodbITableDefGrantReadDataParams]] = pydantic.Field(None, description='Permits an IAM principal all data read operations from this table: BatchGetItem, GetRecords, GetShardIterator, Query, GetItem, Scan.\nAppropriate grants will also be added to the customer-managed KMS key\nif one was configured.')
    grant_read_write_data: typing.Optional[list[AwsDynamodbITableDefGrantReadWriteDataParams]] = pydantic.Field(None, description='Permits an IAM principal to all data read/write operations to this table.\nBatchGetItem, GetRecords, GetShardIterator, Query, GetItem, Scan,\nBatchWriteItem, PutItem, UpdateItem, DeleteItem\n\nAppropriate grants will also be added to the customer-managed KMS key\nif one was configured.')
    grant_stream: typing.Optional[list[AwsDynamodbITableDefGrantStreamParams]] = pydantic.Field(None, description="Adds an IAM policy statement associated with this table's stream to an IAM principal's policy.\nIf ``encryptionKey`` is present, appropriate grants to the key needs to be added\nseparately using the ``table.encryptionKey.grant*`` methods.")
    grant_stream_read: typing.Optional[list[AwsDynamodbITableDefGrantStreamReadParams]] = pydantic.Field(None, description="Permits an IAM principal all stream data read operations for this table's stream: DescribeStream, GetRecords, GetShardIterator, ListStreams.\nAppropriate grants will also be added to the customer-managed KMS key\nif one was configured.")
    grant_table_list_streams: typing.Optional[list[AwsDynamodbITableDefGrantTableListStreamsParams]] = pydantic.Field(None, description='Permits an IAM Principal to list streams attached to current dynamodb table.')
    grant_write_data: typing.Optional[list[AwsDynamodbITableDefGrantWriteDataParams]] = pydantic.Field(None, description='Permits an IAM principal all data write operations to this table: BatchWriteItem, PutItem, UpdateItem, DeleteItem.\nAppropriate grants will also be added to the customer-managed KMS key\nif one was configured.')
    metric: typing.Optional[list[AwsDynamodbITableDefMetricParams]] = pydantic.Field(None, description='Metric for the number of Errors executing all Lambdas.')
    metric_conditional_check_failed_requests: typing.Optional[list[AwsDynamodbITableDefMetricConditionalCheckFailedRequestsParams]] = pydantic.Field(None, description='Metric for the conditional check failed requests.')
    metric_consumed_read_capacity_units: typing.Optional[list[AwsDynamodbITableDefMetricConsumedReadCapacityUnitsParams]] = pydantic.Field(None, description='Metric for the consumed read capacity units.')
    metric_consumed_write_capacity_units: typing.Optional[list[AwsDynamodbITableDefMetricConsumedWriteCapacityUnitsParams]] = pydantic.Field(None, description='Metric for the consumed write capacity units.')
    metric_successful_request_latency: typing.Optional[list[AwsDynamodbITableDefMetricSuccessfulRequestLatencyParams]] = pydantic.Field(None, description='Metric for the successful request latency.')
    metric_system_errors_for_operations: typing.Optional[list[AwsDynamodbITableDefMetricSystemErrorsForOperationsParams]] = pydantic.Field(None, description='Metric for the system errors this table.')
    metric_throttled_requests: typing.Optional[list[AwsDynamodbITableDefMetricThrottledRequestsParams]] = pydantic.Field(None, description='(deprecated) Metric for throttled requests.')
    metric_throttled_requests_for_operations: typing.Optional[list[AwsDynamodbITableDefMetricThrottledRequestsForOperationsParams]] = pydantic.Field(None, description='Metric for throttled requests.')
    metric_user_errors: typing.Optional[list[AwsDynamodbITableDefMetricUserErrorsParams]] = pydantic.Field(None, description='Metric for the user errors.')


class AwsDynamodbITableDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsDynamodbITableDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal (no-op if undefined).\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefGrantFullAccessParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefGrantReadDataParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefGrantReadWriteDataParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefGrantStreamParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal (no-op if undefined).\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefGrantStreamReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefGrantTableListStreamsParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal (no-op if undefined).')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefGrantWriteDataParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefMetricConditionalCheckFailedRequestsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefMetricConsumedReadCapacityUnitsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefMetricConsumedWriteCapacityUnitsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefMetricSuccessfulRequestLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefMetricSystemErrorsForOperationsParams(pydantic.BaseModel):
    operations: typing.Optional[typing.Sequence[aws_cdk.aws_dynamodb.Operation]] = pydantic.Field(None, description='The operations to apply the metric to. Default: - All operations available by DynamoDB tables will be considered.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')

class AwsDynamodbITableDefMetricThrottledRequestsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:deprecated: use ``metricThrottledRequestsForOperations``\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsDynamodbITableDefMetricThrottledRequestsForOperationsParams(pydantic.BaseModel):
    operations: typing.Optional[typing.Sequence[aws_cdk.aws_dynamodb.Operation]] = pydantic.Field(None, description='The operations to apply the metric to. Default: - All operations available by DynamoDB tables will be considered.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')

class AwsDynamodbITableDefMetricUserErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
#  aws-cdk-lib.aws_ec2.IClientVpnConnectionHandler skipped


class AwsEc2IClientVpnEndpointDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IClientVpnEndpointDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IClientVpnEndpointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_ec2.IConnectable skipped


class AwsEc2IFlowLogDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IFlowLogDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IFlowLogDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IGatewayVpcEndpointDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IGatewayVpcEndpointDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IGatewayVpcEndpointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_ec2.IGatewayVpcEndpointService skipped


class AwsEc2IInstanceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IInstanceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IInterfaceVpcEndpointDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IInterfaceVpcEndpointDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IInterfaceVpcEndpointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_ec2.IInterfaceVpcEndpointService skipped


class AwsEc2IIpAddressesDefConfig(pydantic.BaseModel):
    allocate_subnets_cidr: typing.Optional[list[AwsEc2IIpAddressesDefAllocateSubnetsCidrParams]] = pydantic.Field(None, description="Called by the VPC to retrieve Subnet options from the Ipam.\nDon't call this directly, the VPC will call it automatically.")
    allocate_vpc_cidr: typing.Optional[bool] = pydantic.Field(None, description="Called by the VPC to retrieve VPC options from the Ipam.\nDon't call this directly, the VPC will call it automatically.")


class AwsEc2IIpAddressesDefAllocateSubnetsCidrParams(pydantic.BaseModel):
    requested_subnets: typing.Sequence[typing.Union[models.aws_ec2.RequestedSubnetDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The Subnets to be allocated.\n')
    vpc_cidr: str = pydantic.Field(..., description='The IPv4 CIDR block for this Vpc.')

class AwsEc2ILaunchTemplateDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2ILaunchTemplateDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2ILaunchTemplateDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IMachineImageDefConfig(pydantic.BaseModel):
    get_image: typing.Optional[list[AwsEc2IMachineImageDefGetImageParams]] = pydantic.Field(None, description='Return the image to use in the given context.')


class AwsEc2IMachineImageDefGetImageParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsEc2INetworkAclDefConfig(pydantic.BaseModel):
    add_entry: typing.Optional[list[AwsEc2INetworkAclDefAddEntryParams]] = pydantic.Field(None, description='Add a new entry to the ACL.')
    apply_removal_policy: typing.Optional[list[AwsEc2INetworkAclDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2INetworkAclDefAddEntryParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    cidr: models.aws_ec2.AclCidrDef = pydantic.Field(..., description='The CIDR range to allow or deny.\n')
    rule_number: typing.Union[int, float] = pydantic.Field(..., description="Rule number to assign to the entry, such as 100. ACL entries are processed in ascending order by rule number. Entries can't use the same rule number unless one is an egress rule and the other is an ingress rule.\n")
    traffic: models.aws_ec2.AclTrafficDef = pydantic.Field(..., description='What kind of traffic this ACL rule applies to.\n')
    direction: typing.Optional[aws_cdk.aws_ec2.TrafficDirection] = pydantic.Field(None, description='Traffic direction, with respect to the subnet, this rule applies to. Default: TrafficDirection.INGRESS\n')
    network_acl_entry_name: typing.Optional[str] = pydantic.Field(None, description="The name of the NetworkAclEntry. It is not recommended to use an explicit group name. Default: If you don't specify a NetworkAclName, AWS CloudFormation generates a unique physical ID and uses that ID for the group name.\n")
    rule_action: typing.Optional[aws_cdk.aws_ec2.Action] = pydantic.Field(None, description='Whether to allow or deny traffic that matches the rule; valid values are "allow" or "deny". Any traffic that is not explicitly allowed is automatically denied in a custom ACL, all traffic is automatically allowed in a default ACL. Default: ALLOW')
    return_config: typing.Optional[list[models.aws_ec2.NetworkAclEntryDefConfig]] = pydantic.Field(None)

class AwsEc2INetworkAclDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2INetworkAclEntryDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2INetworkAclEntryDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2INetworkAclEntryDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_ec2.IPeer skipped


class AwsEc2IPlacementGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IPlacementGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IPlacementGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IPrefixListDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IPrefixListDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IPrefixListDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IPrivateSubnetDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IPrivateSubnetDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    associate_network_acl: typing.Optional[list[AwsEc2IPrivateSubnetDefAssociateNetworkAclParams]] = pydantic.Field(None, description='Associate a Network ACL with this subnet.')


class AwsEc2IPrivateSubnetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IPrivateSubnetDefAssociateNetworkAclParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    acl: typing.Union[models.aws_ec2.NetworkAclDef] = pydantic.Field(..., description='The Network ACL to associate.')

class AwsEc2IPublicSubnetDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IPublicSubnetDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    associate_network_acl: typing.Optional[list[AwsEc2IPublicSubnetDefAssociateNetworkAclParams]] = pydantic.Field(None, description='Associate a Network ACL with this subnet.')


class AwsEc2IPublicSubnetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IPublicSubnetDefAssociateNetworkAclParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    acl: typing.Union[models.aws_ec2.NetworkAclDef] = pydantic.Field(..., description='The Network ACL to associate.')
#  aws-cdk-lib.aws_ec2.IRouteTable skipped


class AwsEc2ISecurityGroupDefConfig(pydantic.BaseModel):
    add_egress_rule: typing.Optional[list[AwsEc2ISecurityGroupDefAddEgressRuleParams]] = pydantic.Field(None, description='Add an egress rule for the current security group.\n``remoteRule`` controls where the Rule object is created if the peer is also a\nsecurityGroup and they are in different stack. If false (default) the\nrule object is created under the current SecurityGroup object. If true and the\npeer is also a SecurityGroup, the rule object is created under the remote\nSecurityGroup object.')
    add_ingress_rule: typing.Optional[list[AwsEc2ISecurityGroupDefAddIngressRuleParams]] = pydantic.Field(None, description='Add an ingress rule for the current security group.\n``remoteRule`` controls where the Rule object is created if the peer is also a\nsecurityGroup and they are in different stack. If false (default) the\nrule object is created under the current SecurityGroup object. If true and the\npeer is also a SecurityGroup, the rule object is created under the remote\nSecurityGroup object.')
    apply_removal_policy: typing.Optional[list[AwsEc2ISecurityGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2ISecurityGroupDefAddEgressRuleParams(pydantic.BaseModel):
    peer: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    connection: models.aws_ec2.PortDef = pydantic.Field(..., description='-\n')
    description: typing.Optional[str] = pydantic.Field(None, description='-\n')
    remote_rule: typing.Optional[bool] = pydantic.Field(None, description='-')

class AwsEc2ISecurityGroupDefAddIngressRuleParams(pydantic.BaseModel):
    peer: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    connection: models.aws_ec2.PortDef = pydantic.Field(..., description='-\n')
    description: typing.Optional[str] = pydantic.Field(None, description='-\n')
    remote_rule: typing.Optional[bool] = pydantic.Field(None, description='-')

class AwsEc2ISecurityGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2ISubnetDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2ISubnetDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    associate_network_acl: typing.Optional[list[AwsEc2ISubnetDefAssociateNetworkAclParams]] = pydantic.Field(None, description='Associate a Network ACL with this subnet.')


class AwsEc2ISubnetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2ISubnetDefAssociateNetworkAclParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    acl: typing.Union[models.aws_ec2.NetworkAclDef] = pydantic.Field(..., description='The Network ACL to associate.')

class AwsEc2ISubnetNetworkAclAssociationDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2ISubnetNetworkAclAssociationDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2ISubnetNetworkAclAssociationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IVolumeDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IVolumeDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_attach_volume: typing.Optional[list[AwsEc2IVolumeDefGrantAttachVolumeParams]] = pydantic.Field(None, description='Grants permission to attach this Volume to an instance.\nCAUTION: Granting an instance permission to attach to itself using this method will lead to\nan unresolvable circular reference between the instance role and the instance.\nUse ``IVolume.grantAttachVolumeToSelf`` to grant an instance permission to attach this\nvolume to itself.')
    grant_attach_volume_by_resource_tag: typing.Optional[list[AwsEc2IVolumeDefGrantAttachVolumeByResourceTagParams]] = pydantic.Field(None, description='Grants permission to attach the Volume by a ResourceTag condition.\nIf you are looking to\ngrant an Instance, AutoScalingGroup, EC2-Fleet, SpotFleet, ECS host, etc the ability to attach\nthis volume to **itself** then this is the method you want to use.\n\nThis is implemented by adding a Tag with key ``VolumeGrantAttach-<suffix>`` to the given\nconstructs and this Volume, and then conditioning the Grant such that the grantee is only\ngiven the ability to AttachVolume if both the Volume and the destination Instance have that\ntag applied to them.')
    grant_detach_volume: typing.Optional[list[AwsEc2IVolumeDefGrantDetachVolumeParams]] = pydantic.Field(None, description='Grants permission to detach this Volume from an instance CAUTION: Granting an instance permission to detach from itself using this method will lead to an unresolvable circular reference between the instance role and the instance.\nUse ``IVolume.grantDetachVolumeFromSelf`` to grant an instance permission to detach this\nvolume from itself.')
    grant_detach_volume_by_resource_tag: typing.Optional[list[AwsEc2IVolumeDefGrantDetachVolumeByResourceTagParams]] = pydantic.Field(None, description='Grants permission to detach the Volume by a ResourceTag condition.\nThis is implemented via the same mechanism as ``IVolume.grantAttachVolumeByResourceTag``,\nand is subject to the same conditions.')


class AwsEc2IVolumeDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IVolumeDefGrantAttachVolumeParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.\n')
    instances: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.BastionHostLinuxDef, models.aws_ec2.InstanceDef]]] = pydantic.Field(None, description='the instances to which permission is being granted to attach this volume to. If not specified, then permission is granted to attach to all instances in this account.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEc2IVolumeDefGrantAttachVolumeByResourceTagParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.\n')
    constructs: typing.Sequence[models.constructs.ConstructDef] = pydantic.Field(..., description='The list of constructs that will have the generated resource tag applied to them.\n')
    tag_key_suffix: typing.Optional[str] = pydantic.Field(None, description='A suffix to use on the generated Tag key in place of the generated hash value. Defaults to a hash calculated from this volume and list of constructs. (DEPRECATED)')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEc2IVolumeDefGrantDetachVolumeParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.\n')
    instances: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.BastionHostLinuxDef, models.aws_ec2.InstanceDef]]] = pydantic.Field(None, description='the instances to which permission is being granted to detach this volume from. If not specified, then permission is granted to detach from all instances in this account.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEc2IVolumeDefGrantDetachVolumeByResourceTagParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.\n')
    constructs: typing.Sequence[models.constructs.ConstructDef] = pydantic.Field(..., description='The list of constructs that will have the generated resource tag applied to them.\n')
    tag_key_suffix: typing.Optional[str] = pydantic.Field(None, description='A suffix to use on the generated Tag key in place of the generated hash value. Defaults to a hash calculated from this volume and list of constructs. (DEPRECATED)')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEc2IVpcDefConfig(pydantic.BaseModel):
    add_client_vpn_endpoint: typing.Optional[list[AwsEc2IVpcDefAddClientVpnEndpointParams]] = pydantic.Field(None, description='Adds a new client VPN endpoint to this VPC.')
    add_flow_log: typing.Optional[list[AwsEc2IVpcDefAddFlowLogParams]] = pydantic.Field(None, description='Adds a new Flow Log to this VPC.')
    add_gateway_endpoint: typing.Optional[list[AwsEc2IVpcDefAddGatewayEndpointParams]] = pydantic.Field(None, description='Adds a new gateway endpoint to this VPC.')
    add_interface_endpoint: typing.Optional[list[AwsEc2IVpcDefAddInterfaceEndpointParams]] = pydantic.Field(None, description='Adds a new interface endpoint to this VPC.')
    add_vpn_connection: typing.Optional[list[AwsEc2IVpcDefAddVpnConnectionParams]] = pydantic.Field(None, description='Adds a new VPN connection to this VPC.')
    apply_removal_policy: typing.Optional[list[AwsEc2IVpcDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    enable_vpn_gateway: typing.Optional[list[AwsEc2IVpcDefEnableVpnGatewayParams]] = pydantic.Field(None, description='Adds a VPN Gateway to this VPC.')
    select_subnets: typing.Optional[list[AwsEc2IVpcDefSelectSubnetsParams]] = pydantic.Field(None, description='Return information on the subnets appropriate for the given selection strategy.\nRequires that at least one subnet is matched, throws a descriptive\nerror message otherwise.')


class AwsEc2IVpcDefAddClientVpnEndpointParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    cidr: str = pydantic.Field(..., description='The IPv4 address range, in CIDR notation, from which to assign client IP addresses. The address range cannot overlap with the local CIDR of the VPC in which the associated subnet is located, or the routes that you add manually. Changing the address range will replace the Client VPN endpoint. The CIDR block should be /22 or greater.\n')
    server_certificate_arn: str = pydantic.Field(..., description='The ARN of the server certificate.\n')
    authorize_all_users_to_vpc_cidr: typing.Optional[bool] = pydantic.Field(None, description='Whether to authorize all users to the VPC CIDR. This automatically creates an authorization rule. Set this to ``false`` and use ``addAuthorizationRule()`` to create your own rules instead. Default: true\n')
    client_certificate_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of the client certificate for mutual authentication. The certificate must be signed by a certificate authority (CA) and it must be provisioned in AWS Certificate Manager (ACM). Default: - use user-based authentication\n')
    client_connection_handler: typing.Optional[typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef]] = pydantic.Field(None, description='The AWS Lambda function used for connection authorization. The name of the Lambda function must begin with the ``AWSClientVPN-`` prefix Default: - no connection handler\n')
    client_login_banner: typing.Optional[str] = pydantic.Field(None, description='Customizable text that will be displayed in a banner on AWS provided clients when a VPN session is established. UTF-8 encoded characters only. Maximum of 1400 characters. Default: - no banner is presented to the client\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A brief description of the Client VPN endpoint. Default: - no description\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Information about the DNS servers to be used for DNS resolution. A Client VPN endpoint can have up to two DNS servers. Default: - use the DNS address configured on the device\n')
    logging: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable connections logging. Default: true\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='A CloudWatch Logs log group for connection logging. Default: - a new group is created\n')
    log_stream: typing.Optional[typing.Union[models.aws_logs.LogStreamDef]] = pydantic.Field(None, description='A CloudWatch Logs log stream for connection logging. Default: - a new stream is created\n')
    port: typing.Optional[aws_cdk.aws_ec2.VpnPort] = pydantic.Field(None, description='The port number to assign to the Client VPN endpoint for TCP and UDP traffic. Default: VpnPort.HTTPS\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to apply to the target network. Default: - a new security group is created\n')
    self_service_portal: typing.Optional[bool] = pydantic.Field(None, description='Specify whether to enable the self-service portal for the Client VPN endpoint. Default: true\n')
    session_timeout: typing.Optional[aws_cdk.aws_ec2.ClientVpnSessionTimeout] = pydantic.Field(None, description='The maximum VPN session duration time. Default: ClientVpnSessionTimeout.TWENTY_FOUR_HOURS\n')
    split_tunnel: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether split-tunnel is enabled on the AWS Client VPN endpoint. Default: false\n')
    transport_protocol: typing.Optional[aws_cdk.aws_ec2.TransportProtocol] = pydantic.Field(None, description='The transport protocol to be used by the VPN session. Default: TransportProtocol.UDP\n')
    user_based_authentication: typing.Optional[models.aws_ec2.ClientVpnUserBasedAuthenticationDef] = pydantic.Field(None, description='The type of user-based authentication to use. Default: - use mutual authentication\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Subnets to associate to the client VPN endpoint. Default: - the VPC default strategy')
    return_config: typing.Optional[list[models.aws_ec2.ClientVpnEndpointDefConfig]] = pydantic.Field(None)

class AwsEc2IVpcDefAddFlowLogParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    destination: typing.Optional[models.aws_ec2.FlowLogDestinationDef] = pydantic.Field(None, description='Specifies the type of destination to which the flow log data is to be published. Flow log data can be published to CloudWatch Logs or Amazon S3 Default: FlowLogDestinationType.toCloudWatchLogs()\n')
    log_format: typing.Optional[typing.Sequence[models.aws_ec2.LogFormatDef]] = pydantic.Field(None, description='The fields to include in the flow log record, in the order in which they should appear. If multiple fields are specified, they will be separated by spaces. For full control over the literal log format string, pass a single field constructed with ``LogFormat.custom()``. See https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records Default: - default log format is used.\n')
    max_aggregation_interval: typing.Optional[aws_cdk.aws_ec2.FlowLogMaxAggregationInterval] = pydantic.Field(None, description='The maximum interval of time during which a flow of packets is captured and aggregated into a flow log record. Default: FlowLogMaxAggregationInterval.TEN_MINUTES\n')
    traffic_type: typing.Optional[aws_cdk.aws_ec2.FlowLogTrafficType] = pydantic.Field(None, description='The type of traffic to log. You can log traffic that the resource accepts or rejects, or all traffic. Default: ALL')
    return_config: typing.Optional[list[models.aws_ec2.FlowLogDefConfig]] = pydantic.Field(None)

class AwsEc2IVpcDefAddGatewayEndpointParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    service: typing.Union[models.aws_ec2.GatewayVpcEndpointAwsServiceDef] = pydantic.Field(..., description='The service to use for this gateway VPC endpoint.\n')
    subnets: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Where to add endpoint routing. By default, this endpoint will be routable from all subnets in the VPC. Specify a list of subnet selection objects here to be more specific. Default: - All subnets in the VPC')
    return_config: typing.Optional[list[models.aws_ec2.GatewayVpcEndpointDefConfig]] = pydantic.Field(None)

class AwsEc2IVpcDefAddInterfaceEndpointParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    service: typing.Union[models.aws_ec2.InterfaceVpcEndpointAwsServiceDef, models.aws_ec2.InterfaceVpcEndpointServiceDef] = pydantic.Field(..., description='The service to use for this interface VPC endpoint.\n')
    lookup_supported_azs: typing.Optional[bool] = pydantic.Field(None, description="Limit to only those availability zones where the endpoint service can be created. Setting this to 'true' requires a lookup to be performed at synthesis time. Account and region must be set on the containing stack for this to work. Default: false\n")
    open: typing.Optional[bool] = pydantic.Field(None, description="Whether to automatically allow VPC traffic to the endpoint. If enabled, all traffic to the endpoint from within the VPC will be automatically allowed. This is done based on the VPC's CIDR range. Default: true\n")
    private_dns_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether to associate a private hosted zone with the specified VPC. This allows you to make requests to the service using its default DNS hostname. Default: set by the instance of IInterfaceVpcEndpointService, or true if not defined by the instance of IInterfaceVpcEndpointService\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to associate with this interface VPC endpoint. Default: - a new security group is created\n')
    subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets in which to create an endpoint network interface. At most one per availability zone. Default: - private subnets')
    return_config: typing.Optional[list[models.aws_ec2.InterfaceVpcEndpointDefConfig]] = pydantic.Field(None)

class AwsEc2IVpcDefAddVpnConnectionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    ip: str = pydantic.Field(..., description='The ip address of the customer gateway.\n')
    asn: typing.Union[int, float, None] = pydantic.Field(None, description='The ASN of the customer gateway. Default: 65000\n')
    static_routes: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The static routes to be routed from the VPN gateway to the customer gateway. Default: Dynamic routing (BGP)\n')
    tunnel_options: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.VpnTunnelOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The tunnel options for the VPN connection. At most two elements (one per tunnel). Duplicates not allowed. Default: Amazon generated tunnel options')
    return_config: typing.Optional[list[models.aws_ec2.VpnConnectionDefConfig]] = pydantic.Field(None)

class AwsEc2IVpcDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IVpcDefEnableVpnGatewayParams(pydantic.BaseModel):
    vpn_route_propagation: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Provide an array of subnets where the route propagation should be added. Default: noPropagation\n')
    type: str = pydantic.Field(..., description='Default type ipsec.1.\n')
    amazon_side_asn: typing.Union[int, float, None] = pydantic.Field(None, description='Explicitly specify an Asn or let aws pick an Asn for you. Default: 65000')

class AwsEc2IVpcDefSelectSubnetsParams(pydantic.BaseModel):
    availability_zones: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Select subnets only in the given AZs. Default: no filtering on AZs is done\n')
    one_per_az: typing.Optional[bool] = pydantic.Field(None, description='If true, return at most one subnet per AZ. Default: false\n')
    subnet_filters: typing.Optional[typing.Sequence[models.aws_ec2.SubnetFilterDef]] = pydantic.Field(None, description='List of provided subnet filters. Default: - none\n')
    subnet_group_name: typing.Optional[str] = pydantic.Field(None, description='Select the subnet group with the given name. Select the subnet group with the given name. This only needs to be used if you have multiple subnet groups of the same type and you need to distinguish between them. Otherwise, prefer ``subnetType``. This field does not select individual subnets, it selects all subnets that share the given subnet group name. This is the name supplied in ``subnetConfiguration``. At most one of ``subnetType`` and ``subnetGroupName`` can be supplied. Default: - Selection by type instead of by name\n')
    subnets: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.PrivateSubnetDef, models.aws_ec2.PublicSubnetDef, models.aws_ec2.SubnetDef]]] = pydantic.Field(None, description="Explicitly select individual subnets. Use this if you don't want to automatically use all subnets in a group, but have a need to control selection down to individual subnets. Cannot be specified together with ``subnetType`` or ``subnetGroupName``. Default: - Use all subnets in a selected group (all private subnets by default)\n")
    subnet_type: typing.Optional[aws_cdk.aws_ec2.SubnetType] = pydantic.Field(None, description='Select all subnets of the given type. At most one of ``subnetType`` and ``subnetGroupName`` can be supplied. Default: SubnetType.PRIVATE_WITH_EGRESS (or ISOLATED or PUBLIC if there are no PRIVATE_WITH_EGRESS subnets)')

class AwsEc2IVpcEndpointDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IVpcEndpointDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IVpcEndpointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IVpcEndpointServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IVpcEndpointServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IVpcEndpointServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_ec2.IVpcEndpointServiceLoadBalancer skipped


class AwsEc2IVpnConnectionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IVpnConnectionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    metric: typing.Optional[list[AwsEc2IVpnConnectionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this VPNConnection.')
    metric_tunnel_data_in: typing.Optional[list[AwsEc2IVpnConnectionDefMetricTunnelDataInParams]] = pydantic.Field(None, description='The bytes received through the VPN tunnel.\nSum over 5 minutes')
    metric_tunnel_data_out: typing.Optional[list[AwsEc2IVpnConnectionDefMetricTunnelDataOutParams]] = pydantic.Field(None, description='The bytes sent through the VPN tunnel.\nSum over 5 minutes')
    metric_tunnel_state: typing.Optional[list[AwsEc2IVpnConnectionDefMetricTunnelStateParams]] = pydantic.Field(None, description='The state of the tunnel. 0 indicates DOWN and 1 indicates UP.\nAverage over 5 minutes')


class AwsEc2IVpnConnectionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEc2IVpnConnectionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsEc2IVpnConnectionDefMetricTunnelDataInParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsEc2IVpnConnectionDefMetricTunnelDataOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsEc2IVpnConnectionDefMetricTunnelStateParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsEc2IVpnGatewayDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEc2IVpnGatewayDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEc2IVpnGatewayDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcrIRepositoryDefConfig(pydantic.BaseModel):
    add_to_resource_policy: typing.Optional[list[AwsEcrIRepositoryDefAddToResourcePolicyParams]] = pydantic.Field(None, description="Add a policy statement to the repository's resource policy.")
    apply_removal_policy: typing.Optional[list[AwsEcrIRepositoryDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsEcrIRepositoryDefGrantParams]] = pydantic.Field(None, description='Grant the given principal identity permissions to perform the actions on this repository.')
    grant_pull: typing.Optional[list[AwsEcrIRepositoryDefGrantPullParams]] = pydantic.Field(None, description='Grant the given identity permissions to pull images in this repository.')
    grant_pull_push: typing.Optional[list[AwsEcrIRepositoryDefGrantPullPushParams]] = pydantic.Field(None, description='Grant the given identity permissions to pull and push images to this repository.')
    grant_read: typing.Optional[list[AwsEcrIRepositoryDefGrantReadParams]] = pydantic.Field(None, description='Gran tthe given identity permissions to read images in this repository.')
    on_cloud_trail_event: typing.Optional[list[AwsEcrIRepositoryDefOnCloudTrailEventParams]] = pydantic.Field(None, description='Define a CloudWatch event that triggers when something happens to this repository.\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_cloud_trail_image_pushed: typing.Optional[list[AwsEcrIRepositoryDefOnCloudTrailImagePushedParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event rule that can trigger a target when an image is pushed to this repository.\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_event: typing.Optional[list[AwsEcrIRepositoryDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers for repository events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')
    on_image_scan_completed: typing.Optional[list[AwsEcrIRepositoryDefOnImageScanCompletedParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event rule that can trigger a target when the image scan is completed.')
    repository_uri_for_digest: typing.Optional[list[AwsEcrIRepositoryDefRepositoryUriForDigestParams]] = pydantic.Field(None, description='Returns the URI of the repository for a certain digest. Can be used in ``docker push/pull``.\nACCOUNT.dkr.ecr.REGION.amazonaws.com/REPOSITORY[@DIGEST]')
    repository_uri_for_tag: typing.Optional[list[AwsEcrIRepositoryDefRepositoryUriForTagParams]] = pydantic.Field(None, description='Returns the URI of the repository for a certain tag. Can be used in ``docker push/pull``.\nACCOUNT.dkr.ecr.REGION.amazonaws.com/REPOSITORY[:TAG]')
    repository_uri_for_tag_or_digest: typing.Optional[list[AwsEcrIRepositoryDefRepositoryUriForTagOrDigestParams]] = pydantic.Field(None, description='Returns the URI of the repository for a certain tag or digest, inferring based on the syntax of the tag.\nCan be used in ``docker push/pull``.\n\nACCOUNT.dkr.ecr.REGION.amazonaws.com/REPOSITORY[:TAG]\nACCOUNT.dkr.ecr.REGION.amazonaws.com/REPOSITORY[@DIGEST]')


class AwsEcrIRepositoryDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsEcrIRepositoryDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcrIRepositoryDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefGrantPullParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefGrantPullPushParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefOnCloudTrailEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefOnCloudTrailImagePushedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    image_tag: typing.Optional[str] = pydantic.Field(None, description='Only watch changes to this image tag. Default: - Watch changes to all tags\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefOnImageScanCompletedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    image_tags: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to the image tags specified. Leave it undefined to watch the full repository. Default: - Watch the changes to the repository with all image tags\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsEcrIRepositoryDefRepositoryUriForDigestParams(pydantic.BaseModel):
    digest: typing.Optional[str] = pydantic.Field(None, description='Image digest to use (tools usually default to the image with the "latest" tag if omitted).')

class AwsEcrIRepositoryDefRepositoryUriForTagParams(pydantic.BaseModel):
    tag: typing.Optional[str] = pydantic.Field(None, description='Image tag to use (tools usually default to "latest" if omitted).')

class AwsEcrIRepositoryDefRepositoryUriForTagOrDigestParams(pydantic.BaseModel):
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description='Image tag or digest to use (tools usually default to the image with the "latest" tag if omitted).')

class AwsEcsIBaseServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIBaseServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIBaseServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIClusterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIClusterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIEc2ServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIEc2ServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIEc2ServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIEc2TaskDefinitionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIEc2TaskDefinitionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIEc2TaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIEcsLoadBalancerTargetDefConfig(pydantic.BaseModel):
    attach_to_application_target_group: typing.Optional[list[AwsEcsIEcsLoadBalancerTargetDefAttachToApplicationTargetGroupParams]] = pydantic.Field(None, description='Attach load-balanced target to a TargetGroup.\nMay return JSON to directly add to the [Targets] list, or return undefined\nif the target will register itself with the load balancer.')
    attach_to_classic_lb: typing.Optional[list[AwsEcsIEcsLoadBalancerTargetDefAttachToClassicLbParams]] = pydantic.Field(None, description='Attach load-balanced target to a classic ELB.')
    attach_to_network_target_group: typing.Optional[list[AwsEcsIEcsLoadBalancerTargetDefAttachToNetworkTargetGroupParams]] = pydantic.Field(None, description='Attach load-balanced target to a TargetGroup.\nMay return JSON to directly add to the [Targets] list, or return undefined\nif the target will register itself with the load balancer.')


class AwsEcsIEcsLoadBalancerTargetDefAttachToApplicationTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef] = pydantic.Field(..., description='-')

class AwsEcsIEcsLoadBalancerTargetDefAttachToClassicLbParams(pydantic.BaseModel):
    load_balancer: models.aws_elasticloadbalancing.LoadBalancerDef = pydantic.Field(..., description='[disable-awslint:ref-via-interface] The load balancer to attach the target to.')

class AwsEcsIEcsLoadBalancerTargetDefAttachToNetworkTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.NetworkTargetGroupDef] = pydantic.Field(..., description='-')

class AwsEcsIExternalServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIExternalServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIExternalServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIExternalTaskDefinitionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIExternalTaskDefinitionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIExternalTaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIFargateServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIFargateServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIFargateServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIFargateTaskDefinitionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIFargateTaskDefinitionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIFargateTaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsIServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsIServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsIServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsITaskDefinitionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEcsITaskDefinitionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEcsITaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEcsITaskDefinitionExtensionDefConfig(pydantic.BaseModel):
    extend: typing.Optional[list[AwsEcsITaskDefinitionExtensionDefExtendParams]] = pydantic.Field(None, description='Apply the extension to the given TaskDefinition.')


class AwsEcsITaskDefinitionExtensionDefExtendParams(pydantic.BaseModel):
    task_definition: models.aws_ecs.TaskDefinitionDef = pydantic.Field(..., description='[disable-awslint:ref-via-interface].')

class AwsEfsIAccessPointDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEfsIAccessPointDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEfsIAccessPointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEfsIFileSystemDefConfig(pydantic.BaseModel):
    add_to_resource_policy: typing.Optional[list[AwsEfsIFileSystemDefAddToResourcePolicyParams]] = pydantic.Field(None, description="Add a statement to the resource's resource policy.")
    apply_removal_policy: typing.Optional[list[AwsEfsIFileSystemDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsEfsIFileSystemDefGrantParams]] = pydantic.Field(None, description='Grant the actions defined in actions to the given grantee on this File System resource.')


class AwsEfsIFileSystemDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsEfsIFileSystemDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEfsIFileSystemDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEksIClusterDefConfig(pydantic.BaseModel):
    add_cdk8s_chart: typing.Optional[list[AwsEksIClusterDefAddCdk8SChartParams]] = pydantic.Field(None, description='Defines a CDK8s chart in this cluster.')
    add_helm_chart: typing.Optional[list[AwsEksIClusterDefAddHelmChartParams]] = pydantic.Field(None, description='Defines a Helm chart in this cluster.')
    add_manifest: typing.Optional[list[AwsEksIClusterDefAddManifestParams]] = pydantic.Field(None, description='Defines a Kubernetes resource in this cluster.\nThe manifest will be applied/deleted using kubectl as needed.')
    add_service_account: typing.Optional[list[AwsEksIClusterDefAddServiceAccountParams]] = pydantic.Field(None, description='Creates a new service account with corresponding IAM Role (IRSA).')
    apply_removal_policy: typing.Optional[list[AwsEksIClusterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    connect_auto_scaling_group_capacity: typing.Optional[list[AwsEksIClusterDefConnectAutoScalingGroupCapacityParams]] = pydantic.Field(None, description="Connect capacity in the form of an existing AutoScalingGroup to the EKS cluster.\nThe AutoScalingGroup must be running an EKS-optimized AMI containing the\n/etc/eks/bootstrap.sh script. This method will configure Security Groups,\nadd the right policies to the instance role, apply the right tags, and add\nthe required user data to the instance's launch configuration.\n\nSpot instances will be labeled ``lifecycle=Ec2Spot`` and tainted with ``PreferNoSchedule``.\nIf kubectl is enabled, the\n`spot interrupt handler <https://github.com/awslabs/ec2-spot-labs/tree/master/ec2-spot-eks-solution/spot-termination-handler>`_\ndaemon will be installed on all spot instances to handle\n`EC2 Spot Instance Termination Notices <https://aws.amazon.com/blogs/aws/new-ec2-spot-instance-termination-notices/>`_.\n\nPrefer to use ``addAutoScalingGroupCapacity`` if possible.")


class AwsEksIClusterDefAddCdk8SChartParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='logical id of this chart.\n')
    chart: models.constructs.ConstructDef = pydantic.Field(..., description='the cdk8s chart.\n')
    ingress_alb: typing.Optional[bool] = pydantic.Field(None, description='Automatically detect ``Ingress`` resources in the manifest and annotate them so they are picked up by an ALB Ingress Controller. Default: false\n')
    ingress_alb_scheme: typing.Optional[aws_cdk.aws_eks.AlbScheme] = pydantic.Field(None, description='Specify the ALB scheme that should be applied to ``Ingress`` resources. Only applicable if ``ingressAlb`` is set to ``true``. Default: AlbScheme.INTERNAL\n')
    prune: typing.Optional[bool] = pydantic.Field(None, description='When a resource is removed from a Kubernetes manifest, it no longer appears in the manifest, and there is no way to know that this resource needs to be deleted. To address this, ``kubectl apply`` has a ``--prune`` option which will query the cluster for all resources with a specific label and will remove all the labeld resources that are not part of the applied manifest. If this option is disabled and a resource is removed, it will become "orphaned" and will not be deleted from the cluster. When this option is enabled (default), the construct will inject a label to all Kubernetes resources included in this manifest which will be used to prune resources when the manifest changes via ``kubectl apply --prune``. The label name will be ``aws.cdk.eks/prune-<ADDR>`` where ``<ADDR>`` is the 42-char unique address of this construct in the construct tree. Value is empty. Default: - based on the prune option of the cluster, which is ``true`` unless otherwise specified.\n')
    skip_validation: typing.Optional[bool] = pydantic.Field(None, description='A flag to signify if the manifest validation should be skipped. Default: false\n')

class AwsEksIClusterDefAddHelmChartParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='logical id of this chart.\n')
    chart: typing.Optional[str] = pydantic.Field(None, description='The name of the chart. Either this or ``chartAsset`` must be specified. Default: - No chart name. Implies ``chartAsset`` is used.\n')
    chart_asset: typing.Optional[models.aws_s3_assets.AssetDef] = pydantic.Field(None, description='The chart in the form of an asset. Either this or ``chart`` must be specified. Default: - No chart asset. Implies ``chart`` is used.\n')
    create_namespace: typing.Optional[bool] = pydantic.Field(None, description='create namespace if not exist. Default: true\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description='The Kubernetes namespace scope of the requests. Default: default\n')
    release: typing.Optional[str] = pydantic.Field(None, description="The name of the release. Default: - If no release name is given, it will use the last 53 characters of the node's unique id.\n")
    repository: typing.Optional[str] = pydantic.Field(None, description='The repository which contains the chart. For example: https://charts.helm.sh/stable/ Default: - No repository will be used, which means that the chart needs to be an absolute URL.\n')
    skip_crds: typing.Optional[bool] = pydantic.Field(None, description='if set, no CRDs will be installed. Default: - CRDs are installed if not already present\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Amount of time to wait for any individual Kubernetes operation. Maximum 15 minutes. Default: Duration.minutes(5)\n')
    values: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The values to be used by the chart. For nested values use a nested dictionary. For example: values: { installationCRDs: true, webhook: { port: 9443 } } Default: - No values are provided to the chart.\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The chart version to install. Default: - If this is not specified, the latest version is installed\n')
    wait: typing.Optional[bool] = pydantic.Field(None, description='Whether or not Helm should wait until all Pods, PVCs, Services, and minimum number of Pods of a Deployment, StatefulSet, or ReplicaSet are in a ready state before marking the release as successful. Default: - Helm will not wait before marking release as successful\n')

class AwsEksIClusterDefAddManifestParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='logical id of this manifest.\n')
    manifest: list[typing.Mapping[str, typing.Any]] = pydantic.Field(...)

class AwsEksIClusterDefAddServiceAccountParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='logical id of service account.\n')
    annotations: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Additional annotations of the service account. Default: - no additional annotations\n')
    labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Additional labels of the service account. Default: - no additional labels\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the service account. The name of a ServiceAccount object must be a valid DNS subdomain name. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ Default: - If no name is given, it will use the id of the resource.\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description='The namespace of the service account. All namespace names must be valid RFC 1123 DNS labels. https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#namespaces-and-dns Default: "default"')
    return_config: typing.Optional[list[models.aws_eks.ServiceAccountDefConfig]] = pydantic.Field(None)

class AwsEksIClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEksIClusterDefConnectAutoScalingGroupCapacityParams(pydantic.BaseModel):
    auto_scaling_group: models.aws_autoscaling.AutoScalingGroupDef = pydantic.Field(..., description='[disable-awslint:ref-via-interface].\n')
    bootstrap_enabled: typing.Optional[bool] = pydantic.Field(None, description='Configures the EC2 user-data script for instances in this autoscaling group to bootstrap the node (invoke ``/etc/eks/bootstrap.sh``) and associate it with the EKS cluster. If you wish to provide a custom user data script, set this to ``false`` and manually invoke ``autoscalingGroup.addUserData()``. Default: true\n')
    bootstrap_options: typing.Union[models.aws_eks.BootstrapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Allows options for node bootstrapping through EC2 user data. Default: - default options\n')
    machine_image_type: typing.Optional[aws_cdk.aws_eks.MachineImageType] = pydantic.Field(None, description='Allow options to specify different machine image type. Default: MachineImageType.AMAZON_LINUX_2\n')
    map_role: typing.Optional[bool] = pydantic.Field(None, description='Will automatically update the aws-auth ConfigMap to map the IAM instance role to RBAC. This cannot be explicitly set to ``true`` if the cluster has kubectl disabled. Default: - true if the cluster has kubectl enabled (which is the default).\n')
    spot_interrupt_handler: typing.Optional[bool] = pydantic.Field(None, description="Installs the AWS spot instance interrupt handler on the cluster if it's not already added. Only relevant if ``spotPrice`` is configured on the auto-scaling group. Default: true\n\n:see: https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html\n")
#  aws-cdk-lib.aws_eks.IKubectlProvider skipped


class AwsEksINodegroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEksINodegroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEksINodegroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticloadbalancingILoadBalancerTargetDefConfig(pydantic.BaseModel):
    attach_to_classic_lb: typing.Optional[list[AwsElasticloadbalancingILoadBalancerTargetDefAttachToClassicLbParams]] = pydantic.Field(None, description='Attach load-balanced target to a classic ELB.')


class AwsElasticloadbalancingILoadBalancerTargetDefAttachToClassicLbParams(pydantic.BaseModel):
    load_balancer: models.aws_elasticloadbalancing.LoadBalancerDef = pydantic.Field(..., description='[disable-awslint:ref-via-interface] The load balancer to attach the target to.')

class AwsElasticloadbalancingv2IApplicationListenerDefConfig(pydantic.BaseModel):
    add_action: typing.Optional[list[AwsElasticloadbalancingv2IApplicationListenerDefAddActionParams]] = pydantic.Field(None, description="Perform the given action on incoming requests.\nThis allows full control of the default action of the load balancer,\nincluding Action chaining, fixed responses and redirect responses. See\nthe ``ListenerAction`` class for all options.\n\nIt's possible to add routing conditions to the Action added in this way.\n\nIt is not possible to add a default action to an imported IApplicationListener.\nIn order to add actions to an imported IApplicationListener a ``priority``\nmust be provided.")
    add_certificates: typing.Optional[list[AwsElasticloadbalancingv2IApplicationListenerDefAddCertificatesParams]] = pydantic.Field(None, description='Add one or more certificates to this listener.')
    add_target_groups: typing.Optional[list[AwsElasticloadbalancingv2IApplicationListenerDefAddTargetGroupsParams]] = pydantic.Field(None, description="Load balance incoming requests to the given target groups.\nIt's possible to add conditions to the TargetGroups added in this way.\nAt least one TargetGroup must be added without conditions.")
    add_targets: typing.Optional[list[AwsElasticloadbalancingv2IApplicationListenerDefAddTargetsParams]] = pydantic.Field(None, description="Load balance incoming requests to the given load balancing targets.\nThis method implicitly creates an ApplicationTargetGroup for the targets\ninvolved.\n\nIt's possible to add conditions to the targets added in this way. At least\none set of targets must be added without conditions.")
    apply_removal_policy: typing.Optional[list[AwsElasticloadbalancingv2IApplicationListenerDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    register_connectable: typing.Optional[list[AwsElasticloadbalancingv2IApplicationListenerDefRegisterConnectableParams]] = pydantic.Field(None, description="Register that a connectable that has been added to this load balancer.\nDon't call this directly. It is called by ApplicationTargetGroup.")


class AwsElasticloadbalancingv2IApplicationListenerDefAddActionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    action: models.aws_elasticloadbalancingv2.ListenerActionDef = pydantic.Field(..., description='Action to perform.\n')
    conditions: typing.Optional[typing.Sequence[models.aws_elasticloadbalancingv2.ListenerConditionDef]] = pydantic.Field(None, description='Rule applies if matches the conditions. Default: - No conditions.\n')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='Priority of this target group. The rule with the lowest priority will be used for every request. If priority is not given, these target groups will be added as defaults, and must not have conditions. Priorities must be unique. Default: Target groups are used as defaults')

class AwsElasticloadbalancingv2IApplicationListenerDefAddCertificatesParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    certificates: typing.Sequence[typing.Union[models.aws_elasticloadbalancingv2.ListenerCertificateDef]] = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2IApplicationListenerDefAddTargetGroupsParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target_groups: typing.Sequence[typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef]] = pydantic.Field(..., description='Target groups to forward requests to.\n')
    conditions: typing.Optional[typing.Sequence[models.aws_elasticloadbalancingv2.ListenerConditionDef]] = pydantic.Field(None, description='Rule applies if matches the conditions. Default: - No conditions.\n')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='Priority of this target group. The rule with the lowest priority will be used for every request. If priority is not given, these target groups will be added as defaults, and must not have conditions. Priorities must be unique. Default: Target groups are used as defaults')

class AwsElasticloadbalancingv2IApplicationListenerDefAddTargetsParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    deregistration_delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time for Elastic Load Balancing to wait before deregistering a target. The range is 0-3600 seconds. Default: Duration.minutes(5)\n')
    health_check: typing.Union[models.aws_elasticloadbalancingv2.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Health check configuration. Default: - The default value for each property in this configuration varies depending on the target.\n')
    load_balancing_algorithm_type: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.TargetGroupLoadBalancingAlgorithmType] = pydantic.Field(None, description='The load balancing algorithm to select targets for routing requests. Default: round_robin.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port on which the listener listens for requests. Default: Determined from protocol if known\n')
    protocol: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.ApplicationProtocol] = pydantic.Field(None, description='The protocol to use. Default: Determined from port if known\n')
    protocol_version: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.ApplicationProtocolVersion] = pydantic.Field(None, description='The protocol version to use. Default: ApplicationProtocolVersion.HTTP1\n')
    slow_start: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time period during which the load balancer sends a newly registered target a linearly increasing share of the traffic to the target group. The range is 30-900 seconds (15 minutes). Default: 0\n')
    stickiness_cookie_duration: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The stickiness cookie expiration period. Setting this value enables load balancer stickiness. After this period, the cookie is considered stale. The minimum value is 1 second and the maximum value is 7 days (604800 seconds). Default: Stickiness disabled\n')
    stickiness_cookie_name: typing.Optional[str] = pydantic.Field(None, description="The name of an application-based stickiness cookie. Names that start with the following prefixes are not allowed: AWSALB, AWSALBAPP, and AWSALBTG; they're reserved for use by the load balancer. Note: ``stickinessCookieName`` parameter depends on the presence of ``stickinessCookieDuration`` parameter. If ``stickinessCookieDuration`` is not set, ``stickinessCookieName`` will be omitted. Default: - If ``stickinessCookieDuration`` is set, a load-balancer generated cookie is used. Otherwise, no stickiness is defined.\n")
    target_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the target group. This name must be unique per region per account, can have a maximum of 32 characters, must contain only alphanumeric characters or hyphens, and must not begin or end with a hyphen. Default: Automatically generated\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_autoscaling.AutoScalingGroupDef, models.aws_ecs.BaseServiceDef, models.aws_ecs.Ec2ServiceDef, models.aws_ecs.ExternalServiceDef, models.aws_ecs.FargateServiceDef, models.aws_elasticloadbalancingv2_targets.InstanceIdTargetDef, models.aws_elasticloadbalancingv2_targets.InstanceTargetDef, models.aws_elasticloadbalancingv2_targets.IpTargetDef, models.aws_elasticloadbalancingv2_targets.LambdaTargetDef]]] = pydantic.Field(None, description='The targets to add to this target group. Can be ``Instance``, ``IPAddress``, or any self-registering load balancing target. All target must be of the same type.\n')
    conditions: typing.Optional[typing.Sequence[models.aws_elasticloadbalancingv2.ListenerConditionDef]] = pydantic.Field(None, description='Rule applies if matches the conditions. Default: - No conditions.\n')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='Priority of this target group. The rule with the lowest priority will be used for every request. If priority is not given, these target groups will be added as defaults, and must not have conditions. Priorities must be unique. Default: Target groups are used as defaults\n')
    return_config: typing.Optional[list[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationListenerDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2IApplicationListenerDefRegisterConnectableParams(pydantic.BaseModel):
    connectable: typing.Union[models.aws_autoscaling.AutoScalingGroupDef, models.aws_ec2.ConnectionsDef, models.aws_ec2.NatInstanceProviderDef, models.aws_ec2.LaunchTemplateDef, models.aws_elasticloadbalancing.ListenerPortDef, models.aws_elasticloadbalancing.LoadBalancerDef, models.aws_elasticsearch.DomainDef, models.aws_opensearchservice.DomainDef, models.aws_rds.DatabaseProxyDef, models.aws_secretsmanager.HostedRotationDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef] = pydantic.Field(..., description='-\n')
    port_range: models.aws_ec2.PortDef = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2IApplicationLoadBalancerDefConfig(pydantic.BaseModel):
    add_listener: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerDefAddListenerParams]] = pydantic.Field(None, description='Add a new listener to this load balancer.')
    apply_removal_policy: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsElasticloadbalancingv2IApplicationLoadBalancerDefAddListenerParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    certificates: typing.Optional[typing.Sequence[typing.Union[models.aws_elasticloadbalancingv2.ListenerCertificateDef]]] = pydantic.Field(None, description='Certificate list of ACM cert ARNs. You must provide exactly one certificate if the listener protocol is HTTPS or TLS. Default: - No certificates.\n')
    default_action: typing.Optional[models.aws_elasticloadbalancingv2.ListenerActionDef] = pydantic.Field(None, description='Default action to take for requests to this listener. This allows full control of the default action of the load balancer, including Action chaining, fixed responses and redirect responses. See the ``ListenerAction`` class for all options. Cannot be specified together with ``defaultTargetGroups``. Default: - None.\n')
    default_target_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef]]] = pydantic.Field(None, description='Default target groups to load balance to. All target groups will be load balanced to with equal weight and without stickiness. For a more complex configuration than that, use either ``defaultAction`` or ``addAction()``. Cannot be specified together with ``defaultAction``. Default: - None.\n')
    open: typing.Optional[bool] = pydantic.Field(None, description="Allow anyone to connect to the load balancer on the listener port. If this is specified, the load balancer will be opened up to anyone who can reach it. For internal load balancers this is anyone in the same VPC. For public load balancers, this is anyone on the internet. If you want to be more selective about who can access this load balancer, set this to ``false`` and use the listener's ``connections`` object to selectively grant access to the load balancer on the listener port. Default: true\n")
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port on which the listener listens for requests. Default: - Determined from protocol if known.\n')
    protocol: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.ApplicationProtocol] = pydantic.Field(None, description='The protocol to use. Default: - Determined from port if known.\n')
    ssl_policy: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.SslPolicy] = pydantic.Field(None, description='The security policy that defines which ciphers and protocols are supported. Default: - The current predefined security policy.')
    return_config: typing.Optional[list[models.aws_elasticloadbalancingv2.ApplicationListenerDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefConfig(pydantic.BaseModel):
    active_connection_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefActiveConnectionCountParams]] = pydantic.Field(None, description='The total number of concurrent TCP connections active from clients to the load balancer and from the load balancer to targets.')
    client_tls_negotiation_error_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefClientTlsNegotiationErrorCountParams]] = pydantic.Field(None, description='The number of TLS connections initiated by the client that did not establish a session with the load balancer.\nPossible causes include a\nmismatch of ciphers or protocols.')
    consumed_lc_us: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefConsumedLcUsParams]] = pydantic.Field(None, description='The number of load balancer capacity units (LCU) used by your load balancer.')
    custom: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefCustomParams]] = pydantic.Field(None, description='Return the given named metric for this Application Load Balancer.')
    elb_auth_error: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthErrorParams]] = pydantic.Field(None, description="The number of user authentications that could not be completed.\nBecause an authenticate action was misconfigured, the load balancer\ncouldn't establish a connection with the IdP, or the load balancer\ncouldn't complete the authentication flow due to an internal error.")
    elb_auth_failure: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthFailureParams]] = pydantic.Field(None, description='The number of user authentications that could not be completed because the IdP denied access to the user or an authorization code was used more than once.')
    elb_auth_latency: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthLatencyParams]] = pydantic.Field(None, description='The time elapsed, in milliseconds, to query the IdP for the ID token and user info.\nIf one or more of these operations fail, this is the time to failure.')
    elb_auth_success: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthSuccessParams]] = pydantic.Field(None, description='The number of authenticate actions that were successful.\nThis metric is incremented at the end of the authentication workflow,\nafter the load balancer has retrieved the user claims from the IdP.')
    http_code_elb: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpCodeElbParams]] = pydantic.Field(None, description='The number of HTTP 3xx/4xx/5xx codes that originate from the load balancer.\nThis does not include any response codes generated by the targets.')
    http_code_target: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpCodeTargetParams]] = pydantic.Field(None, description='The number of HTTP 2xx/3xx/4xx/5xx response codes generated by all targets in the load balancer.\nThis does not include any response codes generated by the load balancer.')
    http_fixed_response_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpFixedResponseCountParams]] = pydantic.Field(None, description='The number of fixed-response actions that were successful.')
    http_redirect_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpRedirectCountParams]] = pydantic.Field(None, description='The number of redirect actions that were successful.')
    http_redirect_url_limit_exceeded_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpRedirectUrlLimitExceededCountParams]] = pydantic.Field(None, description="The number of redirect actions that couldn't be completed because the URL in the response location header is larger than 8K.")
    ipv6_processed_bytes: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefIpv6ProcessedBytesParams]] = pydantic.Field(None, description='The total number of bytes processed by the load balancer over IPv6.')
    ipv6_request_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefIpv6RequestCountParams]] = pydantic.Field(None, description='The number of IPv6 requests received by the load balancer.')
    new_connection_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefNewConnectionCountParams]] = pydantic.Field(None, description='The total number of new TCP connections established from clients to the load balancer and from the load balancer to targets.')
    processed_bytes: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefProcessedBytesParams]] = pydantic.Field(None, description='The total number of bytes processed by the load balancer over IPv4 and IPv6.')
    rejected_connection_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefRejectedConnectionCountParams]] = pydantic.Field(None, description='The number of connections that were rejected because the load balancer had reached its maximum number of connections.')
    request_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefRequestCountParams]] = pydantic.Field(None, description='The number of requests processed over IPv4 and IPv6.\nThis count includes only the requests with a response generated by a target of the load balancer.')
    rule_evaluations: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefRuleEvaluationsParams]] = pydantic.Field(None, description='The number of rules processed by the load balancer given a request rate averaged over an hour.')
    target_connection_error_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefTargetConnectionErrorCountParams]] = pydantic.Field(None, description='The number of connections that were not successfully established between the load balancer and target.')
    target_response_time: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefTargetResponseTimeParams]] = pydantic.Field(None, description='The time elapsed, in seconds, after the request leaves the load balancer until a response from the target is received.')
    target_tls_negotiation_error_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefTargetTlsNegotiationErrorCountParams]] = pydantic.Field(None, description='The number of TLS connections initiated by the load balancer that did not establish a session with the target.\nPossible causes include a mismatch of ciphers or protocols.')


class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefActiveConnectionCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefClientTlsNegotiationErrorCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefConsumedLcUsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefCustomParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthErrorParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthFailureParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefElbAuthSuccessParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpCodeElbParams(pydantic.BaseModel):
    code: aws_cdk.aws_elasticloadbalancingv2.HttpCodeElb = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpCodeTargetParams(pydantic.BaseModel):
    code: aws_cdk.aws_elasticloadbalancingv2.HttpCodeTarget = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpFixedResponseCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpRedirectCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefHttpRedirectUrlLimitExceededCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefIpv6ProcessedBytesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefIpv6RequestCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefNewConnectionCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefProcessedBytesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefRejectedConnectionCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefRequestCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefRuleEvaluationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefTargetConnectionErrorCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefTargetResponseTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerMetricsDefTargetTlsNegotiationErrorCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationLoadBalancerTargetDefConfig(pydantic.BaseModel):
    attach_to_application_target_group: typing.Optional[list[AwsElasticloadbalancingv2IApplicationLoadBalancerTargetDefAttachToApplicationTargetGroupParams]] = pydantic.Field(None, description='Attach load-balanced target to a TargetGroup.\nMay return JSON to directly add to the [Targets] list, or return undefined\nif the target will register itself with the load balancer.')


class AwsElasticloadbalancingv2IApplicationLoadBalancerTargetDefAttachToApplicationTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef] = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2IApplicationTargetGroupDefConfig(pydantic.BaseModel):
    add_target: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupDefAddTargetParams]] = pydantic.Field(None, description='Add a load balancing target to this target group.')
    register_connectable: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupDefRegisterConnectableParams]] = pydantic.Field(None, description="Register a connectable as a member of this target group.\nDon't call this directly. It will be called by load balancing targets.")
    register_listener: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupDefRegisterListenerParams]] = pydantic.Field(None, description="Register a listener that is load balancing to this target group.\nDon't call this directly. It will be called by listeners.")


class AwsElasticloadbalancingv2IApplicationTargetGroupDefAddTargetParams(pydantic.BaseModel):
    targets: list[typing.Union[models.aws_autoscaling.AutoScalingGroupDef, models.aws_ecs.BaseServiceDef, models.aws_ecs.Ec2ServiceDef, models.aws_ecs.ExternalServiceDef, models.aws_ecs.FargateServiceDef, models.aws_elasticloadbalancingv2_targets.InstanceIdTargetDef, models.aws_elasticloadbalancingv2_targets.InstanceTargetDef, models.aws_elasticloadbalancingv2_targets.IpTargetDef, models.aws_elasticloadbalancingv2_targets.LambdaTargetDef]] = pydantic.Field(...)

class AwsElasticloadbalancingv2IApplicationTargetGroupDefRegisterConnectableParams(pydantic.BaseModel):
    connectable: typing.Union[models.aws_autoscaling.AutoScalingGroupDef, models.aws_ec2.ConnectionsDef, models.aws_ec2.NatInstanceProviderDef, models.aws_ec2.LaunchTemplateDef, models.aws_elasticloadbalancing.ListenerPortDef, models.aws_elasticloadbalancing.LoadBalancerDef, models.aws_elasticsearch.DomainDef, models.aws_opensearchservice.DomainDef, models.aws_rds.DatabaseProxyDef, models.aws_secretsmanager.HostedRotationDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef] = pydantic.Field(..., description='-\n')
    port_range: typing.Optional[models.aws_ec2.PortDef] = pydantic.Field(None, description='-')

class AwsElasticloadbalancingv2IApplicationTargetGroupDefRegisterListenerParams(pydantic.BaseModel):
    listener: typing.Union[models.aws_elasticloadbalancingv2.ApplicationListenerDef] = pydantic.Field(..., description='-\n')
    associating_construct: typing.Optional[models.AnyResource] = pydantic.Field(None, description='-')

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefConfig(pydantic.BaseModel):
    custom: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefCustomParams]] = pydantic.Field(None, description='Return the given named metric for this Network Target Group.')
    healthy_host_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefHealthyHostCountParams]] = pydantic.Field(None, description='The number of healthy hosts in the target group.')
    http_code_target: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefHttpCodeTargetParams]] = pydantic.Field(None, description='The number of HTTP 2xx/3xx/4xx/5xx response codes generated by all targets in this target group.\nThis does not include any response codes generated by the load balancer.')
    ipv6_request_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefIpv6RequestCountParams]] = pydantic.Field(None, description='The number of IPv6 requests received by the target group.')
    request_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefRequestCountParams]] = pydantic.Field(None, description='The number of requests processed over IPv4 and IPv6.\nThis count includes only the requests with a response generated by a target of the load balancer.')
    request_count_per_target: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefRequestCountPerTargetParams]] = pydantic.Field(None, description='The average number of requests received by each target in a target group.\nThe only valid statistic is Sum. Note that this represents the average not the sum.')
    target_connection_error_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefTargetConnectionErrorCountParams]] = pydantic.Field(None, description='The number of connections that were not successfully established between the load balancer and target.')
    target_response_time: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefTargetResponseTimeParams]] = pydantic.Field(None, description='The time elapsed, in seconds, after the request leaves the load balancer until a response from the target is received.')
    target_tls_negotiation_error_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefTargetTlsNegotiationErrorCountParams]] = pydantic.Field(None, description='The number of TLS connections initiated by the load balancer that did not establish a session with the target.\nPossible causes include a mismatch of ciphers or protocols.')
    unhealthy_host_count: typing.Optional[list[AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefUnhealthyHostCountParams]] = pydantic.Field(None, description='The number of unhealthy hosts in the target group.')


class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefCustomParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefHealthyHostCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefHttpCodeTargetParams(pydantic.BaseModel):
    code: aws_cdk.aws_elasticloadbalancingv2.HttpCodeTarget = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefIpv6RequestCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefRequestCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefRequestCountPerTargetParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefTargetConnectionErrorCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefTargetResponseTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefTargetTlsNegotiationErrorCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IApplicationTargetGroupMetricsDefUnhealthyHostCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2IListenerDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsElasticloadbalancingv2IListenerDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsElasticloadbalancingv2IListenerDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2IListenerActionDefConfig(pydantic.BaseModel):
    render_actions: typing.Optional[bool] = pydantic.Field(None, description='Render the listener default actions in this chain.')
    render_rule_actions: typing.Optional[bool] = pydantic.Field(None, description='Render the listener rule actions in this chain.')

#  aws-cdk-lib.aws_elasticloadbalancingv2.IListenerCertificate skipped


class AwsElasticloadbalancingv2ILoadBalancerV2DefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsElasticloadbalancingv2ILoadBalancerV2DefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsElasticloadbalancingv2ILoadBalancerV2DefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2INetworkListenerDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsElasticloadbalancingv2INetworkListenerDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsElasticloadbalancingv2INetworkListenerDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2INetworkLoadBalancerDefConfig(pydantic.BaseModel):
    add_listener: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerDefAddListenerParams]] = pydantic.Field(None, description='Add a listener to this load balancer.')
    apply_removal_policy: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsElasticloadbalancingv2INetworkLoadBalancerDefAddListenerParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    port: typing.Union[int, float] = pydantic.Field(..., description='The port on which the listener listens for requests.\n')
    alpn_policy: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.AlpnPolicy] = pydantic.Field(None, description='Application-Layer Protocol Negotiation (ALPN) is a TLS extension that is sent on the initial TLS handshake hello messages. ALPN enables the application layer to negotiate which protocols should be used over a secure connection, such as HTTP/1 and HTTP/2. Can only be specified together with Protocol TLS. Default: - None\n')
    certificates: typing.Optional[typing.Sequence[typing.Union[models.aws_elasticloadbalancingv2.ListenerCertificateDef]]] = pydantic.Field(None, description='Certificate list of ACM cert ARNs. You must provide exactly one certificate if the listener protocol is HTTPS or TLS. Default: - No certificates.\n')
    default_action: typing.Optional[models.aws_elasticloadbalancingv2.NetworkListenerActionDef] = pydantic.Field(None, description='Default action to take for requests to this listener. This allows full control of the default Action of the load balancer, including weighted forwarding. See the ``NetworkListenerAction`` class for all options. Cannot be specified together with ``defaultTargetGroups``. Default: - None.\n')
    default_target_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_elasticloadbalancingv2.NetworkTargetGroupDef]]] = pydantic.Field(None, description='Default target groups to load balance to. All target groups will be load balanced to with equal weight and without stickiness. For a more complex configuration than that, use either ``defaultAction`` or ``addAction()``. Cannot be specified together with ``defaultAction``. Default: - None.\n')
    protocol: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.Protocol] = pydantic.Field(None, description='Protocol for listener, expects TCP, TLS, UDP, or TCP_UDP. Default: - TLS if certificates are provided. TCP otherwise.\n')
    ssl_policy: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.SslPolicy] = pydantic.Field(None, description='SSL Policy. Default: - Current predefined security policy.\n')
    return_config: typing.Optional[list[models.aws_elasticloadbalancingv2.NetworkListenerDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefConfig(pydantic.BaseModel):
    active_flow_count: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefActiveFlowCountParams]] = pydantic.Field(None, description='The total number of concurrent TCP flows (or connections) from clients to targets.\nThis metric includes connections in the SYN_SENT and ESTABLISHED states.\nTCP connections are not terminated at the load balancer, so a client\nopening a TCP connection to a target counts as a single flow.')
    consumed_lc_us: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefConsumedLcUsParams]] = pydantic.Field(None, description='The number of load balancer capacity units (LCU) used by your load balancer.')
    custom: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefCustomParams]] = pydantic.Field(None, description='Return the given named metric for this Network Load Balancer.')
    new_flow_count: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefNewFlowCountParams]] = pydantic.Field(None, description='The total number of new TCP flows (or connections) established from clients to targets in the time period.')
    processed_bytes: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefProcessedBytesParams]] = pydantic.Field(None, description='The total number of bytes processed by the load balancer, including TCP/IP headers.')
    tcp_client_reset_count: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefTcpClientResetCountParams]] = pydantic.Field(None, description='The total number of reset (RST) packets sent from a client to a target.\nThese resets are generated by the client and forwarded by the load balancer.')
    tcp_elb_reset_count: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefTcpElbResetCountParams]] = pydantic.Field(None, description='The total number of reset (RST) packets generated by the load balancer.')
    tcp_target_reset_count: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefTcpTargetResetCountParams]] = pydantic.Field(None, description='The total number of reset (RST) packets sent from a target to a client.\nThese resets are generated by the target and forwarded by the load balancer.')


class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefActiveFlowCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefConsumedLcUsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefCustomParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefNewFlowCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefProcessedBytesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefTcpClientResetCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefTcpElbResetCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerMetricsDefTcpTargetResetCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkLoadBalancerTargetDefConfig(pydantic.BaseModel):
    attach_to_network_target_group: typing.Optional[list[AwsElasticloadbalancingv2INetworkLoadBalancerTargetDefAttachToNetworkTargetGroupParams]] = pydantic.Field(None, description='Attach load-balanced target to a TargetGroup.\nMay return JSON to directly add to the [Targets] list, or return undefined\nif the target will register itself with the load balancer.')


class AwsElasticloadbalancingv2INetworkLoadBalancerTargetDefAttachToNetworkTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.NetworkTargetGroupDef] = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2INetworkTargetGroupDefConfig(pydantic.BaseModel):
    add_target: typing.Optional[list[AwsElasticloadbalancingv2INetworkTargetGroupDefAddTargetParams]] = pydantic.Field(None, description='Add a load balancing target to this target group.')
    register_listener: typing.Optional[list[AwsElasticloadbalancingv2INetworkTargetGroupDefRegisterListenerParams]] = pydantic.Field(None, description="Register a listener that is load balancing to this target group.\nDon't call this directly. It will be called by listeners.")


class AwsElasticloadbalancingv2INetworkTargetGroupDefAddTargetParams(pydantic.BaseModel):
    targets: list[typing.Union[models.aws_autoscaling.AutoScalingGroupDef, models.aws_ecs.BaseServiceDef, models.aws_ecs.Ec2ServiceDef, models.aws_ecs.ExternalServiceDef, models.aws_ecs.FargateServiceDef, models.aws_elasticloadbalancingv2_targets.AlbArnTargetDef, models.aws_elasticloadbalancingv2_targets.AlbTargetDef, models.aws_elasticloadbalancingv2_targets.InstanceIdTargetDef, models.aws_elasticloadbalancingv2_targets.InstanceTargetDef, models.aws_elasticloadbalancingv2_targets.IpTargetDef]] = pydantic.Field(...)

class AwsElasticloadbalancingv2INetworkTargetGroupDefRegisterListenerParams(pydantic.BaseModel):
    listener: typing.Union[models.aws_elasticloadbalancingv2.NetworkListenerDef] = pydantic.Field(..., description='-')

class AwsElasticloadbalancingv2INetworkTargetGroupMetricsDefConfig(pydantic.BaseModel):
    custom: typing.Optional[list[AwsElasticloadbalancingv2INetworkTargetGroupMetricsDefCustomParams]] = pydantic.Field(None, description='Return the given named metric for this Network Target Group.')
    healthy_host_count: typing.Optional[list[AwsElasticloadbalancingv2INetworkTargetGroupMetricsDefHealthyHostCountParams]] = pydantic.Field(None, description='The number of targets that are considered healthy.')
    un_healthy_host_count: typing.Optional[list[AwsElasticloadbalancingv2INetworkTargetGroupMetricsDefUnHealthyHostCountParams]] = pydantic.Field(None, description='The number of targets that are considered unhealthy.')


class AwsElasticloadbalancingv2INetworkTargetGroupMetricsDefCustomParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkTargetGroupMetricsDefHealthyHostCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticloadbalancingv2INetworkTargetGroupMetricsDefUnHealthyHostCountParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: Average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
#  aws-cdk-lib.aws_elasticloadbalancingv2.ITargetGroup skipped


class AwsElasticsearchIDomainDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsElasticsearchIDomainDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_index_read: typing.Optional[list[AwsElasticsearchIDomainDefGrantIndexReadParams]] = pydantic.Field(None, description='(deprecated) Grant read permissions for an index in this domain to an IAM principal (Role/Group/User).')
    grant_index_read_write: typing.Optional[list[AwsElasticsearchIDomainDefGrantIndexReadWriteParams]] = pydantic.Field(None, description='(deprecated) Grant read/write permissions for an index in this domain to an IAM principal (Role/Group/User).')
    grant_index_write: typing.Optional[list[AwsElasticsearchIDomainDefGrantIndexWriteParams]] = pydantic.Field(None, description='(deprecated) Grant write permissions for an index in this domain to an IAM principal (Role/Group/User).')
    grant_path_read: typing.Optional[list[AwsElasticsearchIDomainDefGrantPathReadParams]] = pydantic.Field(None, description='(deprecated) Grant read permissions for a specific path in this domain to an IAM principal (Role/Group/User).')
    grant_path_read_write: typing.Optional[list[AwsElasticsearchIDomainDefGrantPathReadWriteParams]] = pydantic.Field(None, description='(deprecated) Grant read/write permissions for a specific path in this domain to an IAM principal (Role/Group/User).')
    grant_path_write: typing.Optional[list[AwsElasticsearchIDomainDefGrantPathWriteParams]] = pydantic.Field(None, description='(deprecated) Grant write permissions for a specific path in this domain to an IAM principal (Role/Group/User).')
    grant_read: typing.Optional[list[AwsElasticsearchIDomainDefGrantReadParams]] = pydantic.Field(None, description='(deprecated) Grant read permissions for this domain and its contents to an IAM principal (Role/Group/User).')
    grant_read_write: typing.Optional[list[AwsElasticsearchIDomainDefGrantReadWriteParams]] = pydantic.Field(None, description='(deprecated) Grant read/write permissions for this domain and its contents to an IAM principal (Role/Group/User).')
    grant_write: typing.Optional[list[AwsElasticsearchIDomainDefGrantWriteParams]] = pydantic.Field(None, description='(deprecated) Grant write permissions for this domain and its contents to an IAM principal (Role/Group/User).')
    metric: typing.Optional[list[AwsElasticsearchIDomainDefMetricParams]] = pydantic.Field(None, description='(deprecated) Return the given named metric for this Domain.')
    metric_automated_snapshot_failure: typing.Optional[list[AwsElasticsearchIDomainDefMetricAutomatedSnapshotFailureParams]] = pydantic.Field(None, description='(deprecated) Metric for automated snapshot failures.')
    metric_cluster_index_writes_blocked: typing.Optional[list[AwsElasticsearchIDomainDefMetricClusterIndexWritesBlockedParams]] = pydantic.Field(None, description='(deprecated) Metric for the cluster blocking index writes.')
    metric_cluster_status_red: typing.Optional[list[AwsElasticsearchIDomainDefMetricClusterStatusRedParams]] = pydantic.Field(None, description='(deprecated) Metric for the time the cluster status is red.')
    metric_cluster_status_yellow: typing.Optional[list[AwsElasticsearchIDomainDefMetricClusterStatusYellowParams]] = pydantic.Field(None, description='(deprecated) Metric for the time the cluster status is yellow.')
    metric_cpu_utilization: typing.Optional[list[AwsElasticsearchIDomainDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='(deprecated) Metric for CPU utilization.')
    metric_free_storage_space: typing.Optional[list[AwsElasticsearchIDomainDefMetricFreeStorageSpaceParams]] = pydantic.Field(None, description='(deprecated) Metric for the storage space of nodes in the cluster.')
    metric_indexing_latency: typing.Optional[list[AwsElasticsearchIDomainDefMetricIndexingLatencyParams]] = pydantic.Field(None, description='(deprecated) Metric for indexing latency.')
    metric_jvm_memory_pressure: typing.Optional[list[AwsElasticsearchIDomainDefMetricJvmMemoryPressureParams]] = pydantic.Field(None, description='(deprecated) Metric for JVM memory pressure.')
    metric_kms_key_error: typing.Optional[list[AwsElasticsearchIDomainDefMetricKmsKeyErrorParams]] = pydantic.Field(None, description='(deprecated) Metric for KMS key errors.')
    metric_kms_key_inaccessible: typing.Optional[list[AwsElasticsearchIDomainDefMetricKmsKeyInaccessibleParams]] = pydantic.Field(None, description='(deprecated) Metric for KMS key being inaccessible.')
    metric_master_cpu_utilization: typing.Optional[list[AwsElasticsearchIDomainDefMetricMasterCpuUtilizationParams]] = pydantic.Field(None, description='(deprecated) Metric for master CPU utilization.')
    metric_master_jvm_memory_pressure: typing.Optional[list[AwsElasticsearchIDomainDefMetricMasterJvmMemoryPressureParams]] = pydantic.Field(None, description='(deprecated) Metric for master JVM memory pressure.')
    metric_nodes: typing.Optional[list[AwsElasticsearchIDomainDefMetricNodesParams]] = pydantic.Field(None, description='(deprecated) Metric for the number of nodes.')
    metric_search_latency: typing.Optional[list[AwsElasticsearchIDomainDefMetricSearchLatencyParams]] = pydantic.Field(None, description='(deprecated) Metric for search latency.')
    metric_searchable_documents: typing.Optional[list[AwsElasticsearchIDomainDefMetricSearchableDocumentsParams]] = pydantic.Field(None, description='(deprecated) Metric for number of searchable documents.')


class AwsElasticsearchIDomainDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsElasticsearchIDomainDefGrantIndexReadParams(pydantic.BaseModel):
    index: str = pydantic.Field(..., description='The index to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantIndexReadWriteParams(pydantic.BaseModel):
    index: str = pydantic.Field(..., description='The index to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantIndexWriteParams(pydantic.BaseModel):
    index: str = pydantic.Field(..., description='The index to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantPathReadParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantPathReadWriteParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantPathWriteParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantReadParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantReadWriteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefGrantWriteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricAutomatedSnapshotFailureParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricClusterIndexWritesBlockedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 1 minute\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricClusterStatusRedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricClusterStatusYellowParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricFreeStorageSpaceParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: minimum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricIndexingLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: p99 over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricJvmMemoryPressureParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricKmsKeyErrorParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricKmsKeyInaccessibleParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricMasterCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricMasterJvmMemoryPressureParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricNodesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: minimum over 1 hour\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricSearchLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: p99 over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsElasticsearchIDomainDefMetricSearchableDocumentsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n\n:deprecated: use opensearchservice module instead\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsEventsIApiDestinationDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEventsIApiDestinationDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEventsIApiDestinationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEventsIConnectionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEventsIConnectionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEventsIConnectionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEventsIEventBusDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEventsIEventBusDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    archive: typing.Optional[list[AwsEventsIEventBusDefArchiveParams]] = pydantic.Field(None, description='Create an EventBridge archive to send events to.\nWhen you create an archive, incoming events might not immediately start being sent to the archive.\nAllow a short period of time for changes to take effect.')
    grant_put_events_to: typing.Optional[list[AwsEventsIEventBusDefGrantPutEventsToParams]] = pydantic.Field(None, description='Grants an IAM Principal to send custom events to the eventBus so that they can be matched to rules.')


class AwsEventsIEventBusDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEventsIEventBusDefArchiveParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any]] = pydantic.Field(..., description='An event pattern to use to filter events sent to the archive.\n')
    archive_name: typing.Optional[str] = pydantic.Field(None, description='The name of the archive. Default: - Automatically generated\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description for the archive. Default: - none\n')
    retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days to retain events for. Default value is 0. If set to 0, events are retained indefinitely. Default: - Infinite')
    return_config: typing.Optional[list[models.aws_events.ArchiveDefConfig]] = pydantic.Field(None)

class AwsEventsIEventBusDefGrantPutEventsToParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal (no-op if undefined).')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsEventsIRuleDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsEventsIRuleDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsEventsIRuleDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsEventsIRuleTargetDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsEventsIRuleTargetDefBindParams]] = pydantic.Field(None, description='Returns the rule target specification.\nNOTE: Do not use the various ``inputXxx`` options. They can be set in a call to ``addTarget``.')


class AwsEventsIRuleTargetDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='The EventBridge Rule that would trigger this target.\n')
    id: typing.Optional[str] = pydantic.Field(None, description='The id of the target that will be attached to the rule.')
#  aws-cdk-lib.aws_fsx.IFileSystem skipped


class AwsGlobalacceleratorIAcceleratorDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsGlobalacceleratorIAcceleratorDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsGlobalacceleratorIAcceleratorDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsGlobalacceleratorIEndpointDefConfig(pydantic.BaseModel):
    render_endpoint_configuration: typing.Optional[bool] = pydantic.Field(None, description='Render the endpoint to an endpoint configuration.')


class AwsGlobalacceleratorIEndpointGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsGlobalacceleratorIEndpointGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsGlobalacceleratorIEndpointGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsGlobalacceleratorIListenerDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsGlobalacceleratorIListenerDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsGlobalacceleratorIListenerDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIAccessKeyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsIamIAccessKeyDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsIamIAccessKeyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIAssumeRolePrincipalDefConfig(pydantic.BaseModel):
    add_to_assume_role_policy: typing.Optional[list[AwsIamIAssumeRolePrincipalDefAddToAssumeRolePolicyParams]] = pydantic.Field(None, description='Add the principal to the AssumeRolePolicyDocument.\nAdd the statements to the AssumeRolePolicyDocument necessary to give this principal\npermissions to assume the given role.')
    add_to_principal_policy: typing.Optional[list[AwsIamIAssumeRolePrincipalDefAddToPrincipalPolicyParams]] = pydantic.Field(None, description='Add to the policy of this principal.')


class AwsIamIAssumeRolePrincipalDefAddToAssumeRolePolicyParams(pydantic.BaseModel):
    document: models.aws_iam.PolicyDocumentDef = pydantic.Field(..., description='-')

class AwsIamIAssumeRolePrincipalDefAddToPrincipalPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsIamIComparablePrincipalDefConfig(pydantic.BaseModel):
    add_to_principal_policy: typing.Optional[list[AwsIamIComparablePrincipalDefAddToPrincipalPolicyParams]] = pydantic.Field(None, description='Add to the policy of this principal.')
    dedupe_string: typing.Optional[bool] = pydantic.Field(None, description='Return a string format of this principal which should be identical if the two principals are the same.')


class AwsIamIComparablePrincipalDefAddToPrincipalPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_iam.IGrantable skipped


class AwsIamIGroupDefConfig(pydantic.BaseModel):
    add_managed_policy: typing.Optional[list[AwsIamIGroupDefAddManagedPolicyParams]] = pydantic.Field(None, description='Attaches a managed policy to this principal.')
    add_to_principal_policy: typing.Optional[list[AwsIamIGroupDefAddToPrincipalPolicyParams]] = pydantic.Field(None, description='Add to the policy of this principal.')
    apply_removal_policy: typing.Optional[list[AwsIamIGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    attach_inline_policy: typing.Optional[list[AwsIamIGroupDefAttachInlinePolicyParams]] = pydantic.Field(None, description='Attaches an inline policy to this principal.\nThis is the same as calling ``policy.addToXxx(principal)``.')


class AwsIamIGroupDefAddManagedPolicyParams(pydantic.BaseModel):
    policy: typing.Union[models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_iam.ManagedPolicyDef] = pydantic.Field(..., description='The managed policy.')

class AwsIamIGroupDefAddToPrincipalPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsIamIGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIGroupDefAttachInlinePolicyParams(pydantic.BaseModel):
    policy: models.aws_iam.PolicyDef = pydantic.Field(..., description='The policy resource to attach to this principal [disable-awslint:ref-via-interface].')

class AwsIamIIdentityDefConfig(pydantic.BaseModel):
    add_managed_policy: typing.Optional[list[AwsIamIIdentityDefAddManagedPolicyParams]] = pydantic.Field(None, description='Attaches a managed policy to this principal.')
    add_to_principal_policy: typing.Optional[list[AwsIamIIdentityDefAddToPrincipalPolicyParams]] = pydantic.Field(None, description='Add to the policy of this principal.')
    apply_removal_policy: typing.Optional[list[AwsIamIIdentityDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    attach_inline_policy: typing.Optional[list[AwsIamIIdentityDefAttachInlinePolicyParams]] = pydantic.Field(None, description='Attaches an inline policy to this principal.\nThis is the same as calling ``policy.addToXxx(principal)``.')


class AwsIamIIdentityDefAddManagedPolicyParams(pydantic.BaseModel):
    policy: typing.Union[models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_iam.ManagedPolicyDef] = pydantic.Field(..., description='The managed policy.')

class AwsIamIIdentityDefAddToPrincipalPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsIamIIdentityDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIIdentityDefAttachInlinePolicyParams(pydantic.BaseModel):
    policy: models.aws_iam.PolicyDef = pydantic.Field(..., description='The policy resource to attach to this principal [disable-awslint:ref-via-interface].')
#  aws-cdk-lib.aws_iam.IManagedPolicy skipped


class AwsIamIOpenIdConnectProviderDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsIamIOpenIdConnectProviderDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsIamIOpenIdConnectProviderDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIPolicyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsIamIPolicyDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsIamIPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIPrincipalDefConfig(pydantic.BaseModel):
    add_to_principal_policy: typing.Optional[list[AwsIamIPrincipalDefAddToPrincipalPolicyParams]] = pydantic.Field(None, description='Add to the policy of this principal.')


class AwsIamIPrincipalDefAddToPrincipalPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsIamIResourceWithPolicyDefConfig(pydantic.BaseModel):
    add_to_resource_policy: typing.Optional[list[AwsIamIResourceWithPolicyDefAddToResourcePolicyParams]] = pydantic.Field(None, description="Add a statement to the resource's resource policy.")
    apply_removal_policy: typing.Optional[list[AwsIamIResourceWithPolicyDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsIamIResourceWithPolicyDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsIamIResourceWithPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIRoleDefConfig(pydantic.BaseModel):
    add_managed_policy: typing.Optional[list[AwsIamIRoleDefAddManagedPolicyParams]] = pydantic.Field(None, description='Attaches a managed policy to this principal.')
    add_to_principal_policy: typing.Optional[list[AwsIamIRoleDefAddToPrincipalPolicyParams]] = pydantic.Field(None, description='Add to the policy of this principal.')
    apply_removal_policy: typing.Optional[list[AwsIamIRoleDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    attach_inline_policy: typing.Optional[list[AwsIamIRoleDefAttachInlinePolicyParams]] = pydantic.Field(None, description='Attaches an inline policy to this principal.\nThis is the same as calling ``policy.addToXxx(principal)``.')
    grant: typing.Optional[list[AwsIamIRoleDefGrantParams]] = pydantic.Field(None, description='Grant the actions defined in actions to the identity Principal on this resource.')
    grant_assume_role: typing.Optional[list[AwsIamIRoleDefGrantAssumeRoleParams]] = pydantic.Field(None, description='Grant permissions to the given principal to assume this role.')
    grant_pass_role: typing.Optional[list[AwsIamIRoleDefGrantPassRoleParams]] = pydantic.Field(None, description='Grant permissions to the given principal to pass this role.')


class AwsIamIRoleDefAddManagedPolicyParams(pydantic.BaseModel):
    policy: typing.Union[models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_iam.ManagedPolicyDef] = pydantic.Field(..., description='The managed policy.')

class AwsIamIRoleDefAddToPrincipalPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsIamIRoleDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIRoleDefAttachInlinePolicyParams(pydantic.BaseModel):
    policy: models.aws_iam.PolicyDef = pydantic.Field(..., description='The policy resource to attach to this principal [disable-awslint:ref-via-interface].')

class AwsIamIRoleDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsIamIRoleDefGrantAssumeRoleParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsIamIRoleDefGrantPassRoleParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsIamISamlProviderDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsIamISamlProviderDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsIamISamlProviderDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIUserDefConfig(pydantic.BaseModel):
    add_managed_policy: typing.Optional[list[AwsIamIUserDefAddManagedPolicyParams]] = pydantic.Field(None, description='Attaches a managed policy to this principal.')
    add_to_group: typing.Optional[list[AwsIamIUserDefAddToGroupParams]] = pydantic.Field(None, description='Adds this user to a group.')
    add_to_principal_policy: typing.Optional[list[AwsIamIUserDefAddToPrincipalPolicyParams]] = pydantic.Field(None, description='Add to the policy of this principal.')
    apply_removal_policy: typing.Optional[list[AwsIamIUserDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    attach_inline_policy: typing.Optional[list[AwsIamIUserDefAttachInlinePolicyParams]] = pydantic.Field(None, description='Attaches an inline policy to this principal.\nThis is the same as calling ``policy.addToXxx(principal)``.')


class AwsIamIUserDefAddManagedPolicyParams(pydantic.BaseModel):
    policy: typing.Union[models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_iam.ManagedPolicyDef] = pydantic.Field(..., description='The managed policy.')

class AwsIamIUserDefAddToGroupParams(pydantic.BaseModel):
    group: typing.Union[models.aws_iam.GroupDef] = pydantic.Field(..., description='-')

class AwsIamIUserDefAddToPrincipalPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsIamIUserDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsIamIUserDefAttachInlinePolicyParams(pydantic.BaseModel):
    policy: models.aws_iam.PolicyDef = pydantic.Field(..., description='The policy resource to attach to this principal [disable-awslint:ref-via-interface].')

class AwsKinesisIStreamDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsKinesisIStreamDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsKinesisIStreamDefGrantParams]] = pydantic.Field(None, description='Grant the indicated permissions on this stream to the provided IAM principal.')
    grant_read: typing.Optional[list[AwsKinesisIStreamDefGrantReadParams]] = pydantic.Field(None, description='Grant read permissions for this stream and its contents to an IAM principal (Role/Group/User).\nIf an encryption key is used, permission to ues the key to decrypt the\ncontents of the stream will also be granted.')
    grant_read_write: typing.Optional[list[AwsKinesisIStreamDefGrantReadWriteParams]] = pydantic.Field(None, description='Grants read/write permissions for this stream and its contents to an IAM principal (Role/Group/User).\nIf an encryption key is used, permission to use the key for\nencrypt/decrypt will also be granted.')
    grant_write: typing.Optional[list[AwsKinesisIStreamDefGrantWriteParams]] = pydantic.Field(None, description='Grant write permissions for this stream and its contents to an IAM principal (Role/Group/User).\nIf an encryption key is used, permission to ues the key to encrypt the\ncontents of the stream will also be granted.')
    metric: typing.Optional[list[AwsKinesisIStreamDefMetricParams]] = pydantic.Field(None, description='Return stream metric based from its metric name.')
    metric_get_records: typing.Optional[list[AwsKinesisIStreamDefMetricGetRecordsParams]] = pydantic.Field(None, description='The number of records retrieved from the shard, measured over the specified time period.\nMinimum, Maximum, and\nAverage statistics represent the records in a single GetRecords operation for the stream in the specified time\nperiod.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_get_records_bytes: typing.Optional[list[AwsKinesisIStreamDefMetricGetRecordsBytesParams]] = pydantic.Field(None, description='The number of bytes retrieved from the Kinesis stream, measured over the specified time period.\nMinimum, Maximum,\nand Average statistics represent the bytes in a single GetRecords operation for the stream in the specified time\nperiod.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_get_records_iterator_age_milliseconds: typing.Optional[list[AwsKinesisIStreamDefMetricGetRecordsIteratorAgeMillisecondsParams]] = pydantic.Field(None, description='The age of the last record in all GetRecords calls made against a Kinesis stream, measured over the specified time period.\nAge is the difference between the current time and when the last record of the GetRecords call was written\nto the stream. The Minimum and Maximum statistics can be used to track the progress of Kinesis consumer\napplications. A value of zero indicates that the records being read are completely caught up with the stream.\n\nThe metric defaults to maximum over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_get_records_latency: typing.Optional[list[AwsKinesisIStreamDefMetricGetRecordsLatencyParams]] = pydantic.Field(None, description='The time taken per GetRecords operation, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_get_records_success: typing.Optional[list[AwsKinesisIStreamDefMetricGetRecordsSuccessParams]] = pydantic.Field(None, description='The number of successful GetRecords operations per stream, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_incoming_bytes: typing.Optional[list[AwsKinesisIStreamDefMetricIncomingBytesParams]] = pydantic.Field(None, description='The number of bytes successfully put to the Kinesis stream over the specified time period.\nThis metric includes\nbytes from PutRecord and PutRecords operations. Minimum, Maximum, and Average statistics represent the bytes in a\nsingle put operation for the stream in the specified time period.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_incoming_records: typing.Optional[list[AwsKinesisIStreamDefMetricIncomingRecordsParams]] = pydantic.Field(None, description='The number of records successfully put to the Kinesis stream over the specified time period.\nThis metric includes\nrecord counts from PutRecord and PutRecords operations. Minimum, Maximum, and Average statistics represent the\nrecords in a single put operation for the stream in the specified time period.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_record_bytes: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordBytesParams]] = pydantic.Field(None, description='The number of bytes put to the Kinesis stream using the PutRecord operation over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_record_latency: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordLatencyParams]] = pydantic.Field(None, description='The time taken per PutRecord operation, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_record_success: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordSuccessParams]] = pydantic.Field(None, description='The number of successful PutRecord operations per Kinesis stream, measured over the specified time period.\nAverage\nreflects the percentage of successful writes to a stream.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_records_bytes: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordsBytesParams]] = pydantic.Field(None, description='The number of bytes put to the Kinesis stream using the PutRecords operation over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_records_failed_records: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordsFailedRecordsParams]] = pydantic.Field(None, description='The number of records rejected due to internal failures in a PutRecords operation per Kinesis data stream, measured over the specified time period.\nOccasional internal failures are to be expected and should be retried.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_records_latency: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordsLatencyParams]] = pydantic.Field(None, description='The time taken per PutRecords operation, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_records_success: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordsSuccessParams]] = pydantic.Field(None, description='The number of PutRecords operations where at least one record succeeded, per Kinesis stream, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_records_successful_records: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordsSuccessfulRecordsParams]] = pydantic.Field(None, description='The number of successful records in a PutRecords operation per Kinesis data stream, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_records_throttled_records: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordsThrottledRecordsParams]] = pydantic.Field(None, description='The number of records rejected due to throttling in a PutRecords operation per Kinesis data stream, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_put_records_total_records: typing.Optional[list[AwsKinesisIStreamDefMetricPutRecordsTotalRecordsParams]] = pydantic.Field(None, description='The total number of records sent in a PutRecords operation per Kinesis data stream, measured over the specified time period.\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')
    metric_read_provisioned_throughput_exceeded: typing.Optional[list[AwsKinesisIStreamDefMetricReadProvisionedThroughputExceededParams]] = pydantic.Field(None, description='The number of GetRecords calls throttled for the stream over the specified time period.\nThe most commonly used\nstatistic for this metric is Average.\n\nWhen the Minimum statistic has a value of 1, all records were throttled for the stream during the specified time\nperiod.\n\nWhen the Maximum statistic has a value of 0 (zero), no records were throttled for the stream during the specified\ntime period.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties')
    metric_write_provisioned_throughput_exceeded: typing.Optional[list[AwsKinesisIStreamDefMetricWriteProvisionedThroughputExceededParams]] = pydantic.Field(None, description='The number of records rejected due to throttling for the stream over the specified time period.\nThis metric\nincludes throttling from PutRecord and PutRecords operations.\n\nWhen the Minimum statistic has a non-zero value, records were being throttled for the stream during the specified\ntime period.\n\nWhen the Maximum statistic has a value of 0 (zero), no records were being throttled for the stream during the\nspecified time period.\n\nThe metric defaults to average over 5 minutes, it can be changed by passing ``statistic`` and ``period`` properties.')


class AwsKinesisIStreamDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsKinesisIStreamDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefGrantReadWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='name of the stream metric.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricGetRecordsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricGetRecordsBytesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricGetRecordsIteratorAgeMillisecondsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricGetRecordsLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricGetRecordsSuccessParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricIncomingBytesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricIncomingRecordsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordBytesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordSuccessParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordsBytesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordsFailedRecordsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordsLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordsSuccessParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordsSuccessfulRecordsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordsThrottledRecordsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricPutRecordsTotalRecordsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricReadProvisionedThroughputExceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKinesisIStreamDefMetricWriteProvisionedThroughputExceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsKmsIAliasDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[AwsKmsIAliasDefAddAliasParams]] = pydantic.Field(None, description='Defines a new alias for the key.')
    add_to_resource_policy: typing.Optional[list[AwsKmsIAliasDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the KMS key resource policy.')
    apply_removal_policy: typing.Optional[list[AwsKmsIAliasDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsKmsIAliasDefGrantParams]] = pydantic.Field(None, description='Grant the indicated permissions on this key to the given principal.')
    grant_decrypt: typing.Optional[list[AwsKmsIAliasDefGrantDecryptParams]] = pydantic.Field(None, description='Grant decryption permissions using this key to the given principal.')
    grant_encrypt: typing.Optional[list[AwsKmsIAliasDefGrantEncryptParams]] = pydantic.Field(None, description='Grant encryption permissions using this key to the given principal.')
    grant_encrypt_decrypt: typing.Optional[list[AwsKmsIAliasDefGrantEncryptDecryptParams]] = pydantic.Field(None, description='Grant encryption and decryption permissions using this key to the given principal.')
    grant_generate_mac: typing.Optional[list[AwsKmsIAliasDefGrantGenerateMacParams]] = pydantic.Field(None, description='Grant permissions to generating MACs to the given principal.')
    grant_verify_mac: typing.Optional[list[AwsKmsIAliasDefGrantVerifyMacParams]] = pydantic.Field(None, description='Grant permissions to verifying MACs to the given principal.')


class AwsKmsIAliasDefAddAliasParams(pydantic.BaseModel):
    alias: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_kms.AliasDefConfig]] = pydantic.Field(None)

class AwsKmsIAliasDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='The policy statement to add.\n')
    allow_no_op: typing.Optional[bool] = pydantic.Field(None, description='If this is set to ``false`` and there is no policy defined (i.e. external key), the operation will fail. Otherwise, it will no-op.')

class AwsKmsIAliasDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsKmsIAliasDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIAliasDefGrantDecryptParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIAliasDefGrantEncryptParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIAliasDefGrantEncryptDecryptParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIAliasDefGrantGenerateMacParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIAliasDefGrantVerifyMacParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIKeyDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[AwsKmsIKeyDefAddAliasParams]] = pydantic.Field(None, description='Defines a new alias for the key.')
    add_to_resource_policy: typing.Optional[list[AwsKmsIKeyDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the KMS key resource policy.')
    apply_removal_policy: typing.Optional[list[AwsKmsIKeyDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsKmsIKeyDefGrantParams]] = pydantic.Field(None, description='Grant the indicated permissions on this key to the given principal.')
    grant_decrypt: typing.Optional[list[AwsKmsIKeyDefGrantDecryptParams]] = pydantic.Field(None, description='Grant decryption permissions using this key to the given principal.')
    grant_encrypt: typing.Optional[list[AwsKmsIKeyDefGrantEncryptParams]] = pydantic.Field(None, description='Grant encryption permissions using this key to the given principal.')
    grant_encrypt_decrypt: typing.Optional[list[AwsKmsIKeyDefGrantEncryptDecryptParams]] = pydantic.Field(None, description='Grant encryption and decryption permissions using this key to the given principal.')
    grant_generate_mac: typing.Optional[list[AwsKmsIKeyDefGrantGenerateMacParams]] = pydantic.Field(None, description='Grant permissions to generating MACs to the given principal.')
    grant_verify_mac: typing.Optional[list[AwsKmsIKeyDefGrantVerifyMacParams]] = pydantic.Field(None, description='Grant permissions to verifying MACs to the given principal.')


class AwsKmsIKeyDefAddAliasParams(pydantic.BaseModel):
    alias: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_kms.AliasDefConfig]] = pydantic.Field(None)

class AwsKmsIKeyDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='The policy statement to add.\n')
    allow_no_op: typing.Optional[bool] = pydantic.Field(None, description='If this is set to ``false`` and there is no policy defined (i.e. external key), the operation will fail. Otherwise, it will no-op.')

class AwsKmsIKeyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsKmsIKeyDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIKeyDefGrantDecryptParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIKeyDefGrantEncryptParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIKeyDefGrantEncryptDecryptParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIKeyDefGrantGenerateMacParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsKmsIKeyDefGrantVerifyMacParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefConfig(pydantic.BaseModel):
    add_event_source: typing.Optional[list[AwsLambdaIAliasDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the @aws-cdk/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from '@aws-cdk/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[AwsLambdaIAliasDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[AwsLambdaIAliasDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[AwsLambdaIAliasDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[AwsLambdaIAliasDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[AwsLambdaIAliasDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    configure_async_invoke: typing.Optional[list[AwsLambdaIAliasDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    grant_invoke: typing.Optional[list[AwsLambdaIAliasDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_url: typing.Optional[list[AwsLambdaIAliasDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    metric: typing.Optional[list[AwsLambdaIAliasDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Lambda Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[AwsLambdaIAliasDefMetricDurationParams]] = pydantic.Field(None, description='Metric for the Duration of this Lambda How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[AwsLambdaIAliasDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[AwsLambdaIAliasDefMetricInvocationsParams]] = pydantic.Field(None, description='Metric for the number of invocations of this Lambda How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[AwsLambdaIAliasDefMetricThrottlesParams]] = pydantic.Field(None, description='Metric for the number of throttled invocations of this Lambda How often this Lambda is throttled.\nSum over 5 minutes')


class AwsLambdaIAliasDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')

class AwsLambdaIAliasDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='construct ID.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')

class AwsLambdaIAliasDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsLambdaIAliasDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLambdaIAliasDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')

class AwsLambdaIAliasDefGrantInvokeParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefGrantInvokeUrlParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIAliasDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaICodeSigningConfigDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsLambdaICodeSigningConfigDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsLambdaICodeSigningConfigDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLambdaIDestinationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsLambdaIDestinationDefBindParams]] = pydantic.Field(None, description='Binds this destination to the Lambda function.')


class AwsLambdaIDestinationDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    fn: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-\n')
    type: aws_cdk.aws_lambda.DestinationType = pydantic.Field(..., description='The destination type.')

class AwsLambdaIEventSourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsLambdaIEventSourceDefBindParams]] = pydantic.Field(None, description='Called by ``lambda.addEventSource`` to allow the event source to bind to this function.')


class AwsLambdaIEventSourceDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='That lambda function to bind to.')

class AwsLambdaIEventSourceDlqDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsLambdaIEventSourceDlqDefBindParams]] = pydantic.Field(None, description='Returns the DLQ destination config of the DLQ.')


class AwsLambdaIEventSourceDlqDefBindParams(pydantic.BaseModel):
    target: typing.Union[models.aws_lambda.EventSourceMappingDef] = pydantic.Field(..., description='-\n')
    target_handler: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='-')

class AwsLambdaIEventSourceMappingDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsLambdaIEventSourceMappingDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsLambdaIEventSourceMappingDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLambdaIFunctionDefConfig(pydantic.BaseModel):
    add_event_source: typing.Optional[list[AwsLambdaIFunctionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the @aws-cdk/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from '@aws-cdk/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[AwsLambdaIFunctionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[AwsLambdaIFunctionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[AwsLambdaIFunctionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[AwsLambdaIFunctionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[AwsLambdaIFunctionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    configure_async_invoke: typing.Optional[list[AwsLambdaIFunctionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    grant_invoke: typing.Optional[list[AwsLambdaIFunctionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_url: typing.Optional[list[AwsLambdaIFunctionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    metric: typing.Optional[list[AwsLambdaIFunctionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Lambda Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[AwsLambdaIFunctionDefMetricDurationParams]] = pydantic.Field(None, description='Metric for the Duration of this Lambda How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[AwsLambdaIFunctionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[AwsLambdaIFunctionDefMetricInvocationsParams]] = pydantic.Field(None, description='Metric for the number of invocations of this Lambda How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[AwsLambdaIFunctionDefMetricThrottlesParams]] = pydantic.Field(None, description='Metric for the number of throttled invocations of this Lambda How often this Lambda is throttled.\nSum over 5 minutes')


class AwsLambdaIFunctionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')

class AwsLambdaIFunctionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='construct ID.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')

class AwsLambdaIFunctionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsLambdaIFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLambdaIFunctionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')

class AwsLambdaIFunctionDefGrantInvokeParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefGrantInvokeUrlParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIFunctionUrlDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsLambdaIFunctionUrlDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_invoke_url: typing.Optional[list[AwsLambdaIFunctionUrlDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')


class AwsLambdaIFunctionUrlDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLambdaIFunctionUrlDefGrantInvokeUrlParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaILayerVersionDefConfig(pydantic.BaseModel):
    add_permission: typing.Optional[list[AwsLambdaILayerVersionDefAddPermissionParams]] = pydantic.Field(None, description='Add permission for this layer version to specific entities.\nUsage within\nthe same account where the layer is defined is always allowed and does not\nrequire calling this method. Note that the principal that creates the\nLambda function using the layer (for example, a CloudFormation changeset\nexecution role) also needs to have the ``lambda:GetLayerVersion``\npermission on the layer version.')
    apply_removal_policy: typing.Optional[list[AwsLambdaILayerVersionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsLambdaILayerVersionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='the ID of the grant in the construct tree.\n')
    account_id: str = pydantic.Field(..., description='The AWS Account id of the account that is authorized to use a Lambda Layer Version. The wild-card ``\'*\'`` can be used to grant access to "any" account (or any account in an organization when ``organizationId`` is specified).\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description="The ID of the AWS Organization to which the grant is restricted. Can only be specified if ``accountId`` is ``'*'``")

class AwsLambdaILayerVersionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLambdaIScalableFunctionAttributeDefConfig(pydantic.BaseModel):
    scale_on_schedule: typing.Optional[list[AwsLambdaIScalableFunctionAttributeDefScaleOnScheduleParams]] = pydantic.Field(None, description='Scale out or in based on schedule.')
    scale_on_utilization: typing.Optional[list[AwsLambdaIScalableFunctionAttributeDefScaleOnUtilizationParams]] = pydantic.Field(None, description='Scale out or in to keep utilization at a given level.\nThe utilization is tracked by the\nLambdaProvisionedConcurrencyUtilization metric, emitted by lambda. See:\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-metrics.html#monitoring-metrics-concurrency')


class AwsLambdaIScalableFunctionAttributeDefScaleOnScheduleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    schedule: models.aws_applicationautoscaling.ScheduleDef = pydantic.Field(..., description='When to perform this action.\n')
    end_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action expires. Default: The rule never expires.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new maximum capacity. During the scheduled time, the current capacity is above the maximum capacity, Application Auto Scaling scales in to the maximum capacity. At least one of maxCapacity and minCapacity must be supplied. Default: No new maximum capacity\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new minimum capacity. During the scheduled time, if the current capacity is below the minimum capacity, Application Auto Scaling scales out to the minimum capacity. At least one of maxCapacity and minCapacity must be supplied. Default: No new minimum capacity\n')
    start_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action becomes active. Default: The rule is activate immediately')

class AwsLambdaIScalableFunctionAttributeDefScaleOnUtilizationParams(pydantic.BaseModel):
    utilization_target: typing.Union[int, float] = pydantic.Field(..., description='Utilization target for the attribute. For example, .5 indicates that 50 percent of allocated provisioned concurrency is in use.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency')

class AwsLambdaIVersionDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[AwsLambdaIVersionDefAddAliasParams]] = pydantic.Field(None, description='(deprecated) Defines an alias for this version.')
    add_event_source: typing.Optional[list[AwsLambdaIVersionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the @aws-cdk/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from '@aws-cdk/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[AwsLambdaIVersionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[AwsLambdaIVersionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[AwsLambdaIVersionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[AwsLambdaIVersionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[AwsLambdaIVersionDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    configure_async_invoke: typing.Optional[list[AwsLambdaIVersionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    grant_invoke: typing.Optional[list[AwsLambdaIVersionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_url: typing.Optional[list[AwsLambdaIVersionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    metric: typing.Optional[list[AwsLambdaIVersionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Lambda Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[AwsLambdaIVersionDefMetricDurationParams]] = pydantic.Field(None, description='Metric for the Duration of this Lambda How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[AwsLambdaIVersionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[AwsLambdaIVersionDefMetricInvocationsParams]] = pydantic.Field(None, description='Metric for the number of invocations of this Lambda How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[AwsLambdaIVersionDefMetricThrottlesParams]] = pydantic.Field(None, description='Metric for the number of throttled invocations of this Lambda How often this Lambda is throttled.\nSum over 5 minutes')


class AwsLambdaIVersionDefAddAliasParams(pydantic.BaseModel):
    alias_name: str = pydantic.Field(..., description='The name of the alias.\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n\n:deprecated: Calling ``addAlias`` on a ``Version`` object will cause the Alias to be replaced on every function update. Call ``function.addAlias()`` or ``new Alias()`` instead.\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_lambda.AliasDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')

class AwsLambdaIVersionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='construct ID.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')

class AwsLambdaIVersionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsLambdaIVersionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLambdaIVersionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')

class AwsLambdaIVersionDefGrantInvokeParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefGrantInvokeUrlParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaIVersionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLambdaNodejsICommandHooksDefConfig(pydantic.BaseModel):
    after_bundling: typing.Optional[list[AwsLambdaNodejsICommandHooksDefAfterBundlingParams]] = pydantic.Field(None, description='Returns commands to run after bundling.\nCommands are chained with ``&&``.')
    before_bundling: typing.Optional[list[AwsLambdaNodejsICommandHooksDefBeforeBundlingParams]] = pydantic.Field(None, description='Returns commands to run before bundling.\nCommands are chained with ``&&``.')
    before_install: typing.Optional[list[AwsLambdaNodejsICommandHooksDefBeforeInstallParams]] = pydantic.Field(None, description='Returns commands to run before installing node modules.\nThis hook only runs when node modules are installed.\n\nCommands are chained with ``&&``.')


class AwsLambdaNodejsICommandHooksDefAfterBundlingParams(pydantic.BaseModel):
    input_dir: str = pydantic.Field(..., description='-\n')
    output_dir: str = pydantic.Field(..., description='-')

class AwsLambdaNodejsICommandHooksDefBeforeBundlingParams(pydantic.BaseModel):
    input_dir: str = pydantic.Field(..., description='-\n')
    output_dir: str = pydantic.Field(..., description='-')

class AwsLambdaNodejsICommandHooksDefBeforeInstallParams(pydantic.BaseModel):
    input_dir: str = pydantic.Field(..., description='-\n')
    output_dir: str = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_logs.IFilterPattern skipped


class AwsLogsILogGroupDefConfig(pydantic.BaseModel):
    add_metric_filter: typing.Optional[list[AwsLogsILogGroupDefAddMetricFilterParams]] = pydantic.Field(None, description='Create a new Metric Filter on this Log Group.')
    add_stream: typing.Optional[list[AwsLogsILogGroupDefAddStreamParams]] = pydantic.Field(None, description='Create a new Log Stream for this Log Group.')
    add_subscription_filter: typing.Optional[list[AwsLogsILogGroupDefAddSubscriptionFilterParams]] = pydantic.Field(None, description='Create a new Subscription Filter on this Log Group.')
    add_to_resource_policy: typing.Optional[list[AwsLogsILogGroupDefAddToResourcePolicyParams]] = pydantic.Field(None, description="Add a statement to the resource's resource policy.")
    apply_removal_policy: typing.Optional[list[AwsLogsILogGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    extract_metric: typing.Optional[list[AwsLogsILogGroupDefExtractMetricParams]] = pydantic.Field(None, description='Extract a metric from structured log events in the LogGroup.\nCreates a MetricFilter on this LogGroup that will extract the value\nof the indicated JSON field in all records where it occurs.\n\nThe metric will be available in CloudWatch Metrics under the\nindicated namespace and name.')
    grant: typing.Optional[list[AwsLogsILogGroupDefGrantParams]] = pydantic.Field(None, description='Give the indicated permissions on this log group and all streams.')
    grant_read: typing.Optional[list[AwsLogsILogGroupDefGrantReadParams]] = pydantic.Field(None, description='Give permissions to read from this log group and streams.')
    grant_write: typing.Optional[list[AwsLogsILogGroupDefGrantWriteParams]] = pydantic.Field(None, description='Give permissions to write to create and write to streams in this log group.')
    log_group_physical_name: typing.Optional[bool] = pydantic.Field(None, description='Public method to get the physical name of this log group.')


class AwsLogsILogGroupDefAddMetricFilterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Unique identifier for the construct in its parent.\n')
    filter_pattern: typing.Union[models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(..., description='Pattern to search for log events.\n')
    metric_name: str = pydantic.Field(..., description='The name of the metric to emit.\n')
    metric_namespace: str = pydantic.Field(..., description='The namespace of the metric to emit.\n')
    default_value: typing.Union[int, float, None] = pydantic.Field(None, description='The value to emit if the pattern does not match a particular event. Default: No metric emitted.\n')
    dimensions: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The fields to use as dimensions for the metric. One metric filter can include as many as three dimensions. Default: - No dimensions attached to metrics.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the metric filter. Default: - Cloudformation generated name.\n')
    metric_value: typing.Optional[str] = pydantic.Field(None, description='The value to emit for the metric. Can either be a literal number (typically "1"), or the name of a field in the structure to take the value from the matched event. If you are using a field value, the field value must have been matched using the pattern. If you want to specify a field from a matched JSON structure, use \'$.fieldName\', and make sure the field is in the pattern (if only as \'$.fieldName = *\'). If you want to specify a field from a matched space-delimited structure, use \'$fieldName\'. Default: "1"\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='The unit to assign to the metric. Default: - No unit attached to metrics.')
    return_config: typing.Optional[list[models.aws_logs.MetricFilterDefConfig]] = pydantic.Field(None)

class AwsLogsILogGroupDefAddStreamParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Unique identifier for the construct in its parent.\n')
    log_stream_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log stream to create. The name must be unique within the log group. Default: Automatically generated')
    return_config: typing.Optional[list[models.aws_logs.LogStreamDefConfig]] = pydantic.Field(None)

class AwsLogsILogGroupDefAddSubscriptionFilterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Unique identifier for the construct in its parent.\n')
    destination: typing.Union[models.aws_logs.CrossAccountDestinationDef, models.aws_logs_destinations.KinesisDestinationDef, models.aws_logs_destinations.LambdaDestinationDef] = pydantic.Field(..., description='The destination to send the filtered events to. For example, a Kinesis stream or a Lambda function.\n')
    filter_pattern: typing.Union[models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(..., description='Log events matching this pattern will be sent to the destination.')
    return_config: typing.Optional[list[models.aws_logs.SubscriptionFilterDefConfig]] = pydantic.Field(None)

class AwsLogsILogGroupDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsLogsILogGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLogsILogGroupDefExtractMetricParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description="JSON field to extract (example: '$.myfield').\n")
    metric_namespace: str = pydantic.Field(..., description='Namespace to emit the metric under.\n')
    metric_name: str = pydantic.Field(..., description='Name to emit the metric under.\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsLogsILogGroupDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLogsILogGroupDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLogsILogGroupDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsLogsILogStreamDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsLogsILogStreamDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsLogsILogStreamDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsLogsILogSubscriptionDestinationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsLogsILogSubscriptionDestinationDefBindParams]] = pydantic.Field(None, description='Return the properties required to send subscription events to this destination.\nIf necessary, the destination can use the properties of the SubscriptionFilter\nobject itself to configure its permissions to allow the subscription to write\nto it.\n\nThe destination may reconfigure its own permissions in response to this\nfunction call.')


class AwsLogsILogSubscriptionDestinationDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    source_log_group: typing.Union[models.aws_logs.LogGroupDef] = pydantic.Field(..., description='-')

class AwsOpensearchserviceIDomainDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsOpensearchserviceIDomainDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_index_read: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantIndexReadParams]] = pydantic.Field(None, description='Grant read permissions for an index in this domain to an IAM principal (Role/Group/User).')
    grant_index_read_write: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantIndexReadWriteParams]] = pydantic.Field(None, description='Grant read/write permissions for an index in this domain to an IAM principal (Role/Group/User).')
    grant_index_write: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantIndexWriteParams]] = pydantic.Field(None, description='Grant write permissions for an index in this domain to an IAM principal (Role/Group/User).')
    grant_path_read: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantPathReadParams]] = pydantic.Field(None, description='Grant read permissions for a specific path in this domain to an IAM principal (Role/Group/User).')
    grant_path_read_write: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantPathReadWriteParams]] = pydantic.Field(None, description='Grant read/write permissions for a specific path in this domain to an IAM principal (Role/Group/User).')
    grant_path_write: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantPathWriteParams]] = pydantic.Field(None, description='Grant write permissions for a specific path in this domain to an IAM principal (Role/Group/User).')
    grant_read: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantReadParams]] = pydantic.Field(None, description='Grant read permissions for this domain and its contents to an IAM principal (Role/Group/User).')
    grant_read_write: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantReadWriteParams]] = pydantic.Field(None, description='Grant read/write permissions for this domain and its contents to an IAM principal (Role/Group/User).')
    grant_write: typing.Optional[list[AwsOpensearchserviceIDomainDefGrantWriteParams]] = pydantic.Field(None, description='Grant write permissions for this domain and its contents to an IAM principal (Role/Group/User).')
    metric: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this domain.')
    metric_automated_snapshot_failure: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricAutomatedSnapshotFailureParams]] = pydantic.Field(None, description='Metric for automated snapshot failures.')
    metric_cluster_index_writes_blocked: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricClusterIndexWritesBlockedParams]] = pydantic.Field(None, description='Metric for the cluster blocking index writes.')
    metric_cluster_status_red: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricClusterStatusRedParams]] = pydantic.Field(None, description='Metric for the time the cluster status is red.')
    metric_cluster_status_yellow: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricClusterStatusYellowParams]] = pydantic.Field(None, description='Metric for the time the cluster status is yellow.')
    metric_cpu_utilization: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='Metric for CPU utilization.')
    metric_free_storage_space: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricFreeStorageSpaceParams]] = pydantic.Field(None, description='Metric for the storage space of nodes in the cluster.')
    metric_indexing_latency: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricIndexingLatencyParams]] = pydantic.Field(None, description='Metric for indexing latency.')
    metric_jvm_memory_pressure: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricJvmMemoryPressureParams]] = pydantic.Field(None, description='Metric for JVM memory pressure.')
    metric_kms_key_error: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricKmsKeyErrorParams]] = pydantic.Field(None, description='Metric for KMS key errors.')
    metric_kms_key_inaccessible: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricKmsKeyInaccessibleParams]] = pydantic.Field(None, description='Metric for KMS key being inaccessible.')
    metric_master_cpu_utilization: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricMasterCpuUtilizationParams]] = pydantic.Field(None, description='Metric for master CPU utilization.')
    metric_master_jvm_memory_pressure: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricMasterJvmMemoryPressureParams]] = pydantic.Field(None, description='Metric for master JVM memory pressure.')
    metric_nodes: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricNodesParams]] = pydantic.Field(None, description='Metric for the number of nodes.')
    metric_search_latency: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricSearchLatencyParams]] = pydantic.Field(None, description='Metric for search latency.')
    metric_searchable_documents: typing.Optional[list[AwsOpensearchserviceIDomainDefMetricSearchableDocumentsParams]] = pydantic.Field(None, description='Metric for number of searchable documents.')


class AwsOpensearchserviceIDomainDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsOpensearchserviceIDomainDefGrantIndexReadParams(pydantic.BaseModel):
    index: str = pydantic.Field(..., description='The index to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantIndexReadWriteParams(pydantic.BaseModel):
    index: str = pydantic.Field(..., description='The index to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantIndexWriteParams(pydantic.BaseModel):
    index: str = pydantic.Field(..., description='The index to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantPathReadParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantPathReadWriteParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantPathWriteParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to grant permissions for.\n')
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantReadParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantReadWriteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefGrantWriteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricAutomatedSnapshotFailureParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricClusterIndexWritesBlockedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 1 minute\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricClusterStatusRedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricClusterStatusYellowParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricFreeStorageSpaceParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: minimum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricIndexingLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: p99 over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricJvmMemoryPressureParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricKmsKeyErrorParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricKmsKeyInaccessibleParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricMasterCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricMasterJvmMemoryPressureParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricNodesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: minimum over 1 hour\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricSearchLatencyParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: p99 over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsOpensearchserviceIDomainDefMetricSearchableDocumentsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: maximum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIAuroraClusterInstanceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRdsIAuroraClusterInstanceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsRdsIAuroraClusterInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRdsIClusterEngineDefConfig(pydantic.BaseModel):
    bind_to_cluster: typing.Optional[list[AwsRdsIClusterEngineDefBindToClusterParams]] = pydantic.Field(None, description='Method called when the engine is used to create a new cluster.')


class AwsRdsIClusterEngineDefBindToClusterParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The customer-provided ParameterGroup. Default: - none\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 exporting. Default: - none\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 importing. Default: - none')

class AwsRdsIClusterInstanceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsRdsIClusterInstanceDefBindParams]] = pydantic.Field(None, description='Create the database instance within the provided cluster.')


class AwsRdsIClusterInstanceDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    cluster: typing.Union[models.aws_rds.DatabaseClusterBaseDef, models.aws_rds.DatabaseClusterDef, models.aws_rds.DatabaseClusterFromSnapshotDef] = pydantic.Field(..., description='-\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instances. Default: no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instances monitoring. Default: - A role is automatically created for you\n')
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description='The promotion tier of the cluster instance. This matters more for serverlessV2 instances. If a serverless instance is in tier 0-1 then it will scale with the writer. For provisioned instances this just determines the failover priority. If multiple instances have the same priority then one will be picked at random Default: 2\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy on the cluster. Default: - RemovalPolicy.DESTROY (cluster snapshot can restore)\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. This is only needed when using the isFromLegacyInstanceProps Default: - cluster subnet group is used')
    return_config: typing.Optional[list[models._interface_methods.AwsRdsIAuroraClusterInstanceDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[AwsRdsIDatabaseClusterDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this cluster.')
    apply_removal_policy: typing.Optional[list[AwsRdsIDatabaseClusterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    metric: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBCluster.')
    metric_cpu_utilization: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_deadlocks: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricDeadlocksParams]] = pydantic.Field(None, description='The average number of deadlocks in the database per second.\nAverage over 5 minutes')
    metric_engine_uptime: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricEngineUptimeParams]] = pydantic.Field(None, description='The amount of time that the instance has been running, in seconds.\nAverage over 5 minutes')
    metric_free_local_storage: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricFreeLocalStorageParams]] = pydantic.Field(None, description='The amount of local storage available, in bytes.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory, in bytes.\nAverage over 5 minutes')
    metric_network_receive_throughput: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricNetworkReceiveThroughputParams]] = pydantic.Field(None, description='The amount of network throughput received from clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_throughput: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricNetworkThroughputParams]] = pydantic.Field(None, description='The amount of network throughput both received from and transmitted to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_transmit_throughput: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricNetworkTransmitThroughputParams]] = pydantic.Field(None, description='The amount of network throughput sent to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_snapshot_storage_used: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricSnapshotStorageUsedParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes consumed by all Aurora snapshots outside its backup retention window.\nAverage over 5 minutes')
    metric_total_backup_storage_billed: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricTotalBackupStorageBilledParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes for which you are billed.\nAverage over 5 minutes')
    metric_volume_bytes_used: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricVolumeBytesUsedParams]] = pydantic.Field(None, description='The amount of storage used by your Aurora DB instance, in bytes.\nAverage over 5 minutes')
    metric_volume_read_io_ps: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricVolumeReadIoPsParams]] = pydantic.Field(None, description='The number of billed read I/O operations from a cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')
    metric_volume_write_io_ps: typing.Optional[list[AwsRdsIDatabaseClusterDefMetricVolumeWriteIoPsParams]] = pydantic.Field(None, description='The number of write disk I/O operations to the cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')


class AwsRdsIDatabaseClusterDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRdsIDatabaseClusterDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricDeadlocksParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricEngineUptimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricFreeLocalStorageParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricNetworkReceiveThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricNetworkThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricNetworkTransmitThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricSnapshotStorageUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricTotalBackupStorageBilledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricVolumeBytesUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricVolumeReadIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseClusterDefMetricVolumeWriteIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[AwsRdsIDatabaseInstanceDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this instance.')
    apply_removal_policy: typing.Optional[list[AwsRdsIDatabaseInstanceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_connect: typing.Optional[list[AwsRdsIDatabaseInstanceDefGrantConnectParams]] = pydantic.Field(None, description='Grant the given identity connection access to the database.')
    metric: typing.Optional[list[AwsRdsIDatabaseInstanceDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBInstance.')
    metric_cpu_utilization: typing.Optional[list[AwsRdsIDatabaseInstanceDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[AwsRdsIDatabaseInstanceDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_free_storage_space: typing.Optional[list[AwsRdsIDatabaseInstanceDefMetricFreeStorageSpaceParams]] = pydantic.Field(None, description='The amount of available storage space.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[AwsRdsIDatabaseInstanceDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory.\nAverage over 5 minutes')
    metric_read_iops: typing.Optional[list[AwsRdsIDatabaseInstanceDefMetricReadIopsParams]] = pydantic.Field(None, description='The average number of disk write I/O operations per second.\nAverage over 5 minutes')
    metric_write_iops: typing.Optional[list[AwsRdsIDatabaseInstanceDefMetricWriteIopsParams]] = pydantic.Field(None, description='The average number of disk read I/O operations per second.\nAverage over 5 minutes')
    on_event: typing.Optional[list[AwsRdsIDatabaseInstanceDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers for instance events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')


class AwsRdsIDatabaseInstanceDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRdsIDatabaseInstanceDefGrantConnectParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the Principal to grant the permissions to.\n')
    db_user: typing.Optional[str] = pydantic.Field(None, description='the name of the database user to allow connecting as to the db instance.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefMetricFreeStorageSpaceParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefMetricReadIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefMetricWriteIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseInstanceDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsRdsIDatabaseProxyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRdsIDatabaseProxyDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_connect: typing.Optional[list[AwsRdsIDatabaseProxyDefGrantConnectParams]] = pydantic.Field(None, description='Grant the given identity connection access to the proxy.')


class AwsRdsIDatabaseProxyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRdsIDatabaseProxyDefGrantConnectParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the Principal to grant the permissions to.\n')
    db_user: typing.Optional[str] = pydantic.Field(None, description='the name of the database user to allow connecting as to the proxy.\n\n:default:\n\n- if the Proxy had been provided a single Secret value,\nthe user will be taken from that Secret\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
#  aws-cdk-lib.aws_rds.IEngine skipped


class AwsRdsIInstanceEngineDefConfig(pydantic.BaseModel):
    bind_to_instance: typing.Optional[list[AwsRdsIInstanceEngineDefBindToInstanceParams]] = pydantic.Field(None, description='Method called when the engine is used to create a new instance.')


class AwsRdsIInstanceEngineDefBindToInstanceParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    domain: typing.Optional[str] = pydantic.Field(None, description="The Active Directory directory ID to create the DB instance in. Default: - none (it's an optional field)\n")
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group of the database. Default: - none\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 exporting. Default: - none\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 importing. Default: - none\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description="The timezone of the database, set by the customer. Default: - none (it's an optional field)")

class AwsRdsIOptionGroupDefConfig(pydantic.BaseModel):
    add_configuration: typing.Optional[list[AwsRdsIOptionGroupDefAddConfigurationParams]] = pydantic.Field(None, description='Adds a configuration to this OptionGroup.\nThis method is a no-op for an imported OptionGroup.')
    apply_removal_policy: typing.Optional[list[AwsRdsIOptionGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsRdsIOptionGroupDefAddConfigurationParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='The name of the option.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number that this option uses. If ``port`` is specified then ``vpc`` must also be specified. Default: - no port\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Optional list of security groups to use for this option, if ``vpc`` is specified. If no groups are provided, a default one will be created. Default: - a default group will be created if ``port`` or ``vpc`` are specified.\n')
    settings: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The settings for the option. Default: - no settings\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version for the option. Default: - no version\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where a security group should be created for this option. If ``vpc`` is specified then ``port`` must also be specified. Default: - no VPC\n')

class AwsRdsIOptionGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRdsIParameterGroupDefConfig(pydantic.BaseModel):
    add_parameter: typing.Optional[list[AwsRdsIParameterGroupDefAddParameterParams]] = pydantic.Field(None, description='Adds a parameter to this group.\nIf this is an imported parameter group,\nthis method does nothing.')
    apply_removal_policy: typing.Optional[list[AwsRdsIParameterGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    bind_to_cluster: typing.Optional[bool] = pydantic.Field(None, description='Method called when this Parameter Group is used when defining a database cluster.')
    bind_to_instance: typing.Optional[bool] = pydantic.Field(None, description='Method called when this Parameter Group is used when defining a database instance.')


class AwsRdsIParameterGroupDefAddParameterParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: str = pydantic.Field(..., description='-\n')

class AwsRdsIParameterGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRdsIServerlessClusterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRdsIServerlessClusterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_data_api_access: typing.Optional[list[AwsRdsIServerlessClusterDefGrantDataApiAccessParams]] = pydantic.Field(None, description='Grant the given identity to access to the Data API.')


class AwsRdsIServerlessClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRdsIServerlessClusterDefGrantDataApiAccessParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsRdsISubnetGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRdsISubnetGroupDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsRdsISubnetGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRoute53IAliasRecordTargetDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsRoute53IAliasRecordTargetDefBindParams]] = pydantic.Field(None, description='Return hosted zone ID and DNS name, usable for Route53 alias targets.')


class AwsRoute53IAliasRecordTargetDefBindParams(pydantic.BaseModel):
    record: typing.Union[models.aws_route53.AaaaRecordDef, models.aws_route53.ARecordDef, models.aws_route53.CaaAmazonRecordDef, models.aws_route53.CaaRecordDef, models.aws_route53.CnameRecordDef, models.aws_route53.DsRecordDef, models.aws_route53.MxRecordDef, models.aws_route53.NsRecordDef, models.aws_route53.RecordSetDef, models.aws_route53.SrvRecordDef, models.aws_route53.TxtRecordDef, models.aws_route53.ZoneDelegationRecordDef] = pydantic.Field(..., description='-\n')
    zone: typing.Optional[typing.Union[models.aws_route53.HostedZoneDef, models.aws_route53.PrivateHostedZoneDef, models.aws_route53.PublicHostedZoneDef]] = pydantic.Field(None, description='-')

class AwsRoute53IHostedZoneDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRoute53IHostedZoneDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsRoute53IHostedZoneDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRoute53IPrivateHostedZoneDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRoute53IPrivateHostedZoneDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsRoute53IPrivateHostedZoneDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRoute53IPublicHostedZoneDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRoute53IPublicHostedZoneDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsRoute53IPublicHostedZoneDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsRoute53IRecordSetDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsRoute53IRecordSetDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsRoute53IRecordSetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsS3IBucketDefConfig(pydantic.BaseModel):
    add_event_notification: typing.Optional[list[AwsS3IBucketDefAddEventNotificationParams]] = pydantic.Field(None, description='Adds a bucket notification event destination.')
    add_object_created_notification: typing.Optional[list[AwsS3IBucketDefAddObjectCreatedNotificationParams]] = pydantic.Field(None, description='Subscribes a destination to receive notifications when an object is created in the bucket.\nThis is identical to calling\n``onEvent(s3.EventType.OBJECT_CREATED)``.')
    add_object_removed_notification: typing.Optional[list[AwsS3IBucketDefAddObjectRemovedNotificationParams]] = pydantic.Field(None, description='Subscribes a destination to receive notifications when an object is removed from the bucket.\nThis is identical to calling\n``onEvent(EventType.OBJECT_REMOVED)``.')
    add_to_resource_policy: typing.Optional[list[AwsS3IBucketDefAddToResourcePolicyParams]] = pydantic.Field(None, description="Adds a statement to the resource policy for a principal (i.e. account/role/service) to perform actions on this bucket and/or its contents. Use ``bucketArn`` and ``arnForObjects(keys)`` to obtain ARNs for this bucket or objects.\nNote that the policy statement may or may not be added to the policy.\nFor example, when an ``IBucket`` is created from an existing bucket,\nit's not possible to tell whether the bucket already has a policy\nattached, let alone to re-use that policy to add more statements to it.\nSo it's safest to do nothing in these cases.")
    apply_removal_policy: typing.Optional[list[AwsS3IBucketDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    arn_for_objects: typing.Optional[list[AwsS3IBucketDefArnForObjectsParams]] = pydantic.Field(None, description='Returns an ARN that represents all objects within the bucket that match the key pattern specified.\nTo represent all keys, specify ``"*"``.')
    enable_event_bridge_notification: typing.Optional[bool] = pydantic.Field(None, description='Enables event bridge notification, causing all events below to be sent to EventBridge:.\n- Object Deleted (DeleteObject)\n- Object Deleted (Lifecycle expiration)\n- Object Restore Initiated\n- Object Restore Completed\n- Object Restore Expired\n- Object Storage Class Changed\n- Object Access Tier Changed\n- Object ACL Updated\n- Object Tags Added\n- Object Tags Deleted')
    grant_delete: typing.Optional[list[AwsS3IBucketDefGrantDeleteParams]] = pydantic.Field(None, description='Grants s3:DeleteObject* permission to an IAM principal for objects in this bucket.')
    grant_public_access: typing.Optional[list[AwsS3IBucketDefGrantPublicAccessParams]] = pydantic.Field(None, description='Allows unrestricted access to objects from this bucket.\nIMPORTANT: This permission allows anyone to perform actions on S3 objects\nin this bucket, which is useful for when you configure your bucket as a\nwebsite and want everyone to be able to read objects in the bucket without\nneeding to authenticate.\n\nWithout arguments, this method will grant read ("s3:GetObject") access to\nall objects ("*") in the bucket.\n\nThe method returns the ``iam.Grant`` object, which can then be modified\nas needed. For example, you can add a condition that will restrict access only\nto an IPv4 range like this::\n\n   const grant = bucket.grantPublicAccess();\n   grant.resourceStatement!.addCondition(‘IpAddress’, { “aws:SourceIp”: “54.240.143.0/24” });')
    grant_put: typing.Optional[list[AwsS3IBucketDefGrantPutParams]] = pydantic.Field(None, description='Grants s3:PutObject* and s3:Abort* permissions for this bucket to an IAM principal.\nIf encryption is used, permission to use the key to encrypt the contents\nof written files will also be granted to the same principal.')
    grant_put_acl: typing.Optional[list[AwsS3IBucketDefGrantPutAclParams]] = pydantic.Field(None, description="Grant the given IAM identity permissions to modify the ACLs of objects in the given Bucket.\nIf your application has the '@aws-cdk/aws-s3:grantWriteWithoutAcl' feature flag set,\ncalling ``grantWrite`` or ``grantReadWrite`` no longer grants permissions to modify the ACLs of the objects;\nin this case, if you need to modify object ACLs, call this method explicitly.")
    grant_read: typing.Optional[list[AwsS3IBucketDefGrantReadParams]] = pydantic.Field(None, description="Grant read permissions for this bucket and it's contents to an IAM principal (Role/Group/User).\nIf encryption is used, permission to use the key to decrypt the contents\nof the bucket will also be granted to the same principal.")
    grant_read_write: typing.Optional[list[AwsS3IBucketDefGrantReadWriteParams]] = pydantic.Field(None, description="Grants read/write permissions for this bucket and it's contents to an IAM principal (Role/Group/User).\nIf an encryption key is used, permission to use the key for\nencrypt/decrypt will also be granted.\n\nBefore CDK version 1.85.0, this method granted the ``s3:PutObject*`` permission that included ``s3:PutObjectAcl``,\nwhich could be used to grant read/write object access to IAM principals in other accounts.\nIf you want to get rid of that behavior, update your CDK version to 1.85.0 or later,\nand make sure the ``@aws-cdk/aws-s3:grantWriteWithoutAcl`` feature flag is set to ``true``\nin the ``context`` key of your cdk.json file.\nIf you've already updated, but still need the principal to have permissions to modify the ACLs,\nuse the ``grantPutAcl`` method.")
    grant_write: typing.Optional[list[AwsS3IBucketDefGrantWriteParams]] = pydantic.Field(None, description="Grant write permissions to this bucket to an IAM principal.\nIf encryption is used, permission to use the key to encrypt the contents\nof written files will also be granted to the same principal.\n\nBefore CDK version 1.85.0, this method granted the ``s3:PutObject*`` permission that included ``s3:PutObjectAcl``,\nwhich could be used to grant read/write object access to IAM principals in other accounts.\nIf you want to get rid of that behavior, update your CDK version to 1.85.0 or later,\nand make sure the ``@aws-cdk/aws-s3:grantWriteWithoutAcl`` feature flag is set to ``true``\nin the ``context`` key of your cdk.json file.\nIf you've already updated, but still need the principal to have permissions to modify the ACLs,\nuse the ``grantPutAcl`` method.")
    on_cloud_trail_event: typing.Optional[list[AwsS3IBucketDefOnCloudTrailEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event that triggers when something happens to this bucket.\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_cloud_trail_put_object: typing.Optional[list[AwsS3IBucketDefOnCloudTrailPutObjectParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event that triggers when an object is uploaded to the specified paths (keys) in this bucket using the PutObject API call.\nNote that some tools like ``aws s3 cp`` will automatically use either\nPutObject or the multipart upload API depending on the file size,\nso using ``onCloudTrailWriteObject`` may be preferable.\n\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_cloud_trail_write_object: typing.Optional[list[AwsS3IBucketDefOnCloudTrailWriteObjectParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event that triggers when an object at the specified paths (keys) in this bucket are written to.\nThis includes\nthe events PutObject, CopyObject, and CompleteMultipartUpload.\n\nNote that some tools like ``aws s3 cp`` will automatically use either\nPutObject or the multipart upload API depending on the file size,\nso using this method may be preferable to ``onCloudTrailPutObject``.\n\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    s3_url_for_object: typing.Optional[list[AwsS3IBucketDefS3UrlForObjectParams]] = pydantic.Field(None, description='The S3 URL of an S3 object.\nFor example:\n\n- ``s3://onlybucket``\n- ``s3://bucket/key``')
    transfer_acceleration_url_for_object: typing.Optional[list[AwsS3IBucketDefTransferAccelerationUrlForObjectParams]] = pydantic.Field(None, description='The https Transfer Acceleration URL of an S3 object.\nSpecify ``dualStack: true`` at the options\nfor dual-stack endpoint (connect to the bucket over IPv6). For example:\n\n- ``https://bucket.s3-accelerate.amazonaws.com``\n- ``https://bucket.s3-accelerate.amazonaws.com/key``')
    url_for_object: typing.Optional[list[AwsS3IBucketDefUrlForObjectParams]] = pydantic.Field(None, description='The https URL of an S3 object. For example:.\n- ``https://s3.us-west-1.amazonaws.com/onlybucket``\n- ``https://s3.us-west-1.amazonaws.com/bucket/key``\n- ``https://s3.cn-north-1.amazonaws.com.cn/china-bucket/mykey``')
    virtual_hosted_url_for_object: typing.Optional[list[AwsS3IBucketDefVirtualHostedUrlForObjectParams]] = pydantic.Field(None, description='The virtual hosted-style URL of an S3 object. Specify ``regional: false`` at the options for non-regional URL. For example:.\n- ``https://only-bucket.s3.us-west-1.amazonaws.com``\n- ``https://bucket.s3.us-west-1.amazonaws.com/key``\n- ``https://bucket.s3.amazonaws.com/key``\n- ``https://china-bucket.s3.cn-north-1.amazonaws.com.cn/mykey``')


class AwsS3IBucketDefAddEventNotificationParams(pydantic.BaseModel):
    event: aws_cdk.aws_s3.EventType = pydantic.Field(..., description='The event to trigger the notification.\n')
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (Lambda, SNS Topic or SQS Queue).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)

class AwsS3IBucketDefAddObjectCreatedNotificationParams(pydantic.BaseModel):
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (see onEvent).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)

class AwsS3IBucketDefAddObjectRemovedNotificationParams(pydantic.BaseModel):
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (see onEvent).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)

class AwsS3IBucketDefAddToResourcePolicyParams(pydantic.BaseModel):
    permission: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description="the policy statement to be added to the bucket's policy.\n")

class AwsS3IBucketDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsS3IBucketDefArnForObjectsParams(pydantic.BaseModel):
    key_pattern: str = pydantic.Field(..., description='-')

class AwsS3IBucketDefGrantDeleteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefGrantPublicAccessParams(pydantic.BaseModel):
    key_prefix: typing.Optional[str] = pydantic.Field(None, description='the prefix of S3 object keys (e.g. ``home/*``). Default is "*".\n')
    allowed_actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefGrantPutParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefGrantPutAclParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Optional[str] = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefGrantReadParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefGrantReadWriteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefGrantWriteParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').\n")
    allowed_action_patterns: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Restrict the permissions to certain list of action patterns.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefOnCloudTrailEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefOnCloudTrailPutObjectParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefOnCloudTrailWriteObjectParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)

class AwsS3IBucketDefS3UrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the S3 URL of the bucket is returned.\n')

class AwsS3IBucketDefTransferAccelerationUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    dual_stack: typing.Optional[bool] = pydantic.Field(None, description='Dual-stack support to connect to the bucket over IPv6. Default: - false\n')

class AwsS3IBucketDefUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')

class AwsS3IBucketDefVirtualHostedUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    regional: typing.Optional[bool] = pydantic.Field(None, description='Specifies the URL includes the region. Default: - true\n')

class AwsS3IBucketNotificationDestinationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsS3IBucketNotificationDestinationDefBindParams]] = pydantic.Field(None, description='Registers this resource to receive notifications for the specified bucket.\nThis method will only be called once for each destination/bucket\npair and the result will be cached, so there is no need to implement\nidempotency in each destination.')


class AwsS3IBucketNotificationDestinationDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The bucket object to bind to.')

class AwsS3DeploymentISourceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsS3DeploymentISourceDefBindParams]] = pydantic.Field(None, description='Binds the source to a bucket deployment.')


class AwsS3DeploymentISourceDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The construct tree context.\n')
    handler_role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='The role for the handler.')
    return_config: typing.Optional[list[models.aws_s3_deployment.SourceConfigDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretDefConfig(pydantic.BaseModel):
    add_rotation_schedule: typing.Optional[list[AwsSecretsmanagerISecretDefAddRotationScheduleParams]] = pydantic.Field(None, description='Adds a rotation schedule to the secret.')
    add_to_resource_policy: typing.Optional[list[AwsSecretsmanagerISecretDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM resource policy associated with this secret.\nIf this secret was created in this stack, a resource policy will be\nautomatically created upon the first call to ``addToResourcePolicy``. If\nthe secret is imported, then this is a no-op.')
    apply_removal_policy: typing.Optional[list[AwsSecretsmanagerISecretDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    attach: typing.Optional[list[AwsSecretsmanagerISecretDefAttachParams]] = pydantic.Field(None, description='Attach a target to this secret.')
    deny_account_root_delete: typing.Optional[bool] = pydantic.Field(None, description='Denies the ``DeleteSecret`` action to all principals within the current account.')
    grant_read: typing.Optional[list[AwsSecretsmanagerISecretDefGrantReadParams]] = pydantic.Field(None, description='Grants reading the secret value to some role.')
    grant_write: typing.Optional[list[AwsSecretsmanagerISecretDefGrantWriteParams]] = pydantic.Field(None, description='Grants writing and updating the secret value to some role.')
    secret_value_from_json: typing.Optional[list[AwsSecretsmanagerISecretDefSecretValueFromJsonParams]] = pydantic.Field(None, description="Interpret the secret as a JSON object and return a field's value from it as a ``SecretValue``.")


class AwsSecretsmanagerISecretDefAddRotationScheduleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. A value of zero will disable automatic rotation - ``Duration.days(0)``. Default: Duration.days(30)\n')
    hosted_rotation: typing.Optional[models.aws_secretsmanager.HostedRotationDef] = pydantic.Field(None, description='Hosted rotation. Default: - either ``rotationLambda`` or ``hostedRotation`` must be specified\n')
    rotate_immediately_on_update: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to rotate the secret immediately or wait until the next scheduled rotation window. Default: - secret is rotated immediately\n')
    rotation_lambda: typing.Optional[typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef]] = pydantic.Field(None, description='A Lambda function that can rotate the secret. Default: - either ``rotationLambda`` or ``hostedRotation`` must be specified')
    return_config: typing.Optional[list[models.aws_secretsmanager.RotationScheduleDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsSecretsmanagerISecretDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSecretsmanagerISecretDefAttachParams(pydantic.BaseModel):
    target: typing.Union[models.aws_rds.DatabaseProxyDef] = pydantic.Field(..., description='The target to attach.\n')
    return_config: typing.Optional[list[models._interface_methods.AwsSecretsmanagerISecretDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.\n')
    version_stages: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='the version stages the grant is limited to. If not specified, no restriction on the version stages is applied.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretDefSecretValueFromJsonParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.core.SecretValueDefConfig]] = pydantic.Field(None)
#  aws-cdk-lib.aws_secretsmanager.ISecretAttachmentTarget skipped


class AwsSecretsmanagerISecretTargetAttachmentDefConfig(pydantic.BaseModel):
    add_rotation_schedule: typing.Optional[list[AwsSecretsmanagerISecretTargetAttachmentDefAddRotationScheduleParams]] = pydantic.Field(None, description='Adds a rotation schedule to the secret.')
    add_to_resource_policy: typing.Optional[list[AwsSecretsmanagerISecretTargetAttachmentDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM resource policy associated with this secret.\nIf this secret was created in this stack, a resource policy will be\nautomatically created upon the first call to ``addToResourcePolicy``. If\nthe secret is imported, then this is a no-op.')
    apply_removal_policy: typing.Optional[list[AwsSecretsmanagerISecretTargetAttachmentDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    attach: typing.Optional[list[AwsSecretsmanagerISecretTargetAttachmentDefAttachParams]] = pydantic.Field(None, description='Attach a target to this secret.')
    deny_account_root_delete: typing.Optional[bool] = pydantic.Field(None, description='Denies the ``DeleteSecret`` action to all principals within the current account.')
    grant_read: typing.Optional[list[AwsSecretsmanagerISecretTargetAttachmentDefGrantReadParams]] = pydantic.Field(None, description='Grants reading the secret value to some role.')
    grant_write: typing.Optional[list[AwsSecretsmanagerISecretTargetAttachmentDefGrantWriteParams]] = pydantic.Field(None, description='Grants writing and updating the secret value to some role.')
    secret_value_from_json: typing.Optional[list[AwsSecretsmanagerISecretTargetAttachmentDefSecretValueFromJsonParams]] = pydantic.Field(None, description="Interpret the secret as a JSON object and return a field's value from it as a ``SecretValue``.")


class AwsSecretsmanagerISecretTargetAttachmentDefAddRotationScheduleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. A value of zero will disable automatic rotation - ``Duration.days(0)``. Default: Duration.days(30)\n')
    hosted_rotation: typing.Optional[models.aws_secretsmanager.HostedRotationDef] = pydantic.Field(None, description='Hosted rotation. Default: - either ``rotationLambda`` or ``hostedRotation`` must be specified\n')
    rotate_immediately_on_update: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to rotate the secret immediately or wait until the next scheduled rotation window. Default: - secret is rotated immediately\n')
    rotation_lambda: typing.Optional[typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef]] = pydantic.Field(None, description='A Lambda function that can rotate the secret. Default: - either ``rotationLambda`` or ``hostedRotation`` must be specified')
    return_config: typing.Optional[list[models.aws_secretsmanager.RotationScheduleDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretTargetAttachmentDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsSecretsmanagerISecretTargetAttachmentDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSecretsmanagerISecretTargetAttachmentDefAttachParams(pydantic.BaseModel):
    target: typing.Union[models.aws_rds.DatabaseProxyDef] = pydantic.Field(..., description='The target to attach.\n')
    return_config: typing.Optional[list[models._interface_methods.AwsSecretsmanagerISecretDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretTargetAttachmentDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.\n')
    version_stages: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='the version stages the grant is limited to. If not specified, no restriction on the version stages is applied.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretTargetAttachmentDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the principal being granted permission.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSecretsmanagerISecretTargetAttachmentDefSecretValueFromJsonParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.core.SecretValueDefConfig]] = pydantic.Field(None)

class AwsServicecatalogIPortfolioDefConfig(pydantic.BaseModel):
    add_product: typing.Optional[list[AwsServicecatalogIPortfolioDefAddProductParams]] = pydantic.Field(None, description='Associate portfolio with the given product.')
    apply_removal_policy: typing.Optional[list[AwsServicecatalogIPortfolioDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    associate_tag_options: typing.Optional[list[AwsServicecatalogIPortfolioDefAssociateTagOptionsParams]] = pydantic.Field(None, description='Associate Tag Options.\nA TagOption is a key-value pair managed in AWS Service Catalog.\nIt is not an AWS tag, but serves as a template for creating an AWS tag based on the TagOption.')
    constrain_cloud_formation_parameters: typing.Optional[list[AwsServicecatalogIPortfolioDefConstrainCloudFormationParametersParams]] = pydantic.Field(None, description='Set provisioning rules for the product.')
    constrain_tag_updates: typing.Optional[list[AwsServicecatalogIPortfolioDefConstrainTagUpdatesParams]] = pydantic.Field(None, description='Add a Resource Update Constraint.')
    deploy_with_stack_sets: typing.Optional[list[AwsServicecatalogIPortfolioDefDeployWithStackSetsParams]] = pydantic.Field(None, description='Configure deployment options using AWS Cloudformation StackSets.')
    give_access_to_group: typing.Optional[list[AwsServicecatalogIPortfolioDefGiveAccessToGroupParams]] = pydantic.Field(None, description='Associate portfolio with an IAM Group.')
    give_access_to_role: typing.Optional[list[AwsServicecatalogIPortfolioDefGiveAccessToRoleParams]] = pydantic.Field(None, description='Associate portfolio with an IAM Role.')
    give_access_to_user: typing.Optional[list[AwsServicecatalogIPortfolioDefGiveAccessToUserParams]] = pydantic.Field(None, description='Associate portfolio with an IAM User.')
    notify_on_stack_events: typing.Optional[list[AwsServicecatalogIPortfolioDefNotifyOnStackEventsParams]] = pydantic.Field(None, description='Add notifications for supplied topics on the provisioned product.')
    set_launch_role: typing.Optional[list[AwsServicecatalogIPortfolioDefSetLaunchRoleParams]] = pydantic.Field(None, description='Force users to assume a certain role when launching a product.\nThis sets the launch role using the role arn which is tied to the account this role exists in.\nThis is useful if you will be provisioning products from the account where this role exists.\nIf you intend to share the portfolio across accounts, use a local launch role.')
    set_local_launch_role: typing.Optional[list[AwsServicecatalogIPortfolioDefSetLocalLaunchRoleParams]] = pydantic.Field(None, description='Force users to assume a certain role when launching a product.\nThe role name will be referenced by in the local account and must be set explicitly.\nThis is useful when sharing the portfolio with multiple accounts.')
    set_local_launch_role_name: typing.Optional[list[AwsServicecatalogIPortfolioDefSetLocalLaunchRoleNameParams]] = pydantic.Field(None, description='Force users to assume a certain role when launching a product.\nThe role will be referenced by name in the local account instead of a static role arn.\nA role with this name will automatically be created and assumable by Service Catalog in this account.\nThis is useful when sharing the portfolio with multiple accounts.')
    share_with_account: typing.Optional[list[AwsServicecatalogIPortfolioDefShareWithAccountParams]] = pydantic.Field(None, description='Initiate a portfolio share with another account.')


class AwsServicecatalogIPortfolioDefAddProductParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='A service catalog produt.')

class AwsServicecatalogIPortfolioDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsServicecatalogIPortfolioDefAssociateTagOptionsParams(pydantic.BaseModel):
    tag_options: models.aws_servicecatalog.TagOptionsDef = pydantic.Field(..., description='-')

class AwsServicecatalogIPortfolioDefConstrainCloudFormationParametersParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='A service catalog product.\n')
    rule: typing.Union[models.aws_servicecatalog.TemplateRuleDef, dict[str, typing.Any]] = pydantic.Field(..., description='The rule with condition and assertions to apply to template.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the constraint. Default: - No description provided\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The language code. Configures the language for error messages from service catalog. Default: - English')

class AwsServicecatalogIPortfolioDefConstrainTagUpdatesParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='-\n')
    allow: typing.Optional[bool] = pydantic.Field(None, description='Toggle for if users should be allowed to change/update tags on provisioned products. Default: true\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the constraint. Default: - No description provided\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The language code. Configures the language for error messages from service catalog. Default: - English')

class AwsServicecatalogIPortfolioDefDeployWithStackSetsParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='A service catalog product.\n')
    accounts: typing.Sequence[str] = pydantic.Field(..., description='List of accounts to deploy stacks to.\n')
    admin_role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='IAM role used to administer the StackSets configuration.\n')
    execution_role_name: str = pydantic.Field(..., description='IAM role used to provision the products in the Stacks.\n')
    regions: typing.Sequence[str] = pydantic.Field(..., description='List of regions to deploy stacks to.\n')
    allow_stack_set_instance_operations: typing.Optional[bool] = pydantic.Field(None, description='Wether to allow end users to create, update, and delete stacks. Default: false\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the constraint. Default: - No description provided\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The language code. Configures the language for error messages from service catalog. Default: - English')

class AwsServicecatalogIPortfolioDefGiveAccessToGroupParams(pydantic.BaseModel):
    group: typing.Union[models.aws_iam.GroupDef] = pydantic.Field(..., description='an IAM Group.')

class AwsServicecatalogIPortfolioDefGiveAccessToRoleParams(pydantic.BaseModel):
    role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='an IAM role.')

class AwsServicecatalogIPortfolioDefGiveAccessToUserParams(pydantic.BaseModel):
    user: typing.Union[models.aws_iam.UserDef] = pydantic.Field(..., description='an IAM user.')

class AwsServicecatalogIPortfolioDefNotifyOnStackEventsParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='A service catalog product.\n')
    topic: typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef] = pydantic.Field(..., description='A SNS Topic to receive notifications on events related to the provisioned product.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the constraint. Default: - No description provided\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The language code. Configures the language for error messages from service catalog. Default: - English')

class AwsServicecatalogIPortfolioDefSetLaunchRoleParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='A service catalog product.\n')
    launch_role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='The IAM role a user must assume when provisioning the product.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the constraint. Default: - No description provided\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The language code. Configures the language for error messages from service catalog. Default: - English')

class AwsServicecatalogIPortfolioDefSetLocalLaunchRoleParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='A service catalog product.\n')
    launch_role: typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(..., description='The IAM role a user must assume when provisioning the product. A role with this name must exist in the account where the portolio is created and the accounts it is shared with. The role name must be set explicitly.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the constraint. Default: - No description provided\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The language code. Configures the language for error messages from service catalog. Default: - English')

class AwsServicecatalogIPortfolioDefSetLocalLaunchRoleNameParams(pydantic.BaseModel):
    product: typing.Union[models.aws_servicecatalog.ProductDef, models.aws_servicecatalog.CloudFormationProductDef] = pydantic.Field(..., description='A service catalog product.\n')
    launch_role_name: str = pydantic.Field(..., description='The name of the IAM role a user must assume when provisioning the product. A role with this name must exist in the account where the portolio is created and the accounts it is shared with.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the constraint. Default: - No description provided\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The language code. Configures the language for error messages from service catalog. Default: - English')
    return_config: typing.Optional[list[models._interface_methods.AwsIamIRoleDefConfig]] = pydantic.Field(None)

class AwsServicecatalogIPortfolioDefShareWithAccountParams(pydantic.BaseModel):
    account_id: str = pydantic.Field(..., description='AWS account to share portfolio with.\n')
    message_language: typing.Optional[aws_cdk.aws_servicecatalog.MessageLanguage] = pydantic.Field(None, description='The message language of the share. Controls status and error message language for share. Default: - English\n')
    share_tag_options: typing.Optional[bool] = pydantic.Field(None, description='Whether to share tagOptions as a part of the portfolio share. Default: - share not specified')

class AwsServicecatalogIProductDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsServicecatalogIProductDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    associate_tag_options: typing.Optional[list[AwsServicecatalogIProductDefAssociateTagOptionsParams]] = pydantic.Field(None, description='Associate Tag Options.\nA TagOption is a key-value pair managed in AWS Service Catalog.\nIt is not an AWS tag, but serves as a template for creating an AWS tag based on the TagOption.')


class AwsServicecatalogIProductDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsServicecatalogIProductDefAssociateTagOptionsParams(pydantic.BaseModel):
    tag_options: models.aws_servicecatalog.TagOptionsDef = pydantic.Field(..., description='-')

class AwsServicediscoveryIHttpNamespaceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsServicediscoveryIHttpNamespaceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsServicediscoveryIHttpNamespaceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsServicediscoveryIInstanceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsServicediscoveryIInstanceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsServicediscoveryIInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsServicediscoveryINamespaceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsServicediscoveryINamespaceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsServicediscoveryINamespaceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsServicediscoveryIPrivateDnsNamespaceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsServicediscoveryIPrivateDnsNamespaceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsServicediscoveryIPrivateDnsNamespaceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsServicediscoveryIPublicDnsNamespaceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsServicediscoveryIPublicDnsNamespaceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsServicediscoveryIPublicDnsNamespaceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsServicediscoveryIServiceDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsServicediscoveryIServiceDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsServicediscoveryIServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSesIConfigurationSetDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSesIConfigurationSetDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSesIConfigurationSetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSesIConfigurationSetEventDestinationDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSesIConfigurationSetEventDestinationDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSesIConfigurationSetEventDestinationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSesIDedicatedIpPoolDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSesIDedicatedIpPoolDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSesIDedicatedIpPoolDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSesIEmailIdentityDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSesIEmailIdentityDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSesIEmailIdentityDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSesIReceiptRuleDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSesIReceiptRuleDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSesIReceiptRuleDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSesIReceiptRuleActionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsSesIReceiptRuleActionDefBindParams]] = pydantic.Field(None, description='Returns the receipt rule action specification.')


class AwsSesIReceiptRuleActionDefBindParams(pydantic.BaseModel):
    receipt_rule: typing.Union[models.aws_ses.ReceiptRuleDef] = pydantic.Field(..., description='-')

class AwsSesIReceiptRuleSetDefConfig(pydantic.BaseModel):
    add_rule: typing.Optional[list[AwsSesIReceiptRuleSetDefAddRuleParams]] = pydantic.Field(None, description='Adds a new receipt rule in this rule set.\nThe new rule is added after\nthe last added rule unless ``after`` is specified.')
    apply_removal_policy: typing.Optional[list[AwsSesIReceiptRuleSetDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSesIReceiptRuleSetDefAddRuleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    actions: typing.Optional[typing.Sequence[typing.Union[models.aws_ses_actions.AddHeaderDef, models.aws_ses_actions.BounceDef, models.aws_ses_actions.LambdaDef, models.aws_ses_actions.S3Def, models.aws_ses_actions.SnsDef, models.aws_ses_actions.StopDef]]] = pydantic.Field(None, description='An ordered list of actions to perform on messages that match at least one of the recipient email addresses or domains specified in the receipt rule. Default: - No actions.\n')
    after: typing.Optional[typing.Union[models.aws_ses.ReceiptRuleDef]] = pydantic.Field(None, description='An existing rule after which the new rule will be placed. Default: - The new rule is inserted at the beginning of the rule list.\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether the rule is active. Default: true\n')
    receipt_rule_name: typing.Optional[str] = pydantic.Field(None, description='The name for the rule. Default: - A CloudFormation generated name.\n')
    recipients: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The recipient domains and email addresses that the receipt rule applies to. Default: - Match all recipients under all verified domains.\n')
    scan_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether to scan for spam and viruses. Default: false\n')
    tls_policy: typing.Optional[aws_cdk.aws_ses.TlsPolicy] = pydantic.Field(None, description='Whether Amazon SES should require that incoming email is delivered over a connection encrypted with Transport Layer Security (TLS). Default: - Optional which will not check for TLS.')
    return_config: typing.Optional[list[models.aws_ses.ReceiptRuleDefConfig]] = pydantic.Field(None)

class AwsSesIReceiptRuleSetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSesIVdmAttributesDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSesIVdmAttributesDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSesIVdmAttributesDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSignerISigningProfileDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSignerISigningProfileDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsSignerISigningProfileDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSnsITopicDefConfig(pydantic.BaseModel):
    add_subscription: typing.Optional[list[AwsSnsITopicDefAddSubscriptionParams]] = pydantic.Field(None, description='Subscribe some endpoint to this topic.')
    add_to_resource_policy: typing.Optional[list[AwsSnsITopicDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM resource policy associated with this topic.\nIf this topic was created in this stack (``new Topic``), a topic policy\nwill be automatically created upon the first call to ``addToPolicy``. If\nthe topic is imported (``Topic.import``), then this is a no-op.')
    apply_removal_policy: typing.Optional[list[AwsSnsITopicDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    bind_as_notification_rule_target: typing.Optional[list[AwsSnsITopicDefBindAsNotificationRuleTargetParams]] = pydantic.Field(None, description='Returns a target configuration for notification rule.')
    grant_publish: typing.Optional[list[AwsSnsITopicDefGrantPublishParams]] = pydantic.Field(None, description='Grant topic publishing permissions to the given identity.')
    metric: typing.Optional[list[AwsSnsITopicDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Topic.')
    metric_number_of_messages_published: typing.Optional[list[AwsSnsITopicDefMetricNumberOfMessagesPublishedParams]] = pydantic.Field(None, description='The number of messages published to your Amazon SNS topics.\nSum over 5 minutes')
    metric_number_of_notifications_delivered: typing.Optional[list[AwsSnsITopicDefMetricNumberOfNotificationsDeliveredParams]] = pydantic.Field(None, description='The number of messages successfully delivered from your Amazon SNS topics to subscribing endpoints.\nSum over 5 minutes')
    metric_number_of_notifications_failed: typing.Optional[list[AwsSnsITopicDefMetricNumberOfNotificationsFailedParams]] = pydantic.Field(None, description='The number of messages that Amazon SNS failed to deliver.\nSum over 5 minutes')
    metric_number_of_notifications_filtered_out: typing.Optional[list[AwsSnsITopicDefMetricNumberOfNotificationsFilteredOutParams]] = pydantic.Field(None, description='The number of messages that were rejected by subscription filter policies.\nSum over 5 minutes')
    metric_number_of_notifications_filtered_out_invalid_attributes: typing.Optional[list[AwsSnsITopicDefMetricNumberOfNotificationsFilteredOutInvalidAttributesParams]] = pydantic.Field(None, description="The number of messages that were rejected by subscription filter policies because the messages' attributes are invalid.\nSum over 5 minutes")
    metric_number_of_notifications_filtered_out_no_message_attributes: typing.Optional[list[AwsSnsITopicDefMetricNumberOfNotificationsFilteredOutNoMessageAttributesParams]] = pydantic.Field(None, description='The number of messages that were rejected by subscription filter policies because the messages have no attributes.\nSum over 5 minutes')
    metric_publish_size: typing.Optional[list[AwsSnsITopicDefMetricPublishSizeParams]] = pydantic.Field(None, description='Metric for the size of messages published through this topic.\nAverage over 5 minutes')
    metric_sms_month_to_date_spent_usd: typing.Optional[list[AwsSnsITopicDefMetricSmsMonthToDateSpentUsdParams]] = pydantic.Field(None, description='The charges you have accrued since the start of the current calendar month for sending SMS messages.\nMaximum over 5 minutes')
    metric_sms_success_rate: typing.Optional[list[AwsSnsITopicDefMetricSmsSuccessRateParams]] = pydantic.Field(None, description='The rate of successful SMS message deliveries.\nSum over 5 minutes')


class AwsSnsITopicDefAddSubscriptionParams(pydantic.BaseModel):
    subscription: typing.Union[models.aws_sns_subscriptions.EmailSubscriptionDef, models.aws_sns_subscriptions.LambdaSubscriptionDef, models.aws_sns_subscriptions.SmsSubscriptionDef, models.aws_sns_subscriptions.SqsSubscriptionDef, models.aws_sns_subscriptions.UrlSubscriptionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_sns.SubscriptionDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsSnsITopicDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSnsITopicDefBindAsNotificationRuleTargetParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')

class AwsSnsITopicDefGrantPublishParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricNumberOfMessagesPublishedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricNumberOfNotificationsDeliveredParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricNumberOfNotificationsFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricNumberOfNotificationsFilteredOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricNumberOfNotificationsFilteredOutInvalidAttributesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricNumberOfNotificationsFilteredOutNoMessageAttributesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricPublishSizeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricSmsMonthToDateSpentUsdParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicDefMetricSmsSuccessRateParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSnsITopicSubscriptionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsSnsITopicSubscriptionDefBindParams]] = pydantic.Field(None, description='Returns a configuration used to subscribe to an SNS topic.')


class AwsSnsITopicSubscriptionDefBindParams(pydantic.BaseModel):
    topic: typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef] = pydantic.Field(..., description='topic for which subscription will be configured.')

class AwsSqsIQueueDefConfig(pydantic.BaseModel):
    add_to_resource_policy: typing.Optional[list[AwsSqsIQueueDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM resource policy associated with this queue.\nIf this queue was created in this stack (``new Queue``), a queue policy\nwill be automatically created upon the first call to ``addToPolicy``. If\nthe queue is imported (``Queue.import``), then this is a no-op.')
    apply_removal_policy: typing.Optional[list[AwsSqsIQueueDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsSqsIQueueDefGrantParams]] = pydantic.Field(None, description='Grant the actions defined in queueActions to the identity Principal given on this SQS queue resource.')
    grant_consume_messages: typing.Optional[list[AwsSqsIQueueDefGrantConsumeMessagesParams]] = pydantic.Field(None, description='Grant permissions to consume messages from a queue.\nThis will grant the following permissions:\n\n- sqs:ChangeMessageVisibility\n- sqs:DeleteMessage\n- sqs:ReceiveMessage\n- sqs:GetQueueAttributes\n- sqs:GetQueueUrl')
    grant_purge: typing.Optional[list[AwsSqsIQueueDefGrantPurgeParams]] = pydantic.Field(None, description='Grant an IAM principal permissions to purge all messages from the queue.\nThis will grant the following permissions:\n\n- sqs:PurgeQueue\n- sqs:GetQueueAttributes\n- sqs:GetQueueUrl')
    grant_send_messages: typing.Optional[list[AwsSqsIQueueDefGrantSendMessagesParams]] = pydantic.Field(None, description='Grant access to send messages to a queue to the given identity.\nThis will grant the following permissions:\n\n- sqs:SendMessage\n- sqs:GetQueueAttributes\n- sqs:GetQueueUrl')
    metric: typing.Optional[list[AwsSqsIQueueDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Queue.')
    metric_approximate_age_of_oldest_message: typing.Optional[list[AwsSqsIQueueDefMetricApproximateAgeOfOldestMessageParams]] = pydantic.Field(None, description='The approximate age of the oldest non-deleted message in the queue.\nMaximum over 5 minutes')
    metric_approximate_number_of_messages_delayed: typing.Optional[list[AwsSqsIQueueDefMetricApproximateNumberOfMessagesDelayedParams]] = pydantic.Field(None, description='The number of messages in the queue that are delayed and not available for reading immediately.\nMaximum over 5 minutes')
    metric_approximate_number_of_messages_not_visible: typing.Optional[list[AwsSqsIQueueDefMetricApproximateNumberOfMessagesNotVisibleParams]] = pydantic.Field(None, description='The number of messages that are in flight.\nMaximum over 5 minutes')
    metric_approximate_number_of_messages_visible: typing.Optional[list[AwsSqsIQueueDefMetricApproximateNumberOfMessagesVisibleParams]] = pydantic.Field(None, description='The number of messages available for retrieval from the queue.\nMaximum over 5 minutes')
    metric_number_of_empty_receives: typing.Optional[list[AwsSqsIQueueDefMetricNumberOfEmptyReceivesParams]] = pydantic.Field(None, description='The number of ReceiveMessage API calls that did not return a message.\nSum over 5 minutes')
    metric_number_of_messages_deleted: typing.Optional[list[AwsSqsIQueueDefMetricNumberOfMessagesDeletedParams]] = pydantic.Field(None, description='The number of messages deleted from the queue.\nSum over 5 minutes')
    metric_number_of_messages_received: typing.Optional[list[AwsSqsIQueueDefMetricNumberOfMessagesReceivedParams]] = pydantic.Field(None, description='The number of messages returned by calls to the ReceiveMessage action.\nSum over 5 minutes')
    metric_number_of_messages_sent: typing.Optional[list[AwsSqsIQueueDefMetricNumberOfMessagesSentParams]] = pydantic.Field(None, description='The number of messages added to a queue.\nSum over 5 minutes')
    metric_sent_message_size: typing.Optional[list[AwsSqsIQueueDefMetricSentMessageSizeParams]] = pydantic.Field(None, description='The size of messages added to a queue.\nAverage over 5 minutes')


class AwsSqsIQueueDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')

class AwsSqsIQueueDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSqsIQueueDefGrantParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='Principal to grant right to.\n')
    queue_actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefGrantConsumeMessagesParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='Principal to grant consume rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefGrantPurgeParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='Principal to grant send rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefGrantSendMessagesParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='Principal to grant send rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricApproximateAgeOfOldestMessageParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricApproximateNumberOfMessagesDelayedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricApproximateNumberOfMessagesNotVisibleParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricApproximateNumberOfMessagesVisibleParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricNumberOfEmptyReceivesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricNumberOfMessagesDeletedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricNumberOfMessagesReceivedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricNumberOfMessagesSentParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSqsIQueueDefMetricSentMessageSizeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsSsmIParameterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSsmIParameterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_read: typing.Optional[list[AwsSsmIParameterDefGrantReadParams]] = pydantic.Field(None, description='Grants read (DescribeParameter, GetParameter, GetParameterHistory) permissions on the SSM Parameter.')
    grant_write: typing.Optional[list[AwsSsmIParameterDefGrantWriteParams]] = pydantic.Field(None, description='Grants write (PutParameter) permissions on the SSM Parameter.')


class AwsSsmIParameterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSsmIParameterDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the role to be granted read-only access to the parameter.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSsmIParameterDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the role to be granted write access to the parameter.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSsmIStringListParameterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSsmIStringListParameterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_read: typing.Optional[list[AwsSsmIStringListParameterDefGrantReadParams]] = pydantic.Field(None, description='Grants read (DescribeParameter, GetParameter, GetParameterHistory) permissions on the SSM Parameter.')
    grant_write: typing.Optional[list[AwsSsmIStringListParameterDefGrantWriteParams]] = pydantic.Field(None, description='Grants write (PutParameter) permissions on the SSM Parameter.')


class AwsSsmIStringListParameterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSsmIStringListParameterDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the role to be granted read-only access to the parameter.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSsmIStringListParameterDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the role to be granted write access to the parameter.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSsmIStringParameterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsSsmIStringParameterDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant_read: typing.Optional[list[AwsSsmIStringParameterDefGrantReadParams]] = pydantic.Field(None, description='Grants read (DescribeParameter, GetParameter, GetParameterHistory) permissions on the SSM Parameter.')
    grant_write: typing.Optional[list[AwsSsmIStringParameterDefGrantWriteParams]] = pydantic.Field(None, description='Grants write (PutParameter) permissions on the SSM Parameter.')


class AwsSsmIStringParameterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsSsmIStringParameterDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the role to be granted read-only access to the parameter.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsSsmIStringParameterDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the role to be granted write access to the parameter.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIActivityDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsStepfunctionsIActivityDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")


class AwsStepfunctionsIActivityDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
#  aws-cdk-lib.aws_stepfunctions.IChainable skipped


class AwsStepfunctionsINextableDefConfig(pydantic.BaseModel):
    next: typing.Optional[list[AwsStepfunctionsINextableDefNextParams]] = pydantic.Field(None, description='Go to the indicated state after this state.')


class AwsStepfunctionsINextableDefNextParams(pydantic.BaseModel):
    state: typing.Union[models.aws_stepfunctions.ChainDef, models.aws_stepfunctions.StateDef, models.aws_stepfunctions.StateMachineFragmentDef, models.aws_stepfunctions.TaskStateBaseDef, models.aws_stepfunctions.ChoiceDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.CustomStateDef, models.aws_stepfunctions.FailDef, models.aws_stepfunctions.MapDef, models.aws_stepfunctions.ParallelDef, models.aws_stepfunctions.PassDef, models.aws_stepfunctions.SucceedDef, models.aws_stepfunctions.WaitDef, models.aws_stepfunctions_tasks.AthenaGetQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaGetQueryResultsDef, models.aws_stepfunctions_tasks.AthenaStartQueryExecutionDef, models.aws_stepfunctions_tasks.AthenaStopQueryExecutionDef, models.aws_stepfunctions_tasks.BatchSubmitJobDef, models.aws_stepfunctions_tasks.CallApiGatewayHttpApiEndpointDef, models.aws_stepfunctions_tasks.CallApiGatewayRestApiEndpointDef, models.aws_stepfunctions_tasks.CallAwsServiceDef, models.aws_stepfunctions_tasks.CodeBuildStartBuildDef, models.aws_stepfunctions_tasks.DynamoDeleteItemDef, models.aws_stepfunctions_tasks.DynamoGetItemDef, models.aws_stepfunctions_tasks.DynamoPutItemDef, models.aws_stepfunctions_tasks.DynamoUpdateItemDef, models.aws_stepfunctions_tasks.EcsRunTaskDef, models.aws_stepfunctions_tasks.EksCallDef, models.aws_stepfunctions_tasks.EmrAddStepDef, models.aws_stepfunctions_tasks.EmrCancelStepDef, models.aws_stepfunctions_tasks.EmrContainersCreateVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersDeleteVirtualClusterDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.EmrCreateClusterDef, models.aws_stepfunctions_tasks.EmrModifyInstanceFleetByNameDef, models.aws_stepfunctions_tasks.EmrModifyInstanceGroupByNameDef, models.aws_stepfunctions_tasks.EmrSetClusterTerminationProtectionDef, models.aws_stepfunctions_tasks.EmrTerminateClusterDef, models.aws_stepfunctions_tasks.EvaluateExpressionDef, models.aws_stepfunctions_tasks.EventBridgePutEventsDef, models.aws_stepfunctions_tasks.GlueDataBrewStartJobRunDef, models.aws_stepfunctions_tasks.GlueStartJobRunDef, models.aws_stepfunctions_tasks.LambdaInvokeDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointDef, models.aws_stepfunctions_tasks.SageMakerCreateEndpointConfigDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.aws_stepfunctions_tasks.SageMakerCreateTransformJobDef, models.aws_stepfunctions_tasks.SageMakerUpdateEndpointDef, models.aws_stepfunctions_tasks.SnsPublishDef, models.aws_stepfunctions_tasks.SqsSendMessageDef, models.aws_stepfunctions_tasks.StepFunctionsInvokeActivityDef, models.aws_stepfunctions_tasks.StepFunctionsStartExecutionDef] = pydantic.Field(..., description='-\n')
    return_config: typing.Optional[list[models.aws_stepfunctions.ChainDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[AwsStepfunctionsIStateMachineDefApplyRemovalPolicyParams]] = pydantic.Field(None, description="Apply the given removal policy to this resource.\nThe Removal Policy controls what happens to this resource when it stops\nbeing managed by CloudFormation, either because you've removed it from the\nCDK application or because you've made a change that requires the resource\nto be replaced.\n\nThe resource can be deleted (``RemovalPolicy.DESTROY``), or left in your AWS\naccount for data recovery and cleanup later (``RemovalPolicy.RETAIN``).")
    grant: typing.Optional[list[AwsStepfunctionsIStateMachineDefGrantParams]] = pydantic.Field(None, description='Grant the given identity custom permissions.')
    grant_execution: typing.Optional[list[AwsStepfunctionsIStateMachineDefGrantExecutionParams]] = pydantic.Field(None, description='Grant the given identity permissions for all executions of a state machine.')
    grant_read: typing.Optional[list[AwsStepfunctionsIStateMachineDefGrantReadParams]] = pydantic.Field(None, description='Grant the given identity read permissions for this state machine.')
    grant_start_execution: typing.Optional[list[AwsStepfunctionsIStateMachineDefGrantStartExecutionParams]] = pydantic.Field(None, description='Grant the given identity permissions to start an execution of this state machine.')
    grant_start_sync_execution: typing.Optional[list[AwsStepfunctionsIStateMachineDefGrantStartSyncExecutionParams]] = pydantic.Field(None, description='Grant the given identity permissions to start a synchronous execution of this state machine.')
    grant_task_response: typing.Optional[list[AwsStepfunctionsIStateMachineDefGrantTaskResponseParams]] = pydantic.Field(None, description='Grant the given identity read permissions for this state machine.')
    metric: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricParams]] = pydantic.Field(None, description="Return the given named metric for this State Machine's executions.")
    metric_aborted: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricAbortedParams]] = pydantic.Field(None, description='Metric for the number of executions that were aborted.')
    metric_failed: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricFailedParams]] = pydantic.Field(None, description='Metric for the number of executions that failed.')
    metric_started: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricStartedParams]] = pydantic.Field(None, description='Metric for the number of executions that were started.')
    metric_succeeded: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricSucceededParams]] = pydantic.Field(None, description='Metric for the number of executions that succeeded.')
    metric_throttled: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricThrottledParams]] = pydantic.Field(None, description='Metric for the number of executions that were throttled.')
    metric_time: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricTimeParams]] = pydantic.Field(None, description='Metric for the interval, in milliseconds, between the time the execution starts and the time it closes.')
    metric_timed_out: typing.Optional[list[AwsStepfunctionsIStateMachineDefMetricTimedOutParams]] = pydantic.Field(None, description='Metric for the number of executions that timed out.')


class AwsStepfunctionsIStateMachineDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')

class AwsStepfunctionsIStateMachineDefGrantParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefGrantExecutionParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefGrantReadParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefGrantStartExecutionParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefGrantStartSyncExecutionParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefGrantTaskResponseParams(pydantic.BaseModel):
    identity: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricAbortedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricFailedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricStartedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricSucceededParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricThrottledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricTimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsIStateMachineDefMetricTimedOutParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: - sum over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)

class AwsStepfunctionsTasksIContainerDefinitionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsStepfunctionsTasksIContainerDefinitionDefBindParams]] = pydantic.Field(None, description='Called when the ContainerDefinition is used by a SageMaker task.')


class AwsStepfunctionsTasksIContainerDefinitionDefBindParams(pydantic.BaseModel):
    task: models.UnsupportedResource = pydantic.Field(..., description='-')

class AwsStepfunctionsTasksIEcsLaunchTargetDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[AwsStepfunctionsTasksIEcsLaunchTargetDefBindParams]] = pydantic.Field(None, description='called when the ECS launch target is configured on RunTask.')


class AwsStepfunctionsTasksIEcsLaunchTargetDefBindParams(pydantic.BaseModel):
    task: models.aws_stepfunctions_tasks.EcsRunTaskDef = pydantic.Field(..., description='-\n')
    task_definition: typing.Union[models.aws_ecs.Ec2TaskDefinitionDef, models.aws_ecs.ExternalTaskDefinitionDef, models.aws_ecs.FargateTaskDefinitionDef, models.aws_ecs.TaskDefinitionDef] = pydantic.Field(..., description='Task definition to run Docker containers in Amazon ECS.\n')
    cluster: typing.Optional[typing.Union[models.aws_ecs.ClusterDef]] = pydantic.Field(None, description='A regional grouping of one or more container instances on which you can run tasks and services. Default: - No cluster')
#  aws-cdk-lib.aws_stepfunctions_tasks.ISageMakerTask skipped


class CxApiIEnvironmentPlaceholderProviderDefConfig(pydantic.BaseModel):
    account_id: typing.Optional[bool] = pydantic.Field(None, description='Return the account.')
    partition: typing.Optional[bool] = pydantic.Field(None, description='Return the partition.')
    region: typing.Optional[bool] = pydantic.Field(None, description='Return the region.')


class PipelinesICodePipelineActionFactoryDefConfig(pydantic.BaseModel):
    produce_action: typing.Optional[list[PipelinesICodePipelineActionFactoryDefProduceActionParams]] = pydantic.Field(None, description='Create the desired Action and add it to the pipeline.')


class PipelinesICodePipelineActionFactoryDefProduceActionParams(pydantic.BaseModel):
    stage: models.UnsupportedResource = pydantic.Field(..., description='-\n')
    action_name: str = pydantic.Field(..., description='Name the action should get.\n')
    artifacts: models.pipelines.ArtifactMapDef = pydantic.Field(..., description='Helper object to translate FileSets to CodePipeline Artifacts.\n')
    pipeline: models.pipelines.CodePipelineDef = pydantic.Field(..., description='The pipeline the action is being generated for.\n')
    run_order: typing.Union[int, float] = pydantic.Field(..., description='RunOrder the action should get.\n')
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='Scope in which to create constructs.\n')
    stack_outputs_map: models.pipelines.StackOutputsMapDef = pydantic.Field(..., description='Helper object to produce variables exported from stack deployments. If your step references outputs from a stack deployment, use this to map the output references to Codepipeline variable names. Note - Codepipeline variables can only be referenced in action configurations.\n')
    before_self_mutation: typing.Optional[bool] = pydantic.Field(None, description='Whether or not this action is inserted before self mutation. If it is, the action should take care to reflect some part of its own definition in the pipeline action definition, to trigger a restart after self-mutation (if necessary). Default: false\n')
    code_build_defaults: typing.Union[models.pipelines.CodeBuildOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='If this action factory creates a CodeBuild step, default options to inherit. Default: - No CodeBuild project defaults\n')
    fallback_artifact: typing.Optional[models.aws_codepipeline.ArtifactDef] = pydantic.Field(None, description="An input artifact that CodeBuild projects that don't actually need an input artifact can use. CodeBuild Projects MUST have an input artifact in order to be added to the Pipeline. If the Project doesn't actually care about its input (it can be anything), it can use the Artifact passed here. Default: - A fallback artifact does not exist\n")
    variables_namespace: typing.Optional[str] = pydantic.Field(None, description="If this step is producing outputs, the variables namespace assigned to it. Pass this on to the Action you are creating. Default: - Step doesn't produce any outputs")
#  aws-cdk-lib.pipelines.IFileSetProducer skipped

#  aws-cdk-lib.region_info.IFact skipped


class TriggersITriggerDefConfig(pydantic.BaseModel):
    execute_after: typing.Optional[list[TriggersITriggerDefExecuteAfterParams]] = pydantic.Field(None, description='Adds trigger dependencies.\nExecute this trigger only after these construct\nscopes have been provisioned.')
    execute_before: typing.Optional[list[TriggersITriggerDefExecuteBeforeParams]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs.\nThis means that this\ntrigger will get executed *before* the given construct(s).')


class TriggersITriggerDefExecuteAfterParams(pydantic.BaseModel):
    scopes: list[models.constructs.ConstructDef] = pydantic.Field(...)

class TriggersITriggerDefExecuteBeforeParams(pydantic.BaseModel):
    scopes: list[models.constructs.ConstructDef] = pydantic.Field(...)
#  constructs.IConstruct skipped

#  constructs.IDependable skipped


class ConstructsIValidationDefConfig(pydantic.BaseModel):
    validate_: typing.Optional[bool] = pydantic.Field(None, description='Validate the current construct.\nThis method can be implemented by derived constructs in order to perform\nvalidation logic. It is called on all constructs before synthesis.\n\n:return: An array of validation error messages, or an empty array if there the construct is valid.', alias='validate')


import models