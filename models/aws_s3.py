from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_s3.BlockPublicAccess
class BlockPublicAccessDef(BaseClass):
    block_public_acls: typing.Optional[bool] = pydantic.Field(None, description='Whether to block public ACLs.')
    block_public_policy: typing.Optional[bool] = pydantic.Field(None, description='Whether to block public policy.\n')
    ignore_public_acls: typing.Optional[bool] = pydantic.Field(None, description='Whether to ignore public ACLs.\n')
    restrict_public_buckets: typing.Optional[bool] = pydantic.Field(None, description='Whether to restrict public access.')
    _init_params: typing.ClassVar[list[str]] = ['block_public_acls', 'block_public_policy', 'ignore_public_acls', 'restrict_public_buckets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BlockPublicAccess'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.BucketBase
class BucketBaseDef(BaseClass):
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID this resource belongs to. Default: - the resource is in the same account as the stack it belongs to\n')
    environment_from_arn: typing.Optional[str] = pydantic.Field(None, description='ARN to deduce region and account from. The ARN is parsed and the account and region are taken from the ARN. This should be used for imported resources. Cannot be supplied together with either ``account`` or ``region``. Default: - take environment from ``account``, ``region`` parameters, or use Stack environment.\n')
    physical_name: typing.Optional[str] = pydantic.Field(None, description='The value passed in by users to the physical name prop of the resource. - ``undefined`` implies that a physical name will be allocated by CloudFormation during deployment. - a concrete value implies a specific physical name - ``PhysicalName.GENERATE_IF_NEEDED`` is a marker that indicates that a physical will only be generated by the CDK if it is needed for cross-environment references. Otherwise, it will be allocated by CloudFormation. Default: - The physical name will be allocated by CloudFormation at deployment time\n')
    region: typing.Optional[str] = pydantic.Field(None, description='The AWS region this resource belongs to. Default: - the resource is in the same region as the stack it belongs to')
    _init_params: typing.ClassVar[list[str]] = ['account', 'environment_from_arn', 'physical_name', 'region']
    _method_names: typing.ClassVar[list[str]] = ['add_event_notification', 'add_object_created_notification', 'add_object_removed_notification', 'add_to_resource_policy', 'apply_removal_policy', 'arn_for_objects', 'enable_event_bridge_notification', 'grant_delete', 'grant_public_access', 'grant_put', 'grant_put_acl', 'grant_read', 'grant_read_write', 'grant_write', 'on_cloud_trail_event', 'on_cloud_trail_put_object', 'on_cloud_trail_write_object', 's3_url_for_object', 'transfer_acceleration_url_for_object', 'url_for_object', 'virtual_hosted_url_for_object']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BucketBase'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.BucketBaseDefConfig] = pydantic.Field(None)


class BucketBaseDefConfig(pydantic.BaseModel):
    add_event_notification: typing.Optional[list[models.aws_s3.BucketBaseDefAddEventNotificationParams]] = pydantic.Field(None, description='Adds a bucket notification event destination.')
    add_object_created_notification: typing.Optional[list[models.aws_s3.BucketBaseDefAddObjectCreatedNotificationParams]] = pydantic.Field(None, description='Subscribes a destination to receive notifications when an object is created in the bucket.\nThis is identical to calling\n``onEvent(EventType.OBJECT_CREATED)``.')
    add_object_removed_notification: typing.Optional[list[models.aws_s3.BucketBaseDefAddObjectRemovedNotificationParams]] = pydantic.Field(None, description='Subscribes a destination to receive notifications when an object is removed from the bucket.\nThis is identical to calling\n``onEvent(EventType.OBJECT_REMOVED)``.')
    add_to_resource_policy: typing.Optional[list[models.aws_s3.BucketBaseDefAddToResourcePolicyParams]] = pydantic.Field(None, description="Adds a statement to the resource policy for a principal (i.e. account/role/service) to perform actions on this bucket and/or its contents. Use ``bucketArn`` and ``arnForObjects(keys)`` to obtain ARNs for this bucket or objects.\nNote that the policy statement may or may not be added to the policy.\nFor example, when an ``IBucket`` is created from an existing bucket,\nit's not possible to tell whether the bucket already has a policy\nattached, let alone to re-use that policy to add more statements to it.\nSo it's safest to do nothing in these cases.")
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    arn_for_objects: typing.Optional[list[models.aws_s3.BucketBaseDefArnForObjectsParams]] = pydantic.Field(None, description='Returns an ARN that represents all objects within the bucket that match the key pattern specified.\nTo represent all keys, specify ``"*"``.\n\nIf you need to specify a keyPattern with multiple components, concatenate them into a single string, e.g.:\n\narnForObjects(``home/${team}/${user}/*``)')
    enable_event_bridge_notification: typing.Optional[bool] = pydantic.Field(None, description='Enables event bridge notification, causing all events below to be sent to EventBridge:.\n- Object Deleted (DeleteObject)\n- Object Deleted (Lifecycle expiration)\n- Object Restore Initiated\n- Object Restore Completed\n- Object Restore Expired\n- Object Storage Class Changed\n- Object Access Tier Changed\n- Object ACL Updated\n- Object Tags Added\n- Object Tags Deleted')
    grant_delete: typing.Optional[list[models.aws_s3.BucketBaseDefGrantDeleteParams]] = pydantic.Field(None, description='Grants s3:DeleteObject* permission to an IAM principal for objects in this bucket.')
    grant_public_access: typing.Optional[list[models.aws_s3.BucketBaseDefGrantPublicAccessParams]] = pydantic.Field(None, description='Allows unrestricted access to objects from this bucket.\nIMPORTANT: This permission allows anyone to perform actions on S3 objects\nin this bucket, which is useful for when you configure your bucket as a\nwebsite and want everyone to be able to read objects in the bucket without\nneeding to authenticate.\n\nWithout arguments, this method will grant read ("s3:GetObject") access to\nall objects ("*") in the bucket.\n\nThe method returns the ``iam.Grant`` object, which can then be modified\nas needed. For example, you can add a condition that will restrict access only\nto an IPv4 range like this::\n\n   const grant = bucket.grantPublicAccess();\n   grant.resourceStatement!.addCondition(‘IpAddress’, { “aws:SourceIp”: “54.240.143.0/24” });\n\nNote that if this ``IBucket`` refers to an existing bucket, possibly not\nmanaged by CloudFormation, this method will have no effect, since it\'s\nimpossible to modify the policy of an existing bucket.')
    grant_put: typing.Optional[list[models.aws_s3.BucketBaseDefGrantPutParams]] = pydantic.Field(None, description='Grants s3:PutObject* and s3:Abort* permissions for this bucket to an IAM principal.\nIf encryption is used, permission to use the key to encrypt the contents\nof written files will also be granted to the same principal.')
    grant_put_acl: typing.Optional[list[models.aws_s3.BucketBaseDefGrantPutAclParams]] = pydantic.Field(None, description="Grant the given IAM identity permissions to modify the ACLs of objects in the given Bucket.\nIf your application has the '@aws-cdk/aws-s3:grantWriteWithoutAcl' feature flag set,\ncalling ``grantWrite`` or ``grantReadWrite`` no longer grants permissions to modify the ACLs of the objects;\nin this case, if you need to modify object ACLs, call this method explicitly.")
    grant_read: typing.Optional[list[models.aws_s3.BucketBaseDefGrantReadParams]] = pydantic.Field(None, description="Grant read permissions for this bucket and it's contents to an IAM principal (Role/Group/User).\nIf encryption is used, permission to use the key to decrypt the contents\nof the bucket will also be granted to the same principal.")
    grant_read_write: typing.Optional[list[models.aws_s3.BucketBaseDefGrantReadWriteParams]] = pydantic.Field(None, description="Grants read/write permissions for this bucket and it's contents to an IAM principal (Role/Group/User).\nIf an encryption key is used, permission to use the key for\nencrypt/decrypt will also be granted.\n\nBefore CDK version 1.85.0, this method granted the ``s3:PutObject*`` permission that included ``s3:PutObjectAcl``,\nwhich could be used to grant read/write object access to IAM principals in other accounts.\nIf you want to get rid of that behavior, update your CDK version to 1.85.0 or later,\nand make sure the ``@aws-cdk/aws-s3:grantWriteWithoutAcl`` feature flag is set to ``true``\nin the ``context`` key of your cdk.json file.\nIf you've already updated, but still need the principal to have permissions to modify the ACLs,\nuse the ``grantPutAcl`` method.")
    grant_write: typing.Optional[list[models.aws_s3.BucketBaseDefGrantWriteParams]] = pydantic.Field(None, description="Grant write permissions to this bucket to an IAM principal.\nIf encryption is used, permission to use the key to encrypt the contents\nof written files will also be granted to the same principal.\n\nBefore CDK version 1.85.0, this method granted the ``s3:PutObject*`` permission that included ``s3:PutObjectAcl``,\nwhich could be used to grant read/write object access to IAM principals in other accounts.\nIf you want to get rid of that behavior, update your CDK version to 1.85.0 or later,\nand make sure the ``@aws-cdk/aws-s3:grantWriteWithoutAcl`` feature flag is set to ``true``\nin the ``context`` key of your cdk.json file.\nIf you've already updated, but still need the principal to have permissions to modify the ACLs,\nuse the ``grantPutAcl`` method.")
    on_cloud_trail_event: typing.Optional[list[models.aws_s3.BucketBaseDefOnCloudTrailEventParams]] = pydantic.Field(None, description='Define a CloudWatch event that triggers when something happens to this repository.\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_cloud_trail_put_object: typing.Optional[list[models.aws_s3.BucketBaseDefOnCloudTrailPutObjectParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event that triggers when an object is uploaded to the specified paths (keys) in this bucket using the PutObject API call.\nNote that some tools like ``aws s3 cp`` will automatically use either\nPutObject or the multipart upload API depending on the file size,\nso using ``onCloudTrailWriteObject`` may be preferable.\n\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_cloud_trail_write_object: typing.Optional[list[models.aws_s3.BucketBaseDefOnCloudTrailWriteObjectParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event that triggers when an object at the specified paths (keys) in this bucket are written to.\nThis includes\nthe events PutObject, CopyObject, and CompleteMultipartUpload.\n\nNote that some tools like ``aws s3 cp`` will automatically use either\nPutObject or the multipart upload API depending on the file size,\nso using this method may be preferable to ``onCloudTrailPutObject``.\n\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    s3_url_for_object: typing.Optional[list[models.aws_s3.BucketBaseDefS3UrlForObjectParams]] = pydantic.Field(None, description='The S3 URL of an S3 object. For example:.\n- ``s3://onlybucket``\n- ``s3://bucket/key``')
    transfer_acceleration_url_for_object: typing.Optional[list[models.aws_s3.BucketBaseDefTransferAccelerationUrlForObjectParams]] = pydantic.Field(None, description='The https Transfer Acceleration URL of an S3 object.\nSpecify ``dualStack: true`` at the options\nfor dual-stack endpoint (connect to the bucket over IPv6). For example:\n\n- ``https://bucket.s3-accelerate.amazonaws.com``\n- ``https://bucket.s3-accelerate.amazonaws.com/key``')
    url_for_object: typing.Optional[list[models.aws_s3.BucketBaseDefUrlForObjectParams]] = pydantic.Field(None, description='The https URL of an S3 object. Specify ``regional: false`` at the options for non-regional URLs. For example:.\n- ``https://s3.us-west-1.amazonaws.com/onlybucket``\n- ``https://s3.us-west-1.amazonaws.com/bucket/key``\n- ``https://s3.cn-north-1.amazonaws.com.cn/china-bucket/mykey``')
    virtual_hosted_url_for_object: typing.Optional[list[models.aws_s3.BucketBaseDefVirtualHostedUrlForObjectParams]] = pydantic.Field(None, description='The virtual hosted-style URL of an S3 object. Specify ``regional: false`` at the options for non-regional URL. For example:.\n- ``https://only-bucket.s3.us-west-1.amazonaws.com``\n- ``https://bucket.s3.us-west-1.amazonaws.com/key``\n- ``https://bucket.s3.amazonaws.com/key``\n- ``https://china-bucket.s3.cn-north-1.amazonaws.com.cn/mykey``')

class BucketBaseDefAddEventNotificationParams(pydantic.BaseModel):
    event: aws_cdk.aws_s3.EventType = pydantic.Field(..., description='The event to trigger the notification.\n')
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (Lambda, SNS Topic or SQS Queue).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)
    ...

class BucketBaseDefAddObjectCreatedNotificationParams(pydantic.BaseModel):
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (see onEvent).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)
    ...

class BucketBaseDefAddObjectRemovedNotificationParams(pydantic.BaseModel):
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (see onEvent).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)
    ...

class BucketBaseDefAddToResourcePolicyParams(pydantic.BaseModel):
    permission: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description="the policy statement to be added to the bucket's policy.\n")
    ...

class BucketBaseDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class BucketBaseDefArnForObjectsParams(pydantic.BaseModel):
    key_pattern: str = pydantic.Field(..., description='-')
    ...

class BucketBaseDefGrantDeleteParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefGrantPublicAccessParams(pydantic.BaseModel):
    key_prefix: typing.Optional[str] = pydantic.Field(None, description='the prefix of S3 object keys (e.g. ``home/*``). Default is "*".\n')
    allowed_actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefGrantPutParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefGrantPutAclParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    objects_key_pattern: typing.Optional[str] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefGrantReadParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefGrantReadWriteParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefGrantWriteParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description='-\n')
    allowed_action_patterns: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefOnCloudTrailEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefOnCloudTrailPutObjectParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefOnCloudTrailWriteObjectParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class BucketBaseDefS3UrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the S3 URL of the bucket is returned.\n')
    ...

class BucketBaseDefTransferAccelerationUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    dual_stack: typing.Optional[bool] = pydantic.Field(None, description='Dual-stack support to connect to the bucket over IPv6. Default: - false\n')
    ...

class BucketBaseDefUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    ...

class BucketBaseDefVirtualHostedUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    regional: typing.Optional[bool] = pydantic.Field(None, description='Specifies the URL includes the region. Default: - true\n')
    ...


#  autogenerated from aws_cdk.aws_s3.ObjectLockRetention
class ObjectLockRetentionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['compliance', 'governance']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.ObjectLockRetention'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.ObjectLockRetentionDefConfig] = pydantic.Field(None)


class ObjectLockRetentionDefConfig(pydantic.BaseModel):
    compliance: typing.Optional[list[models.aws_s3.ObjectLockRetentionDefComplianceParams]] = pydantic.Field(None, description="Configure for Compliance retention for a specified duration.\nWhen an object is locked in compliance mode, its retention mode can't be changed, and\nits retention period can't be shortened. Compliance mode helps ensure that an object\nversion can't be overwritten or deleted for the duration of the retention period.")
    governance: typing.Optional[list[models.aws_s3.ObjectLockRetentionDefGovernanceParams]] = pydantic.Field(None, description='Configure for Governance retention for a specified duration.\nWith governance mode, you protect objects against being deleted by most users, but you can\nstill grant some users permission to alter the retention settings or delete the object if\nnecessary. You can also use governance mode to test retention-period settings before\ncreating a compliance-mode retention period.')
    duration_config: typing.Optional[models.core.DurationDefConfig] = pydantic.Field(None)

class ObjectLockRetentionDefComplianceParams(pydantic.BaseModel):
    duration: models.DurationDef = pydantic.Field(..., description='the length of time for which objects should be retained.\n')
    return_config: typing.Optional[list[models.aws_s3.ObjectLockRetentionDefConfig]] = pydantic.Field(None)
    ...

class ObjectLockRetentionDefGovernanceParams(pydantic.BaseModel):
    duration: models.DurationDef = pydantic.Field(..., description='the length of time for which objects should retained.\n')
    return_config: typing.Optional[list[models.aws_s3.ObjectLockRetentionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_s3.ReplaceKey
class ReplaceKeyDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['prefix_with', 'with_']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.ReplaceKey'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.ReplaceKeyDefConfig] = pydantic.Field(None)


class ReplaceKeyDefConfig(pydantic.BaseModel):
    prefix_with: typing.Optional[list[models.aws_s3.ReplaceKeyDefPrefixWithParams]] = pydantic.Field(None, description='The object key prefix to use in the redirect request.')
    with_: typing.Optional[list[models.aws_s3.ReplaceKeyDefWithParams]] = pydantic.Field(None, description='The specific object key to use in the redirect request.')

class ReplaceKeyDefPrefixWithParams(pydantic.BaseModel):
    key_replacement: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_s3.ReplaceKeyDefConfig]] = pydantic.Field(None)
    ...

class ReplaceKeyDefWithParams(pydantic.BaseModel):
    key_replacement: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_s3.ReplaceKeyDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_s3.StorageClass
class StorageClassDef(BaseClass):
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.StorageClass'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.Bucket
class BucketDef(BaseConstruct):
    access_control: typing.Optional[aws_cdk.aws_s3.BucketAccessControl] = pydantic.Field(None, description='Specifies a canned ACL that grants predefined permissions to the bucket. Default: BucketAccessControl.PRIVATE\n')
    auto_delete_objects: typing.Optional[bool] = pydantic.Field(None, description='Whether all objects should be automatically deleted when the bucket is removed from the stack or when the stack is deleted. Requires the ``removalPolicy`` to be set to ``RemovalPolicy.DESTROY``. **Warning** if you have deployed a bucket with ``autoDeleteObjects: true``, switching this to ``false`` in a CDK version *before* ``1.126.0`` will lead to all objects in the bucket being deleted. Be sure to update your bucket resources by deploying with CDK version ``1.126.0`` or later **before** switching this value to ``false``. Default: false\n')
    block_public_access: typing.Optional[models.aws_s3.BlockPublicAccessDef] = pydantic.Field(None, description="The block public access configuration of this bucket. Default: - CloudFormation defaults will apply. New buckets and objects don't allow public access, but users can modify bucket policies or object permissions to allow public access\n")
    bucket_key_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether Amazon S3 should use its own intermediary key to generate data keys. Only relevant when using KMS for encryption. - If not enabled, every object GET and PUT will cause an API call to KMS (with the attendant cost implications of that). - If enabled, S3 will use its own time-limited key instead. Only relevant, when Encryption is set to ``BucketEncryption.KMS`` or ``BucketEncryption.KMS_MANAGED``. Default: - false\n')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description='Physical name of this bucket. Default: - Assigned by CloudFormation (recommended).\n')
    cors: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.CorsRuleDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The CORS configuration of this bucket. Default: - No CORS configuration.\n')
    encryption: typing.Optional[aws_cdk.aws_s3.BucketEncryption] = pydantic.Field(None, description='The kind of server-side encryption to apply to this bucket. If you choose KMS, you can specify a KMS key via ``encryptionKey``. If encryption key is not specified, a key will automatically be created. Default: - ``KMS`` if ``encryptionKey`` is specified, or ``UNENCRYPTED`` otherwise. But if ``UNENCRYPTED`` is specified, the bucket will be encrypted as ``S3_MANAGED`` automatically.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='External KMS key to use for bucket encryption. The ``encryption`` property must be either not specified or set to ``KMS`` or ``DSSE``. An error will be emitted if ``encryption`` is set to ``UNENCRYPTED`` or ``S3_MANAGED``. Default: - If ``encryption`` is set to ``KMS`` and this property is undefined, a new KMS key will be created and associated with this bucket.\n')
    enforce_ssl: typing.Optional[bool] = pydantic.Field(None, description='Enforces SSL for requests. S3.5 of the AWS Foundational Security Best Practices Regarding S3. Default: false\n')
    event_bridge_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether this bucket should send notifications to Amazon EventBridge or not. Default: false\n')
    intelligent_tiering_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.IntelligentTieringConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Inteligent Tiering Configurations. Default: No Intelligent Tiiering Configurations.\n')
    inventories: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.InventoryDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The inventory configuration of the bucket. Default: - No inventory configuration\n')
    lifecycle_rules: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.LifecycleRuleDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Rules that define how Amazon S3 manages objects during their lifetime. Default: - No lifecycle rules.\n')
    metrics: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketMetricsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metrics configuration of this bucket. Default: - No metrics configuration.\n')
    minimum_tls_version: typing.Union[int, float, None] = pydantic.Field(None, description='Enforces minimum TLS version for requests. Requires ``enforceSSL`` to be enabled. Default: No minimum TLS version is enforced.\n')
    notifications_handler_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to be used by the notifications handler. Default: - a new role will be created.\n')
    object_lock_default_retention: typing.Optional[models.aws_s3.ObjectLockRetentionDef] = pydantic.Field(None, description='The default retention mode and rules for S3 Object Lock. Default retention can be configured after a bucket is created if the bucket already has object lock enabled. Enabling object lock for existing buckets is not supported. Default: no default retention period\n')
    object_lock_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enable object lock on the bucket. Enabling object lock for existing buckets is not supported. Object lock must be enabled when the bucket is created. Default: false, unless objectLockDefaultRetention is set (then, true)\n')
    object_ownership: typing.Optional[aws_cdk.aws_s3.ObjectOwnership] = pydantic.Field(None, description='The objectOwnership of the bucket. Default: - No ObjectOwnership configuration, uploading account will own the object.\n')
    public_read_access: typing.Optional[bool] = pydantic.Field(None, description='Grants public read access to all objects in the bucket. Similar to calling ``bucket.grantPublicAccess()`` Default: false\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Policy to apply when the bucket is removed from this stack. Default: - The bucket will be orphaned.\n')
    server_access_logs_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='Destination bucket for the server access logs. Default: - If "serverAccessLogsPrefix" undefined - access logs disabled, otherwise - log to current bucket.\n')
    server_access_logs_prefix: typing.Optional[str] = pydantic.Field(None, description='Optional log file prefix to use for the bucket\'s access logs. If defined without "serverAccessLogsBucket", enables access logs to current bucket with this prefix. Default: - No log file prefix\n')
    transfer_acceleration: typing.Optional[bool] = pydantic.Field(None, description='Whether this bucket should have transfer acceleration turned on or not. Default: false\n')
    versioned: typing.Optional[bool] = pydantic.Field(None, description='Whether this bucket should have versioning turned on or not. Default: false (unless object lock is enabled, then true)\n')
    website_error_document: typing.Optional[str] = pydantic.Field(None, description='The name of the error document (e.g. "404.html") for the website. ``websiteIndexDocument`` must also be set if this is set. Default: - No error document.\n')
    website_index_document: typing.Optional[str] = pydantic.Field(None, description='The name of the index document (e.g. "index.html") for the website. Enables static website hosting for this bucket. Default: - No index document.\n')
    website_redirect: typing.Union[models.aws_s3.RedirectTargetDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the redirect behavior of all requests to a website endpoint of a bucket. If you specify this property, you can\'t specify "websiteIndexDocument", "websiteErrorDocument" nor , "websiteRoutingRules". Default: - No redirection.\n')
    website_routing_rules: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.RoutingRuleDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Rules that define when a redirect is applied and the redirect behavior. Default: - No redirection rules.')
    _init_params: typing.ClassVar[list[str]] = ['access_control', 'auto_delete_objects', 'block_public_access', 'bucket_key_enabled', 'bucket_name', 'cors', 'encryption', 'encryption_key', 'enforce_ssl', 'event_bridge_enabled', 'intelligent_tiering_configurations', 'inventories', 'lifecycle_rules', 'metrics', 'minimum_tls_version', 'notifications_handler_role', 'object_lock_default_retention', 'object_lock_enabled', 'object_ownership', 'public_read_access', 'removal_policy', 'server_access_logs_bucket', 'server_access_logs_prefix', 'transfer_acceleration', 'versioned', 'website_error_document', 'website_index_document', 'website_redirect', 'website_routing_rules']
    _method_names: typing.ClassVar[list[str]] = ['add_cors_rule', 'add_event_notification', 'add_inventory', 'add_lifecycle_rule', 'add_metric', 'add_object_created_notification', 'add_object_removed_notification', 'add_to_resource_policy', 'apply_removal_policy', 'arn_for_objects', 'enable_event_bridge_notification', 'grant_delete', 'grant_public_access', 'grant_put', 'grant_put_acl', 'grant_read', 'grant_read_write', 'grant_write', 'on_cloud_trail_event', 'on_cloud_trail_put_object', 'on_cloud_trail_write_object', 's3_url_for_object', 'transfer_acceleration_url_for_object', 'url_for_object', 'virtual_hosted_url_for_object']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_bucket_arn', 'from_bucket_attributes', 'from_bucket_name', 'from_cfn_bucket', 'validate_bucket_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.Bucket'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_bucket_arn', 'from_bucket_attributes', 'from_bucket_name', 'from_cfn_bucket']
    ...


    from_bucket_arn: typing.Optional[models.aws_s3.BucketDefFromBucketArnParams] = pydantic.Field(None, description='')
    from_bucket_attributes: typing.Optional[models.aws_s3.BucketDefFromBucketAttributesParams] = pydantic.Field(None, description='Creates a Bucket construct that represents an external bucket.')
    from_bucket_name: typing.Optional[models.aws_s3.BucketDefFromBucketNameParams] = pydantic.Field(None, description='')
    from_cfn_bucket: typing.Optional[models.aws_s3.BucketDefFromCfnBucketParams] = pydantic.Field(None, description='Create a mutable ``IBucket`` based on a low-level ``CfnBucket``.')
    resource_config: typing.Optional[models.aws_s3.BucketDefConfig] = pydantic.Field(None)


class BucketDefConfig(pydantic.BaseModel):
    add_cors_rule: typing.Optional[list[models.aws_s3.BucketDefAddCorsRuleParams]] = pydantic.Field(None, description='Adds a cross-origin access configuration for objects in an Amazon S3 bucket.')
    add_event_notification: typing.Optional[list[models.aws_s3.BucketDefAddEventNotificationParams]] = pydantic.Field(None, description='Adds a bucket notification event destination.')
    add_inventory: typing.Optional[list[models.aws_s3.BucketDefAddInventoryParams]] = pydantic.Field(None, description='Add an inventory configuration.')
    add_lifecycle_rule: typing.Optional[list[models.aws_s3.BucketDefAddLifecycleRuleParams]] = pydantic.Field(None, description='Add a lifecycle rule to the bucket.')
    add_metric: typing.Optional[list[models.aws_s3.BucketDefAddMetricParams]] = pydantic.Field(None, description='Adds a metrics configuration for the CloudWatch request metrics from the bucket.')
    add_object_created_notification: typing.Optional[list[models.aws_s3.BucketDefAddObjectCreatedNotificationParams]] = pydantic.Field(None, description='Subscribes a destination to receive notifications when an object is created in the bucket.\nThis is identical to calling\n``onEvent(EventType.OBJECT_CREATED)``.')
    add_object_removed_notification: typing.Optional[list[models.aws_s3.BucketDefAddObjectRemovedNotificationParams]] = pydantic.Field(None, description='Subscribes a destination to receive notifications when an object is removed from the bucket.\nThis is identical to calling\n``onEvent(EventType.OBJECT_REMOVED)``.')
    add_to_resource_policy: typing.Optional[list[models.aws_s3.BucketDefAddToResourcePolicyParams]] = pydantic.Field(None, description="Adds a statement to the resource policy for a principal (i.e. account/role/service) to perform actions on this bucket and/or its contents. Use ``bucketArn`` and ``arnForObjects(keys)`` to obtain ARNs for this bucket or objects.\nNote that the policy statement may or may not be added to the policy.\nFor example, when an ``IBucket`` is created from an existing bucket,\nit's not possible to tell whether the bucket already has a policy\nattached, let alone to re-use that policy to add more statements to it.\nSo it's safest to do nothing in these cases.")
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    arn_for_objects: typing.Optional[list[models.aws_s3.BucketDefArnForObjectsParams]] = pydantic.Field(None, description='Returns an ARN that represents all objects within the bucket that match the key pattern specified.\nTo represent all keys, specify ``"*"``.\n\nIf you need to specify a keyPattern with multiple components, concatenate them into a single string, e.g.:\n\narnForObjects(``home/${team}/${user}/*``)')
    enable_event_bridge_notification: typing.Optional[bool] = pydantic.Field(None, description='Enables event bridge notification, causing all events below to be sent to EventBridge:.\n- Object Deleted (DeleteObject)\n- Object Deleted (Lifecycle expiration)\n- Object Restore Initiated\n- Object Restore Completed\n- Object Restore Expired\n- Object Storage Class Changed\n- Object Access Tier Changed\n- Object ACL Updated\n- Object Tags Added\n- Object Tags Deleted')
    grant_delete: typing.Optional[list[models.aws_s3.BucketDefGrantDeleteParams]] = pydantic.Field(None, description='Grants s3:DeleteObject* permission to an IAM principal for objects in this bucket.')
    grant_public_access: typing.Optional[list[models.aws_s3.BucketDefGrantPublicAccessParams]] = pydantic.Field(None, description='Allows unrestricted access to objects from this bucket.\nIMPORTANT: This permission allows anyone to perform actions on S3 objects\nin this bucket, which is useful for when you configure your bucket as a\nwebsite and want everyone to be able to read objects in the bucket without\nneeding to authenticate.\n\nWithout arguments, this method will grant read ("s3:GetObject") access to\nall objects ("*") in the bucket.\n\nThe method returns the ``iam.Grant`` object, which can then be modified\nas needed. For example, you can add a condition that will restrict access only\nto an IPv4 range like this::\n\n   const grant = bucket.grantPublicAccess();\n   grant.resourceStatement!.addCondition(‘IpAddress’, { “aws:SourceIp”: “54.240.143.0/24” });\n\nNote that if this ``IBucket`` refers to an existing bucket, possibly not\nmanaged by CloudFormation, this method will have no effect, since it\'s\nimpossible to modify the policy of an existing bucket.')
    grant_put: typing.Optional[list[models.aws_s3.BucketDefGrantPutParams]] = pydantic.Field(None, description='Grants s3:PutObject* and s3:Abort* permissions for this bucket to an IAM principal.\nIf encryption is used, permission to use the key to encrypt the contents\nof written files will also be granted to the same principal.')
    grant_put_acl: typing.Optional[list[models.aws_s3.BucketDefGrantPutAclParams]] = pydantic.Field(None, description="Grant the given IAM identity permissions to modify the ACLs of objects in the given Bucket.\nIf your application has the '@aws-cdk/aws-s3:grantWriteWithoutAcl' feature flag set,\ncalling ``grantWrite`` or ``grantReadWrite`` no longer grants permissions to modify the ACLs of the objects;\nin this case, if you need to modify object ACLs, call this method explicitly.")
    grant_read: typing.Optional[list[models.aws_s3.BucketDefGrantReadParams]] = pydantic.Field(None, description="Grant read permissions for this bucket and it's contents to an IAM principal (Role/Group/User).\nIf encryption is used, permission to use the key to decrypt the contents\nof the bucket will also be granted to the same principal.")
    grant_read_write: typing.Optional[list[models.aws_s3.BucketDefGrantReadWriteParams]] = pydantic.Field(None, description="Grants read/write permissions for this bucket and it's contents to an IAM principal (Role/Group/User).\nIf an encryption key is used, permission to use the key for\nencrypt/decrypt will also be granted.\n\nBefore CDK version 1.85.0, this method granted the ``s3:PutObject*`` permission that included ``s3:PutObjectAcl``,\nwhich could be used to grant read/write object access to IAM principals in other accounts.\nIf you want to get rid of that behavior, update your CDK version to 1.85.0 or later,\nand make sure the ``@aws-cdk/aws-s3:grantWriteWithoutAcl`` feature flag is set to ``true``\nin the ``context`` key of your cdk.json file.\nIf you've already updated, but still need the principal to have permissions to modify the ACLs,\nuse the ``grantPutAcl`` method.")
    grant_write: typing.Optional[list[models.aws_s3.BucketDefGrantWriteParams]] = pydantic.Field(None, description="Grant write permissions to this bucket to an IAM principal.\nIf encryption is used, permission to use the key to encrypt the contents\nof written files will also be granted to the same principal.\n\nBefore CDK version 1.85.0, this method granted the ``s3:PutObject*`` permission that included ``s3:PutObjectAcl``,\nwhich could be used to grant read/write object access to IAM principals in other accounts.\nIf you want to get rid of that behavior, update your CDK version to 1.85.0 or later,\nand make sure the ``@aws-cdk/aws-s3:grantWriteWithoutAcl`` feature flag is set to ``true``\nin the ``context`` key of your cdk.json file.\nIf you've already updated, but still need the principal to have permissions to modify the ACLs,\nuse the ``grantPutAcl`` method.")
    on_cloud_trail_event: typing.Optional[list[models.aws_s3.BucketDefOnCloudTrailEventParams]] = pydantic.Field(None, description='Define a CloudWatch event that triggers when something happens to this repository.\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_cloud_trail_put_object: typing.Optional[list[models.aws_s3.BucketDefOnCloudTrailPutObjectParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event that triggers when an object is uploaded to the specified paths (keys) in this bucket using the PutObject API call.\nNote that some tools like ``aws s3 cp`` will automatically use either\nPutObject or the multipart upload API depending on the file size,\nso using ``onCloudTrailWriteObject`` may be preferable.\n\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    on_cloud_trail_write_object: typing.Optional[list[models.aws_s3.BucketDefOnCloudTrailWriteObjectParams]] = pydantic.Field(None, description='Defines an AWS CloudWatch event that triggers when an object at the specified paths (keys) in this bucket are written to.\nThis includes\nthe events PutObject, CopyObject, and CompleteMultipartUpload.\n\nNote that some tools like ``aws s3 cp`` will automatically use either\nPutObject or the multipart upload API depending on the file size,\nso using this method may be preferable to ``onCloudTrailPutObject``.\n\nRequires that there exists at least one CloudTrail Trail in your account\nthat captures the event. This method will not create the Trail.')
    s3_url_for_object: typing.Optional[list[models.aws_s3.BucketDefS3UrlForObjectParams]] = pydantic.Field(None, description='The S3 URL of an S3 object. For example:.\n- ``s3://onlybucket``\n- ``s3://bucket/key``')
    transfer_acceleration_url_for_object: typing.Optional[list[models.aws_s3.BucketDefTransferAccelerationUrlForObjectParams]] = pydantic.Field(None, description='The https Transfer Acceleration URL of an S3 object.\nSpecify ``dualStack: true`` at the options\nfor dual-stack endpoint (connect to the bucket over IPv6). For example:\n\n- ``https://bucket.s3-accelerate.amazonaws.com``\n- ``https://bucket.s3-accelerate.amazonaws.com/key``')
    url_for_object: typing.Optional[list[models.aws_s3.BucketDefUrlForObjectParams]] = pydantic.Field(None, description='The https URL of an S3 object. Specify ``regional: false`` at the options for non-regional URLs. For example:.\n- ``https://s3.us-west-1.amazonaws.com/onlybucket``\n- ``https://s3.us-west-1.amazonaws.com/bucket/key``\n- ``https://s3.cn-north-1.amazonaws.com.cn/china-bucket/mykey``')
    validate_bucket_name: typing.Optional[list[models.aws_s3.BucketDefValidateBucketNameParams]] = pydantic.Field(None, description='Thrown an exception if the given bucket name is not valid.')
    virtual_hosted_url_for_object: typing.Optional[list[models.aws_s3.BucketDefVirtualHostedUrlForObjectParams]] = pydantic.Field(None, description='The virtual hosted-style URL of an S3 object. Specify ``regional: false`` at the options for non-regional URL. For example:.\n- ``https://only-bucket.s3.us-west-1.amazonaws.com``\n- ``https://bucket.s3.us-west-1.amazonaws.com/key``\n- ``https://bucket.s3.amazonaws.com/key``\n- ``https://china-bucket.s3.cn-north-1.amazonaws.com.cn/mykey``')

class BucketDefAddCorsRuleParams(pydantic.BaseModel):
    allowed_methods: typing.Sequence[aws_cdk.aws_s3.HttpMethods] = pydantic.Field(..., description='An HTTP method that you allow the origin to execute.\n')
    allowed_origins: typing.Sequence[str] = pydantic.Field(..., description='One or more origins you want customers to be able to access the bucket from.\n')
    allowed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Headers that are specified in the Access-Control-Request-Headers header. Default: - No headers allowed.\n')
    exposed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more headers in the response that you want customers to be able to access from their applications. Default: - No headers exposed.\n')
    id: typing.Optional[str] = pydantic.Field(None, description='A unique identifier for this rule. Default: - No id specified.\n')
    max_age: typing.Union[int, float, None] = pydantic.Field(None, description='The time in seconds that your browser is to cache the preflight response for the specified resource. Default: - No caching.')
    ...

class BucketDefAddEventNotificationParams(pydantic.BaseModel):
    event: aws_cdk.aws_s3.EventType = pydantic.Field(..., description='The event to trigger the notification.\n')
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (Lambda, SNS Topic or SQS Queue).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)
    ...

class BucketDefAddInventoryParams(pydantic.BaseModel):
    destination: typing.Union[models.aws_s3.InventoryDestinationDef, dict[str, typing.Any]] = pydantic.Field(..., description='The destination of the inventory.\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether the inventory is enabled or disabled. Default: true\n')
    format: typing.Optional[aws_cdk.aws_s3.InventoryFormat] = pydantic.Field(None, description='The format of the inventory. Default: InventoryFormat.CSV\n')
    frequency: typing.Optional[aws_cdk.aws_s3.InventoryFrequency] = pydantic.Field(None, description='Frequency at which the inventory should be generated. Default: InventoryFrequency.WEEKLY\n')
    include_object_versions: typing.Optional[aws_cdk.aws_s3.InventoryObjectVersion] = pydantic.Field(None, description='If the inventory should contain all the object versions or only the current one. Default: InventoryObjectVersion.ALL\n')
    inventory_id: typing.Optional[str] = pydantic.Field(None, description='The inventory configuration ID. Default: - generated ID.\n')
    objects_prefix: typing.Optional[str] = pydantic.Field(None, description='The inventory will only include objects that meet the prefix filter criteria. Default: - No objects prefix\n')
    optional_fields: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of optional fields to be included in the inventory result. Default: - No optional fields.')
    ...

class BucketDefAddLifecycleRuleParams(pydantic.BaseModel):
    abort_incomplete_multipart_upload_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies a lifecycle rule that aborts incomplete multipart uploads to an Amazon S3 bucket. The AbortIncompleteMultipartUpload property type creates a lifecycle rule that aborts incomplete multipart uploads to an Amazon S3 bucket. When Amazon S3 aborts a multipart upload, it deletes all parts associated with the multipart upload. The underlying configuration is expressed in whole numbers of days. Providing a Duration that does not represent a whole number of days will result in a runtime or deployment error. Default: - Incomplete uploads are never aborted\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether this rule is enabled. Default: true\n')
    expiration: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Indicates the number of days after creation when objects are deleted from Amazon S3 and Amazon Glacier. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. The underlying configuration is expressed in whole numbers of days. Providing a Duration that does not represent a whole number of days will result in a runtime or deployment error. Default: - No expiration timeout\n')
    expiration_date: typing.Optional[datetime.datetime] = pydantic.Field(None, description='Indicates when objects are deleted from Amazon S3 and Amazon Glacier. The date value must be in ISO 8601 format. The time is always midnight UTC. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. Default: - No expiration date\n')
    expired_object_delete_marker: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether Amazon S3 will remove a delete marker with no noncurrent versions. If set to true, the delete marker will be expired. Default: false\n')
    id: typing.Optional[str] = pydantic.Field(None, description='A unique identifier for this rule. The value cannot be more than 255 characters.\n')
    noncurrent_version_expiration: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time between when a new version of the object is uploaded to the bucket and when old versions of the object expire. For buckets with versioning enabled (or suspended), specifies the time, in days, between when a new version of the object is uploaded to the bucket and when old versions of the object expire. When object versions expire, Amazon S3 permanently deletes them. If you specify a transition and expiration time, the expiration time must be later than the transition time. The underlying configuration is expressed in whole numbers of days. Providing a Duration that does not represent a whole number of days will result in a runtime or deployment error. Default: - No noncurrent version expiration\n')
    noncurrent_versions_to_retain: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates a maximum number of noncurrent versions to retain. If there are this many more noncurrent versions, Amazon S3 permanently deletes them. Default: - No noncurrent versions to retain\n')
    noncurrent_version_transitions: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.NoncurrentVersionTransitionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more transition rules that specify when non-current objects transition to a specified storage class. Only for for buckets with versioning enabled (or suspended). If you specify a transition and expiration time, the expiration time must be later than the transition time.\n')
    object_size_greater_than: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the minimum object size in bytes for this rule to apply to. Default: - No rule\n')
    object_size_less_than: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum object size in bytes for this rule to apply to. Default: - No rule\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='Object key prefix that identifies one or more objects to which this rule applies. Default: - Rule applies to all objects\n')
    tag_filters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The TagFilter property type specifies tags to use to identify a subset of objects for an Amazon S3 bucket. Default: - Rule applies to all objects\n')
    transitions: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.TransitionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more transition rules that specify when an object transitions to a specified storage class. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. Default: - No transition rules')
    ...

class BucketDefAddMetricParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The ID used to identify the metrics configuration.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix that an object must have to be included in the metrics results.\n')
    tag_filters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description="Specifies a list of tag filters to use as a metrics configuration filter. The metrics configuration includes only objects that meet the filter's criteria.")
    ...

class BucketDefAddObjectCreatedNotificationParams(pydantic.BaseModel):
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (see onEvent).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)
    ...

class BucketDefAddObjectRemovedNotificationParams(pydantic.BaseModel):
    dest: typing.Union[models.aws_s3_notifications.LambdaDestinationDef, models.aws_s3_notifications.SnsDestinationDef, models.aws_s3_notifications.SqsDestinationDef] = pydantic.Field(..., description='The notification destination (see onEvent).\n')
    filters: list[models.aws_s3.NotificationKeyFilterDef] = pydantic.Field(...)
    ...

class BucketDefAddToResourcePolicyParams(pydantic.BaseModel):
    permission: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description="the policy statement to be added to the bucket's policy.\n")
    ...

class BucketDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class BucketDefArnForObjectsParams(pydantic.BaseModel):
    key_pattern: str = pydantic.Field(..., description='-')
    ...

class BucketDefFromBucketArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    id: str = pydantic.Field(..., description='-\n')
    bucket_arn: str = pydantic.Field(..., description='-')
    ...

class BucketDefFromBucketAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The parent creating construct (usually ``this``).\n')
    id: str = pydantic.Field(..., description="The construct's name.\n")
    account: typing.Optional[str] = pydantic.Field(None, description="The account this existing bucket belongs to. Default: - it's assumed the bucket belongs to the same account as the scope it's being imported into\n")
    bucket_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of the bucket. At least one of bucketArn or bucketName must be defined in order to initialize a bucket ref.\n')
    bucket_domain_name: typing.Optional[str] = pydantic.Field(None, description='The domain name of the bucket. Default: - Inferred from bucket name\n')
    bucket_dual_stack_domain_name: typing.Optional[str] = pydantic.Field(None, description='The IPv6 DNS name of the specified bucket.\n')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description="The name of the bucket. If the underlying value of ARN is a string, the name will be parsed from the ARN. Otherwise, the name is optional, but some features that require the bucket name such as auto-creating a bucket policy, won't work.\n")
    bucket_regional_domain_name: typing.Optional[str] = pydantic.Field(None, description='The regional domain name of the specified bucket.\n')
    bucket_website_new_url_format: typing.Optional[bool] = pydantic.Field(None, description='(deprecated) Force the format of the website URL of the bucket. This should be true for regions launched since 2014. Default: - inferred from available region information, ``false`` otherwise\n')
    bucket_website_url: typing.Optional[str] = pydantic.Field(None, description='The website URL of the bucket (if static web hosting is enabled). Default: - Inferred from bucket name and region\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key associated with this bucket. Default: - no encryption key\n')
    is_website: typing.Optional[bool] = pydantic.Field(None, description='If this bucket has been configured for static website hosting. Default: false\n')
    notifications_handler_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to be used by the notifications handler. Default: - a new role will be created.\n')
    region: typing.Optional[str] = pydantic.Field(None, description="The region this existing bucket is in. Features that require the region (e.g. ``bucketWebsiteUrl``) won't fully work if the region cannot be correctly inferred. Default: - it's assumed the bucket is in the same region as the scope it's being imported into")
    ...

class BucketDefFromBucketNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    id: str = pydantic.Field(..., description='-\n')
    bucket_name: str = pydantic.Field(..., description='-')
    ...

class BucketDefFromCfnBucketParams(pydantic.BaseModel):
    cfn_bucket: models.aws_s3.CfnBucketDef = pydantic.Field(..., description='-')
    ...

class BucketDefGrantDeleteParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketDefGrantPublicAccessParams(pydantic.BaseModel):
    key_prefix: typing.Optional[str] = pydantic.Field(None, description='the prefix of S3 object keys (e.g. ``home/*``). Default is "*".\n')
    allowed_actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketDefGrantPutParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketDefGrantPutAclParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    objects_key_pattern: typing.Optional[str] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketDefGrantReadParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='The principal.\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description="Restrict the permission to a certain key pattern (default '*').")
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketDefGrantReadWriteParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketDefGrantWriteParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    objects_key_pattern: typing.Any = pydantic.Field(None, description='-\n')
    allowed_action_patterns: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class BucketDefOnCloudTrailEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class BucketDefOnCloudTrailPutObjectParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class BucketDefOnCloudTrailWriteObjectParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id of the rule.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...

class BucketDefS3UrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the S3 URL of the bucket is returned.\n')
    ...

class BucketDefTransferAccelerationUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    dual_stack: typing.Optional[bool] = pydantic.Field(None, description='Dual-stack support to connect to the bucket over IPv6. Default: - false\n')
    ...

class BucketDefUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    ...

class BucketDefValidateBucketNameParams(pydantic.BaseModel):
    physical_name: str = pydantic.Field(..., description='name of the bucket.')
    ...

class BucketDefVirtualHostedUrlForObjectParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='The S3 key of the object. If not specified, the URL of the bucket is returned.\n')
    regional: typing.Optional[bool] = pydantic.Field(None, description='Specifies the URL includes the region. Default: - true\n')
    ...


#  autogenerated from aws_cdk.aws_s3.BucketPolicy
class BucketPolicyDef(BaseConstruct):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon S3 bucket that the policy applies to.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Policy to apply when the policy is removed from this stack. Default: - RemovalPolicy.DESTROY.')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'removal_policy']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_cfn_bucket_policy']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BucketPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_cfn_bucket_policy']
    ...


    from_cfn_bucket_policy: typing.Optional[models.aws_s3.BucketPolicyDefFromCfnBucketPolicyParams] = pydantic.Field(None, description='Create a mutable ``BucketPolicy`` from a ``CfnBucketPolicy``.')
    resource_config: typing.Optional[models.aws_s3.BucketPolicyDefConfig] = pydantic.Field(None)


class BucketPolicyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    document_config: typing.Optional[models.aws_iam.PolicyDocumentDefConfig] = pydantic.Field(None)

class BucketPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    removal_policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='the RemovalPolicy to set.')
    ...

class BucketPolicyDefFromCfnBucketPolicyParams(pydantic.BaseModel):
    cfn_bucket_policy: models.aws_s3.CfnBucketPolicyDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_s3.BlockPublicAccessOptions
class BlockPublicAccessOptionsDef(BaseStruct):
    block_public_acls: typing.Optional[bool] = pydantic.Field(None, description='Whether to block public ACLs.')
    block_public_policy: typing.Optional[bool] = pydantic.Field(None, description='Whether to block public policy.\n')
    ignore_public_acls: typing.Optional[bool] = pydantic.Field(None, description='Whether to ignore public ACLs.\n')
    restrict_public_buckets: typing.Optional[bool] = pydantic.Field(None, description='Whether to restrict public access.\n\n:exampleMetadata: infused\n\nExample::\n\n    bucket = s3.Bucket(self, "MyBlockedBucket",\n        block_public_access=s3.BlockPublicAccess(block_public_policy=True)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['block_public_acls', 'block_public_policy', 'ignore_public_acls', 'restrict_public_buckets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BlockPublicAccessOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.BucketAttributes
class BucketAttributesDef(BaseStruct):
    account: typing.Optional[str] = pydantic.Field(None, description="The account this existing bucket belongs to. Default: - it's assumed the bucket belongs to the same account as the scope it's being imported into\n")
    bucket_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of the bucket. At least one of bucketArn or bucketName must be defined in order to initialize a bucket ref.\n')
    bucket_domain_name: typing.Optional[str] = pydantic.Field(None, description='The domain name of the bucket. Default: - Inferred from bucket name\n')
    bucket_dual_stack_domain_name: typing.Optional[str] = pydantic.Field(None, description='The IPv6 DNS name of the specified bucket.\n')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description="The name of the bucket. If the underlying value of ARN is a string, the name will be parsed from the ARN. Otherwise, the name is optional, but some features that require the bucket name such as auto-creating a bucket policy, won't work.\n")
    bucket_regional_domain_name: typing.Optional[str] = pydantic.Field(None, description='The regional domain name of the specified bucket.\n')
    bucket_website_new_url_format: typing.Optional[bool] = pydantic.Field(None, description='(deprecated) Force the format of the website URL of the bucket. This should be true for regions launched since 2014. Default: - inferred from available region information, ``false`` otherwise\n')
    bucket_website_url: typing.Optional[str] = pydantic.Field(None, description='The website URL of the bucket (if static web hosting is enabled). Default: - Inferred from bucket name and region\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key associated with this bucket. Default: - no encryption key\n')
    is_website: typing.Optional[bool] = pydantic.Field(None, description='If this bucket has been configured for static website hosting. Default: false\n')
    notifications_handler_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to be used by the notifications handler. Default: - a new role will be created.\n')
    region: typing.Optional[str] = pydantic.Field(None, description='The region this existing bucket is in. Features that require the region (e.g. ``bucketWebsiteUrl``) won\'t fully work if the region cannot be correctly inferred. Default: - it\'s assumed the bucket is in the same region as the scope it\'s being imported into\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_lambda: lambda.Function\n\n    bucket = s3.Bucket.from_bucket_attributes(self, "ImportedBucket",\n        bucket_arn="arn:aws:s3:::my-bucket"\n    )\n\n    # now you can just call methods on the bucket\n    bucket.add_event_notification(s3.EventType.OBJECT_CREATED, s3n.LambdaDestination(my_lambda),\n        prefix="home/myusername/*"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['account', 'bucket_arn', 'bucket_domain_name', 'bucket_dual_stack_domain_name', 'bucket_name', 'bucket_regional_domain_name', 'bucket_website_new_url_format', 'bucket_website_url', 'encryption_key', 'is_website', 'notifications_handler_role', 'region']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BucketAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.BucketMetrics
class BucketMetricsDef(BaseStruct):
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix that an object must have to be included in the metrics results.\n')
    tag_filters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='Specifies a list of tag filters to use as a metrics configuration filter. The metrics configuration includes only objects that meet the filter\'s criteria.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # tag_filters: Any\n\n    bucket_metrics = s3.BucketMetrics(\n        id="id",\n\n        # the properties below are optional\n        prefix="prefix",\n        tag_filters={\n            "tag_filters_key": tag_filters\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['prefix', 'tag_filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BucketMetrics'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.BucketNotificationDestinationConfig
class BucketNotificationDestinationConfigDef(BaseStruct):
    arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of the destination (i.e. Lambda, SNS, SQS).\n')
    type: typing.Union[aws_cdk.aws_s3.BucketNotificationDestinationType, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The notification type.\n')
    dependencies: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='Any additional dependencies that should be resolved before the bucket notification can be configured (for example, the SNS Topic Policy resource).\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n    import constructs as constructs\n\n    # dependable: constructs.IDependable\n\n    bucket_notification_destination_config = s3.BucketNotificationDestinationConfig(\n        arn="arn",\n        type=s3.BucketNotificationDestinationType.LAMBDA,\n\n        # the properties below are optional\n        dependencies=[dependable]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['arn', 'type', 'dependencies']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BucketNotificationDestinationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.BucketPolicyProps
class BucketPolicyPropsDef(BaseStruct):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon S3 bucket that the policy applies to.')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Policy to apply when the policy is removed from this stack. Default: - RemovalPolicy.DESTROY.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_s3 as s3\n\n    # bucket: s3.Bucket\n\n    bucket_policy_props = s3.BucketPolicyProps(\n        bucket=bucket,\n\n        # the properties below are optional\n        removal_policy=cdk.RemovalPolicy.DESTROY\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'removal_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BucketPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.BucketProps
class BucketPropsDef(BaseStruct):
    access_control: typing.Optional[aws_cdk.aws_s3.BucketAccessControl] = pydantic.Field(None, description='Specifies a canned ACL that grants predefined permissions to the bucket. Default: BucketAccessControl.PRIVATE')
    auto_delete_objects: typing.Optional[bool] = pydantic.Field(None, description='Whether all objects should be automatically deleted when the bucket is removed from the stack or when the stack is deleted. Requires the ``removalPolicy`` to be set to ``RemovalPolicy.DESTROY``. **Warning** if you have deployed a bucket with ``autoDeleteObjects: true``, switching this to ``false`` in a CDK version *before* ``1.126.0`` will lead to all objects in the bucket being deleted. Be sure to update your bucket resources by deploying with CDK version ``1.126.0`` or later **before** switching this value to ``false``. Default: false\n')
    block_public_access: typing.Optional[models.aws_s3.BlockPublicAccessDef] = pydantic.Field(None, description="The block public access configuration of this bucket. Default: - CloudFormation defaults will apply. New buckets and objects don't allow public access, but users can modify bucket policies or object permissions to allow public access\n")
    bucket_key_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether Amazon S3 should use its own intermediary key to generate data keys. Only relevant when using KMS for encryption. - If not enabled, every object GET and PUT will cause an API call to KMS (with the attendant cost implications of that). - If enabled, S3 will use its own time-limited key instead. Only relevant, when Encryption is set to ``BucketEncryption.KMS`` or ``BucketEncryption.KMS_MANAGED``. Default: - false\n')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description='Physical name of this bucket. Default: - Assigned by CloudFormation (recommended).\n')
    cors: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.CorsRuleDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The CORS configuration of this bucket. Default: - No CORS configuration.\n')
    encryption: typing.Optional[aws_cdk.aws_s3.BucketEncryption] = pydantic.Field(None, description='The kind of server-side encryption to apply to this bucket. If you choose KMS, you can specify a KMS key via ``encryptionKey``. If encryption key is not specified, a key will automatically be created. Default: - ``KMS`` if ``encryptionKey`` is specified, or ``UNENCRYPTED`` otherwise. But if ``UNENCRYPTED`` is specified, the bucket will be encrypted as ``S3_MANAGED`` automatically.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='External KMS key to use for bucket encryption. The ``encryption`` property must be either not specified or set to ``KMS`` or ``DSSE``. An error will be emitted if ``encryption`` is set to ``UNENCRYPTED`` or ``S3_MANAGED``. Default: - If ``encryption`` is set to ``KMS`` and this property is undefined, a new KMS key will be created and associated with this bucket.\n')
    enforce_ssl: typing.Optional[bool] = pydantic.Field(None, description='Enforces SSL for requests. S3.5 of the AWS Foundational Security Best Practices Regarding S3. Default: false\n')
    event_bridge_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether this bucket should send notifications to Amazon EventBridge or not. Default: false\n')
    intelligent_tiering_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.IntelligentTieringConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Inteligent Tiering Configurations. Default: No Intelligent Tiiering Configurations.\n')
    inventories: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.InventoryDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The inventory configuration of the bucket. Default: - No inventory configuration\n')
    lifecycle_rules: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.LifecycleRuleDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Rules that define how Amazon S3 manages objects during their lifetime. Default: - No lifecycle rules.\n')
    metrics: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketMetricsDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metrics configuration of this bucket. Default: - No metrics configuration.\n')
    minimum_tls_version: typing.Union[int, float, None] = pydantic.Field(None, description='Enforces minimum TLS version for requests. Requires ``enforceSSL`` to be enabled. Default: No minimum TLS version is enforced.\n')
    notifications_handler_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to be used by the notifications handler. Default: - a new role will be created.\n')
    object_lock_default_retention: typing.Optional[models.aws_s3.ObjectLockRetentionDef] = pydantic.Field(None, description='The default retention mode and rules for S3 Object Lock. Default retention can be configured after a bucket is created if the bucket already has object lock enabled. Enabling object lock for existing buckets is not supported. Default: no default retention period\n')
    object_lock_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enable object lock on the bucket. Enabling object lock for existing buckets is not supported. Object lock must be enabled when the bucket is created. Default: false, unless objectLockDefaultRetention is set (then, true)\n')
    object_ownership: typing.Optional[aws_cdk.aws_s3.ObjectOwnership] = pydantic.Field(None, description='The objectOwnership of the bucket. Default: - No ObjectOwnership configuration, uploading account will own the object.\n')
    public_read_access: typing.Optional[bool] = pydantic.Field(None, description='Grants public read access to all objects in the bucket. Similar to calling ``bucket.grantPublicAccess()`` Default: false\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Policy to apply when the bucket is removed from this stack. Default: - The bucket will be orphaned.\n')
    server_access_logs_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='Destination bucket for the server access logs. Default: - If "serverAccessLogsPrefix" undefined - access logs disabled, otherwise - log to current bucket.\n')
    server_access_logs_prefix: typing.Optional[str] = pydantic.Field(None, description='Optional log file prefix to use for the bucket\'s access logs. If defined without "serverAccessLogsBucket", enables access logs to current bucket with this prefix. Default: - No log file prefix\n')
    transfer_acceleration: typing.Optional[bool] = pydantic.Field(None, description='Whether this bucket should have transfer acceleration turned on or not. Default: false\n')
    versioned: typing.Optional[bool] = pydantic.Field(None, description='Whether this bucket should have versioning turned on or not. Default: false (unless object lock is enabled, then true)\n')
    website_error_document: typing.Optional[str] = pydantic.Field(None, description='The name of the error document (e.g. "404.html") for the website. ``websiteIndexDocument`` must also be set if this is set. Default: - No error document.\n')
    website_index_document: typing.Optional[str] = pydantic.Field(None, description='The name of the index document (e.g. "index.html") for the website. Enables static website hosting for this bucket. Default: - No index document.\n')
    website_redirect: typing.Union[models.aws_s3.RedirectTargetDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the redirect behavior of all requests to a website endpoint of a bucket. If you specify this property, you can\'t specify "websiteIndexDocument", "websiteErrorDocument" nor , "websiteRoutingRules". Default: - No redirection.\n')
    website_routing_rules: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.RoutingRuleDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Rules that define when a redirect is applied and the redirect behavior. Default: - No redirection rules.\n\n:exampleMetadata: infused\n\nExample::\n\n    source_bucket = s3.Bucket(self, "MyBucket",\n        versioned=True\n    )\n\n    pipeline = codepipeline.Pipeline(self, "MyPipeline")\n    source_output = codepipeline.Artifact()\n    source_action = codepipeline_actions.S3SourceAction(\n        action_name="S3Source",\n        bucket=source_bucket,\n        bucket_key="path/to/file.zip",\n        output=source_output\n    )\n    pipeline.add_stage(\n        stage_name="Source",\n        actions=[source_action]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_control', 'auto_delete_objects', 'block_public_access', 'bucket_key_enabled', 'bucket_name', 'cors', 'encryption', 'encryption_key', 'enforce_ssl', 'event_bridge_enabled', 'intelligent_tiering_configurations', 'inventories', 'lifecycle_rules', 'metrics', 'minimum_tls_version', 'notifications_handler_role', 'object_lock_default_retention', 'object_lock_enabled', 'object_ownership', 'public_read_access', 'removal_policy', 'server_access_logs_bucket', 'server_access_logs_prefix', 'transfer_acceleration', 'versioned', 'website_error_document', 'website_index_document', 'website_redirect', 'website_routing_rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.BucketProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnAccessPoint.PublicAccessBlockConfigurationProperty
class CfnAccessPoint_PublicAccessBlockConfigurationPropertyDef(BaseStruct):
    block_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should block public access control lists (ACLs) for this bucket and objects in this bucket. Setting this element to ``TRUE`` causes the following behavior: - PUT Bucket ACL and PUT Object ACL calls fail if the specified ACL is public. - PUT Object calls fail if the request includes a public ACL. - PUT Bucket calls fail if the request includes a public ACL. Enabling this setting doesn't affect existing policies or ACLs.\n")
    block_public_policy: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should block public bucket policies for this bucket. Setting this element to ``TRUE`` causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access. Enabling this setting doesn't affect existing bucket policies.\n")
    ignore_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should ignore public ACLs for this bucket and objects in this bucket. Setting this element to ``TRUE`` causes Amazon S3 to ignore all public ACLs on this bucket and objects in this bucket. Enabling this setting doesn't affect the persistence of any existing ACLs and doesn't prevent new public ACLs from being set.\n")
    restrict_public_buckets: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should restrict public bucket policies for this bucket. Setting this element to ``TRUE`` restricts access to this bucket to only AWS service principals and authorized users within this account if the bucket has a public policy. Enabling this setting doesn't affect previously stored bucket policies, except that public and cross-account access within any public bucket policy, including non-public delegation to specific accounts, is blocked.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-accesspoint-publicaccessblockconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    public_access_block_configuration_property = s3.CfnAccessPoint.PublicAccessBlockConfigurationProperty(\n        block_public_acls=False,\n        block_public_policy=False,\n        ignore_public_acls=False,\n        restrict_public_buckets=False\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['block_public_acls', 'block_public_policy', 'ignore_public_acls', 'restrict_public_buckets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnAccessPoint.PublicAccessBlockConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnAccessPoint.VpcConfigurationProperty
class CfnAccessPoint_VpcConfigurationPropertyDef(BaseStruct):
    vpc_id: typing.Optional[str] = pydantic.Field(None, description='If this field is specified, the access point will only allow connections from the specified VPC ID.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-accesspoint-vpcconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    vpc_configuration_property = s3.CfnAccessPoint.VpcConfigurationProperty(\n        vpc_id="vpcId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnAccessPoint.VpcConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.AbortIncompleteMultipartUploadProperty
class CfnBucket_AbortIncompleteMultipartUploadPropertyDef(BaseStruct):
    days_after_initiation: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the number of days after which Amazon S3 stops an incomplete multipart upload.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-abortincompletemultipartupload.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    abort_incomplete_multipart_upload_property = s3.CfnBucket.AbortIncompleteMultipartUploadProperty(\n        days_after_initiation=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['days_after_initiation']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.AbortIncompleteMultipartUploadProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.AccelerateConfigurationProperty
class CfnBucket_AccelerateConfigurationPropertyDef(BaseStruct):
    acceleration_status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the transfer acceleration status of the bucket.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-accelerateconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    accelerate_configuration_property = s3.CfnBucket.AccelerateConfigurationProperty(\n        acceleration_status="accelerationStatus"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['acceleration_status']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.AccelerateConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.AccessControlTranslationProperty
class CfnBucket_AccessControlTranslationPropertyDef(BaseStruct):
    owner: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the replica ownership. For default and valid values, see `PUT bucket replication <https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html>`_ in the *Amazon S3 API Reference* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-accesscontroltranslation.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    access_control_translation_property = s3.CfnBucket.AccessControlTranslationProperty(\n        owner="owner"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['owner']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.AccessControlTranslationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.AnalyticsConfigurationProperty
class CfnBucket_AnalyticsConfigurationPropertyDef(BaseStruct):
    storage_class_analysis: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnBucket_StorageClassAnalysisPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Contains data related to access patterns to be collected and made available to analyze the tradeoffs between different storage classes.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix that an object must have to be included in the analytics results.\n')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The tags to use when evaluating an analytics filter. The analytics only includes objects that meet the filter\'s criteria. If no filter is specified, all of the contents of the bucket are included in the analysis.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-analyticsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    analytics_configuration_property = s3.CfnBucket.AnalyticsConfigurationProperty(\n        id="id",\n        storage_class_analysis=s3.CfnBucket.StorageClassAnalysisProperty(\n            data_export=s3.CfnBucket.DataExportProperty(\n                destination=s3.CfnBucket.DestinationProperty(\n                    bucket_arn="bucketArn",\n                    format="format",\n\n                    # the properties below are optional\n                    bucket_account_id="bucketAccountId",\n                    prefix="prefix"\n                ),\n                output_schema_version="outputSchemaVersion"\n            )\n        ),\n\n        # the properties below are optional\n        prefix="prefix",\n        tag_filters=[s3.CfnBucket.TagFilterProperty(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_class_analysis', 'prefix', 'tag_filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.AnalyticsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.BucketEncryptionProperty
class CfnBucket_BucketEncryptionPropertyDef(BaseStruct):
    server_side_encryption_configuration: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ServerSideEncryptionRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the default server-side-encryption configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-bucketencryption.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    bucket_encryption_property = s3.CfnBucket.BucketEncryptionProperty(\n        server_side_encryption_configuration=[s3.CfnBucket.ServerSideEncryptionRuleProperty(\n            bucket_key_enabled=False,\n            server_side_encryption_by_default=s3.CfnBucket.ServerSideEncryptionByDefaultProperty(\n                sse_algorithm="sseAlgorithm",\n\n                # the properties below are optional\n                kms_master_key_id="kmsMasterKeyId"\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['server_side_encryption_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.BucketEncryptionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.CorsConfigurationProperty
class CfnBucket_CorsConfigurationPropertyDef(BaseStruct):
    cors_rules: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_CorsRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A set of origins and methods (cross-origin access that you want to allow). You can add up to 100 rules to the configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-corsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    cors_configuration_property = s3.CfnBucket.CorsConfigurationProperty(\n        cors_rules=[s3.CfnBucket.CorsRuleProperty(\n            allowed_methods=["allowedMethods"],\n            allowed_origins=["allowedOrigins"],\n\n            # the properties below are optional\n            allowed_headers=["allowedHeaders"],\n            exposed_headers=["exposedHeaders"],\n            id="id",\n            max_age=123\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cors_rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.CorsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.CorsRuleProperty
class CfnBucket_CorsRulePropertyDef(BaseStruct):
    allowed_methods: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An HTTP method that you allow the origin to run. *Allowed values* : ``GET`` | ``PUT`` | ``HEAD`` | ``POST`` | ``DELETE``\n')
    allowed_origins: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='One or more origins you want customers to be able to access the bucket from.\n')
    allowed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Headers that are specified in the ``Access-Control-Request-Headers`` header. These headers are allowed in a preflight OPTIONS request. In response to any preflight OPTIONS request, Amazon S3 returns any requested headers that are allowed.\n')
    exposed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more headers in the response that you want customers to be able to access from their applications (for example, from a JavaScript ``XMLHttpRequest`` object).\n')
    max_age: typing.Union[int, float, None] = pydantic.Field(None, description='The time in seconds that your browser is to cache the preflight response for the specified resource.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-corsrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    cors_rule_property = s3.CfnBucket.CorsRuleProperty(\n        allowed_methods=["allowedMethods"],\n        allowed_origins=["allowedOrigins"],\n\n        # the properties below are optional\n        allowed_headers=["allowedHeaders"],\n        exposed_headers=["exposedHeaders"],\n        id="id",\n        max_age=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allowed_methods', 'allowed_origins', 'allowed_headers', 'exposed_headers', 'max_age']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.CorsRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.DataExportProperty
class CfnBucket_DataExportPropertyDef(BaseStruct):
    destination: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnBucket_DestinationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The place to store the data for an analysis.\n')
    output_schema_version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The version of the output schema to use when exporting data. Must be ``V_1`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-dataexport.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    data_export_property = s3.CfnBucket.DataExportProperty(\n        destination=s3.CfnBucket.DestinationProperty(\n            bucket_arn="bucketArn",\n            format="format",\n\n            # the properties below are optional\n            bucket_account_id="bucketAccountId",\n            prefix="prefix"\n        ),\n        output_schema_version="outputSchemaVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination', 'output_schema_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.DataExportProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.DefaultRetentionProperty
class CfnBucket_DefaultRetentionPropertyDef(BaseStruct):
    days: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days that you want to specify for the default retention period. If Object Lock is turned on, you must specify ``Mode`` and specify either ``Days`` or ``Years`` .\n')
    mode: typing.Optional[str] = pydantic.Field(None, description='The default Object Lock retention mode you want to apply to new objects placed in the specified bucket. If Object Lock is turned on, you must specify ``Mode`` and specify either ``Days`` or ``Years`` .\n')
    years: typing.Union[int, float, None] = pydantic.Field(None, description='The number of years that you want to specify for the default retention period. If Object Lock is turned on, you must specify ``Mode`` and specify either ``Days`` or ``Years`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-defaultretention.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    default_retention_property = s3.CfnBucket.DefaultRetentionProperty(\n        days=123,\n        mode="mode",\n        years=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['days', 'mode', 'years']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.DefaultRetentionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.DeleteMarkerReplicationProperty
class CfnBucket_DeleteMarkerReplicationPropertyDef(BaseStruct):
    status: typing.Optional[str] = pydantic.Field(None, description='Indicates whether to replicate delete markers. Disabled by default.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-deletemarkerreplication.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    delete_marker_replication_property = s3.CfnBucket.DeleteMarkerReplicationProperty(\n        status="status"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.DeleteMarkerReplicationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.DestinationProperty
class CfnBucket_DestinationPropertyDef(BaseStruct):
    bucket_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the bucket to which data is exported.\n')
    format: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the file format used when exporting data to Amazon S3. *Allowed values* : ``CSV`` | ``ORC`` | ``Parquet``\n')
    bucket_account_id: typing.Optional[str] = pydantic.Field(None, description='The account ID that owns the destination S3 bucket. If no account ID is provided, the owner is not validated before exporting data. .. epigraph:: Although this value is optional, we strongly recommend that you set it to help prevent problems if the destination bucket ownership changes.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix to use when exporting data. The prefix is prepended to all results.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-destination.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    destination_property = s3.CfnBucket.DestinationProperty(\n        bucket_arn="bucketArn",\n        format="format",\n\n        # the properties below are optional\n        bucket_account_id="bucketAccountId",\n        prefix="prefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket_arn', 'format', 'bucket_account_id', 'prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.DestinationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.EncryptionConfigurationProperty
class CfnBucket_EncryptionConfigurationPropertyDef(BaseStruct):
    replica_kms_key_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the ID (Key ARN or Alias ARN) of the customer managed AWS KMS key stored in AWS Key Management Service (KMS) for the destination bucket. Amazon S3 uses this key to encrypt replica objects. Amazon S3 only supports symmetric encryption KMS keys. For more information, see `Asymmetric keys in AWS KMS <https://docs.aws.amazon.com//kms/latest/developerguide/symmetric-asymmetric.html>`_ in the *AWS Key Management Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-encryptionconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    encryption_configuration_property = s3.CfnBucket.EncryptionConfigurationProperty(\n        replica_kms_key_id="replicaKmsKeyId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['replica_kms_key_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.EncryptionConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.EventBridgeConfigurationProperty
class CfnBucket_EventBridgeConfigurationPropertyDef(BaseStruct):
    event_bridge_enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Enables delivery of events to Amazon EventBridge.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-eventbridgeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    event_bridge_configuration_property = s3.CfnBucket.EventBridgeConfigurationProperty(\n        event_bridge_enabled=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['event_bridge_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.EventBridgeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.FilterRuleProperty
class CfnBucket_FilterRulePropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The object key name prefix or suffix identifying one or more objects to which the filtering rule applies. The maximum length is 1,024 characters. Overlapping prefixes and suffixes are not supported. For more information, see `Configuring Event Notifications <https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html>`_ in the *Amazon S3 User Guide* .\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The value that the filter searches for in object key names.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-filterrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    filter_rule_property = s3.CfnBucket.FilterRuleProperty(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.FilterRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.IntelligentTieringConfigurationProperty
class CfnBucket_IntelligentTieringConfigurationPropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the status of the configuration.\n')
    tierings: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TieringPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies a list of S3 Intelligent-Tiering storage class tiers in the configuration. At least one tier must be defined in the list. At most, you can specify two tiers in the list, one for each available AccessTier: ``ARCHIVE_ACCESS`` and ``DEEP_ARCHIVE_ACCESS`` . .. epigraph:: You only need Intelligent Tiering Configuration enabled on a bucket if you want to automatically move objects stored in the Intelligent-Tiering storage class to Archive Access or Deep Archive Access tiers.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='An object key name prefix that identifies the subset of objects to which the rule applies.\n')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A container for a key-value pair.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-intelligenttieringconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    intelligent_tiering_configuration_property = s3.CfnBucket.IntelligentTieringConfigurationProperty(\n        id="id",\n        status="status",\n        tierings=[s3.CfnBucket.TieringProperty(\n            access_tier="accessTier",\n            days=123\n        )],\n\n        # the properties below are optional\n        prefix="prefix",\n        tag_filters=[s3.CfnBucket.TagFilterProperty(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status', 'tierings', 'prefix', 'tag_filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.IntelligentTieringConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.InventoryConfigurationProperty
class CfnBucket_InventoryConfigurationPropertyDef(BaseStruct):
    destination: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnBucket_DestinationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Contains information about where to publish the inventory results.\n')
    enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether the inventory is enabled or disabled. If set to ``True`` , an inventory list is generated. If set to ``False`` , no inventory list is generated.\n')
    included_object_versions: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Object versions to include in the inventory list. If set to ``All`` , the list includes all the object versions, which adds the version-related fields ``VersionId`` , ``IsLatest`` , and ``DeleteMarker`` to the list. If set to ``Current`` , the list does not contain these version-related fields.\n')
    schedule_frequency: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the schedule for generating inventory results. *Allowed values* : ``Daily`` | ``Weekly``\n')
    optional_fields: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Contains the optional fields that are included in the inventory results. *Valid values* : ``Size | LastModifiedDate | StorageClass | ETag | IsMultipartUploaded | ReplicationStatus | EncryptionStatus | ObjectLockRetainUntilDate | ObjectLockMode | ObjectLockLegalHoldStatus | IntelligentTieringAccessTier | BucketKeyStatus``\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='Specifies the inventory filter prefix.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-inventoryconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    inventory_configuration_property = s3.CfnBucket.InventoryConfigurationProperty(\n        destination=s3.CfnBucket.DestinationProperty(\n            bucket_arn="bucketArn",\n            format="format",\n\n            # the properties below are optional\n            bucket_account_id="bucketAccountId",\n            prefix="prefix"\n        ),\n        enabled=False,\n        id="id",\n        included_object_versions="includedObjectVersions",\n        schedule_frequency="scheduleFrequency",\n\n        # the properties below are optional\n        optional_fields=["optionalFields"],\n        prefix="prefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination', 'enabled', 'included_object_versions', 'schedule_frequency', 'optional_fields', 'prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.InventoryConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.LambdaConfigurationProperty
class CfnBucket_LambdaConfigurationPropertyDef(BaseStruct):
    event: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon S3 bucket event for which to invoke the AWS Lambda function. For more information, see `Supported Event Types <https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html>`_ in the *Amazon S3 User Guide* .\n')
    function: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the AWS Lambda function that Amazon S3 invokes when the specified event type occurs.\n')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The filtering rules that determine which objects invoke the AWS Lambda function. For example, you can create a filter so that only image files with a ``.jpg`` extension invoke the function when they are added to the Amazon S3 bucket.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-lambdaconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    lambda_configuration_property = s3.CfnBucket.LambdaConfigurationProperty(\n        event="event",\n        function="function",\n\n        # the properties below are optional\n        filter=s3.CfnBucket.NotificationFilterProperty(\n            s3_key=s3.CfnBucket.S3KeyFilterProperty(\n                rules=[s3.CfnBucket.FilterRuleProperty(\n                    name="name",\n                    value="value"\n                )]\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['event', 'function', 'filter']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.LambdaConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.LifecycleConfigurationProperty
class CfnBucket_LifecycleConfigurationPropertyDef(BaseStruct):
    rules: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A lifecycle rule for individual objects in an Amazon S3 bucket.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-lifecycleconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    lifecycle_configuration_property = s3.CfnBucket.LifecycleConfigurationProperty(\n        rules=[s3.CfnBucket.RuleProperty(\n            status="status",\n\n            # the properties below are optional\n            abort_incomplete_multipart_upload=s3.CfnBucket.AbortIncompleteMultipartUploadProperty(\n                days_after_initiation=123\n            ),\n            expiration_date=Date(),\n            expiration_in_days=123,\n            expired_object_delete_marker=False,\n            id="id",\n            noncurrent_version_expiration=s3.CfnBucket.NoncurrentVersionExpirationProperty(\n                noncurrent_days=123,\n\n                # the properties below are optional\n                newer_noncurrent_versions=123\n            ),\n            noncurrent_version_expiration_in_days=123,\n            noncurrent_version_transition=s3.CfnBucket.NoncurrentVersionTransitionProperty(\n                storage_class="storageClass",\n                transition_in_days=123,\n\n                # the properties below are optional\n                newer_noncurrent_versions=123\n            ),\n            noncurrent_version_transitions=[s3.CfnBucket.NoncurrentVersionTransitionProperty(\n                storage_class="storageClass",\n                transition_in_days=123,\n\n                # the properties below are optional\n                newer_noncurrent_versions=123\n            )],\n            object_size_greater_than=123,\n            object_size_less_than=123,\n            prefix="prefix",\n            tag_filters=[s3.CfnBucket.TagFilterProperty(\n                key="key",\n                value="value"\n            )],\n            transition=s3.CfnBucket.TransitionProperty(\n                storage_class="storageClass",\n\n                # the properties below are optional\n                transition_date=Date(),\n                transition_in_days=123\n            ),\n            transitions=[s3.CfnBucket.TransitionProperty(\n                storage_class="storageClass",\n\n                # the properties below are optional\n                transition_date=Date(),\n                transition_in_days=123\n            )]\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.LifecycleConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.LoggingConfigurationProperty
class CfnBucket_LoggingConfigurationPropertyDef(BaseStruct):
    destination_bucket_name: typing.Optional[str] = pydantic.Field(None, description='The name of the bucket where Amazon S3 should store server access log files. You can store log files in any bucket that you own. By default, logs are stored in the bucket where the ``LoggingConfiguration`` property is defined.\n')
    log_file_prefix: typing.Optional[str] = pydantic.Field(None, description='A prefix for all log object keys. If you store log files from multiple Amazon S3 buckets in a single bucket, you can use a prefix to distinguish which log files came from which bucket.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-loggingconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    logging_configuration_property = s3.CfnBucket.LoggingConfigurationProperty(\n        destination_bucket_name="destinationBucketName",\n        log_file_prefix="logFilePrefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination_bucket_name', 'log_file_prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.LoggingConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.MetricsConfigurationProperty
class CfnBucket_MetricsConfigurationPropertyDef(BaseStruct):
    access_point_arn: typing.Optional[str] = pydantic.Field(None, description="The access point that was used while performing operations on the object. The metrics configuration only includes objects that meet the filter's criteria.\n")
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix that an object must have to be included in the metrics results.\n')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies a list of tag filters to use as a metrics configuration filter. The metrics configuration includes only objects that meet the filter\'s criteria.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-metricsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    metrics_configuration_property = s3.CfnBucket.MetricsConfigurationProperty(\n        id="id",\n\n        # the properties below are optional\n        access_point_arn="accessPointArn",\n        prefix="prefix",\n        tag_filters=[s3.CfnBucket.TagFilterProperty(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_point_arn', 'prefix', 'tag_filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.MetricsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.MetricsProperty
class CfnBucket_MetricsPropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether the replication metrics are enabled.\n')
    event_threshold: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationTimeValuePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container specifying the time threshold for emitting the ``s3:Replication:OperationMissedThreshold`` event.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-metrics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    metrics_property = s3.CfnBucket.MetricsProperty(\n        status="status",\n\n        # the properties below are optional\n        event_threshold=s3.CfnBucket.ReplicationTimeValueProperty(\n            minutes=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status', 'event_threshold']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.MetricsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.NoncurrentVersionExpirationProperty
class CfnBucket_NoncurrentVersionExpirationPropertyDef(BaseStruct):
    noncurrent_days: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the number of days an object is noncurrent before Amazon S3 can perform the associated action. For information about the noncurrent days calculations, see `How Amazon S3 Calculates When an Object Became Noncurrent <https://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html#non-current-days-calculations>`_ in the *Amazon S3 User Guide* .\n')
    newer_noncurrent_versions: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies how many noncurrent versions Amazon S3 will retain. If there are this many more recent noncurrent versions, Amazon S3 will take the associated action. For more information about noncurrent versions, see `Lifecycle configuration elements <https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html>`_ in the *Amazon S3 User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-noncurrentversionexpiration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    noncurrent_version_expiration_property = s3.CfnBucket.NoncurrentVersionExpirationProperty(\n        noncurrent_days=123,\n\n        # the properties below are optional\n        newer_noncurrent_versions=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['noncurrent_days', 'newer_noncurrent_versions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.NoncurrentVersionExpirationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.NoncurrentVersionTransitionProperty
class CfnBucket_NoncurrentVersionTransitionPropertyDef(BaseStruct):
    storage_class: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The class of storage used to store the object.\n')
    transition_in_days: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the number of days an object is noncurrent before Amazon S3 can perform the associated action. For information about the noncurrent days calculations, see `How Amazon S3 Calculates How Long an Object Has Been Noncurrent <https://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html#non-current-days-calculations>`_ in the *Amazon S3 User Guide* .\n')
    newer_noncurrent_versions: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies how many noncurrent versions Amazon S3 will retain. If there are this many more recent noncurrent versions, Amazon S3 will take the associated action. For more information about noncurrent versions, see `Lifecycle configuration elements <https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html>`_ in the *Amazon S3 User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-noncurrentversiontransition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    noncurrent_version_transition_property = s3.CfnBucket.NoncurrentVersionTransitionProperty(\n        storage_class="storageClass",\n        transition_in_days=123,\n\n        # the properties below are optional\n        newer_noncurrent_versions=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_class', 'transition_in_days', 'newer_noncurrent_versions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.NoncurrentVersionTransitionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.NotificationConfigurationProperty
class CfnBucket_NotificationConfigurationPropertyDef(BaseStruct):
    event_bridge_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_EventBridgeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Enables delivery of events to Amazon EventBridge.\n')
    lambda_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_LambdaConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Describes the AWS Lambda functions to invoke and the events for which to invoke them.\n')
    queue_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_QueueConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The Amazon Simple Queue Service queues to publish messages to and the events for which to publish messages.\n')
    topic_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TopicConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The topic to which notifications are sent and the events for which notifications are generated.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-notificationconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    notification_configuration_property = s3.CfnBucket.NotificationConfigurationProperty(\n        event_bridge_configuration=s3.CfnBucket.EventBridgeConfigurationProperty(\n            event_bridge_enabled=False\n        ),\n        lambda_configurations=[s3.CfnBucket.LambdaConfigurationProperty(\n            event="event",\n            function="function",\n\n            # the properties below are optional\n            filter=s3.CfnBucket.NotificationFilterProperty(\n                s3_key=s3.CfnBucket.S3KeyFilterProperty(\n                    rules=[s3.CfnBucket.FilterRuleProperty(\n                        name="name",\n                        value="value"\n                    )]\n                )\n            )\n        )],\n        queue_configurations=[s3.CfnBucket.QueueConfigurationProperty(\n            event="event",\n            queue="queue",\n\n            # the properties below are optional\n            filter=s3.CfnBucket.NotificationFilterProperty(\n                s3_key=s3.CfnBucket.S3KeyFilterProperty(\n                    rules=[s3.CfnBucket.FilterRuleProperty(\n                        name="name",\n                        value="value"\n                    )]\n                )\n            )\n        )],\n        topic_configurations=[s3.CfnBucket.TopicConfigurationProperty(\n            event="event",\n            topic="topic",\n\n            # the properties below are optional\n            filter=s3.CfnBucket.NotificationFilterProperty(\n                s3_key=s3.CfnBucket.S3KeyFilterProperty(\n                    rules=[s3.CfnBucket.FilterRuleProperty(\n                        name="name",\n                        value="value"\n                    )]\n                )\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['event_bridge_configuration', 'lambda_configurations', 'queue_configurations', 'topic_configurations']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.NotificationConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.NotificationFilterProperty
class CfnBucket_NotificationFilterPropertyDef(BaseStruct):
    s3_key: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnBucket_S3KeyFilterPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A container for object key name prefix and suffix filtering rules.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-notificationfilter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    notification_filter_property = s3.CfnBucket.NotificationFilterProperty(\n        s3_key=s3.CfnBucket.S3KeyFilterProperty(\n            rules=[s3.CfnBucket.FilterRuleProperty(\n                name="name",\n                value="value"\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.NotificationFilterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ObjectLockConfigurationProperty
class CfnBucket_ObjectLockConfigurationPropertyDef(BaseStruct):
    object_lock_enabled: typing.Optional[str] = pydantic.Field(None, description='Indicates whether this bucket has an Object Lock configuration enabled. Enable ``ObjectLockEnabled`` when you apply ``ObjectLockConfiguration`` to a bucket.\n')
    rule: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ObjectLockRulePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the Object Lock rule for the specified object. Enable this rule when you apply ``ObjectLockConfiguration`` to a bucket. If Object Lock is turned on, bucket settings require both ``Mode`` and a period of either ``Days`` or ``Years`` . You cannot specify ``Days`` and ``Years`` at the same time. For more information, see `ObjectLockRule <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-objectlockrule.html>`_ and `DefaultRetention <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-defaultretention.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-objectlockconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    object_lock_configuration_property = s3.CfnBucket.ObjectLockConfigurationProperty(\n        object_lock_enabled="objectLockEnabled",\n        rule=s3.CfnBucket.ObjectLockRuleProperty(\n            default_retention=s3.CfnBucket.DefaultRetentionProperty(\n                days=123,\n                mode="mode",\n                years=123\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['object_lock_enabled', 'rule']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ObjectLockConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ObjectLockRuleProperty
class CfnBucket_ObjectLockRulePropertyDef(BaseStruct):
    default_retention: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DefaultRetentionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The default Object Lock retention mode and period that you want to apply to new objects placed in the specified bucket. If Object Lock is turned on, bucket settings require both ``Mode`` and a period of either ``Days`` or ``Years`` . You cannot specify ``Days`` and ``Years`` at the same time. For more information about allowable values for mode and period, see `DefaultRetention <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-defaultretention.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-objectlockrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    object_lock_rule_property = s3.CfnBucket.ObjectLockRuleProperty(\n        default_retention=s3.CfnBucket.DefaultRetentionProperty(\n            days=123,\n            mode="mode",\n            years=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['default_retention']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ObjectLockRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.OwnershipControlsProperty
class CfnBucket_OwnershipControlsPropertyDef(BaseStruct):
    rules: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_OwnershipControlsRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the container element for Object Ownership rules.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-ownershipcontrols.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    ownership_controls_property = s3.CfnBucket.OwnershipControlsProperty(\n        rules=[s3.CfnBucket.OwnershipControlsRuleProperty(\n            object_ownership="objectOwnership"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.OwnershipControlsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.OwnershipControlsRuleProperty
class CfnBucket_OwnershipControlsRulePropertyDef(BaseStruct):
    object_ownership: typing.Optional[str] = pydantic.Field(None, description='Specifies an Object Ownership rule. *Allowed values* : ``BucketOwnerEnforced`` | ``ObjectWriter`` | ``BucketOwnerPreferred``\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-ownershipcontrolsrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    ownership_controls_rule_property = s3.CfnBucket.OwnershipControlsRuleProperty(\n        object_ownership="objectOwnership"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['object_ownership']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.OwnershipControlsRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.PublicAccessBlockConfigurationProperty
class CfnBucket_PublicAccessBlockConfigurationPropertyDef(BaseStruct):
    block_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should block public access control lists (ACLs) for this bucket and objects in this bucket. Setting this element to ``TRUE`` causes the following behavior: - PUT Bucket ACL and PUT Object ACL calls fail if the specified ACL is public. - PUT Object calls fail if the request includes a public ACL. - PUT Bucket calls fail if the request includes a public ACL. Enabling this setting doesn't affect existing policies or ACLs.\n")
    block_public_policy: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should block public bucket policies for this bucket. Setting this element to ``TRUE`` causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access. Enabling this setting doesn't affect existing bucket policies.\n")
    ignore_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should ignore public ACLs for this bucket and objects in this bucket. Setting this element to ``TRUE`` causes Amazon S3 to ignore all public ACLs on this bucket and objects in this bucket. Enabling this setting doesn't affect the persistence of any existing ACLs and doesn't prevent new public ACLs from being set.\n")
    restrict_public_buckets: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should restrict public bucket policies for this bucket. Setting this element to ``TRUE`` restricts access to this bucket to only AWS service principals and authorized users within this account if the bucket has a public policy. Enabling this setting doesn't affect previously stored bucket policies, except that public and cross-account access within any public bucket policy, including non-public delegation to specific accounts, is blocked.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-publicaccessblockconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    public_access_block_configuration_property = s3.CfnBucket.PublicAccessBlockConfigurationProperty(\n        block_public_acls=False,\n        block_public_policy=False,\n        ignore_public_acls=False,\n        restrict_public_buckets=False\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['block_public_acls', 'block_public_policy', 'ignore_public_acls', 'restrict_public_buckets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.PublicAccessBlockConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.QueueConfigurationProperty
class CfnBucket_QueueConfigurationPropertyDef(BaseStruct):
    event: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon S3 bucket event about which you want to publish messages to Amazon SQS. For more information, see `Supported Event Types <https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html>`_ in the *Amazon S3 User Guide* .\n')
    queue: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the Amazon SQS queue to which Amazon S3 publishes a message when it detects events of the specified type. FIFO queues are not allowed when enabling an SQS queue as the event notification destination.\n')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The filtering rules that determine which objects trigger notifications. For example, you can create a filter so that Amazon S3 sends notifications only when image files with a ``.jpg`` extension are added to the bucket. For more information, see `Configuring event notifications using object key name filtering <https://docs.aws.amazon.com/AmazonS3/latest/user-guide/notification-how-to-filtering.html>`_ in the *Amazon S3 User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-queueconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    queue_configuration_property = s3.CfnBucket.QueueConfigurationProperty(\n        event="event",\n        queue="queue",\n\n        # the properties below are optional\n        filter=s3.CfnBucket.NotificationFilterProperty(\n            s3_key=s3.CfnBucket.S3KeyFilterProperty(\n                rules=[s3.CfnBucket.FilterRuleProperty(\n                    name="name",\n                    value="value"\n                )]\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['event', 'queue', 'filter']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.QueueConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.RedirectAllRequestsToProperty
class CfnBucket_RedirectAllRequestsToPropertyDef(BaseStruct):
    host_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name of the host where requests are redirected.\n')
    protocol: typing.Optional[str] = pydantic.Field(None, description='Protocol to use when redirecting requests. The default is the protocol that is used in the original request.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-redirectallrequeststo.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    redirect_all_requests_to_property = s3.CfnBucket.RedirectAllRequestsToProperty(\n        host_name="hostName",\n\n        # the properties below are optional\n        protocol="protocol"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['host_name', 'protocol']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.RedirectAllRequestsToProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.RedirectRuleProperty
class CfnBucket_RedirectRulePropertyDef(BaseStruct):
    host_name: typing.Optional[str] = pydantic.Field(None, description='The host name to use in the redirect request.\n')
    http_redirect_code: typing.Optional[str] = pydantic.Field(None, description='The HTTP redirect code to use on the response. Not required if one of the siblings is present.\n')
    protocol: typing.Optional[str] = pydantic.Field(None, description='Protocol to use when redirecting requests. The default is the protocol that is used in the original request.\n')
    replace_key_prefix_with: typing.Optional[str] = pydantic.Field(None, description='The object key prefix to use in the redirect request. For example, to redirect requests for all pages with prefix ``docs/`` (objects in the ``docs/`` folder) to ``documents/`` , you can set a condition block with ``KeyPrefixEquals`` set to ``docs/`` and in the Redirect set ``ReplaceKeyPrefixWith`` to ``/documents`` . Not required if one of the siblings is present. Can be present only if ``ReplaceKeyWith`` is not provided. .. epigraph:: Replacement must be made for object keys containing special characters (such as carriage returns) when using XML requests. For more information, see `XML related object key constraints <https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html#object-key-xml-related-constraints>`_ .\n')
    replace_key_with: typing.Optional[str] = pydantic.Field(None, description='The specific object key to use in the redirect request. For example, redirect request to ``error.html`` . Not required if one of the siblings is present. Can be present only if ``ReplaceKeyPrefixWith`` is not provided. .. epigraph:: Replacement must be made for object keys containing special characters (such as carriage returns) when using XML requests. For more information, see `XML related object key constraints <https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html#object-key-xml-related-constraints>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-redirectrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    redirect_rule_property = s3.CfnBucket.RedirectRuleProperty(\n        host_name="hostName",\n        http_redirect_code="httpRedirectCode",\n        protocol="protocol",\n        replace_key_prefix_with="replaceKeyPrefixWith",\n        replace_key_with="replaceKeyWith"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['host_name', 'http_redirect_code', 'protocol', 'replace_key_prefix_with', 'replace_key_with']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.RedirectRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicaModificationsProperty
class CfnBucket_ReplicaModificationsPropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether Amazon S3 replicates modifications on replicas. *Allowed values* : ``Enabled`` | ``Disabled``\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicamodifications.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replica_modifications_property = s3.CfnBucket.ReplicaModificationsProperty(\n        status="status"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicaModificationsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicationConfigurationProperty
class CfnBucket_ReplicationConfigurationPropertyDef(BaseStruct):
    role: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that Amazon S3 assumes when replicating objects. For more information, see `How to Set Up Replication <https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-how-setup.html>`_ in the *Amazon S3 User Guide* .\n')
    rules: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A container for one or more replication rules. A replication configuration must have at least one rule and can contain a maximum of 1,000 rules.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicationconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replication_configuration_property = s3.CfnBucket.ReplicationConfigurationProperty(\n        role="role",\n        rules=[s3.CfnBucket.ReplicationRuleProperty(\n            destination=s3.CfnBucket.ReplicationDestinationProperty(\n                bucket="bucket",\n\n                # the properties below are optional\n                access_control_translation=s3.CfnBucket.AccessControlTranslationProperty(\n                    owner="owner"\n                ),\n                account="account",\n                encryption_configuration=s3.CfnBucket.EncryptionConfigurationProperty(\n                    replica_kms_key_id="replicaKmsKeyId"\n                ),\n                metrics=s3.CfnBucket.MetricsProperty(\n                    status="status",\n\n                    # the properties below are optional\n                    event_threshold=s3.CfnBucket.ReplicationTimeValueProperty(\n                        minutes=123\n                    )\n                ),\n                replication_time=s3.CfnBucket.ReplicationTimeProperty(\n                    status="status",\n                    time=s3.CfnBucket.ReplicationTimeValueProperty(\n                        minutes=123\n                    )\n                ),\n                storage_class="storageClass"\n            ),\n            status="status",\n\n            # the properties below are optional\n            delete_marker_replication=s3.CfnBucket.DeleteMarkerReplicationProperty(\n                status="status"\n            ),\n            filter=s3.CfnBucket.ReplicationRuleFilterProperty(\n                and=s3.CfnBucket.ReplicationRuleAndOperatorProperty(\n                    prefix="prefix",\n                    tag_filters=[s3.CfnBucket.TagFilterProperty(\n                        key="key",\n                        value="value"\n                    )]\n                ),\n                prefix="prefix",\n                tag_filter=s3.CfnBucket.TagFilterProperty(\n                    key="key",\n                    value="value"\n                )\n            ),\n            id="id",\n            prefix="prefix",\n            priority=123,\n            source_selection_criteria=s3.CfnBucket.SourceSelectionCriteriaProperty(\n                replica_modifications=s3.CfnBucket.ReplicaModificationsProperty(\n                    status="status"\n                ),\n                sse_kms_encrypted_objects=s3.CfnBucket.SseKmsEncryptedObjectsProperty(\n                    status="status"\n                )\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['role', 'rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicationConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicationDestinationProperty
class CfnBucket_ReplicationDestinationPropertyDef(BaseStruct):
    bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the bucket where you want Amazon S3 to store the results.\n')
    access_control_translation: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AccessControlTranslationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify this only in a cross-account scenario (where source and destination bucket owners are not the same), and you want to change replica ownership to the AWS account that owns the destination bucket. If this is not specified in the replication configuration, the replicas are owned by same AWS account that owns the source object.\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Destination bucket owner account ID. In a cross-account scenario, if you direct Amazon S3 to change replica ownership to the AWS account that owns the destination bucket by specifying the ``AccessControlTranslation`` property, this is the account ID of the destination bucket owner. For more information, see `Cross-Region Replication Additional Configuration: Change Replica Owner <https://docs.aws.amazon.com/AmazonS3/latest/dev/crr-change-owner.html>`_ in the *Amazon S3 User Guide* . If you specify the ``AccessControlTranslation`` property, the ``Account`` property is required.\n')
    encryption_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_EncryptionConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies encryption-related information.\n')
    metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_MetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container specifying replication metrics-related settings enabling replication metrics and events.\n')
    replication_time: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationTimePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container specifying S3 Replication Time Control (S3 RTC), including whether S3 RTC is enabled and the time when all objects and operations on objects must be replicated. Must be specified together with a ``Metrics`` block.\n')
    storage_class: typing.Optional[str] = pydantic.Field(None, description='The storage class to use when replicating objects, such as S3 Standard or reduced redundancy. By default, Amazon S3 uses the storage class of the source object to create the object replica. For valid values, see the ``StorageClass`` element of the `PUT Bucket replication <https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html>`_ action in the *Amazon S3 API Reference* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicationdestination.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replication_destination_property = s3.CfnBucket.ReplicationDestinationProperty(\n        bucket="bucket",\n\n        # the properties below are optional\n        access_control_translation=s3.CfnBucket.AccessControlTranslationProperty(\n            owner="owner"\n        ),\n        account="account",\n        encryption_configuration=s3.CfnBucket.EncryptionConfigurationProperty(\n            replica_kms_key_id="replicaKmsKeyId"\n        ),\n        metrics=s3.CfnBucket.MetricsProperty(\n            status="status",\n\n            # the properties below are optional\n            event_threshold=s3.CfnBucket.ReplicationTimeValueProperty(\n                minutes=123\n            )\n        ),\n        replication_time=s3.CfnBucket.ReplicationTimeProperty(\n            status="status",\n            time=s3.CfnBucket.ReplicationTimeValueProperty(\n                minutes=123\n            )\n        ),\n        storage_class="storageClass"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'access_control_translation', 'account', 'encryption_configuration', 'metrics', 'replication_time', 'storage_class']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicationDestinationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicationRuleAndOperatorProperty
class CfnBucket_ReplicationRuleAndOperatorPropertyDef(BaseStruct):
    prefix: typing.Optional[str] = pydantic.Field(None, description='An object key name prefix that identifies the subset of objects to which the rule applies.\n')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of tags containing key and value pairs.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicationruleandoperator.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replication_rule_and_operator_property = s3.CfnBucket.ReplicationRuleAndOperatorProperty(\n        prefix="prefix",\n        tag_filters=[s3.CfnBucket.TagFilterProperty(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['prefix', 'tag_filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicationRuleAndOperatorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicationRuleFilterProperty
class CfnBucket_ReplicationRuleFilterPropertyDef(BaseStruct):
    and_: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationRuleAndOperatorPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container for specifying rule filters. The filters determine the subset of objects to which the rule applies. This element is required only if you specify more than one filter. For example: - If you specify both a ``Prefix`` and a ``TagFilter`` , wrap these filters in an ``And`` tag. - If you specify a filter based on multiple tags, wrap the ``TagFilter`` elements in an ``And`` tag.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='An object key name prefix that identifies the subset of objects to which the rule applies. .. epigraph:: Replacement must be made for object keys containing special characters (such as carriage returns) when using XML requests. For more information, see `XML related object key constraints <https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html#object-key-xml-related-constraints>`_ .\n')
    tag_filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container for specifying a tag key and value. The rule applies only to objects that have the tag in their tag set.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicationrulefilter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replication_rule_filter_property = s3.CfnBucket.ReplicationRuleFilterProperty(\n        and=s3.CfnBucket.ReplicationRuleAndOperatorProperty(\n            prefix="prefix",\n            tag_filters=[s3.CfnBucket.TagFilterProperty(\n                key="key",\n                value="value"\n            )]\n        ),\n        prefix="prefix",\n        tag_filter=s3.CfnBucket.TagFilterProperty(\n            key="key",\n            value="value"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['and_', 'prefix', 'tag_filter']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicationRuleFilterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicationRuleProperty
class CfnBucket_ReplicationRulePropertyDef(BaseStruct):
    destination: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationDestinationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A container for information about the replication destination and its configurations including enabling the S3 Replication Time Control (S3 RTC).\n')
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether the rule is enabled.\n')
    delete_marker_replication: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DeleteMarkerReplicationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies whether Amazon S3 replicates delete markers. If you specify a ``Filter`` in your replication configuration, you must also include a ``DeleteMarkerReplication`` element. If your ``Filter`` includes a ``Tag`` element, the ``DeleteMarkerReplication`` ``Status`` must be set to Disabled, because Amazon S3 does not support replicating delete markers for tag-based rules. For an example configuration, see `Basic Rule Configuration <https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-add-config.html#replication-config-min-rule-config>`_ . For more information about delete marker replication, see `Basic Rule Configuration <https://docs.aws.amazon.com/AmazonS3/latest/dev/delete-marker-replication.html>`_ . .. epigraph:: If you are using an earlier version of the replication configuration, Amazon S3 handles replication of delete markers differently. For more information, see `Backward Compatibility <https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-add-config.html#replication-backward-compat-considerations>`_ .\n')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationRuleFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="A filter that identifies the subset of objects to which the replication rule applies. A ``Filter`` must specify exactly one ``Prefix`` , ``TagFilter`` , or an ``And`` child element. The use of the filter field indicates that this is a V2 replication configuration. This field isn't supported in a V1 replication configuration. .. epigraph:: V1 replication configuration only supports filtering by key prefix. To filter using a V1 replication configuration, add the ``Prefix`` directly as a child element of the ``Rule`` element.\n")
    prefix: typing.Optional[str] = pydantic.Field(None, description='An object key name prefix that identifies the object or objects to which the rule applies. The maximum prefix length is 1,024 characters. To include all objects in a bucket, specify an empty string. To filter using a V1 replication configuration, add the ``Prefix`` directly as a child element of the ``Rule`` element. .. epigraph:: Replacement must be made for object keys containing special characters (such as carriage returns) when using XML requests. For more information, see `XML related object key constraints <https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html#object-key-xml-related-constraints>`_ .\n')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority indicates which rule has precedence whenever two or more replication rules conflict. Amazon S3 will attempt to replicate objects according to all replication rules. However, if there are two or more rules with the same destination bucket, then objects will be replicated according to the rule with the highest priority. The higher the number, the higher the priority. For more information, see `Replication <https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html>`_ in the *Amazon S3 User Guide* .\n')
    source_selection_criteria: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_SourceSelectionCriteriaPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container that describes additional filters for identifying the source objects that you want to replicate. You can choose to enable or disable the replication of these objects.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicationrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replication_rule_property = s3.CfnBucket.ReplicationRuleProperty(\n        destination=s3.CfnBucket.ReplicationDestinationProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            access_control_translation=s3.CfnBucket.AccessControlTranslationProperty(\n                owner="owner"\n            ),\n            account="account",\n            encryption_configuration=s3.CfnBucket.EncryptionConfigurationProperty(\n                replica_kms_key_id="replicaKmsKeyId"\n            ),\n            metrics=s3.CfnBucket.MetricsProperty(\n                status="status",\n\n                # the properties below are optional\n                event_threshold=s3.CfnBucket.ReplicationTimeValueProperty(\n                    minutes=123\n                )\n            ),\n            replication_time=s3.CfnBucket.ReplicationTimeProperty(\n                status="status",\n                time=s3.CfnBucket.ReplicationTimeValueProperty(\n                    minutes=123\n                )\n            ),\n            storage_class="storageClass"\n        ),\n        status="status",\n\n        # the properties below are optional\n        delete_marker_replication=s3.CfnBucket.DeleteMarkerReplicationProperty(\n            status="status"\n        ),\n        filter=s3.CfnBucket.ReplicationRuleFilterProperty(\n            and=s3.CfnBucket.ReplicationRuleAndOperatorProperty(\n                prefix="prefix",\n                tag_filters=[s3.CfnBucket.TagFilterProperty(\n                    key="key",\n                    value="value"\n                )]\n            ),\n            prefix="prefix",\n            tag_filter=s3.CfnBucket.TagFilterProperty(\n                key="key",\n                value="value"\n            )\n        ),\n        id="id",\n        prefix="prefix",\n        priority=123,\n        source_selection_criteria=s3.CfnBucket.SourceSelectionCriteriaProperty(\n            replica_modifications=s3.CfnBucket.ReplicaModificationsProperty(\n                status="status"\n            ),\n            sse_kms_encrypted_objects=s3.CfnBucket.SseKmsEncryptedObjectsProperty(\n                status="status"\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination', 'status', 'delete_marker_replication', 'filter', 'prefix', 'priority', 'source_selection_criteria']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicationRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicationTimeProperty
class CfnBucket_ReplicationTimePropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether the replication time is enabled.\n')
    time: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationTimeValuePropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A container specifying the time by which replication should be complete for all objects and operations on objects.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicationtime.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replication_time_property = s3.CfnBucket.ReplicationTimeProperty(\n        status="status",\n        time=s3.CfnBucket.ReplicationTimeValueProperty(\n            minutes=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status', 'time']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicationTimeProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ReplicationTimeValueProperty
class CfnBucket_ReplicationTimeValuePropertyDef(BaseStruct):
    minutes: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Contains an integer specifying time in minutes. Valid value: 15\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-replicationtimevalue.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    replication_time_value_property = s3.CfnBucket.ReplicationTimeValueProperty(\n        minutes=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['minutes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ReplicationTimeValueProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.RoutingRuleConditionProperty
class CfnBucket_RoutingRuleConditionPropertyDef(BaseStruct):
    http_error_code_returned_equals: typing.Optional[str] = pydantic.Field(None, description='The HTTP error code when the redirect is applied. In the event of an error, if the error code equals this value, then the specified redirect is applied. Required when parent element ``Condition`` is specified and sibling ``KeyPrefixEquals`` is not specified. If both are specified, then both must be true for the redirect to be applied.\n')
    key_prefix_equals: typing.Optional[str] = pydantic.Field(None, description='The object key name prefix when the redirect is applied. For example, to redirect requests for ``ExamplePage.html`` , the key prefix will be ``ExamplePage.html`` . To redirect request for all pages with the prefix ``docs/`` , the key prefix will be ``/docs`` , which identifies all objects in the docs/ folder. Required when the parent element ``Condition`` is specified and sibling ``HttpErrorCodeReturnedEquals`` is not specified. If both conditions are specified, both must be true for the redirect to be applied.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-routingrulecondition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    routing_rule_condition_property = s3.CfnBucket.RoutingRuleConditionProperty(\n        http_error_code_returned_equals="httpErrorCodeReturnedEquals",\n        key_prefix_equals="keyPrefixEquals"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['http_error_code_returned_equals', 'key_prefix_equals']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.RoutingRuleConditionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.RoutingRuleProperty
class CfnBucket_RoutingRulePropertyDef(BaseStruct):
    redirect_rule: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnBucket_RedirectRulePropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Container for redirect information. You can redirect requests to another host, to another page, or with another protocol. In the event of an error, you can specify a different error code to return.\n')
    routing_rule_condition: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RoutingRuleConditionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container for describing a condition that must be met for the specified redirect to apply. For example, 1. If request is for pages in the ``/docs`` folder, redirect to the ``/documents`` folder. 2. If request results in HTTP error 4xx, redirect request to another host where you might process the error.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-routingrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    routing_rule_property = s3.CfnBucket.RoutingRuleProperty(\n        redirect_rule=s3.CfnBucket.RedirectRuleProperty(\n            host_name="hostName",\n            http_redirect_code="httpRedirectCode",\n            protocol="protocol",\n            replace_key_prefix_with="replaceKeyPrefixWith",\n            replace_key_with="replaceKeyWith"\n        ),\n\n        # the properties below are optional\n        routing_rule_condition=s3.CfnBucket.RoutingRuleConditionProperty(\n            http_error_code_returned_equals="httpErrorCodeReturnedEquals",\n            key_prefix_equals="keyPrefixEquals"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['redirect_rule', 'routing_rule_condition']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.RoutingRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.RuleProperty
class CfnBucket_RulePropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='If ``Enabled`` , the rule is currently being applied. If ``Disabled`` , the rule is not currently being applied.\n')
    abort_incomplete_multipart_upload: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AbortIncompleteMultipartUploadPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies a lifecycle rule that stops incomplete multipart uploads to an Amazon S3 bucket.\n')
    expiration_date: typing.Union[models.UnsupportedResource, datetime.datetime, None] = pydantic.Field(None, description='Indicates when objects are deleted from Amazon S3 and Amazon S3 Glacier. The date value must be in ISO 8601 format. The time is always midnight UTC. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time.\n')
    expiration_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates the number of days after creation when objects are deleted from Amazon S3 and Amazon S3 Glacier. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time.\n')
    expired_object_delete_marker: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether Amazon S3 will remove a delete marker without any noncurrent versions. If set to true, the delete marker will be removed if there are no noncurrent versions. This cannot be specified with ``ExpirationInDays`` , ``ExpirationDate`` , or ``TagFilters`` .\n')
    noncurrent_version_expiration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NoncurrentVersionExpirationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Specifies when noncurrent object versions expire. Upon expiration, Amazon S3 permanently deletes the noncurrent object versions. You set this lifecycle configuration action on a bucket that has versioning enabled (or suspended) to request that Amazon S3 delete noncurrent object versions at a specific period in the object's lifetime.\n")
    noncurrent_version_expiration_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='(Deprecated.) For buckets with versioning enabled (or suspended), specifies the time, in days, between when a new version of the object is uploaded to the bucket and when old versions of the object expire. When object versions expire, Amazon S3 permanently deletes them. If you specify a transition and expiration time, the expiration time must be later than the transition time.\n')
    noncurrent_version_transition: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NoncurrentVersionTransitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="(Deprecated.) For buckets with versioning enabled (or suspended), specifies when non-current objects transition to a specified storage class. If you specify a transition and expiration time, the expiration time must be later than the transition time. If you specify this property, don't specify the ``NoncurrentVersionTransitions`` property.\n")
    noncurrent_version_transitions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NoncurrentVersionTransitionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="For buckets with versioning enabled (or suspended), one or more transition rules that specify when non-current objects transition to a specified storage class. If you specify a transition and expiration time, the expiration time must be later than the transition time. If you specify this property, don't specify the ``NoncurrentVersionTransition`` property.\n")
    object_size_greater_than: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the minimum object size in bytes for this rule to apply to. Objects must be larger than this value in bytes. For more information about size based rules, see `Lifecycle configuration using size-based rules <https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html#lc-size-rules>`_ in the *Amazon S3 User Guide* .\n')
    object_size_less_than: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum object size in bytes for this rule to apply to. Objects must be smaller than this value in bytes. For more information about sized based rules, see `Lifecycle configuration using size-based rules <https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html#lc-size-rules>`_ in the *Amazon S3 User Guide* .\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='Object key prefix that identifies one or more objects to which this rule applies. .. epigraph:: Replacement must be made for object keys containing special characters (such as carriage returns) when using XML requests. For more information, see `XML related object key constraints <https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html#object-key-xml-related-constraints>`_ .\n')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Tags to use to identify a subset of objects to which the lifecycle rule applies.\n')
    transition: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TransitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="(Deprecated.) Specifies when an object transitions to a specified storage class. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. If you specify this property, don't specify the ``Transitions`` property.\n")
    transitions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TransitionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more transition rules that specify when an object transitions to a specified storage class. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. If you specify this property, don\'t specify the ``Transition`` property.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-rule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    rule_property = s3.CfnBucket.RuleProperty(\n        status="status",\n\n        # the properties below are optional\n        abort_incomplete_multipart_upload=s3.CfnBucket.AbortIncompleteMultipartUploadProperty(\n            days_after_initiation=123\n        ),\n        expiration_date=Date(),\n        expiration_in_days=123,\n        expired_object_delete_marker=False,\n        id="id",\n        noncurrent_version_expiration=s3.CfnBucket.NoncurrentVersionExpirationProperty(\n            noncurrent_days=123,\n\n            # the properties below are optional\n            newer_noncurrent_versions=123\n        ),\n        noncurrent_version_expiration_in_days=123,\n        noncurrent_version_transition=s3.CfnBucket.NoncurrentVersionTransitionProperty(\n            storage_class="storageClass",\n            transition_in_days=123,\n\n            # the properties below are optional\n            newer_noncurrent_versions=123\n        ),\n        noncurrent_version_transitions=[s3.CfnBucket.NoncurrentVersionTransitionProperty(\n            storage_class="storageClass",\n            transition_in_days=123,\n\n            # the properties below are optional\n            newer_noncurrent_versions=123\n        )],\n        object_size_greater_than=123,\n        object_size_less_than=123,\n        prefix="prefix",\n        tag_filters=[s3.CfnBucket.TagFilterProperty(\n            key="key",\n            value="value"\n        )],\n        transition=s3.CfnBucket.TransitionProperty(\n            storage_class="storageClass",\n\n            # the properties below are optional\n            transition_date=Date(),\n            transition_in_days=123\n        ),\n        transitions=[s3.CfnBucket.TransitionProperty(\n            storage_class="storageClass",\n\n            # the properties below are optional\n            transition_date=Date(),\n            transition_in_days=123\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status', 'abort_incomplete_multipart_upload', 'expiration_date', 'expiration_in_days', 'expired_object_delete_marker', 'noncurrent_version_expiration', 'noncurrent_version_expiration_in_days', 'noncurrent_version_transition', 'noncurrent_version_transitions', 'object_size_greater_than', 'object_size_less_than', 'prefix', 'tag_filters', 'transition', 'transitions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.RuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.S3KeyFilterProperty
class CfnBucket_S3KeyFilterPropertyDef(BaseStruct):
    rules: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_FilterRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A list of containers for the key-value pair that defines the criteria for the filter rule.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-s3keyfilter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    s3_key_filter_property = s3.CfnBucket.S3KeyFilterProperty(\n        rules=[s3.CfnBucket.FilterRuleProperty(\n            name="name",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.S3KeyFilterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ServerSideEncryptionByDefaultProperty
class CfnBucket_ServerSideEncryptionByDefaultPropertyDef(BaseStruct):
    sse_algorithm: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Server-side encryption algorithm to use for the default encryption.\n')
    kms_master_key_id: typing.Optional[str] = pydantic.Field(None, description='KMS key ID to use for the default encryption. This parameter is allowed if SSEAlgorithm is aws:kms. You can specify the key ID, key alias, or the Amazon Resource Name (ARN) of the CMK. However, if you are using encryption with cross-account operations, you must use a fully qualified CMK ARN. For more information, see `Using encryption for cross-account operations <https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html#bucket-encryption-update-bucket-policy>`_ . For example: - Key ID: ``1234abcd-12ab-34cd-56ef-1234567890ab`` - Key ARN: ``arn:aws:kms:us-east-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab`` .. epigraph:: Amazon S3 only supports symmetric KMS keys and not asymmetric KMS keys. For more information, see `Using Symmetric and Asymmetric Keys <https://docs.aws.amazon.com//kms/latest/developerguide/symmetric-asymmetric.html>`_ in the *AWS Key Management Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-serversideencryptionbydefault.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    server_side_encryption_by_default_property = s3.CfnBucket.ServerSideEncryptionByDefaultProperty(\n        sse_algorithm="sseAlgorithm",\n\n        # the properties below are optional\n        kms_master_key_id="kmsMasterKeyId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['sse_algorithm', 'kms_master_key_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ServerSideEncryptionByDefaultProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.ServerSideEncryptionRuleProperty
class CfnBucket_ServerSideEncryptionRulePropertyDef(BaseStruct):
    bucket_key_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether Amazon S3 should use an S3 Bucket Key with server-side encryption using KMS (SSE-KMS) for new objects in the bucket. Existing objects are not affected. Setting the ``BucketKeyEnabled`` element to ``true`` causes Amazon S3 to use an S3 Bucket Key. By default, S3 Bucket Key is not enabled. For more information, see `Amazon S3 Bucket Keys <https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-key.html>`_ in the *Amazon S3 User Guide* .\n')
    server_side_encryption_by_default: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ServerSideEncryptionByDefaultPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the default server-side encryption to apply to new objects in the bucket. If a PUT Object request doesn\'t specify any server-side encryption, this default encryption will be applied.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-serversideencryptionrule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    server_side_encryption_rule_property = s3.CfnBucket.ServerSideEncryptionRuleProperty(\n        bucket_key_enabled=False,\n        server_side_encryption_by_default=s3.CfnBucket.ServerSideEncryptionByDefaultProperty(\n            sse_algorithm="sseAlgorithm",\n\n            # the properties below are optional\n            kms_master_key_id="kmsMasterKeyId"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket_key_enabled', 'server_side_encryption_by_default']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.ServerSideEncryptionRuleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.SourceSelectionCriteriaProperty
class CfnBucket_SourceSelectionCriteriaPropertyDef(BaseStruct):
    replica_modifications: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicaModificationsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A filter that you can specify for selection for modifications on replicas.\n')
    sse_kms_encrypted_objects: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_SseKmsEncryptedObjectsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A container for filter information for the selection of Amazon S3 objects encrypted with AWS KMS.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-sourceselectioncriteria.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    source_selection_criteria_property = s3.CfnBucket.SourceSelectionCriteriaProperty(\n        replica_modifications=s3.CfnBucket.ReplicaModificationsProperty(\n            status="status"\n        ),\n        sse_kms_encrypted_objects=s3.CfnBucket.SseKmsEncryptedObjectsProperty(\n            status="status"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['replica_modifications', 'sse_kms_encrypted_objects']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.SourceSelectionCriteriaProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.SseKmsEncryptedObjectsProperty
class CfnBucket_SseKmsEncryptedObjectsPropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether Amazon S3 replicates objects created with server-side encryption using an AWS KMS key stored in AWS Key Management Service.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-ssekmsencryptedobjects.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    sse_kms_encrypted_objects_property = s3.CfnBucket.SseKmsEncryptedObjectsProperty(\n        status="status"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.SseKmsEncryptedObjectsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.StorageClassAnalysisProperty
class CfnBucket_StorageClassAnalysisPropertyDef(BaseStruct):
    data_export: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DataExportPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies how data related to the storage class analysis for an Amazon S3 bucket should be exported.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-storageclassanalysis.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    storage_class_analysis_property = s3.CfnBucket.StorageClassAnalysisProperty(\n        data_export=s3.CfnBucket.DataExportProperty(\n            destination=s3.CfnBucket.DestinationProperty(\n                bucket_arn="bucketArn",\n                format="format",\n\n                # the properties below are optional\n                bucket_account_id="bucketAccountId",\n                prefix="prefix"\n            ),\n            output_schema_version="outputSchemaVersion"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_export']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.StorageClassAnalysisProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.TagFilterProperty
class CfnBucket_TagFilterPropertyDef(BaseStruct):
    key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The tag key.\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The tag value.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-tagfilter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    tag_filter_property = s3.CfnBucket.TagFilterProperty(\n        key="key",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.TagFilterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.TieringProperty
class CfnBucket_TieringPropertyDef(BaseStruct):
    access_tier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='S3 Intelligent-Tiering access tier. See `Storage class for automatically optimizing frequently and infrequently accessed objects <https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-dynamic-data-access>`_ for a list of access tiers in the S3 Intelligent-Tiering storage class.\n')
    days: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of consecutive days of no access after which an object will be eligible to be transitioned to the corresponding tier. The minimum number of days specified for Archive Access tier must be at least 90 days and Deep Archive Access tier must be at least 180 days. The maximum can be up to 2 years (730 days).\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-tiering.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    tiering_property = s3.CfnBucket.TieringProperty(\n        access_tier="accessTier",\n        days=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_tier', 'days']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.TieringProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.TopicConfigurationProperty
class CfnBucket_TopicConfigurationPropertyDef(BaseStruct):
    event: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon S3 bucket event about which to send notifications. For more information, see `Supported Event Types <https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html>`_ in the *Amazon S3 User Guide* .\n')
    topic: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the Amazon SNS topic to which Amazon S3 publishes a message when it detects events of the specified type.\n')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The filtering rules that determine for which objects to send notifications. For example, you can create a filter so that Amazon S3 sends notifications only when image files with a ``.jpg`` extension are added to the bucket.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-topicconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    topic_configuration_property = s3.CfnBucket.TopicConfigurationProperty(\n        event="event",\n        topic="topic",\n\n        # the properties below are optional\n        filter=s3.CfnBucket.NotificationFilterProperty(\n            s3_key=s3.CfnBucket.S3KeyFilterProperty(\n                rules=[s3.CfnBucket.FilterRuleProperty(\n                    name="name",\n                    value="value"\n                )]\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['event', 'topic', 'filter']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.TopicConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.TransitionProperty
class CfnBucket_TransitionPropertyDef(BaseStruct):
    storage_class: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The storage class to which you want the object to transition.\n')
    transition_date: typing.Union[models.UnsupportedResource, datetime.datetime, None] = pydantic.Field(None, description='Indicates when objects are transitioned to the specified storage class. The date value must be in ISO 8601 format. The time is always midnight UTC.\n')
    transition_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates the number of days after creation when objects are transitioned to the specified storage class. The value must be a positive integer.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-transition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    transition_property = s3.CfnBucket.TransitionProperty(\n        storage_class="storageClass",\n\n        # the properties below are optional\n        transition_date=Date(),\n        transition_in_days=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_class', 'transition_date', 'transition_in_days']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.TransitionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.VersioningConfigurationProperty
class CfnBucket_VersioningConfigurationPropertyDef(BaseStruct):
    status: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The versioning state of the bucket. Default: - "Suspended"\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-versioningconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    versioning_configuration_property = s3.CfnBucket.VersioningConfigurationProperty(\n        status="status"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['status']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.VersioningConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucket.WebsiteConfigurationProperty
class CfnBucket_WebsiteConfigurationPropertyDef(BaseStruct):
    error_document: typing.Optional[str] = pydantic.Field(None, description='The name of the error document for the website.\n')
    index_document: typing.Optional[str] = pydantic.Field(None, description='The name of the index document for the website.\n')
    redirect_all_requests_to: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RedirectAllRequestsToPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The redirect behavior for every request to this bucket's website endpoint. .. epigraph:: If you specify this property, you can't specify any other property.\n")
    routing_rules: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RoutingRulePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Rules that define when a redirect is applied and the redirect behavior.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-websiteconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    website_configuration_property = s3.CfnBucket.WebsiteConfigurationProperty(\n        error_document="errorDocument",\n        index_document="indexDocument",\n        redirect_all_requests_to=s3.CfnBucket.RedirectAllRequestsToProperty(\n            host_name="hostName",\n\n            # the properties below are optional\n            protocol="protocol"\n        ),\n        routing_rules=[s3.CfnBucket.RoutingRuleProperty(\n            redirect_rule=s3.CfnBucket.RedirectRuleProperty(\n                host_name="hostName",\n                http_redirect_code="httpRedirectCode",\n                protocol="protocol",\n                replace_key_prefix_with="replaceKeyPrefixWith",\n                replace_key_with="replaceKeyWith"\n            ),\n\n            # the properties below are optional\n            routing_rule_condition=s3.CfnBucket.RoutingRuleConditionProperty(\n                http_error_code_returned_equals="httpErrorCodeReturnedEquals",\n                key_prefix_equals="keyPrefixEquals"\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['error_document', 'index_document', 'redirect_all_requests_to', 'routing_rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket.WebsiteConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnMultiRegionAccessPoint.PublicAccessBlockConfigurationProperty
class CfnMultiRegionAccessPoint_PublicAccessBlockConfigurationPropertyDef(BaseStruct):
    block_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should block public access control lists (ACLs) for this bucket and objects in this bucket. Setting this element to ``TRUE`` causes the following behavior: - PUT Bucket ACL and PUT Object ACL calls fail if the specified ACL is public. - PUT Object calls fail if the request includes a public ACL. - PUT Bucket calls fail if the request includes a public ACL. Enabling this setting doesn't affect existing policies or ACLs.\n")
    block_public_policy: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should block public bucket policies for this bucket. Setting this element to ``TRUE`` causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access. Enabling this setting doesn't affect existing bucket policies.\n")
    ignore_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should ignore public ACLs for this bucket and objects in this bucket. Setting this element to ``TRUE`` causes Amazon S3 to ignore all public ACLs on this bucket and objects in this bucket. Enabling this setting doesn't affect the persistence of any existing ACLs and doesn't prevent new public ACLs from being set.\n")
    restrict_public_buckets: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether Amazon S3 should restrict public bucket policies for this bucket. Setting this element to ``TRUE`` restricts access to this bucket to only AWS service principals and authorized users within this account if the bucket has a public policy. Enabling this setting doesn't affect previously stored bucket policies, except that public and cross-account access within any public bucket policy, including non-public delegation to specific accounts, is blocked.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-multiregionaccesspoint-publicaccessblockconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    public_access_block_configuration_property = s3.CfnMultiRegionAccessPoint.PublicAccessBlockConfigurationProperty(\n        block_public_acls=False,\n        block_public_policy=False,\n        ignore_public_acls=False,\n        restrict_public_buckets=False\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['block_public_acls', 'block_public_policy', 'ignore_public_acls', 'restrict_public_buckets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnMultiRegionAccessPoint.PublicAccessBlockConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnMultiRegionAccessPoint.RegionProperty
class CfnMultiRegionAccessPoint_RegionPropertyDef(BaseStruct):
    bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the associated bucket for the Region.\n')
    bucket_account_id: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID that owns the Amazon S3 bucket that\'s associated with this Multi-Region Access Point.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-multiregionaccesspoint-region.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    region_property = s3.CfnMultiRegionAccessPoint.RegionProperty(\n        bucket="bucket",\n\n        # the properties below are optional\n        bucket_account_id="bucketAccountId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_account_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnMultiRegionAccessPoint.RegionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnMultiRegionAccessPointPolicy.PolicyStatusProperty
class CfnMultiRegionAccessPointPolicy_PolicyStatusPropertyDef(BaseStruct):
    is_public: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The policy status for this bucket. ``TRUE`` indicates that this bucket is public. ``FALSE`` indicates that the bucket is not public.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-multiregionaccesspointpolicy-policystatus.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    policy_status_property = s3.CfnMultiRegionAccessPointPolicy.PolicyStatusProperty(\n        is_public="isPublic"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_public']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnMultiRegionAccessPointPolicy.PolicyStatusProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.AccountLevelProperty
class CfnStorageLens_AccountLevelPropertyDef(BaseStruct):
    bucket_level: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnStorageLens_BucketLevelPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property contains the details of the account-level bucket-level configurations for Amazon S3 Storage Lens.\n')
    activity_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_ActivityMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of account-level activity metrics for S3 Storage Lens.\n')
    advanced_cost_optimization_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedCostOptimizationMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of account-level advanced cost optimization metrics for S3 Storage Lens.\n')
    advanced_data_protection_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedDataProtectionMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of account-level advanced data protection metrics for S3 Storage Lens.\n')
    detailed_status_codes_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_DetailedStatusCodesMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of account-level detailed status code metrics for S3 Storage Lens.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-accountlevel.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    account_level_property = s3.CfnStorageLens.AccountLevelProperty(\n        bucket_level=s3.CfnStorageLens.BucketLevelProperty(\n            activity_metrics=s3.CfnStorageLens.ActivityMetricsProperty(\n                is_enabled=False\n            ),\n            advanced_cost_optimization_metrics=s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n                is_enabled=False\n            ),\n            advanced_data_protection_metrics=s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n                is_enabled=False\n            ),\n            detailed_status_codes_metrics=s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n                is_enabled=False\n            ),\n            prefix_level=s3.CfnStorageLens.PrefixLevelProperty(\n                storage_metrics=s3.CfnStorageLens.PrefixLevelStorageMetricsProperty(\n                    is_enabled=False,\n                    selection_criteria=s3.CfnStorageLens.SelectionCriteriaProperty(\n                        delimiter="delimiter",\n                        max_depth=123,\n                        min_storage_bytes_percentage=123\n                    )\n                )\n            )\n        ),\n\n        # the properties below are optional\n        activity_metrics=s3.CfnStorageLens.ActivityMetricsProperty(\n            is_enabled=False\n        ),\n        advanced_cost_optimization_metrics=s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n            is_enabled=False\n        ),\n        advanced_data_protection_metrics=s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n            is_enabled=False\n        ),\n        detailed_status_codes_metrics=s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n            is_enabled=False\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket_level', 'activity_metrics', 'advanced_cost_optimization_metrics', 'advanced_data_protection_metrics', 'detailed_status_codes_metrics']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.AccountLevelProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.ActivityMetricsProperty
class CfnStorageLens_ActivityMetricsPropertyDef(BaseStruct):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A property that indicates whether the activity metrics is enabled.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-activitymetrics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    activity_metrics_property = s3.CfnStorageLens.ActivityMetricsProperty(\n        is_enabled=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.ActivityMetricsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty
class CfnStorageLens_AdvancedCostOptimizationMetricsPropertyDef(BaseStruct):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether advanced cost optimization metrics are enabled.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-advancedcostoptimizationmetrics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    advanced_cost_optimization_metrics_property = s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n        is_enabled=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty
class CfnStorageLens_AdvancedDataProtectionMetricsPropertyDef(BaseStruct):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether advanced data protection metrics are enabled.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-advanceddataprotectionmetrics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    advanced_data_protection_metrics_property = s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n        is_enabled=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.AwsOrgProperty
class CfnStorageLens_AwsOrgPropertyDef(BaseStruct):
    arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='This resource contains the ARN of the AWS Organization.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-awsorg.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    aws_org_property = s3.CfnStorageLens.AwsOrgProperty(\n        arn="arn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.AwsOrgProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.BucketLevelProperty
class CfnStorageLens_BucketLevelPropertyDef(BaseStruct):
    activity_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_ActivityMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A property for bucket-level activity metrics for S3 Storage Lens.\n')
    advanced_cost_optimization_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedCostOptimizationMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A property for bucket-level advanced cost optimization metrics for S3 Storage Lens.\n')
    advanced_data_protection_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedDataProtectionMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A property for bucket-level advanced data protection metrics for S3 Storage Lens.\n')
    detailed_status_codes_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_DetailedStatusCodesMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A property for bucket-level detailed status code metrics for S3 Storage Lens.\n')
    prefix_level: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_PrefixLevelPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A property for bucket-level prefix-level storage metrics for S3 Storage Lens.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-bucketlevel.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    bucket_level_property = s3.CfnStorageLens.BucketLevelProperty(\n        activity_metrics=s3.CfnStorageLens.ActivityMetricsProperty(\n            is_enabled=False\n        ),\n        advanced_cost_optimization_metrics=s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n            is_enabled=False\n        ),\n        advanced_data_protection_metrics=s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n            is_enabled=False\n        ),\n        detailed_status_codes_metrics=s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n            is_enabled=False\n        ),\n        prefix_level=s3.CfnStorageLens.PrefixLevelProperty(\n            storage_metrics=s3.CfnStorageLens.PrefixLevelStorageMetricsProperty(\n                is_enabled=False,\n                selection_criteria=s3.CfnStorageLens.SelectionCriteriaProperty(\n                    delimiter="delimiter",\n                    max_depth=123,\n                    min_storage_bytes_percentage=123\n                )\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['activity_metrics', 'advanced_cost_optimization_metrics', 'advanced_data_protection_metrics', 'detailed_status_codes_metrics', 'prefix_level']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.BucketLevelProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.BucketsAndRegionsProperty
class CfnStorageLens_BucketsAndRegionsPropertyDef(BaseStruct):
    buckets: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='This property contains the details of the buckets for the Amazon S3 Storage Lens configuration. This should be the bucket Amazon Resource Name(ARN). For valid values, see `Buckets ARN format here <https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_Include.html#API_control_Include_Contents>`_ in the *Amazon S3 API Reference* .\n')
    regions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='This property contains the details of the Regions for the S3 Storage Lens configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-bucketsandregions.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    buckets_and_regions_property = s3.CfnStorageLens.BucketsAndRegionsProperty(\n        buckets=["buckets"],\n        regions=["regions"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['buckets', 'regions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.BucketsAndRegionsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.CloudWatchMetricsProperty
class CfnStorageLens_CloudWatchMetricsPropertyDef(BaseStruct):
    is_enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property identifies whether the CloudWatch publishing option for S3 Storage Lens is enabled.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-cloudwatchmetrics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    cloud_watch_metrics_property = s3.CfnStorageLens.CloudWatchMetricsProperty(\n        is_enabled=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.CloudWatchMetricsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.DataExportProperty
class CfnStorageLens_DataExportPropertyDef(BaseStruct):
    cloud_watch_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_CloudWatchMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property enables the Amazon CloudWatch publishing option for S3 Storage Lens metrics.\n')
    s3_bucket_destination: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_S3BucketDestinationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of the bucket where the S3 Storage Lens metrics export will be placed.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-dataexport.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # sses3: Any\n\n    data_export_property = s3.CfnStorageLens.DataExportProperty(\n        cloud_watch_metrics=s3.CfnStorageLens.CloudWatchMetricsProperty(\n            is_enabled=False\n        ),\n        s3_bucket_destination=s3.CfnStorageLens.S3BucketDestinationProperty(\n            account_id="accountId",\n            arn="arn",\n            format="format",\n            output_schema_version="outputSchemaVersion",\n\n            # the properties below are optional\n            encryption=s3.CfnStorageLens.EncryptionProperty(\n                ssekms=s3.CfnStorageLens.SSEKMSProperty(\n                    key_id="keyId"\n                ),\n                sses3=sses3\n            ),\n            prefix="prefix"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_watch_metrics', 's3_bucket_destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.DataExportProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.DetailedStatusCodesMetricsProperty
class CfnStorageLens_DetailedStatusCodesMetricsPropertyDef(BaseStruct):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether detailed status code metrics are enabled.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-detailedstatuscodesmetrics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    detailed_status_codes_metrics_property = s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n        is_enabled=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.DetailedStatusCodesMetricsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.EncryptionProperty
class CfnStorageLens_EncryptionPropertyDef(BaseStruct):
    ssekms: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_SSEKMSPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the use of AWS Key Management Service keys (SSE-KMS) to encrypt the S3 Storage Lens metrics export file.\n')
    sses3: typing.Any = pydantic.Field(None, description='Specifies the use of an Amazon S3-managed key (SSE-S3) to encrypt the S3 Storage Lens metrics export file.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-encryption.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # sses3: Any\n\n    encryption_property = s3.CfnStorageLens.EncryptionProperty(\n        ssekms=s3.CfnStorageLens.SSEKMSProperty(\n            key_id="keyId"\n        ),\n        sses3=sses3\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['ssekms', 'sses3']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.EncryptionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.PrefixLevelProperty
class CfnStorageLens_PrefixLevelPropertyDef(BaseStruct):
    storage_metrics: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnStorageLens_PrefixLevelStorageMetricsPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A property for the prefix-level storage metrics for Amazon S3 Storage Lens.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-prefixlevel.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    prefix_level_property = s3.CfnStorageLens.PrefixLevelProperty(\n        storage_metrics=s3.CfnStorageLens.PrefixLevelStorageMetricsProperty(\n            is_enabled=False,\n            selection_criteria=s3.CfnStorageLens.SelectionCriteriaProperty(\n                delimiter="delimiter",\n                max_depth=123,\n                min_storage_bytes_percentage=123\n            )\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_metrics']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.PrefixLevelProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.PrefixLevelStorageMetricsProperty
class CfnStorageLens_PrefixLevelStorageMetricsPropertyDef(BaseStruct):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='This property identifies whether the details of the prefix-level storage metrics for S3 Storage Lens are enabled.\n')
    selection_criteria: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_SelectionCriteriaPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property identifies whether the details of the prefix-level storage metrics for S3 Storage Lens are enabled.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-prefixlevelstoragemetrics.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    prefix_level_storage_metrics_property = s3.CfnStorageLens.PrefixLevelStorageMetricsProperty(\n        is_enabled=False,\n        selection_criteria=s3.CfnStorageLens.SelectionCriteriaProperty(\n            delimiter="delimiter",\n            max_depth=123,\n            min_storage_bytes_percentage=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_enabled', 'selection_criteria']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.PrefixLevelStorageMetricsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.S3BucketDestinationProperty
class CfnStorageLens_S3BucketDestinationPropertyDef(BaseStruct):
    account_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property contains the details of the AWS account ID of the S3 Storage Lens export bucket destination.\n')
    arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property contains the details of the ARN of the bucket destination of the S3 Storage Lens export.\n')
    format: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property contains the details of the format of the S3 Storage Lens export bucket destination.\n')
    output_schema_version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property contains the details of the output schema version of the S3 Storage Lens export bucket destination.\n')
    encryption: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_EncryptionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of the encryption of the bucket destination of the Amazon S3 Storage Lens metrics export.\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='This property contains the details of the prefix of the bucket destination of the S3 Storage Lens export .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-s3bucketdestination.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # sses3: Any\n\n    s3_bucket_destination_property = s3.CfnStorageLens.S3BucketDestinationProperty(\n        account_id="accountId",\n        arn="arn",\n        format="format",\n        output_schema_version="outputSchemaVersion",\n\n        # the properties below are optional\n        encryption=s3.CfnStorageLens.EncryptionProperty(\n            ssekms=s3.CfnStorageLens.SSEKMSProperty(\n                key_id="keyId"\n            ),\n            sses3=sses3\n        ),\n        prefix="prefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['account_id', 'arn', 'format', 'output_schema_version', 'encryption', 'prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.S3BucketDestinationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.SelectionCriteriaProperty
class CfnStorageLens_SelectionCriteriaPropertyDef(BaseStruct):
    delimiter: typing.Optional[str] = pydantic.Field(None, description='This property contains the details of the S3 Storage Lens delimiter being used.\n')
    max_depth: typing.Union[int, float, None] = pydantic.Field(None, description='This property contains the details of the max depth that S3 Storage Lens will collect metrics up to.\n')
    min_storage_bytes_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='This property contains the details of the minimum storage bytes percentage threshold that S3 Storage Lens will collect metrics up to.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-selectioncriteria.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    selection_criteria_property = s3.CfnStorageLens.SelectionCriteriaProperty(\n        delimiter="delimiter",\n        max_depth=123,\n        min_storage_bytes_percentage=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['delimiter', 'max_depth', 'min_storage_bytes_percentage']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.SelectionCriteriaProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.SSEKMSProperty
class CfnStorageLens_SSEKMSPropertyDef(BaseStruct):
    key_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the Amazon Resource Name (ARN) of the customer managed AWS KMS key to use for encrypting the S3 Storage Lens metrics export file. Amazon S3 only supports symmetric encryption keys. For more information, see `Special-purpose keys <https://docs.aws.amazon.com/kms/latest/developerguide/key-types.html>`_ in the *AWS Key Management Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-ssekms.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    s_sEKMSProperty = s3.CfnStorageLens.SSEKMSProperty(\n        key_id="keyId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.SSEKMSProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLens.StorageLensConfigurationProperty
class CfnStorageLens_StorageLensConfigurationPropertyDef(BaseStruct):
    account_level: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnStorageLens_AccountLevelPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property contains the details of the account-level metrics for Amazon S3 Storage Lens configuration.\n')
    is_enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='This property contains the details of whether the Amazon S3 Storage Lens configuration is enabled.\n')
    aws_org: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AwsOrgPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of the AWS Organization for the S3 Storage Lens configuration.\n')
    data_export: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_DataExportPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This property contains the details of this S3 Storage Lens configuration's metrics export.\n")
    exclude: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_BucketsAndRegionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of the bucket and or Regions excluded for Amazon S3 Storage Lens configuration.\n')
    include: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_BucketsAndRegionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property contains the details of the bucket and or Regions included for Amazon S3 Storage Lens configuration.\n')
    storage_lens_arn: typing.Optional[str] = pydantic.Field(None, description='This property contains the details of the ARN of the S3 Storage Lens configuration. This property is read-only.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-storagelens-storagelensconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # sses3: Any\n\n    storage_lens_configuration_property = s3.CfnStorageLens.StorageLensConfigurationProperty(\n        account_level=s3.CfnStorageLens.AccountLevelProperty(\n            bucket_level=s3.CfnStorageLens.BucketLevelProperty(\n                activity_metrics=s3.CfnStorageLens.ActivityMetricsProperty(\n                    is_enabled=False\n                ),\n                advanced_cost_optimization_metrics=s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n                    is_enabled=False\n                ),\n                advanced_data_protection_metrics=s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n                    is_enabled=False\n                ),\n                detailed_status_codes_metrics=s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n                    is_enabled=False\n                ),\n                prefix_level=s3.CfnStorageLens.PrefixLevelProperty(\n                    storage_metrics=s3.CfnStorageLens.PrefixLevelStorageMetricsProperty(\n                        is_enabled=False,\n                        selection_criteria=s3.CfnStorageLens.SelectionCriteriaProperty(\n                            delimiter="delimiter",\n                            max_depth=123,\n                            min_storage_bytes_percentage=123\n                        )\n                    )\n                )\n            ),\n\n            # the properties below are optional\n            activity_metrics=s3.CfnStorageLens.ActivityMetricsProperty(\n                is_enabled=False\n            ),\n            advanced_cost_optimization_metrics=s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n                is_enabled=False\n            ),\n            advanced_data_protection_metrics=s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n                is_enabled=False\n            ),\n            detailed_status_codes_metrics=s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n                is_enabled=False\n            )\n        ),\n        id="id",\n        is_enabled=False,\n\n        # the properties below are optional\n        aws_org=s3.CfnStorageLens.AwsOrgProperty(\n            arn="arn"\n        ),\n        data_export=s3.CfnStorageLens.DataExportProperty(\n            cloud_watch_metrics=s3.CfnStorageLens.CloudWatchMetricsProperty(\n                is_enabled=False\n            ),\n            s3_bucket_destination=s3.CfnStorageLens.S3BucketDestinationProperty(\n                account_id="accountId",\n                arn="arn",\n                format="format",\n                output_schema_version="outputSchemaVersion",\n\n                # the properties below are optional\n                encryption=s3.CfnStorageLens.EncryptionProperty(\n                    ssekms=s3.CfnStorageLens.SSEKMSProperty(\n                        key_id="keyId"\n                    ),\n                    sses3=sses3\n                ),\n                prefix="prefix"\n            )\n        ),\n        exclude=s3.CfnStorageLens.BucketsAndRegionsProperty(\n            buckets=["buckets"],\n            regions=["regions"]\n        ),\n        include=s3.CfnStorageLens.BucketsAndRegionsProperty(\n            buckets=["buckets"],\n            regions=["regions"]\n        ),\n        storage_lens_arn="storageLensArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['account_level', 'is_enabled', 'aws_org', 'data_export', 'exclude', 'include', 'storage_lens_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens.StorageLensConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CorsRule
class CorsRuleDef(BaseStruct):
    allowed_methods: typing.Union[typing.Sequence[aws_cdk.aws_s3.HttpMethods], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An HTTP method that you allow the origin to execute.\n')
    allowed_origins: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='One or more origins you want customers to be able to access the bucket from.\n')
    allowed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Headers that are specified in the Access-Control-Request-Headers header. Default: - No headers allowed.\n')
    exposed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more headers in the response that you want customers to be able to access from their applications. Default: - No headers exposed.\n')
    max_age: typing.Union[int, float, None] = pydantic.Field(None, description='The time in seconds that your browser is to cache the preflight response for the specified resource. Default: - No caching.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    cors_rule = s3.CorsRule(\n        allowed_methods=[s3.HttpMethods.GET],\n        allowed_origins=["allowedOrigins"],\n\n        # the properties below are optional\n        allowed_headers=["allowedHeaders"],\n        exposed_headers=["exposedHeaders"],\n        id="id",\n        max_age=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allowed_methods', 'allowed_origins', 'allowed_headers', 'exposed_headers', 'max_age']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CorsRule'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.IntelligentTieringConfiguration
class IntelligentTieringConfigurationDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Configuration name.\n')
    archive_access_tier_time: typing.Optional[models.DurationDef] = pydantic.Field(None, description='When enabled, Intelligent-Tiering will automatically move objects that haven’t been accessed for a minimum of 90 days to the Archive Access tier. Default: Objects will not move to Glacier\n')
    deep_archive_access_tier_time: typing.Optional[models.DurationDef] = pydantic.Field(None, description='When enabled, Intelligent-Tiering will automatically move objects that haven’t been accessed for a minimum of 180 days to the Deep Archive Access tier. Default: Objects will not move to Glacier Deep Access\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='Add a filter to limit the scope of this configuration to a single prefix. Default: this configuration will apply to **all** objects in the bucket.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.TagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='You can limit the scope of this rule to the key value pairs added below. Default: No filtering will be performed on tags\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_s3 as s3\n\n    intelligent_tiering_configuration = s3.IntelligentTieringConfiguration(\n        name="name",\n\n        # the properties below are optional\n        archive_access_tier_time=cdk.Duration.minutes(30),\n        deep_archive_access_tier_time=cdk.Duration.minutes(30),\n        prefix="prefix",\n        tags=[s3.Tag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'archive_access_tier_time', 'deep_archive_access_tier_time', 'prefix', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.IntelligentTieringConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.Inventory
class InventoryDef(BaseStruct):
    destination: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.InventoryDestinationDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The destination of the inventory.\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether the inventory is enabled or disabled. Default: true\n')
    format: typing.Optional[aws_cdk.aws_s3.InventoryFormat] = pydantic.Field(None, description='The format of the inventory. Default: InventoryFormat.CSV\n')
    frequency: typing.Optional[aws_cdk.aws_s3.InventoryFrequency] = pydantic.Field(None, description='Frequency at which the inventory should be generated. Default: InventoryFrequency.WEEKLY\n')
    include_object_versions: typing.Optional[aws_cdk.aws_s3.InventoryObjectVersion] = pydantic.Field(None, description='If the inventory should contain all the object versions or only the current one. Default: InventoryObjectVersion.ALL\n')
    inventory_id: typing.Optional[str] = pydantic.Field(None, description='The inventory configuration ID. Default: - generated ID.\n')
    objects_prefix: typing.Optional[str] = pydantic.Field(None, description='The inventory will only include objects that meet the prefix filter criteria. Default: - No objects prefix\n')
    optional_fields: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of optional fields to be included in the inventory result. Default: - No optional fields.\n\n:see: https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # bucket: s3.Bucket\n\n    inventory = s3.Inventory(\n        destination=s3.InventoryDestination(\n            bucket=bucket,\n\n            # the properties below are optional\n            bucket_owner="bucketOwner",\n            prefix="prefix"\n        ),\n\n        # the properties below are optional\n        enabled=False,\n        format=s3.InventoryFormat.CSV,\n        frequency=s3.InventoryFrequency.DAILY,\n        include_object_versions=s3.InventoryObjectVersion.ALL,\n        inventory_id="inventoryId",\n        objects_prefix="objectsPrefix",\n        optional_fields=["optionalFields"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination', 'enabled', 'format', 'frequency', 'include_object_versions', 'inventory_id', 'objects_prefix', 'optional_fields']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.Inventory'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.InventoryDestination
class InventoryDestinationDef(BaseStruct):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Bucket where all inventories will be saved in.\n')
    bucket_owner: typing.Optional[str] = pydantic.Field(None, description="The account ID that owns the destination S3 bucket. If no account ID is provided, the owner is not validated before exporting data. It's recommended to set an account ID to prevent problems if the destination bucket ownership changes. Default: - No account ID.\n")
    prefix: typing.Optional[str] = pydantic.Field(None, description='The prefix to be used when saving the inventory. Default: - No prefix.\n\n:exampleMetadata: infused\n\nExample::\n\n    inventory_bucket = s3.Bucket(self, "InventoryBucket")\n\n    data_bucket = s3.Bucket(self, "DataBucket",\n        inventories=[s3.Inventory(\n            frequency=s3.InventoryFrequency.DAILY,\n            include_object_versions=s3.InventoryObjectVersion.CURRENT,\n            destination=s3.InventoryDestination(\n                bucket=inventory_bucket\n            )\n        ), s3.Inventory(\n            frequency=s3.InventoryFrequency.WEEKLY,\n            include_object_versions=s3.InventoryObjectVersion.ALL,\n            destination=s3.InventoryDestination(\n                bucket=inventory_bucket,\n                prefix="with-all-versions"\n            )\n        )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_owner', 'prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.InventoryDestination'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.InventoryDestinationDefConfig] = pydantic.Field(None)


class InventoryDestinationDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_s3.LifecycleRule
class LifecycleRuleDef(BaseStruct):
    abort_incomplete_multipart_upload_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies a lifecycle rule that aborts incomplete multipart uploads to an Amazon S3 bucket. The AbortIncompleteMultipartUpload property type creates a lifecycle rule that aborts incomplete multipart uploads to an Amazon S3 bucket. When Amazon S3 aborts a multipart upload, it deletes all parts associated with the multipart upload. The underlying configuration is expressed in whole numbers of days. Providing a Duration that does not represent a whole number of days will result in a runtime or deployment error. Default: - Incomplete uploads are never aborted\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether this rule is enabled. Default: true\n')
    expiration: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Indicates the number of days after creation when objects are deleted from Amazon S3 and Amazon Glacier. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. The underlying configuration is expressed in whole numbers of days. Providing a Duration that does not represent a whole number of days will result in a runtime or deployment error. Default: - No expiration timeout\n')
    expiration_date: typing.Optional[datetime.datetime] = pydantic.Field(None, description='Indicates when objects are deleted from Amazon S3 and Amazon Glacier. The date value must be in ISO 8601 format. The time is always midnight UTC. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. Default: - No expiration date\n')
    expired_object_delete_marker: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether Amazon S3 will remove a delete marker with no noncurrent versions. If set to true, the delete marker will be expired. Default: false\n')
    noncurrent_version_expiration: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time between when a new version of the object is uploaded to the bucket and when old versions of the object expire. For buckets with versioning enabled (or suspended), specifies the time, in days, between when a new version of the object is uploaded to the bucket and when old versions of the object expire. When object versions expire, Amazon S3 permanently deletes them. If you specify a transition and expiration time, the expiration time must be later than the transition time. The underlying configuration is expressed in whole numbers of days. Providing a Duration that does not represent a whole number of days will result in a runtime or deployment error. Default: - No noncurrent version expiration\n')
    noncurrent_versions_to_retain: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates a maximum number of noncurrent versions to retain. If there are this many more noncurrent versions, Amazon S3 permanently deletes them. Default: - No noncurrent versions to retain\n')
    noncurrent_version_transitions: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.NoncurrentVersionTransitionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more transition rules that specify when non-current objects transition to a specified storage class. Only for for buckets with versioning enabled (or suspended). If you specify a transition and expiration time, the expiration time must be later than the transition time.\n')
    object_size_greater_than: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the minimum object size in bytes for this rule to apply to. Default: - No rule\n')
    object_size_less_than: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum object size in bytes for this rule to apply to. Default: - No rule\n')
    prefix: typing.Optional[str] = pydantic.Field(None, description='Object key prefix that identifies one or more objects to which this rule applies. Default: - Rule applies to all objects\n')
    tag_filters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The TagFilter property type specifies tags to use to identify a subset of objects for an Amazon S3 bucket. Default: - Rule applies to all objects\n')
    transitions: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.TransitionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more transition rules that specify when an object transitions to a specified storage class. If you specify an expiration and transition time, you must use the same time unit for both properties (either in days or by date). The expiration time must also be later than the transition time. Default: - No transition rules\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_s3 as s3\n\n    # storage_class: s3.StorageClass\n    # tag_filters: Any\n\n    lifecycle_rule = s3.LifecycleRule(\n        abort_incomplete_multipart_upload_after=cdk.Duration.minutes(30),\n        enabled=False,\n        expiration=cdk.Duration.minutes(30),\n        expiration_date=Date(),\n        expired_object_delete_marker=False,\n        id="id",\n        noncurrent_version_expiration=cdk.Duration.minutes(30),\n        noncurrent_versions_to_retain=123,\n        noncurrent_version_transitions=[s3.NoncurrentVersionTransition(\n            storage_class=storage_class,\n            transition_after=cdk.Duration.minutes(30),\n\n            # the properties below are optional\n            noncurrent_versions_to_retain=123\n        )],\n        object_size_greater_than=123,\n        object_size_less_than=123,\n        prefix="prefix",\n        tag_filters={\n            "tag_filters_key": tag_filters\n        },\n        transitions=[s3.Transition(\n            storage_class=storage_class,\n\n            # the properties below are optional\n            transition_after=cdk.Duration.minutes(30),\n            transition_date=Date()\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['abort_incomplete_multipart_upload_after', 'enabled', 'expiration', 'expiration_date', 'expired_object_delete_marker', 'noncurrent_version_expiration', 'noncurrent_versions_to_retain', 'noncurrent_version_transitions', 'object_size_greater_than', 'object_size_less_than', 'prefix', 'tag_filters', 'transitions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.LifecycleRule'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.Location
class LocationDef(BaseStruct):
    bucket_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the S3 Bucket the object is in.\n')
    object_key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path inside the Bucket where the object is located at.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='The S3 object version.\n\n:exampleMetadata: lit=aws-codepipeline-actions/test/integ.lambda-deployed-through-codepipeline.lit.ts infused\n\nExample::\n\n    lambda_stack = cdk.Stack(app, "LambdaStack")\n    lambda_code = lambda_.Code.from_cfn_parameters()\n    lambda_.Function(lambda_stack, "Lambda",\n        code=lambda_code,\n        handler="index.handler",\n        runtime=lambda_.Runtime.NODEJS_LATEST\n    )\n    # other resources that your Lambda needs, added to the lambdaStack...\n\n    pipeline_stack = cdk.Stack(app, "PipelineStack")\n    pipeline = codepipeline.Pipeline(pipeline_stack, "Pipeline")\n\n    # add the source code repository containing this code to your Pipeline,\n    # and the source code of the Lambda Function, if they\'re separate\n    cdk_source_output = codepipeline.Artifact()\n    cdk_source_action = codepipeline_actions.CodeCommitSourceAction(\n        repository=codecommit.Repository(pipeline_stack, "CdkCodeRepo",\n            repository_name="CdkCodeRepo"\n        ),\n        action_name="CdkCode_Source",\n        output=cdk_source_output\n    )\n    lambda_source_output = codepipeline.Artifact()\n    lambda_source_action = codepipeline_actions.CodeCommitSourceAction(\n        repository=codecommit.Repository(pipeline_stack, "LambdaCodeRepo",\n            repository_name="LambdaCodeRepo"\n        ),\n        action_name="LambdaCode_Source",\n        output=lambda_source_output\n    )\n    pipeline.add_stage(\n        stage_name="Source",\n        actions=[cdk_source_action, lambda_source_action]\n    )\n\n    # synthesize the Lambda CDK template, using CodeBuild\n    # the below values are just examples, assuming your CDK code is in TypeScript/JavaScript -\n    # adjust the build environment and/or commands accordingly\n    cdk_build_project = codebuild.Project(pipeline_stack, "CdkBuildProject",\n        environment=codebuild.BuildEnvironment(\n            build_image=codebuild.LinuxBuildImage.STANDARD_7_0\n        ),\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2",\n            "phases": {\n                "install": {\n                    "commands": "npm install"\n                },\n                "build": {\n                    "commands": ["npm run build", "npm run cdk synth LambdaStack -- -o ."\n                    ]\n                }\n            },\n            "artifacts": {\n                "files": "LambdaStack.template.yaml"\n            }\n        })\n    )\n    cdk_build_output = codepipeline.Artifact()\n    cdk_build_action = codepipeline_actions.CodeBuildAction(\n        action_name="CDK_Build",\n        project=cdk_build_project,\n        input=cdk_source_output,\n        outputs=[cdk_build_output]\n    )\n\n    # build your Lambda code, using CodeBuild\n    # again, this example assumes your Lambda is written in TypeScript/JavaScript -\n    # make sure to adjust the build environment and/or commands if they don\'t match your specific situation\n    lambda_build_project = codebuild.Project(pipeline_stack, "LambdaBuildProject",\n        environment=codebuild.BuildEnvironment(\n            build_image=codebuild.LinuxBuildImage.STANDARD_7_0\n        ),\n        build_spec=codebuild.BuildSpec.from_object({\n            "version": "0.2",\n            "phases": {\n                "install": {\n                    "commands": "npm install"\n                },\n                "build": {\n                    "commands": "npm run build"\n                }\n            },\n            "artifacts": {\n                "files": ["index.js", "node_modules/**/*"\n                ]\n            }\n        })\n    )\n    lambda_build_output = codepipeline.Artifact()\n    lambda_build_action = codepipeline_actions.CodeBuildAction(\n        action_name="Lambda_Build",\n        project=lambda_build_project,\n        input=lambda_source_output,\n        outputs=[lambda_build_output]\n    )\n\n    pipeline.add_stage(\n        stage_name="Build",\n        actions=[cdk_build_action, lambda_build_action]\n    )\n\n    # finally, deploy your Lambda Stack\n    pipeline.add_stage(\n        stage_name="Deploy",\n        actions=[\n            codepipeline_actions.CloudFormationCreateUpdateStackAction(\n                action_name="Lambda_CFN_Deploy",\n                template_path=cdk_build_output.at_path("LambdaStack.template.yaml"),\n                stack_name="LambdaStackDeployedName",\n                admin_permissions=True,\n                parameter_overrides=lambda_code.assign(lambda_build_output.s3_location),\n                extra_inputs=[lambda_build_output\n                ]\n            )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket_name', 'object_key', 'object_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.Location'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.NoncurrentVersionTransition
class NoncurrentVersionTransitionDef(BaseStruct):
    storage_class: typing.Union[models.aws_s3.StorageClassDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The storage class to which you want the object to transition.\n')
    transition_after: typing.Union[models.DurationDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Indicates the number of days after creation when objects are transitioned to the specified storage class. Default: - No transition count.\n')
    noncurrent_versions_to_retain: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates the number of noncurrent version objects to be retained. Can be up to 100 noncurrent versions retained. Default: - No noncurrent version retained.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_s3 as s3\n\n    # storage_class: s3.StorageClass\n\n    noncurrent_version_transition = s3.NoncurrentVersionTransition(\n        storage_class=storage_class,\n        transition_after=cdk.Duration.minutes(30),\n\n        # the properties below are optional\n        noncurrent_versions_to_retain=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_class', 'transition_after', 'noncurrent_versions_to_retain']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.NoncurrentVersionTransition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.NoncurrentVersionTransitionDefConfig] = pydantic.Field(None)


class NoncurrentVersionTransitionDefConfig(pydantic.BaseModel):
    transition_after_config: typing.Optional[models.core.DurationDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_s3.NotificationKeyFilter
class NotificationKeyFilterDef(BaseStruct):
    prefix: typing.Optional[str] = pydantic.Field(None, description='S3 keys must have the specified prefix.')
    suffix: typing.Optional[str] = pydantic.Field(None, description='S3 keys must have the specified suffix.\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_queue: sqs.Queue\n\n    bucket = s3.Bucket(self, "MyBucket")\n    bucket.add_event_notification(s3.EventType.OBJECT_REMOVED, s3n.SqsDestination(my_queue),\n        prefix="foo/",\n        suffix=".jpg"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['prefix', 'suffix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.NotificationKeyFilter'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.OnCloudTrailBucketEventOptions
class OnCloudTrailBucketEventOptionsDef(BaseStruct):
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    paths: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Only watch changes to these object paths. Default: - Watch changes to all objects\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events as events\n    from aws_cdk import aws_s3 as s3\n    import constructs as constructs\n\n    # construct: constructs.Construct\n    # detail: Any\n    # rule_target: events.IRuleTarget\n\n    on_cloud_trail_bucket_event_options = s3.OnCloudTrailBucketEventOptions(\n        cross_stack_scope=construct,\n        description="description",\n        event_pattern=events.EventPattern(\n            account=["account"],\n            detail={\n                "detail_key": detail\n            },\n            detail_type=["detailType"],\n            id=["id"],\n            region=["region"],\n            resources=["resources"],\n            source=["source"],\n            time=["time"],\n            version=["version"]\n        ),\n        paths=["paths"],\n        rule_name="ruleName",\n        target=rule_target\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cross_stack_scope', 'description', 'event_pattern', 'rule_name', 'target', 'paths']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.OnCloudTrailBucketEventOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.RedirectTarget
class RedirectTargetDef(BaseStruct):
    host_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name of the host where requests are redirected.\n')
    protocol: typing.Optional[aws_cdk.aws_s3.RedirectProtocol] = pydantic.Field(None, description='Protocol to use when redirecting requests. Default: - The protocol used in the original request.\n\n:exampleMetadata: infused\n\nExample::\n\n    bucket = s3.Bucket(self, "MyRedirectedBucket",\n        website_redirect=s3.RedirectTarget(host_name="www.example.com")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['host_name', 'protocol']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.RedirectTarget'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.RoutingRule
class RoutingRuleDef(BaseStruct):
    condition: typing.Union[models.aws_s3.RoutingRuleConditionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies a condition that must be met for the specified redirect to apply. Default: - No condition\n')
    host_name: typing.Optional[str] = pydantic.Field(None, description='The host name to use in the redirect request. Default: - The host name used in the original request.\n')
    http_redirect_code: typing.Optional[str] = pydantic.Field(None, description='The HTTP redirect code to use on the response. Default: "301" - Moved Permanently\n')
    protocol: typing.Optional[aws_cdk.aws_s3.RedirectProtocol] = pydantic.Field(None, description='Protocol to use when redirecting requests. Default: - The protocol used in the original request.\n')
    replace_key: typing.Optional[models.aws_s3.ReplaceKeyDef] = pydantic.Field(None, description='Specifies the object key prefix to use in the redirect request. Default: - The key will not be replaced\n\n:see: https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # replace_key: s3.ReplaceKey\n\n    routing_rule = s3.RoutingRule(\n        condition=s3.RoutingRuleCondition(\n            http_error_code_returned_equals="httpErrorCodeReturnedEquals",\n            key_prefix_equals="keyPrefixEquals"\n        ),\n        host_name="hostName",\n        http_redirect_code="httpRedirectCode",\n        protocol=s3.RedirectProtocol.HTTP,\n        replace_key=replace_key\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['condition', 'host_name', 'http_redirect_code', 'protocol', 'replace_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.RoutingRule'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.RoutingRuleCondition
class RoutingRuleConditionDef(BaseStruct):
    http_error_code_returned_equals: typing.Optional[str] = pydantic.Field(None, description='The HTTP error code when the redirect is applied. In the event of an error, if the error code equals this value, then the specified redirect is applied. If both condition properties are specified, both must be true for the redirect to be applied. Default: - The HTTP error code will not be verified')
    key_prefix_equals: typing.Optional[str] = pydantic.Field(None, description='The object key name prefix when the redirect is applied. If both condition properties are specified, both must be true for the redirect to be applied. Default: - The object key name will not be verified\n\n:exampleMetadata: infused\n\nExample::\n\n    bucket = s3.Bucket(self, "MyRedirectedBucket",\n        website_routing_rules=[s3.RoutingRule(\n            host_name="www.example.com",\n            http_redirect_code="302",\n            protocol=s3.RedirectProtocol.HTTPS,\n            replace_key=s3.ReplaceKey.prefix_with("test/"),\n            condition=s3.RoutingRuleCondition(\n                http_error_code_returned_equals="200",\n                key_prefix_equals="prefix"\n            )\n        )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['http_error_code_returned_equals', 'key_prefix_equals']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.RoutingRuleCondition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.Tag
class TagDef(BaseStruct):
    key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='key to e tagged.\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='additional value.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    tag = s3.Tag(\n        key="key",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.Tag'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.TransferAccelerationUrlOptions
class TransferAccelerationUrlOptionsDef(BaseStruct):
    dual_stack: typing.Optional[bool] = pydantic.Field(None, description='Dual-stack support to connect to the bucket over IPv6. Default: - false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    transfer_acceleration_url_options = s3.TransferAccelerationUrlOptions(\n        dual_stack=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dual_stack']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.TransferAccelerationUrlOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.Transition
class TransitionDef(BaseStruct):
    storage_class: typing.Union[models.aws_s3.StorageClassDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The storage class to which you want the object to transition.\n')
    transition_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Indicates the number of days after creation when objects are transitioned to the specified storage class. Default: - No transition count.\n')
    transition_date: typing.Optional[datetime.datetime] = pydantic.Field(None, description='Indicates when objects are transitioned to the specified storage class. The date value must be in ISO 8601 format. The time is always midnight UTC. Default: - No transition date.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_s3 as s3\n\n    # storage_class: s3.StorageClass\n\n    transition = s3.Transition(\n        storage_class=storage_class,\n\n        # the properties below are optional\n        transition_after=cdk.Duration.minutes(30),\n        transition_date=Date()\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_class', 'transition_after', 'transition_date']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.Transition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.VirtualHostedStyleUrlOptions
class VirtualHostedStyleUrlOptionsDef(BaseStruct):
    regional: typing.Optional[bool] = pydantic.Field(None, description='Specifies the URL includes the region. Default: - true\n\n:exampleMetadata: infused\n\nExample::\n\n    bucket = s3.Bucket(self, "MyBucket")\n    bucket.url_for_object("objectname") # Path-Style URL\n    bucket.virtual_hosted_url_for_object("objectname") # Virtual Hosted-Style URL\n    bucket.virtual_hosted_url_for_object("objectname", regional=False)\n')
    _init_params: typing.ClassVar[list[str]] = ['regional']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.VirtualHostedStyleUrlOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.BucketAccessControl
# skipping emum

#  autogenerated from aws_cdk.aws_s3.BucketEncryption
# skipping emum

#  autogenerated from aws_cdk.aws_s3.BucketNotificationDestinationType
# skipping emum

#  autogenerated from aws_cdk.aws_s3.EventType
# skipping emum

#  autogenerated from aws_cdk.aws_s3.HttpMethods
# skipping emum

#  autogenerated from aws_cdk.aws_s3.InventoryFormat
# skipping emum

#  autogenerated from aws_cdk.aws_s3.InventoryFrequency
# skipping emum

#  autogenerated from aws_cdk.aws_s3.InventoryObjectVersion
# skipping emum

#  autogenerated from aws_cdk.aws_s3.ObjectLockMode
# skipping emum

#  autogenerated from aws_cdk.aws_s3.ObjectOwnership
# skipping emum

#  autogenerated from aws_cdk.aws_s3.RedirectProtocol
# skipping emum

#  autogenerated from aws_cdk.aws_s3.IBucket
#  skipping Interface

#  autogenerated from aws_cdk.aws_s3.IBucketNotificationDestination
#  skipping Interface

#  autogenerated from aws_cdk.aws_s3.CfnAccessPoint
class CfnAccessPointDef(BaseCfnResource):
    bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the bucket associated with this access point.\n')
    bucket_account_id: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID associated with the S3 bucket associated with this access point.\n')
    name: typing.Optional[str] = pydantic.Field(None, description="The name of this access point. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the access point name.\n")
    policy: typing.Any = pydantic.Field(None, description='The access point policy associated with this access point.\n')
    public_access_block_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnAccessPoint_PublicAccessBlockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The PublicAccessBlock configuration that you want to apply to this Amazon S3 bucket. You can enable the configuration options in any combination. For more information about when Amazon S3 considers a bucket or object public, see `The Meaning of "Public" <https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html#access-control-block-public-access-policy-status>`_ in the *Amazon S3 User Guide* .\n')
    vpc_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnAccessPoint_VpcConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Virtual Private Cloud (VPC) configuration for this access point, if one exists.')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_account_id', 'name', 'policy', 'public_access_block_configuration', 'vpc_configuration']
    _method_names: typing.ClassVar[list[str]] = ['PublicAccessBlockConfigurationProperty', 'VpcConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnAccessPoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.CfnAccessPointDefConfig] = pydantic.Field(None)


class CfnAccessPointDefConfig(pydantic.BaseModel):
    PublicAccessBlockConfigurationProperty: typing.Optional[list[models.aws_s3.CfnAccessPointDefPublicaccessblockconfigurationpropertyParams]] = pydantic.Field(None, description='')
    VpcConfigurationProperty: typing.Optional[list[models.aws_s3.CfnAccessPointDefVpcconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_s3.CfnAccessPointDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_s3.CfnAccessPointDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_s3.CfnAccessPointDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_s3.CfnAccessPointDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_s3.CfnAccessPointDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_s3.CfnAccessPointDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_s3.CfnAccessPointDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_s3.CfnAccessPointDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_s3.CfnAccessPointDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_s3.CfnAccessPointDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_s3.CfnAccessPointDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_s3.CfnAccessPointDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_s3.CfnAccessPointDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnAccessPointDefPublicaccessblockconfigurationpropertyParams(pydantic.BaseModel):
    block_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    block_public_policy: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ignore_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    restrict_public_buckets: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnAccessPointDefVpcconfigurationpropertyParams(pydantic.BaseModel):
    vpc_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnAccessPointDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnAccessPointDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnAccessPointDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnAccessPointDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnAccessPointDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnAccessPointDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnAccessPointDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnAccessPointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnAccessPointDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnAccessPointDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnAccessPointDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnAccessPointDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnAccessPointDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnAccessPointDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_s3.CfnBucket
class CfnBucketDef(BaseCfnResource):
    accelerate_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AccelerateConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configures the transfer acceleration state for an Amazon S3 bucket. For more information, see `Amazon S3 Transfer Acceleration <https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html>`_ in the *Amazon S3 User Guide* .\n')
    access_control: typing.Optional[str] = pydantic.Field(None, description='.. epigraph:: This is a legacy property, and it is not recommended for most use cases. A majority of modern use cases in Amazon S3 no longer require the use of ACLs, and we recommend that you keep ACLs disabled. For more information, see `Controlling object ownership <https://docs.aws.amazon.com//AmazonS3/latest/userguide/about-object-ownership.html>`_ in the *Amazon S3 User Guide* . A canned access control list (ACL) that grants predefined permissions to the bucket. For more information about canned ACLs, see `Canned ACL <https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl>`_ in the *Amazon S3 User Guide* . S3 buckets are created with ACLs disabled by default. Therefore, unless you explicitly set the `AWS::S3::OwnershipControls <https://docs.aws.amazon.com//AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-ownershipcontrols.html>`_ property to enable ACLs, your resource will fail to deploy with any value other than Private. Use cases requiring ACLs are uncommon. The majority of access control configurations can be successfully and more easily achieved with bucket policies. For more information, see `AWS::S3::BucketPolicy <https://docs.aws.amazon.com//AWSCloudFormation/latest/UserGuide/aws-properties-s3-policy.html>`_ . For examples of common policy configurations, including S3 Server Access Logs buckets and more, see `Bucket policy examples <https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html>`_ in the *Amazon S3 User Guide* .\n')
    analytics_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AnalyticsConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the configuration and any analyses for the analytics filter of an Amazon S3 bucket.\n')
    bucket_encryption: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_BucketEncryptionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies default encryption for a bucket using server-side encryption with Amazon S3-managed keys (SSE-S3), AWS KMS-managed keys (SSE-KMS), or dual-layer server-side encryption with KMS-managed keys (DSSE-KMS). For information about the Amazon S3 default encryption feature, see `Amazon S3 Default Encryption for S3 Buckets <https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html>`_ in the *Amazon S3 User Guide* .\n')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description="A name for the bucket. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the bucket name. The bucket name must contain only lowercase letters, numbers, periods (.), and dashes (-) and must follow `Amazon S3 bucket restrictions and limitations <https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html>`_ . For more information, see `Rules for naming Amazon S3 buckets <https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules>`_ in the *Amazon S3 User Guide* . .. epigraph:: If you specify a name, you can't perform updates that require replacement of this resource. You can perform updates that require no or some interruption. If you need to replace the resource, specify a new name.\n")
    cors_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_CorsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Describes the cross-origin access configuration for objects in an Amazon S3 bucket. For more information, see `Enabling Cross-Origin Resource Sharing <https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html>`_ in the *Amazon S3 User Guide* .\n')
    intelligent_tiering_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_IntelligentTieringConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Defines how Amazon S3 handles Intelligent-Tiering storage.\n')
    inventory_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_InventoryConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the inventory configuration for an Amazon S3 bucket. For more information, see `GET Bucket inventory <https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETInventoryConfig.html>`_ in the *Amazon S3 API Reference* .\n')
    lifecycle_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_LifecycleConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the lifecycle configuration for objects in an Amazon S3 bucket. For more information, see `Object Lifecycle Management <https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html>`_ in the *Amazon S3 User Guide* .\n')
    logging_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_LoggingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings that define where logs are stored.\n')
    metrics_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_MetricsConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Specifies a metrics configuration for the CloudWatch request metrics (specified by the metrics configuration ID) from an Amazon S3 bucket. If you're updating an existing metrics configuration, note that this is a full replacement of the existing metrics configuration. If you don't include the elements you want to keep, they are erased. For more information, see `PutBucketMetricsConfiguration <https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTMetricConfiguration.html>`_ .\n")
    notification_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration that defines how Amazon S3 handles bucket notifications.\n')
    object_lock_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ObjectLockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Places an Object Lock configuration on the specified bucket. The rule specified in the Object Lock configuration will be applied by default to every new object placed in the specified bucket. For more information, see `Locking Objects <https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html>`_ . .. epigraph:: - The ``DefaultRetention`` settings require both a mode and a period. - The ``DefaultRetention`` period can be either ``Days`` or ``Years`` but you must select one. You cannot specify ``Days`` and ``Years`` at the same time. - You can only enable Object Lock for new buckets. If you want to turn on Object Lock for an existing bucket, contact AWS Support.\n')
    object_lock_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether this bucket has an Object Lock configuration enabled. Enable ``ObjectLockEnabled`` when you apply ``ObjectLockConfiguration`` to a bucket.\n')
    ownership_controls: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_OwnershipControlsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration that defines how Amazon S3 handles Object Ownership rules.\n')
    public_access_block_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_PublicAccessBlockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration that defines how Amazon S3 handles public access.\n')
    replication_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for replicating objects in an S3 bucket. To enable replication, you must also enable versioning by using the ``VersioningConfiguration`` property. Amazon S3 can store replicated objects in a single destination bucket or multiple destination buckets. The destination bucket or buckets must already exist.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An arbitrary set of tags (key-value pairs) for this S3 bucket.\n')
    versioning_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_VersioningConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Enables multiple versions of all objects in this bucket. You might enable versioning to prevent objects from being deleted or overwritten by mistake or to archive objects so that you can retrieve previous versions of them.\n')
    website_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_WebsiteConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information used to configure the bucket as a static website. For more information, see `Hosting Websites on Amazon S3 <https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html>`_ .')
    _init_params: typing.ClassVar[list[str]] = ['accelerate_configuration', 'access_control', 'analytics_configurations', 'bucket_encryption', 'bucket_name', 'cors_configuration', 'intelligent_tiering_configurations', 'inventory_configurations', 'lifecycle_configuration', 'logging_configuration', 'metrics_configurations', 'notification_configuration', 'object_lock_configuration', 'object_lock_enabled', 'ownership_controls', 'public_access_block_configuration', 'replication_configuration', 'tags', 'versioning_configuration', 'website_configuration']
    _method_names: typing.ClassVar[list[str]] = ['AbortIncompleteMultipartUploadProperty', 'AccelerateConfigurationProperty', 'AccessControlTranslationProperty', 'AnalyticsConfigurationProperty', 'BucketEncryptionProperty', 'CorsConfigurationProperty', 'CorsRuleProperty', 'DataExportProperty', 'DefaultRetentionProperty', 'DeleteMarkerReplicationProperty', 'DestinationProperty', 'EncryptionConfigurationProperty', 'EventBridgeConfigurationProperty', 'FilterRuleProperty', 'IntelligentTieringConfigurationProperty', 'InventoryConfigurationProperty', 'LambdaConfigurationProperty', 'LifecycleConfigurationProperty', 'LoggingConfigurationProperty', 'MetricsConfigurationProperty', 'MetricsProperty', 'NoncurrentVersionExpirationProperty', 'NoncurrentVersionTransitionProperty', 'NotificationConfigurationProperty', 'NotificationFilterProperty', 'ObjectLockConfigurationProperty', 'ObjectLockRuleProperty', 'OwnershipControlsProperty', 'OwnershipControlsRuleProperty', 'PublicAccessBlockConfigurationProperty', 'QueueConfigurationProperty', 'RedirectAllRequestsToProperty', 'RedirectRuleProperty', 'ReplicaModificationsProperty', 'ReplicationConfigurationProperty', 'ReplicationDestinationProperty', 'ReplicationRuleAndOperatorProperty', 'ReplicationRuleFilterProperty', 'ReplicationRuleProperty', 'ReplicationTimeProperty', 'ReplicationTimeValueProperty', 'RoutingRuleConditionProperty', 'RoutingRuleProperty', 'RuleProperty', 'S3KeyFilterProperty', 'ServerSideEncryptionByDefaultProperty', 'ServerSideEncryptionRuleProperty', 'SourceSelectionCriteriaProperty', 'SseKmsEncryptedObjectsProperty', 'StorageClassAnalysisProperty', 'TagFilterProperty', 'TieringProperty', 'TopicConfigurationProperty', 'TransitionProperty', 'VersioningConfigurationProperty', 'WebsiteConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucket'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.CfnBucketDefConfig] = pydantic.Field(None)


class CfnBucketDefConfig(pydantic.BaseModel):
    AbortIncompleteMultipartUploadProperty: typing.Optional[list[models.aws_s3.CfnBucketDefAbortincompletemultipartuploadpropertyParams]] = pydantic.Field(None, description='')
    AccelerateConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefAccelerateconfigurationpropertyParams]] = pydantic.Field(None, description='')
    AccessControlTranslationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefAccesscontroltranslationpropertyParams]] = pydantic.Field(None, description='')
    AnalyticsConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefAnalyticsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    BucketEncryptionProperty: typing.Optional[list[models.aws_s3.CfnBucketDefBucketencryptionpropertyParams]] = pydantic.Field(None, description='')
    CorsConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefCorsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    CorsRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefCorsrulepropertyParams]] = pydantic.Field(None, description='')
    DataExportProperty: typing.Optional[list[models.aws_s3.CfnBucketDefDataexportpropertyParams]] = pydantic.Field(None, description='')
    DefaultRetentionProperty: typing.Optional[list[models.aws_s3.CfnBucketDefDefaultretentionpropertyParams]] = pydantic.Field(None, description='')
    DeleteMarkerReplicationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefDeletemarkerreplicationpropertyParams]] = pydantic.Field(None, description='')
    DestinationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefDestinationpropertyParams]] = pydantic.Field(None, description='')
    EncryptionConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefEncryptionconfigurationpropertyParams]] = pydantic.Field(None, description='')
    EventBridgeConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefEventbridgeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    FilterRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefFilterrulepropertyParams]] = pydantic.Field(None, description='')
    IntelligentTieringConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefIntelligenttieringconfigurationpropertyParams]] = pydantic.Field(None, description='')
    InventoryConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefInventoryconfigurationpropertyParams]] = pydantic.Field(None, description='')
    LambdaConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefLambdaconfigurationpropertyParams]] = pydantic.Field(None, description='')
    LifecycleConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefLifecycleconfigurationpropertyParams]] = pydantic.Field(None, description='')
    LoggingConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefLoggingconfigurationpropertyParams]] = pydantic.Field(None, description='')
    MetricsConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefMetricsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    MetricsProperty: typing.Optional[list[models.aws_s3.CfnBucketDefMetricspropertyParams]] = pydantic.Field(None, description='')
    NoncurrentVersionExpirationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefNoncurrentversionexpirationpropertyParams]] = pydantic.Field(None, description='')
    NoncurrentVersionTransitionProperty: typing.Optional[list[models.aws_s3.CfnBucketDefNoncurrentversiontransitionpropertyParams]] = pydantic.Field(None, description='')
    NotificationConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefNotificationconfigurationpropertyParams]] = pydantic.Field(None, description='')
    NotificationFilterProperty: typing.Optional[list[models.aws_s3.CfnBucketDefNotificationfilterpropertyParams]] = pydantic.Field(None, description='')
    ObjectLockConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefObjectlockconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ObjectLockRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefObjectlockrulepropertyParams]] = pydantic.Field(None, description='')
    OwnershipControlsProperty: typing.Optional[list[models.aws_s3.CfnBucketDefOwnershipcontrolspropertyParams]] = pydantic.Field(None, description='')
    OwnershipControlsRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefOwnershipcontrolsrulepropertyParams]] = pydantic.Field(None, description='')
    PublicAccessBlockConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefPublicaccessblockconfigurationpropertyParams]] = pydantic.Field(None, description='')
    QueueConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefQueueconfigurationpropertyParams]] = pydantic.Field(None, description='')
    RedirectAllRequestsToProperty: typing.Optional[list[models.aws_s3.CfnBucketDefRedirectallrequeststopropertyParams]] = pydantic.Field(None, description='')
    RedirectRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefRedirectrulepropertyParams]] = pydantic.Field(None, description='')
    ReplicaModificationsProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicamodificationspropertyParams]] = pydantic.Field(None, description='')
    ReplicationConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicationconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ReplicationDestinationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicationdestinationpropertyParams]] = pydantic.Field(None, description='')
    ReplicationRuleAndOperatorProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicationruleandoperatorpropertyParams]] = pydantic.Field(None, description='')
    ReplicationRuleFilterProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicationrulefilterpropertyParams]] = pydantic.Field(None, description='')
    ReplicationRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicationrulepropertyParams]] = pydantic.Field(None, description='')
    ReplicationTimeProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicationtimepropertyParams]] = pydantic.Field(None, description='')
    ReplicationTimeValueProperty: typing.Optional[list[models.aws_s3.CfnBucketDefReplicationtimevaluepropertyParams]] = pydantic.Field(None, description='')
    RoutingRuleConditionProperty: typing.Optional[list[models.aws_s3.CfnBucketDefRoutingruleconditionpropertyParams]] = pydantic.Field(None, description='')
    RoutingRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefRoutingrulepropertyParams]] = pydantic.Field(None, description='')
    RuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefRulepropertyParams]] = pydantic.Field(None, description='')
    S3KeyFilterProperty: typing.Optional[list[models.aws_s3.CfnBucketDefS3KeyfilterpropertyParams]] = pydantic.Field(None, description='')
    ServerSideEncryptionByDefaultProperty: typing.Optional[list[models.aws_s3.CfnBucketDefServersideencryptionbydefaultpropertyParams]] = pydantic.Field(None, description='')
    ServerSideEncryptionRuleProperty: typing.Optional[list[models.aws_s3.CfnBucketDefServersideencryptionrulepropertyParams]] = pydantic.Field(None, description='')
    SourceSelectionCriteriaProperty: typing.Optional[list[models.aws_s3.CfnBucketDefSourceselectioncriteriapropertyParams]] = pydantic.Field(None, description='')
    SseKmsEncryptedObjectsProperty: typing.Optional[list[models.aws_s3.CfnBucketDefSsekmsencryptedobjectspropertyParams]] = pydantic.Field(None, description='')
    StorageClassAnalysisProperty: typing.Optional[list[models.aws_s3.CfnBucketDefStorageclassanalysispropertyParams]] = pydantic.Field(None, description='')
    TagFilterProperty: typing.Optional[list[models.aws_s3.CfnBucketDefTagfilterpropertyParams]] = pydantic.Field(None, description='')
    TieringProperty: typing.Optional[list[models.aws_s3.CfnBucketDefTieringpropertyParams]] = pydantic.Field(None, description='')
    TopicConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefTopicconfigurationpropertyParams]] = pydantic.Field(None, description='')
    TransitionProperty: typing.Optional[list[models.aws_s3.CfnBucketDefTransitionpropertyParams]] = pydantic.Field(None, description='')
    VersioningConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefVersioningconfigurationpropertyParams]] = pydantic.Field(None, description='')
    WebsiteConfigurationProperty: typing.Optional[list[models.aws_s3.CfnBucketDefWebsiteconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_s3.CfnBucketDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_s3.CfnBucketDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_s3.CfnBucketDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_s3.CfnBucketDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_s3.CfnBucketDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_s3.CfnBucketDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_s3.CfnBucketDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_s3.CfnBucketDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_s3.CfnBucketDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_s3.CfnBucketDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_s3.CfnBucketDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_s3.CfnBucketDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_s3.CfnBucketDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnBucketDefAbortincompletemultipartuploadpropertyParams(pydantic.BaseModel):
    days_after_initiation: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnBucketDefAccelerateconfigurationpropertyParams(pydantic.BaseModel):
    acceleration_status: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefAccesscontroltranslationpropertyParams(pydantic.BaseModel):
    owner: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefAnalyticsconfigurationpropertyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='')
    storage_class_analysis: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_StorageClassAnalysisPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefBucketencryptionpropertyParams(pydantic.BaseModel):
    server_side_encryption_configuration: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ServerSideEncryptionRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefCorsconfigurationpropertyParams(pydantic.BaseModel):
    cors_rules: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_CorsRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefCorsrulepropertyParams(pydantic.BaseModel):
    allowed_methods: typing.Sequence[str] = pydantic.Field(..., description='')
    allowed_origins: typing.Sequence[str] = pydantic.Field(..., description='')
    allowed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    exposed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    id: typing.Optional[str] = pydantic.Field(None, description='')
    max_age: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefDataexportpropertyParams(pydantic.BaseModel):
    destination: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DestinationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    output_schema_version: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefDefaultretentionpropertyParams(pydantic.BaseModel):
    days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    mode: typing.Optional[str] = pydantic.Field(None, description='')
    years: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefDeletemarkerreplicationpropertyParams(pydantic.BaseModel):
    status: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefDestinationpropertyParams(pydantic.BaseModel):
    bucket_arn: str = pydantic.Field(..., description='')
    format: str = pydantic.Field(..., description='')
    bucket_account_id: typing.Optional[str] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefEncryptionconfigurationpropertyParams(pydantic.BaseModel):
    replica_kms_key_id: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefEventbridgeconfigurationpropertyParams(pydantic.BaseModel):
    event_bridge_enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    ...

class CfnBucketDefFilterrulepropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefIntelligenttieringconfigurationpropertyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='')
    status: str = pydantic.Field(..., description='')
    tierings: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TieringPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefInventoryconfigurationpropertyParams(pydantic.BaseModel):
    destination: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DestinationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    id: str = pydantic.Field(..., description='')
    included_object_versions: str = pydantic.Field(..., description='')
    schedule_frequency: str = pydantic.Field(..., description='')
    optional_fields: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefLambdaconfigurationpropertyParams(pydantic.BaseModel):
    event: str = pydantic.Field(..., description='')
    function: str = pydantic.Field(..., description='')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefLifecycleconfigurationpropertyParams(pydantic.BaseModel):
    rules: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefLoggingconfigurationpropertyParams(pydantic.BaseModel):
    destination_bucket_name: typing.Optional[str] = pydantic.Field(None, description='')
    log_file_prefix: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefMetricsconfigurationpropertyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='')
    access_point_arn: typing.Optional[str] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefMetricspropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    event_threshold: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationTimeValuePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefNoncurrentversionexpirationpropertyParams(pydantic.BaseModel):
    noncurrent_days: typing.Union[int, float] = pydantic.Field(..., description='')
    newer_noncurrent_versions: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefNoncurrentversiontransitionpropertyParams(pydantic.BaseModel):
    storage_class: str = pydantic.Field(..., description='')
    transition_in_days: typing.Union[int, float] = pydantic.Field(..., description='')
    newer_noncurrent_versions: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefNotificationconfigurationpropertyParams(pydantic.BaseModel):
    event_bridge_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_EventBridgeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    lambda_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_LambdaConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    queue_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_QueueConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    topic_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TopicConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefNotificationfilterpropertyParams(pydantic.BaseModel):
    s3_key: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_S3KeyFilterPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefObjectlockconfigurationpropertyParams(pydantic.BaseModel):
    object_lock_enabled: typing.Optional[str] = pydantic.Field(None, description='')
    rule: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ObjectLockRulePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefObjectlockrulepropertyParams(pydantic.BaseModel):
    default_retention: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DefaultRetentionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefOwnershipcontrolspropertyParams(pydantic.BaseModel):
    rules: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_OwnershipControlsRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefOwnershipcontrolsrulepropertyParams(pydantic.BaseModel):
    object_ownership: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefPublicaccessblockconfigurationpropertyParams(pydantic.BaseModel):
    block_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    block_public_policy: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ignore_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    restrict_public_buckets: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefQueueconfigurationpropertyParams(pydantic.BaseModel):
    event: str = pydantic.Field(..., description='')
    queue: str = pydantic.Field(..., description='')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefRedirectallrequeststopropertyParams(pydantic.BaseModel):
    host_name: str = pydantic.Field(..., description='')
    protocol: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefRedirectrulepropertyParams(pydantic.BaseModel):
    host_name: typing.Optional[str] = pydantic.Field(None, description='')
    http_redirect_code: typing.Optional[str] = pydantic.Field(None, description='')
    protocol: typing.Optional[str] = pydantic.Field(None, description='')
    replace_key_prefix_with: typing.Optional[str] = pydantic.Field(None, description='')
    replace_key_with: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefReplicamodificationspropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefReplicationconfigurationpropertyParams(pydantic.BaseModel):
    role: str = pydantic.Field(..., description='')
    rules: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefReplicationdestinationpropertyParams(pydantic.BaseModel):
    bucket: str = pydantic.Field(..., description='')
    access_control_translation: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AccessControlTranslationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    account: typing.Optional[str] = pydantic.Field(None, description='')
    encryption_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_EncryptionConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_MetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    replication_time: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationTimePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    storage_class: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefReplicationruleandoperatorpropertyParams(pydantic.BaseModel):
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefReplicationrulefilterpropertyParams(pydantic.BaseModel):
    and_: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationRuleAndOperatorPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    tag_filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefReplicationrulepropertyParams(pydantic.BaseModel):
    destination: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationDestinationPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    status: str = pydantic.Field(..., description='')
    delete_marker_replication: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DeleteMarkerReplicationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationRuleFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    id: typing.Optional[str] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='')
    source_selection_criteria: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_SourceSelectionCriteriaPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefReplicationtimepropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    time: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationTimeValuePropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefReplicationtimevaluepropertyParams(pydantic.BaseModel):
    minutes: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnBucketDefRoutingruleconditionpropertyParams(pydantic.BaseModel):
    http_error_code_returned_equals: typing.Optional[str] = pydantic.Field(None, description='')
    key_prefix_equals: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefRoutingrulepropertyParams(pydantic.BaseModel):
    redirect_rule: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RedirectRulePropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    routing_rule_condition: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RoutingRuleConditionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefRulepropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    abort_incomplete_multipart_upload: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AbortIncompleteMultipartUploadPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    expiration_date: typing.Union[models.UnsupportedResource, datetime.datetime, None] = pydantic.Field(None, description='')
    expiration_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    expired_object_delete_marker: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    id: typing.Optional[str] = pydantic.Field(None, description='')
    noncurrent_version_expiration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NoncurrentVersionExpirationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    noncurrent_version_expiration_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    noncurrent_version_transition: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NoncurrentVersionTransitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    noncurrent_version_transitions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NoncurrentVersionTransitionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    object_size_greater_than: typing.Union[int, float, None] = pydantic.Field(None, description='')
    object_size_less_than: typing.Union[int, float, None] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    tag_filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TagFilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    transition: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TransitionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    transitions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_TransitionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefS3KeyfilterpropertyParams(pydantic.BaseModel):
    rules: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_FilterRulePropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnBucketDefServersideencryptionbydefaultpropertyParams(pydantic.BaseModel):
    sse_algorithm: str = pydantic.Field(..., description='')
    kms_master_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnBucketDefServersideencryptionrulepropertyParams(pydantic.BaseModel):
    bucket_key_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    server_side_encryption_by_default: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ServerSideEncryptionByDefaultPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefSourceselectioncriteriapropertyParams(pydantic.BaseModel):
    replica_modifications: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicaModificationsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    sse_kms_encrypted_objects: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_SseKmsEncryptedObjectsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefSsekmsencryptedobjectspropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefStorageclassanalysispropertyParams(pydantic.BaseModel):
    data_export: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_DataExportPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefTagfilterpropertyParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='')
    value: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefTieringpropertyParams(pydantic.BaseModel):
    access_tier: str = pydantic.Field(..., description='')
    days: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnBucketDefTopicconfigurationpropertyParams(pydantic.BaseModel):
    event: str = pydantic.Field(..., description='')
    topic: str = pydantic.Field(..., description='')
    filter: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationFilterPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefTransitionpropertyParams(pydantic.BaseModel):
    storage_class: str = pydantic.Field(..., description='')
    transition_date: typing.Union[models.UnsupportedResource, datetime.datetime, None] = pydantic.Field(None, description='')
    transition_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefVersioningconfigurationpropertyParams(pydantic.BaseModel):
    status: str = pydantic.Field(..., description='')
    ...

class CfnBucketDefWebsiteconfigurationpropertyParams(pydantic.BaseModel):
    error_document: typing.Optional[str] = pydantic.Field(None, description='')
    index_document: typing.Optional[str] = pydantic.Field(None, description='')
    redirect_all_requests_to: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RedirectAllRequestsToPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    routing_rules: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_RoutingRulePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnBucketDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnBucketDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnBucketDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnBucketDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnBucketDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnBucketDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnBucketDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnBucketDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnBucketDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnBucketDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnBucketDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnBucketDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnBucketDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnBucketDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_s3.CfnBucketPolicy
class CfnBucketPolicyDef(BaseCfnResource):
    bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Amazon S3 bucket to which the policy applies.\n')
    policy_document: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A policy document containing permissions to add to the specified bucket. In IAM, you must provide policy documents in JSON format. However, in CloudFormation you can provide the policy in JSON or YAML format because CloudFormation converts YAML to JSON before submitting it to IAM. For more information, see the AWS::IAM::Policy `PolicyDocument <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-policy.html#cfn-iam-policy-policydocument>`_ resource description in this guide and `Access Policy Language Overview <https://docs.aws.amazon.com/AmazonS3/latest/dev/access-policy-language-overview.html>`_ in the *Amazon S3 User Guide* .')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'policy_document']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucketPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.CfnBucketPolicyDefConfig] = pydantic.Field(None)


class CfnBucketPolicyDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_s3.CfnBucketPolicyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnBucketPolicyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnBucketPolicyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnBucketPolicyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnBucketPolicyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnBucketPolicyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnBucketPolicyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnBucketPolicyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnBucketPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnBucketPolicyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnBucketPolicyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnBucketPolicyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnBucketPolicyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnBucketPolicyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnBucketPolicyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_s3.CfnMultiRegionAccessPoint
class CfnMultiRegionAccessPointDef(BaseCfnResource):
    regions: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnMultiRegionAccessPoint_RegionPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A collection of the Regions and buckets associated with the Multi-Region Access Point.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the Multi-Region Access Point.\n')
    public_access_block_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnMultiRegionAccessPoint_PublicAccessBlockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The PublicAccessBlock configuration that you want to apply to this Multi-Region Access Point. You can enable the configuration options in any combination. For more information about when Amazon S3 considers an object public, see `The Meaning of "Public" <https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html#access-control-block-public-access-policy-status>`_ in the *Amazon S3 User Guide* .')
    _init_params: typing.ClassVar[list[str]] = ['regions', 'name', 'public_access_block_configuration']
    _method_names: typing.ClassVar[list[str]] = ['PublicAccessBlockConfigurationProperty', 'RegionProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnMultiRegionAccessPoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.CfnMultiRegionAccessPointDefConfig] = pydantic.Field(None)


class CfnMultiRegionAccessPointDefConfig(pydantic.BaseModel):
    PublicAccessBlockConfigurationProperty: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefPublicaccessblockconfigurationpropertyParams]] = pydantic.Field(None, description='')
    RegionProperty: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefRegionpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnMultiRegionAccessPointDefPublicaccessblockconfigurationpropertyParams(pydantic.BaseModel):
    block_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    block_public_policy: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ignore_public_acls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    restrict_public_buckets: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnMultiRegionAccessPointDefRegionpropertyParams(pydantic.BaseModel):
    bucket: str = pydantic.Field(..., description='')
    bucket_account_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnMultiRegionAccessPointDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnMultiRegionAccessPointDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMultiRegionAccessPointDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnMultiRegionAccessPointDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMultiRegionAccessPointDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnMultiRegionAccessPointDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnMultiRegionAccessPointDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnMultiRegionAccessPointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnMultiRegionAccessPointDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnMultiRegionAccessPointDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMultiRegionAccessPointDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnMultiRegionAccessPointDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnMultiRegionAccessPointDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMultiRegionAccessPointDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_s3.CfnMultiRegionAccessPointPolicy
class CfnMultiRegionAccessPointPolicyDef(BaseCfnResource):
    mrap_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Multi-Region Access Point.\n')
    policy: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The access policy associated with the Multi-Region Access Point.')
    _init_params: typing.ClassVar[list[str]] = ['mrap_name', 'policy']
    _method_names: typing.ClassVar[list[str]] = ['PolicyStatusProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnMultiRegionAccessPointPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.CfnMultiRegionAccessPointPolicyDefConfig] = pydantic.Field(None)


class CfnMultiRegionAccessPointPolicyDefConfig(pydantic.BaseModel):
    PolicyStatusProperty: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefPolicystatuspropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_s3.CfnMultiRegionAccessPointPolicyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    attr_policy_status_config: typing.Optional[models._interface_methods.CoreIResolvableDefConfig] = pydantic.Field(None)

class CfnMultiRegionAccessPointPolicyDefPolicystatuspropertyParams(pydantic.BaseModel):
    is_public: str = pydantic.Field(..., description='')
    ...

class CfnMultiRegionAccessPointPolicyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnMultiRegionAccessPointPolicyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMultiRegionAccessPointPolicyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnMultiRegionAccessPointPolicyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMultiRegionAccessPointPolicyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnMultiRegionAccessPointPolicyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnMultiRegionAccessPointPolicyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnMultiRegionAccessPointPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnMultiRegionAccessPointPolicyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnMultiRegionAccessPointPolicyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMultiRegionAccessPointPolicyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnMultiRegionAccessPointPolicyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnMultiRegionAccessPointPolicyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMultiRegionAccessPointPolicyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_s3.CfnStorageLens
class CfnStorageLensDef(BaseCfnResource):
    storage_lens_configuration: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnStorageLens_StorageLensConfigurationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='This resource contains the details Amazon S3 Storage Lens configuration.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A set of tags (key–value pairs) to associate with the Storage Lens configuration.')
    _init_params: typing.ClassVar[list[str]] = ['storage_lens_configuration', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['AccountLevelProperty', 'ActivityMetricsProperty', 'AdvancedCostOptimizationMetricsProperty', 'AdvancedDataProtectionMetricsProperty', 'AwsOrgProperty', 'BucketLevelProperty', 'BucketsAndRegionsProperty', 'CloudWatchMetricsProperty', 'DataExportProperty', 'DetailedStatusCodesMetricsProperty', 'EncryptionProperty', 'PrefixLevelProperty', 'PrefixLevelStorageMetricsProperty', 'S3BucketDestinationProperty', 'SSEKMSProperty', 'SelectionCriteriaProperty', 'StorageLensConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLens'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_s3.CfnStorageLensDefConfig] = pydantic.Field(None)


class CfnStorageLensDefConfig(pydantic.BaseModel):
    AccountLevelProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefAccountlevelpropertyParams]] = pydantic.Field(None, description='')
    ActivityMetricsProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefActivitymetricspropertyParams]] = pydantic.Field(None, description='')
    AdvancedCostOptimizationMetricsProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefAdvancedcostoptimizationmetricspropertyParams]] = pydantic.Field(None, description='')
    AdvancedDataProtectionMetricsProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefAdvanceddataprotectionmetricspropertyParams]] = pydantic.Field(None, description='')
    AwsOrgProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefAwsorgpropertyParams]] = pydantic.Field(None, description='')
    BucketLevelProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefBucketlevelpropertyParams]] = pydantic.Field(None, description='')
    BucketsAndRegionsProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefBucketsandregionspropertyParams]] = pydantic.Field(None, description='')
    CloudWatchMetricsProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefCloudwatchmetricspropertyParams]] = pydantic.Field(None, description='')
    DataExportProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefDataexportpropertyParams]] = pydantic.Field(None, description='')
    DetailedStatusCodesMetricsProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefDetailedstatuscodesmetricspropertyParams]] = pydantic.Field(None, description='')
    EncryptionProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefEncryptionpropertyParams]] = pydantic.Field(None, description='')
    PrefixLevelProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefPrefixlevelpropertyParams]] = pydantic.Field(None, description='')
    PrefixLevelStorageMetricsProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefPrefixlevelstoragemetricspropertyParams]] = pydantic.Field(None, description='')
    S3BucketDestinationProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefS3BucketdestinationpropertyParams]] = pydantic.Field(None, description='')
    SSEKMSProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefSsekmspropertyParams]] = pydantic.Field(None, description='')
    SelectionCriteriaProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefSelectioncriteriapropertyParams]] = pydantic.Field(None, description='')
    StorageLensConfigurationProperty: typing.Optional[list[models.aws_s3.CfnStorageLensDefStoragelensconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_s3.CfnStorageLensDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_s3.CfnStorageLensDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_s3.CfnStorageLensDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_s3.CfnStorageLensDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_s3.CfnStorageLensDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_s3.CfnStorageLensDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_s3.CfnStorageLensDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_s3.CfnStorageLensDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_s3.CfnStorageLensDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_s3.CfnStorageLensDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_s3.CfnStorageLensDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_s3.CfnStorageLensDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_s3.CfnStorageLensDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnStorageLensDefAccountlevelpropertyParams(pydantic.BaseModel):
    bucket_level: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_BucketLevelPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    activity_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_ActivityMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    advanced_cost_optimization_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedCostOptimizationMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    advanced_data_protection_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedDataProtectionMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    detailed_status_codes_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_DetailedStatusCodesMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefActivitymetricspropertyParams(pydantic.BaseModel):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefAdvancedcostoptimizationmetricspropertyParams(pydantic.BaseModel):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefAdvanceddataprotectionmetricspropertyParams(pydantic.BaseModel):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefAwsorgpropertyParams(pydantic.BaseModel):
    arn: str = pydantic.Field(..., description='')
    ...

class CfnStorageLensDefBucketlevelpropertyParams(pydantic.BaseModel):
    activity_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_ActivityMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    advanced_cost_optimization_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedCostOptimizationMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    advanced_data_protection_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AdvancedDataProtectionMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    detailed_status_codes_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_DetailedStatusCodesMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    prefix_level: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_PrefixLevelPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefBucketsandregionspropertyParams(pydantic.BaseModel):
    buckets: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    regions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefCloudwatchmetricspropertyParams(pydantic.BaseModel):
    is_enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    ...

class CfnStorageLensDefDataexportpropertyParams(pydantic.BaseModel):
    cloud_watch_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_CloudWatchMetricsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    s3_bucket_destination: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_S3BucketDestinationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefDetailedstatuscodesmetricspropertyParams(pydantic.BaseModel):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefEncryptionpropertyParams(pydantic.BaseModel):
    ssekms: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_SSEKMSPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    sses3: typing.Any = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefPrefixlevelpropertyParams(pydantic.BaseModel):
    storage_metrics: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_PrefixLevelStorageMetricsPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    ...

class CfnStorageLensDefPrefixlevelstoragemetricspropertyParams(pydantic.BaseModel):
    is_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    selection_criteria: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_SelectionCriteriaPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefS3BucketdestinationpropertyParams(pydantic.BaseModel):
    account_id: str = pydantic.Field(..., description='')
    arn: str = pydantic.Field(..., description='')
    format: str = pydantic.Field(..., description='')
    output_schema_version: str = pydantic.Field(..., description='')
    encryption: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_EncryptionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    prefix: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefSsekmspropertyParams(pydantic.BaseModel):
    key_id: str = pydantic.Field(..., description='')
    ...

class CfnStorageLensDefSelectioncriteriapropertyParams(pydantic.BaseModel):
    delimiter: typing.Optional[str] = pydantic.Field(None, description='')
    max_depth: typing.Union[int, float, None] = pydantic.Field(None, description='')
    min_storage_bytes_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefStoragelensconfigurationpropertyParams(pydantic.BaseModel):
    account_level: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AccountLevelPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    id: str = pydantic.Field(..., description='')
    is_enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    aws_org: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_AwsOrgPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    data_export: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_DataExportPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    exclude: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_BucketsAndRegionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    include: typing.Union[models.UnsupportedResource, models.aws_s3.CfnStorageLens_BucketsAndRegionsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    storage_lens_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnStorageLensDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnStorageLensDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnStorageLensDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnStorageLensDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnStorageLensDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnStorageLensDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnStorageLensDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnStorageLensDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnStorageLensDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnStorageLensDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnStorageLensDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnStorageLensDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnStorageLensDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnStorageLensDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_s3.CfnAccessPointProps
class CfnAccessPointPropsDef(BaseCfnProperty):
    bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the bucket associated with this access point.\n')
    bucket_account_id: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID associated with the S3 bucket associated with this access point.\n')
    name: typing.Optional[str] = pydantic.Field(None, description="The name of this access point. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the access point name.\n")
    policy: typing.Any = pydantic.Field(None, description='The access point policy associated with this access point.\n')
    public_access_block_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnAccessPoint_PublicAccessBlockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The PublicAccessBlock configuration that you want to apply to this Amazon S3 bucket. You can enable the configuration options in any combination. For more information about when Amazon S3 considers a bucket or object public, see `The Meaning of "Public" <https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html#access-control-block-public-access-policy-status>`_ in the *Amazon S3 User Guide* .\n')
    vpc_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnAccessPoint_VpcConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Virtual Private Cloud (VPC) configuration for this access point, if one exists.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-accesspoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # policy: Any\n\n    cfn_access_point_props = s3.CfnAccessPointProps(\n        bucket="bucket",\n\n        # the properties below are optional\n        bucket_account_id="bucketAccountId",\n        name="name",\n        policy=policy,\n        public_access_block_configuration=s3.CfnAccessPoint.PublicAccessBlockConfigurationProperty(\n            block_public_acls=False,\n            block_public_policy=False,\n            ignore_public_acls=False,\n            restrict_public_buckets=False\n        ),\n        vpc_configuration=s3.CfnAccessPoint.VpcConfigurationProperty(\n            vpc_id="vpcId"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'bucket_account_id', 'name', 'policy', 'public_access_block_configuration', 'vpc_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnAccessPointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucketPolicyProps
class CfnBucketPolicyPropsDef(BaseCfnProperty):
    bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Amazon S3 bucket to which the policy applies.\n')
    policy_document: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A policy document containing permissions to add to the specified bucket. In IAM, you must provide policy documents in JSON format. However, in CloudFormation you can provide the policy in JSON or YAML format because CloudFormation converts YAML to JSON before submitting it to IAM. For more information, see the AWS::IAM::Policy `PolicyDocument <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-policy.html#cfn-iam-policy-policydocument>`_ resource description in this guide and `Access Policy Language Overview <https://docs.aws.amazon.com/AmazonS3/latest/dev/access-policy-language-overview.html>`_ in the *Amazon S3 User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-bucketpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # policy_document: Any\n\n    cfn_bucket_policy_props = s3.CfnBucketPolicyProps(\n        bucket="bucket",\n        policy_document=policy_document\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'policy_document']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucketPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnBucketProps
class CfnBucketPropsDef(BaseCfnProperty):
    accelerate_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AccelerateConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configures the transfer acceleration state for an Amazon S3 bucket. For more information, see `Amazon S3 Transfer Acceleration <https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html>`_ in the *Amazon S3 User Guide* .\n')
    access_control: typing.Optional[str] = pydantic.Field(None, description='.. epigraph:: This is a legacy property, and it is not recommended for most use cases. A majority of modern use cases in Amazon S3 no longer require the use of ACLs, and we recommend that you keep ACLs disabled. For more information, see `Controlling object ownership <https://docs.aws.amazon.com//AmazonS3/latest/userguide/about-object-ownership.html>`_ in the *Amazon S3 User Guide* . A canned access control list (ACL) that grants predefined permissions to the bucket. For more information about canned ACLs, see `Canned ACL <https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl>`_ in the *Amazon S3 User Guide* . S3 buckets are created with ACLs disabled by default. Therefore, unless you explicitly set the `AWS::S3::OwnershipControls <https://docs.aws.amazon.com//AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-ownershipcontrols.html>`_ property to enable ACLs, your resource will fail to deploy with any value other than Private. Use cases requiring ACLs are uncommon. The majority of access control configurations can be successfully and more easily achieved with bucket policies. For more information, see `AWS::S3::BucketPolicy <https://docs.aws.amazon.com//AWSCloudFormation/latest/UserGuide/aws-properties-s3-policy.html>`_ . For examples of common policy configurations, including S3 Server Access Logs buckets and more, see `Bucket policy examples <https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html>`_ in the *Amazon S3 User Guide* .\n')
    analytics_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_AnalyticsConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the configuration and any analyses for the analytics filter of an Amazon S3 bucket.\n')
    bucket_encryption: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_BucketEncryptionPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies default encryption for a bucket using server-side encryption with Amazon S3-managed keys (SSE-S3), AWS KMS-managed keys (SSE-KMS), or dual-layer server-side encryption with KMS-managed keys (DSSE-KMS). For information about the Amazon S3 default encryption feature, see `Amazon S3 Default Encryption for S3 Buckets <https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html>`_ in the *Amazon S3 User Guide* .\n')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description="A name for the bucket. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the bucket name. The bucket name must contain only lowercase letters, numbers, periods (.), and dashes (-) and must follow `Amazon S3 bucket restrictions and limitations <https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html>`_ . For more information, see `Rules for naming Amazon S3 buckets <https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules>`_ in the *Amazon S3 User Guide* . .. epigraph:: If you specify a name, you can't perform updates that require replacement of this resource. You can perform updates that require no or some interruption. If you need to replace the resource, specify a new name.\n")
    cors_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_CorsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Describes the cross-origin access configuration for objects in an Amazon S3 bucket. For more information, see `Enabling Cross-Origin Resource Sharing <https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html>`_ in the *Amazon S3 User Guide* .\n')
    intelligent_tiering_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_IntelligentTieringConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Defines how Amazon S3 handles Intelligent-Tiering storage.\n')
    inventory_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_InventoryConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the inventory configuration for an Amazon S3 bucket. For more information, see `GET Bucket inventory <https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETInventoryConfig.html>`_ in the *Amazon S3 API Reference* .\n')
    lifecycle_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_LifecycleConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the lifecycle configuration for objects in an Amazon S3 bucket. For more information, see `Object Lifecycle Management <https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html>`_ in the *Amazon S3 User Guide* .\n')
    logging_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_LoggingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings that define where logs are stored.\n')
    metrics_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_MetricsConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Specifies a metrics configuration for the CloudWatch request metrics (specified by the metrics configuration ID) from an Amazon S3 bucket. If you're updating an existing metrics configuration, note that this is a full replacement of the existing metrics configuration. If you don't include the elements you want to keep, they are erased. For more information, see `PutBucketMetricsConfiguration <https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTMetricConfiguration.html>`_ .\n")
    notification_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_NotificationConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration that defines how Amazon S3 handles bucket notifications.\n')
    object_lock_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ObjectLockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Places an Object Lock configuration on the specified bucket. The rule specified in the Object Lock configuration will be applied by default to every new object placed in the specified bucket. For more information, see `Locking Objects <https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html>`_ . .. epigraph:: - The ``DefaultRetention`` settings require both a mode and a period. - The ``DefaultRetention`` period can be either ``Days`` or ``Years`` but you must select one. You cannot specify ``Days`` and ``Years`` at the same time. - You can only enable Object Lock for new buckets. If you want to turn on Object Lock for an existing bucket, contact AWS Support.\n')
    object_lock_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether this bucket has an Object Lock configuration enabled. Enable ``ObjectLockEnabled`` when you apply ``ObjectLockConfiguration`` to a bucket.\n')
    ownership_controls: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_OwnershipControlsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration that defines how Amazon S3 handles Object Ownership rules.\n')
    public_access_block_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_PublicAccessBlockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration that defines how Amazon S3 handles public access.\n')
    replication_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_ReplicationConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for replicating objects in an S3 bucket. To enable replication, you must also enable versioning by using the ``VersioningConfiguration`` property. Amazon S3 can store replicated objects in a single destination bucket or multiple destination buckets. The destination bucket or buckets must already exist.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An arbitrary set of tags (key-value pairs) for this S3 bucket.\n')
    versioning_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_VersioningConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Enables multiple versions of all objects in this bucket. You might enable versioning to prevent objects from being deleted or overwritten by mistake or to archive objects so that you can retrieve previous versions of them.\n')
    website_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnBucket_WebsiteConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information used to configure the bucket as a static website. For more information, see `Hosting Websites on Amazon S3 <https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-bucket.html\n:exampleMetadata: infused\n\nExample::\n\n    raw_bucket = s3.CfnBucket(self, "Bucket")\n    # -or-\n    raw_bucket_alt = my_bucket.node.default_child\n\n    # then\n    raw_bucket.cfn_options.condition = CfnCondition(self, "EnableBucket")\n    raw_bucket.cfn_options.metadata = {\n        "metadata_key": "MetadataValue"\n    }\n')
    _init_params: typing.ClassVar[list[str]] = ['accelerate_configuration', 'access_control', 'analytics_configurations', 'bucket_encryption', 'bucket_name', 'cors_configuration', 'intelligent_tiering_configurations', 'inventory_configurations', 'lifecycle_configuration', 'logging_configuration', 'metrics_configurations', 'notification_configuration', 'object_lock_configuration', 'object_lock_enabled', 'ownership_controls', 'public_access_block_configuration', 'replication_configuration', 'tags', 'versioning_configuration', 'website_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnBucketProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnMultiRegionAccessPointPolicyProps
class CfnMultiRegionAccessPointPolicyPropsDef(BaseCfnProperty):
    mrap_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Multi-Region Access Point.\n')
    policy: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The access policy associated with the Multi-Region Access Point.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-multiregionaccesspointpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # policy: Any\n\n    cfn_multi_region_access_point_policy_props = s3.CfnMultiRegionAccessPointPolicyProps(\n        mrap_name="mrapName",\n        policy=policy\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['mrap_name', 'policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnMultiRegionAccessPointPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnMultiRegionAccessPointProps
class CfnMultiRegionAccessPointPropsDef(BaseCfnProperty):
    regions: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_s3.CfnMultiRegionAccessPoint_RegionPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A collection of the Regions and buckets associated with the Multi-Region Access Point.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the Multi-Region Access Point.\n')
    public_access_block_configuration: typing.Union[models.UnsupportedResource, models.aws_s3.CfnMultiRegionAccessPoint_PublicAccessBlockConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The PublicAccessBlock configuration that you want to apply to this Multi-Region Access Point. You can enable the configuration options in any combination. For more information about when Amazon S3 considers an object public, see `The Meaning of "Public" <https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html#access-control-block-public-access-policy-status>`_ in the *Amazon S3 User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-multiregionaccesspoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    cfn_multi_region_access_point_props = s3.CfnMultiRegionAccessPointProps(\n        regions=[s3.CfnMultiRegionAccessPoint.RegionProperty(\n            bucket="bucket",\n\n            # the properties below are optional\n            bucket_account_id="bucketAccountId"\n        )],\n\n        # the properties below are optional\n        name="name",\n        public_access_block_configuration=s3.CfnMultiRegionAccessPoint.PublicAccessBlockConfigurationProperty(\n            block_public_acls=False,\n            block_public_policy=False,\n            ignore_public_acls=False,\n            restrict_public_buckets=False\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['regions', 'name', 'public_access_block_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnMultiRegionAccessPointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_s3.CfnStorageLensProps
class CfnStorageLensPropsDef(BaseCfnProperty):
    storage_lens_configuration: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_s3.CfnStorageLens_StorageLensConfigurationPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='This resource contains the details Amazon S3 Storage Lens configuration.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A set of tags (key–value pairs) to associate with the Storage Lens configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-storagelens.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_s3 as s3\n\n    # sses3: Any\n\n    cfn_storage_lens_props = s3.CfnStorageLensProps(\n        storage_lens_configuration=s3.CfnStorageLens.StorageLensConfigurationProperty(\n            account_level=s3.CfnStorageLens.AccountLevelProperty(\n                bucket_level=s3.CfnStorageLens.BucketLevelProperty(\n                    activity_metrics=s3.CfnStorageLens.ActivityMetricsProperty(\n                        is_enabled=False\n                    ),\n                    advanced_cost_optimization_metrics=s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n                        is_enabled=False\n                    ),\n                    advanced_data_protection_metrics=s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n                        is_enabled=False\n                    ),\n                    detailed_status_codes_metrics=s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n                        is_enabled=False\n                    ),\n                    prefix_level=s3.CfnStorageLens.PrefixLevelProperty(\n                        storage_metrics=s3.CfnStorageLens.PrefixLevelStorageMetricsProperty(\n                            is_enabled=False,\n                            selection_criteria=s3.CfnStorageLens.SelectionCriteriaProperty(\n                                delimiter="delimiter",\n                                max_depth=123,\n                                min_storage_bytes_percentage=123\n                            )\n                        )\n                    )\n                ),\n\n                # the properties below are optional\n                activity_metrics=s3.CfnStorageLens.ActivityMetricsProperty(\n                    is_enabled=False\n                ),\n                advanced_cost_optimization_metrics=s3.CfnStorageLens.AdvancedCostOptimizationMetricsProperty(\n                    is_enabled=False\n                ),\n                advanced_data_protection_metrics=s3.CfnStorageLens.AdvancedDataProtectionMetricsProperty(\n                    is_enabled=False\n                ),\n                detailed_status_codes_metrics=s3.CfnStorageLens.DetailedStatusCodesMetricsProperty(\n                    is_enabled=False\n                )\n            ),\n            id="id",\n            is_enabled=False,\n\n            # the properties below are optional\n            aws_org=s3.CfnStorageLens.AwsOrgProperty(\n                arn="arn"\n            ),\n            data_export=s3.CfnStorageLens.DataExportProperty(\n                cloud_watch_metrics=s3.CfnStorageLens.CloudWatchMetricsProperty(\n                    is_enabled=False\n                ),\n                s3_bucket_destination=s3.CfnStorageLens.S3BucketDestinationProperty(\n                    account_id="accountId",\n                    arn="arn",\n                    format="format",\n                    output_schema_version="outputSchemaVersion",\n\n                    # the properties below are optional\n                    encryption=s3.CfnStorageLens.EncryptionProperty(\n                        ssekms=s3.CfnStorageLens.SSEKMSProperty(\n                            key_id="keyId"\n                        ),\n                        sses3=sses3\n                    ),\n                    prefix="prefix"\n                )\n            ),\n            exclude=s3.CfnStorageLens.BucketsAndRegionsProperty(\n                buckets=["buckets"],\n                regions=["regions"]\n            ),\n            include=s3.CfnStorageLens.BucketsAndRegionsProperty(\n                buckets=["buckets"],\n                regions=["regions"]\n            ),\n            storage_lens_arn="storageLensArn"\n        ),\n\n        # the properties below are optional\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['storage_lens_configuration', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_s3.CfnStorageLensProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    BlockPublicAccess: typing.Optional[dict[str, models.aws_s3.BlockPublicAccessDef]] = pydantic.Field(None)
    BucketBase: typing.Optional[dict[str, models.aws_s3.BucketBaseDef]] = pydantic.Field(None)
    ObjectLockRetention: typing.Optional[dict[str, models.aws_s3.ObjectLockRetentionDef]] = pydantic.Field(None)
    ReplaceKey: typing.Optional[dict[str, models.aws_s3.ReplaceKeyDef]] = pydantic.Field(None)
    StorageClass: typing.Optional[dict[str, models.aws_s3.StorageClassDef]] = pydantic.Field(None)
    Bucket: typing.Optional[dict[str, models.aws_s3.BucketDef]] = pydantic.Field(None)
    BucketPolicy: typing.Optional[dict[str, models.aws_s3.BucketPolicyDef]] = pydantic.Field(None)
    BlockPublicAccessOptions: typing.Optional[dict[str, models.aws_s3.BlockPublicAccessOptionsDef]] = pydantic.Field(None)
    BucketAttributes: typing.Optional[dict[str, models.aws_s3.BucketAttributesDef]] = pydantic.Field(None)
    BucketMetrics: typing.Optional[dict[str, models.aws_s3.BucketMetricsDef]] = pydantic.Field(None)
    BucketNotificationDestinationConfig: typing.Optional[dict[str, models.aws_s3.BucketNotificationDestinationConfigDef]] = pydantic.Field(None)
    BucketPolicyProps: typing.Optional[dict[str, models.aws_s3.BucketPolicyPropsDef]] = pydantic.Field(None)
    BucketProps: typing.Optional[dict[str, models.aws_s3.BucketPropsDef]] = pydantic.Field(None)
    CfnAccessPoint_PublicAccessBlockConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnAccessPoint_PublicAccessBlockConfigurationPropertyDef]] = pydantic.Field(None)
    CfnAccessPoint_VpcConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnAccessPoint_VpcConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_AbortIncompleteMultipartUploadProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_AbortIncompleteMultipartUploadPropertyDef]] = pydantic.Field(None)
    CfnBucket_AccelerateConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_AccelerateConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_AccessControlTranslationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_AccessControlTranslationPropertyDef]] = pydantic.Field(None)
    CfnBucket_AnalyticsConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_AnalyticsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_BucketEncryptionProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_BucketEncryptionPropertyDef]] = pydantic.Field(None)
    CfnBucket_CorsConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_CorsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_CorsRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_CorsRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_DataExportProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_DataExportPropertyDef]] = pydantic.Field(None)
    CfnBucket_DefaultRetentionProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_DefaultRetentionPropertyDef]] = pydantic.Field(None)
    CfnBucket_DeleteMarkerReplicationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_DeleteMarkerReplicationPropertyDef]] = pydantic.Field(None)
    CfnBucket_DestinationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_DestinationPropertyDef]] = pydantic.Field(None)
    CfnBucket_EncryptionConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_EncryptionConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_EventBridgeConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_EventBridgeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_FilterRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_FilterRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_IntelligentTieringConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_IntelligentTieringConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_InventoryConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_InventoryConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_LambdaConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_LambdaConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_LifecycleConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_LifecycleConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_LoggingConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_LoggingConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_MetricsConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_MetricsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_MetricsProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_MetricsPropertyDef]] = pydantic.Field(None)
    CfnBucket_NoncurrentVersionExpirationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_NoncurrentVersionExpirationPropertyDef]] = pydantic.Field(None)
    CfnBucket_NoncurrentVersionTransitionProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_NoncurrentVersionTransitionPropertyDef]] = pydantic.Field(None)
    CfnBucket_NotificationConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_NotificationConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_NotificationFilterProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_NotificationFilterPropertyDef]] = pydantic.Field(None)
    CfnBucket_ObjectLockConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ObjectLockConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_ObjectLockRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ObjectLockRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_OwnershipControlsProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_OwnershipControlsPropertyDef]] = pydantic.Field(None)
    CfnBucket_OwnershipControlsRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_OwnershipControlsRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_PublicAccessBlockConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_PublicAccessBlockConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_QueueConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_QueueConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_RedirectAllRequestsToProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_RedirectAllRequestsToPropertyDef]] = pydantic.Field(None)
    CfnBucket_RedirectRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_RedirectRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicaModificationsProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicaModificationsPropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicationConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicationConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicationDestinationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicationDestinationPropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicationRuleAndOperatorProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicationRuleAndOperatorPropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicationRuleFilterProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicationRuleFilterPropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicationRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicationRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicationTimeProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicationTimePropertyDef]] = pydantic.Field(None)
    CfnBucket_ReplicationTimeValueProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ReplicationTimeValuePropertyDef]] = pydantic.Field(None)
    CfnBucket_RoutingRuleConditionProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_RoutingRuleConditionPropertyDef]] = pydantic.Field(None)
    CfnBucket_RoutingRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_RoutingRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_RuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_RulePropertyDef]] = pydantic.Field(None)
    CfnBucket_S3KeyFilterProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_S3KeyFilterPropertyDef]] = pydantic.Field(None)
    CfnBucket_ServerSideEncryptionByDefaultProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ServerSideEncryptionByDefaultPropertyDef]] = pydantic.Field(None)
    CfnBucket_ServerSideEncryptionRuleProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_ServerSideEncryptionRulePropertyDef]] = pydantic.Field(None)
    CfnBucket_SourceSelectionCriteriaProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_SourceSelectionCriteriaPropertyDef]] = pydantic.Field(None)
    CfnBucket_SseKmsEncryptedObjectsProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_SseKmsEncryptedObjectsPropertyDef]] = pydantic.Field(None)
    CfnBucket_StorageClassAnalysisProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_StorageClassAnalysisPropertyDef]] = pydantic.Field(None)
    CfnBucket_TagFilterProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_TagFilterPropertyDef]] = pydantic.Field(None)
    CfnBucket_TieringProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_TieringPropertyDef]] = pydantic.Field(None)
    CfnBucket_TopicConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_TopicConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_TransitionProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_TransitionPropertyDef]] = pydantic.Field(None)
    CfnBucket_VersioningConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_VersioningConfigurationPropertyDef]] = pydantic.Field(None)
    CfnBucket_WebsiteConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnBucket_WebsiteConfigurationPropertyDef]] = pydantic.Field(None)
    CfnMultiRegionAccessPoint_PublicAccessBlockConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnMultiRegionAccessPoint_PublicAccessBlockConfigurationPropertyDef]] = pydantic.Field(None)
    CfnMultiRegionAccessPoint_RegionProperty: typing.Optional[dict[str, models.aws_s3.CfnMultiRegionAccessPoint_RegionPropertyDef]] = pydantic.Field(None)
    CfnMultiRegionAccessPointPolicy_PolicyStatusProperty: typing.Optional[dict[str, models.aws_s3.CfnMultiRegionAccessPointPolicy_PolicyStatusPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_AccountLevelProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_AccountLevelPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_ActivityMetricsProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_ActivityMetricsPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_AdvancedCostOptimizationMetricsProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_AdvancedCostOptimizationMetricsPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_AdvancedDataProtectionMetricsProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_AdvancedDataProtectionMetricsPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_AwsOrgProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_AwsOrgPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_BucketLevelProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_BucketLevelPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_BucketsAndRegionsProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_BucketsAndRegionsPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_CloudWatchMetricsProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_CloudWatchMetricsPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_DataExportProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_DataExportPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_DetailedStatusCodesMetricsProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_DetailedStatusCodesMetricsPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_EncryptionProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_EncryptionPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_PrefixLevelProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_PrefixLevelPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_PrefixLevelStorageMetricsProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_PrefixLevelStorageMetricsPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_S3BucketDestinationProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_S3BucketDestinationPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_SelectionCriteriaProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_SelectionCriteriaPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_SSEKMSProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_SSEKMSPropertyDef]] = pydantic.Field(None)
    CfnStorageLens_StorageLensConfigurationProperty: typing.Optional[dict[str, models.aws_s3.CfnStorageLens_StorageLensConfigurationPropertyDef]] = pydantic.Field(None)
    CorsRule: typing.Optional[dict[str, models.aws_s3.CorsRuleDef]] = pydantic.Field(None)
    IntelligentTieringConfiguration: typing.Optional[dict[str, models.aws_s3.IntelligentTieringConfigurationDef]] = pydantic.Field(None)
    Inventory: typing.Optional[dict[str, models.aws_s3.InventoryDef]] = pydantic.Field(None)
    InventoryDestination: typing.Optional[dict[str, models.aws_s3.InventoryDestinationDef]] = pydantic.Field(None)
    LifecycleRule: typing.Optional[dict[str, models.aws_s3.LifecycleRuleDef]] = pydantic.Field(None)
    Location: typing.Optional[dict[str, models.aws_s3.LocationDef]] = pydantic.Field(None)
    NoncurrentVersionTransition: typing.Optional[dict[str, models.aws_s3.NoncurrentVersionTransitionDef]] = pydantic.Field(None)
    NotificationKeyFilter: typing.Optional[dict[str, models.aws_s3.NotificationKeyFilterDef]] = pydantic.Field(None)
    OnCloudTrailBucketEventOptions: typing.Optional[dict[str, models.aws_s3.OnCloudTrailBucketEventOptionsDef]] = pydantic.Field(None)
    RedirectTarget: typing.Optional[dict[str, models.aws_s3.RedirectTargetDef]] = pydantic.Field(None)
    RoutingRule: typing.Optional[dict[str, models.aws_s3.RoutingRuleDef]] = pydantic.Field(None)
    RoutingRuleCondition: typing.Optional[dict[str, models.aws_s3.RoutingRuleConditionDef]] = pydantic.Field(None)
    Tag: typing.Optional[dict[str, models.aws_s3.TagDef]] = pydantic.Field(None)
    TransferAccelerationUrlOptions: typing.Optional[dict[str, models.aws_s3.TransferAccelerationUrlOptionsDef]] = pydantic.Field(None)
    Transition: typing.Optional[dict[str, models.aws_s3.TransitionDef]] = pydantic.Field(None)
    VirtualHostedStyleUrlOptions: typing.Optional[dict[str, models.aws_s3.VirtualHostedStyleUrlOptionsDef]] = pydantic.Field(None)
    CfnAccessPoint: typing.Optional[dict[str, models.aws_s3.CfnAccessPointDef]] = pydantic.Field(None)
    CfnBucket: typing.Optional[dict[str, models.aws_s3.CfnBucketDef]] = pydantic.Field(None)
    CfnBucketPolicy: typing.Optional[dict[str, models.aws_s3.CfnBucketPolicyDef]] = pydantic.Field(None)
    CfnMultiRegionAccessPoint: typing.Optional[dict[str, models.aws_s3.CfnMultiRegionAccessPointDef]] = pydantic.Field(None)
    CfnMultiRegionAccessPointPolicy: typing.Optional[dict[str, models.aws_s3.CfnMultiRegionAccessPointPolicyDef]] = pydantic.Field(None)
    CfnStorageLens: typing.Optional[dict[str, models.aws_s3.CfnStorageLensDef]] = pydantic.Field(None)
    CfnAccessPointProps: typing.Optional[dict[str, models.aws_s3.CfnAccessPointPropsDef]] = pydantic.Field(None)
    CfnBucketPolicyProps: typing.Optional[dict[str, models.aws_s3.CfnBucketPolicyPropsDef]] = pydantic.Field(None)
    CfnBucketProps: typing.Optional[dict[str, models.aws_s3.CfnBucketPropsDef]] = pydantic.Field(None)
    CfnMultiRegionAccessPointPolicyProps: typing.Optional[dict[str, models.aws_s3.CfnMultiRegionAccessPointPolicyPropsDef]] = pydantic.Field(None)
    CfnMultiRegionAccessPointProps: typing.Optional[dict[str, models.aws_s3.CfnMultiRegionAccessPointPropsDef]] = pydantic.Field(None)
    CfnStorageLensProps: typing.Optional[dict[str, models.aws_s3.CfnStorageLensPropsDef]] = pydantic.Field(None)
    ...

import models
