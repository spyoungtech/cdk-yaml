from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_ecs.AppMeshProxyConfiguration
class AppMeshProxyConfigurationDef(BaseClass):
    container_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the container that will serve as the App Mesh proxy.\n')
    properties: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.AppMeshProxyConfigurationPropsDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The set of network configuration parameters to provide the Container Network Interface (CNI) plugin.')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'properties']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AppMeshProxyConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.AppMeshProxyConfigurationDefConfig] = pydantic.Field(None)


class AppMeshProxyConfigurationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.AppMeshProxyConfigurationDefBindParams]] = pydantic.Field(None, description='Called when the proxy configuration is configured on a task definition.')

class AppMeshProxyConfigurationDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.AppProtocol
class AppProtocolDef(BaseClass):
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AppProtocol'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.AssetEnvironmentFile
class AssetEnvironmentFileDef(BaseClass):
    path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path to the asset file or directory.')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    _init_params: typing.ClassVar[list[str]] = ['path', 'deploy_time', 'readers', 'asset_hash', 'asset_hash_type', 'bundling', 'exclude', 'follow_symlinks', 'ignore_mode']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_bucket']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AssetEnvironmentFile'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_bucket']
    ...


    from_asset: typing.Optional[models.aws_ecs.AssetEnvironmentFileDefFromAssetParams] = pydantic.Field(None, description='Loads the environment file from a local disk path.')
    from_bucket: typing.Optional[models.aws_ecs.AssetEnvironmentFileDefFromBucketParams] = pydantic.Field(None, description='Loads the environment file from an S3 bucket.')
    resource_config: typing.Optional[models.aws_ecs.AssetEnvironmentFileDefConfig] = pydantic.Field(None)


class AssetEnvironmentFileDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.AssetEnvironmentFileDefBindParams]] = pydantic.Field(None, description='Called when the container is initialized to allow this object to bind to the stack.')

class AssetEnvironmentFileDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    ...

class AssetEnvironmentFileDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Local disk path.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetEnvironmentFileDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.\n')
    ...


#  autogenerated from aws_cdk.aws_ecs.AssetImage
class AssetImageDef(BaseClass):
    directory: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The directory containing the Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    _init_params: typing.ClassVar[list[str]] = ['directory', 'asset_name', 'build_args', 'build_secrets', 'build_ssh', 'cache_disabled', 'cache_from', 'cache_to', 'file', 'invalidation', 'network_mode', 'outputs', 'platform', 'target', 'extra_hash', 'exclude', 'follow_symlinks', 'ignore_mode']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AssetImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    ...


    from_asset: typing.Optional[models.aws_ecs.AssetImageDefFromAssetParams] = pydantic.Field(None, description="Reference an image that's constructed directly from sources on disk.\nIf you already have a ``DockerImageAsset`` instance, you can use the\n``ContainerImage.fromDockerImageAsset`` method instead.")
    from_docker_image_asset: typing.Optional[models.aws_ecs.AssetImageDefFromDockerImageAssetParams] = pydantic.Field(None, description='Use an existing ``DockerImageAsset`` for this container image.')
    from_ecr_repository: typing.Optional[models.aws_ecs.AssetImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='Reference an image in an ECR repository.')
    from_registry: typing.Optional[models.aws_ecs.AssetImageDefFromRegistryParams] = pydantic.Field(None, description='Reference an image on DockerHub or another online registry.')
    from_tarball: typing.Optional[models.aws_ecs.AssetImageDefFromTarballParams] = pydantic.Field(None, description='Use an existing tarball for this container image.\nUse this method if the container image has already been created by another process (e.g. jib)\nand you want to add it as a container image asset.')
    resource_config: typing.Optional[models.aws_ecs.AssetImageDefConfig] = pydantic.Field(None)


class AssetImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.AssetImageDefBindParams]] = pydantic.Field(None, description='Called when the image is used by a ContainerDefinition.')

class AssetImageDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-')
    ...

class AssetImageDefFromAssetParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='The directory containing the Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetImageDefFromDockerImageAssetParams(pydantic.BaseModel):
    asset: models.aws_ecr_assets.DockerImageAssetDef = pydantic.Field(..., description='The ``DockerImageAsset`` to use for this container definition.')
    ...

class AssetImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='-\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='-')
    ...

class AssetImageDefFromRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret to expose to the container that contains the credentials for the image repository. The supported value is the full ARN of an AWS Secrets Manager secret.')
    ...

class AssetImageDefFromTarballParams(pydantic.BaseModel):
    tarball_file: str = pydantic.Field(..., description='Absolute path to the tarball. You can use language-specific idioms (such as ``__dirname`` in Node.js) to create an absolute path based on the current script running directory.')
    ...


#  autogenerated from aws_cdk.aws_ecs.AwsLogDriver
class AwsLogDriverDef(BaseClass):
    stream_prefix: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    _init_params: typing.ClassVar[list[str]] = ['stream_prefix', 'datetime_format', 'log_group', 'log_retention', 'max_buffer_size', 'mode', 'multiline_pattern']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AwsLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.AwsLogDriverDefConfig] = pydantic.Field(None)


class AwsLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.AwsLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.AwsLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class AwsLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class AwsLogDriverDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_ecs.BaseService
class BaseServiceDef(BaseClass):
    props: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.BaseServicePropsDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='-\n')
    additional_props: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-\n')
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['props', 'additional_props', 'task_definition']
    _method_names: typing.ClassVar[list[str]] = ['add_volume', 'apply_removal_policy', 'associate_cloud_map_service', 'attach_to_application_target_group', 'attach_to_classic_lb', 'attach_to_network_target_group', 'auto_scale_task_count', 'enable_cloud_map', 'enable_deployment_alarms', 'enable_service_connect', 'load_balancer_target', 'metric', 'metric_cpu_utilization', 'metric_memory_utilization', 'register_load_balancer_targets']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_service_arn_with_cluster']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BaseService'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_service_arn_with_cluster']
    ...


    from_service_arn_with_cluster: typing.Optional[models.aws_ecs.BaseServiceDefFromServiceArnWithClusterParams] = pydantic.Field(None, description='Import an existing ECS/Fargate Service using the service cluster format.\nThe format is the "new" format "arn:aws:ecs:region:aws_account_id:service/cluster-name/service-name".')
    resource_config: typing.Optional[models.aws_ecs.BaseServiceDefConfig] = pydantic.Field(None)


class BaseServiceDefConfig(pydantic.BaseModel):
    add_volume: typing.Optional[list[models.aws_ecs.BaseServiceDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the Service.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    associate_cloud_map_service: typing.Optional[list[models.aws_ecs.BaseServiceDefAssociateCloudMapServiceParams]] = pydantic.Field(None, description='Associates this service with a CloudMap service.')
    attach_to_application_target_group: typing.Optional[list[models.aws_ecs.BaseServiceDefAttachToApplicationTargetGroupParams]] = pydantic.Field(None, description="This method is called to attach this service to an Application Load Balancer.\nDon't call this function directly. Instead, call ``listener.addTargets()``\nto add this service to a load balancer.")
    attach_to_classic_lb: typing.Optional[list[models.aws_ecs.BaseServiceDefAttachToClassicLbParams]] = pydantic.Field(None, description="Registers the service as a target of a Classic Load Balancer (CLB).\nDon't call this. Call ``loadBalancer.addTarget()`` instead.")
    attach_to_network_target_group: typing.Optional[list[models.aws_ecs.BaseServiceDefAttachToNetworkTargetGroupParams]] = pydantic.Field(None, description="This method is called to attach this service to a Network Load Balancer.\nDon't call this function directly. Instead, call ``listener.addTargets()``\nto add this service to a load balancer.")
    auto_scale_task_count: typing.Optional[list[models.aws_ecs.BaseServiceDefAutoScaleTaskCountParams]] = pydantic.Field(None, description='An attribute representing the minimum and maximum task count for an AutoScalingGroup.')
    enable_cloud_map: typing.Optional[list[models.aws_ecs.BaseServiceDefEnableCloudMapParams]] = pydantic.Field(None, description='Enable CloudMap service discovery for the service.')
    enable_deployment_alarms: typing.Optional[list[models.aws_ecs.BaseServiceDefEnableDeploymentAlarmsParams]] = pydantic.Field(None, description="Enable Deployment Alarms which take advantage of arbitrary alarms and configure them after service initialization.\nIf you have already enabled deployment alarms, this function can be used to tell ECS about additional alarms that\nshould interrupt a deployment.\n\nNew alarms specified in subsequent calls of this function will be appended to the existing list of alarms.\n\nThe same Alarm Behavior must be used on all deployment alarms. If you specify different AlarmBehavior values in\nmultiple calls to this function, or the Alarm Behavior used here doesn't match the one used in the service\nconstructor, an error will be thrown.\n\nIf the alarm's metric references the service, you cannot pass ``Alarm.alarmName`` here. That will cause a circular\ndependency between the service and its deployment alarm. See this package's README for options to alarm on service\nmetrics, and avoid this circular dependency.")
    enable_service_connect: typing.Optional[list[models.aws_ecs.BaseServiceDefEnableServiceConnectParams]] = pydantic.Field(None, description='Enable Service Connect on this service.')
    load_balancer_target: typing.Optional[list[models.aws_ecs.BaseServiceDefLoadBalancerTargetParams]] = pydantic.Field(None, description='Return a load balancing target for a specific container and port.\nUse this function to create a load balancer target if you want to load balance to\nanother container than the first essential container or the first mapped port on\nthe container.\n\nUse the return value of this function where you would normally use a load balancer\ntarget, instead of the ``Service`` object itself.')
    metric: typing.Optional[list[models.aws_ecs.BaseServiceDefMetricParams]] = pydantic.Field(None, description='This method returns the specified CloudWatch metric name for this service.')
    metric_cpu_utilization: typing.Optional[list[models.aws_ecs.BaseServiceDefMetricCpuUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's CPU utilization.")
    metric_memory_utilization: typing.Optional[list[models.aws_ecs.BaseServiceDefMetricMemoryUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's memory utilization.")
    register_load_balancer_targets: typing.Optional[list[models.aws_ecs.BaseServiceDefRegisterLoadBalancerTargetsParams]] = pydantic.Field(None, description='Use this function to create all load balancer targets to be registered in this service, add them to target groups, and attach target groups to listeners accordingly.\nAlternatively, you can use ``listener.addTargets()`` to create targets and add them to target groups.')
    cluster_config: typing.Optional[models._interface_methods.AwsEcsIClusterDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    task_definition_config: typing.Optional[models.aws_ecs.TaskDefinitionDefConfig] = pydantic.Field(None)

class BaseServiceDefAddVolumeParams(pydantic.BaseModel):
    volume: models.aws_ecs.ServiceManagedVolumeDef = pydantic.Field(..., description='-')
    ...

class BaseServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class BaseServiceDefAssociateCloudMapServiceParams(pydantic.BaseModel):
    service: typing.Union[models.aws_servicediscovery.ServiceDef] = pydantic.Field(..., description='The cloudmap service to register with.\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container")
    ...

class BaseServiceDefAttachToApplicationTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef] = pydantic.Field(..., description='-')
    ...

class BaseServiceDefAttachToClassicLbParams(pydantic.BaseModel):
    load_balancer: models.aws_elasticloadbalancing.LoadBalancerDef = pydantic.Field(..., description='-')
    ...

class BaseServiceDefAttachToNetworkTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.NetworkTargetGroupDef] = pydantic.Field(..., description='-')
    ...

class BaseServiceDefAutoScaleTaskCountParams(pydantic.BaseModel):
    max_capacity: typing.Union[int, float] = pydantic.Field(..., description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1')
    return_config: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefConfig]] = pydantic.Field(None)
    ...

class BaseServiceDefEnableCloudMapParams(pydantic.BaseModel):
    cloud_map_namespace: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='The service discovery namespace for the Cloud Map service to attach to the ECS service. Default: - the defaultCloudMapNamespace associated to the cluster\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container\n")
    dns_record_type: typing.Optional[aws_cdk.aws_servicediscovery.DnsRecordType] = pydantic.Field(None, description='The DNS record type that you want AWS Cloud Map to create. The supported record types are A or SRV. Default: - DnsRecordType.A if TaskDefinition.networkMode = AWS_VPC, otherwise DnsRecordType.SRV\n')
    dns_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time that you want DNS resolvers to cache the settings for this record. Default: Duration.minutes(1)\n')
    failure_threshold: typing.Union[int, float, None] = pydantic.Field(None, description='The number of 30-second intervals that you want Cloud Map to wait after receiving an UpdateInstanceCustomHealthStatus request before it changes the health status of a service instance. NOTE: This is used for HealthCheckCustomConfig\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the Cloud Map service to attach to the ECS service. Default: CloudFormation-generated name\n')
    return_config: typing.Optional[list[models.aws_servicediscovery.ServiceDefConfig]] = pydantic.Field(None)
    ...

class BaseServiceDefEnableDeploymentAlarmsParams(pydantic.BaseModel):
    alarm_names: typing.Sequence[str] = pydantic.Field(..., description='-\n')
    behavior: typing.Optional[aws_cdk.aws_ecs.AlarmBehavior] = pydantic.Field(None, description='Default rollback on alarm. Default: AlarmBehavior.ROLLBACK_ON_ALARM')
    ...

class BaseServiceDefEnableServiceConnectParams(pydantic.BaseModel):
    log_driver: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log driver configuration to use for the Service Connect agent logs. Default: - none\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description='The cloudmap namespace to register this service into. Default: the cloudmap namespace specified on the cluster.\n')
    services: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.ServiceConnectServiceDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of Services, including a port mapping, terse client alias, and optional intermediate DNS name. This property may be left blank if the current ECS service does not need to advertise any ports via Service Connect. Default: none')
    ...

class BaseServiceDefFromServiceArnWithClusterParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    service_arn: str = pydantic.Field(..., description='-\n\n:see: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-account-settings.html#ecs-resource-ids\n')
    ...

class BaseServiceDefLoadBalancerTargetParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='The name of the container.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number of the container. Only applicable when using application/network load balancers. Default: - Container port of the first added port mapping.\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Only applicable when using application load balancers. Default: Protocol.TCP\n\nExample::\n\n    # listener: elbv2.ApplicationListener\n    # service: ecs.BaseService\n\n    listener.add_targets("ECS",\n        port=80,\n        targets=[service.load_balancer_target(\n            container_name="MyContainer",\n            container_port=1234\n        )]\n    )\n')
    return_config: typing.Optional[list[models._interface_methods.AwsEcsIEcsLoadBalancerTargetDefConfig]] = pydantic.Field(None)
    ...

class BaseServiceDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BaseServiceDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BaseServiceDefMetricMemoryUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class BaseServiceDefRegisterLoadBalancerTargetsParams(pydantic.BaseModel):
    targets: list[models.aws_ecs.EcsTargetDef] = pydantic.Field(...)
    ...


#  autogenerated from aws_cdk.aws_ecs.BottleRocketImage
class BottleRocketImageDef(BaseClass):
    architecture: typing.Optional[aws_cdk.aws_ec2.InstanceArchitecture] = pydantic.Field(None, description='The CPU architecture. Default: - x86_64\n')
    cached_in_context: typing.Optional[bool] = pydantic.Field(None, description="Whether the AMI ID is cached to be stable between deployments. By default, the newest image is used on each deployment. This will cause instances to be replaced whenever a new version is released, and may cause downtime if there aren't enough running instances in the AutoScalingGroup to reschedule the tasks on. If set to true, the AMI ID will be cached in ``cdk.context.json`` and the same value will be used on future runs. Your instances will not be replaced but your AMI version will grow old over time. To refresh the AMI lookup, you will have to evict the value from the cache using the ``cdk context`` command. See https://docs.aws.amazon.com/cdk/latest/guide/context.html for more information. Can not be set to ``true`` in environment-agnostic stacks. Default: false\n")
    variant: typing.Optional[aws_cdk.aws_ecs.BottlerocketEcsVariant] = pydantic.Field(None, description='The Amazon ECS variant to use. Default: - BottlerocketEcsVariant.AWS_ECS_1')
    _init_params: typing.ClassVar[list[str]] = ['architecture', 'cached_in_context', 'variant']
    _method_names: typing.ClassVar[list[str]] = ['get_image']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BottleRocketImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.BottleRocketImageDefConfig] = pydantic.Field(None)


class BottleRocketImageDefConfig(pydantic.BaseModel):
    get_image: typing.Optional[list[models.aws_ecs.BottleRocketImageDefGetImageParams]] = pydantic.Field(None, description='Return the correct image.')

class BottleRocketImageDefGetImageParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_ecs.BuiltInAttributes
class BuiltInAttributesDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BuiltInAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ContainerImage
class ContainerImageDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ContainerImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    ...


    from_asset: typing.Optional[models.aws_ecs.ContainerImageDefFromAssetParams] = pydantic.Field(None, description="Reference an image that's constructed directly from sources on disk.\nIf you already have a ``DockerImageAsset`` instance, you can use the\n``ContainerImage.fromDockerImageAsset`` method instead.")
    from_docker_image_asset: typing.Optional[models.aws_ecs.ContainerImageDefFromDockerImageAssetParams] = pydantic.Field(None, description='Use an existing ``DockerImageAsset`` for this container image.')
    from_ecr_repository: typing.Optional[models.aws_ecs.ContainerImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='Reference an image in an ECR repository.')
    from_registry: typing.Optional[models.aws_ecs.ContainerImageDefFromRegistryParams] = pydantic.Field(None, description='Reference an image on DockerHub or another online registry.')
    from_tarball: typing.Optional[models.aws_ecs.ContainerImageDefFromTarballParams] = pydantic.Field(None, description='Use an existing tarball for this container image.\nUse this method if the container image has already been created by another process (e.g. jib)\nand you want to add it as a container image asset.')
    resource_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)


class ContainerImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.ContainerImageDefBindParams]] = pydantic.Field(None, description='Called when the image is used by a ContainerDefinition.')

class ContainerImageDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-')
    ...

class ContainerImageDefFromAssetParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='The directory containing the Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class ContainerImageDefFromDockerImageAssetParams(pydantic.BaseModel):
    asset: models.aws_ecr_assets.DockerImageAssetDef = pydantic.Field(..., description='The ``DockerImageAsset`` to use for this container definition.')
    ...

class ContainerImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='-\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='-')
    ...

class ContainerImageDefFromRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret to expose to the container that contains the credentials for the image repository. The supported value is the full ARN of an AWS Secrets Manager secret.')
    ...

class ContainerImageDefFromTarballParams(pydantic.BaseModel):
    tarball_file: str = pydantic.Field(..., description='Absolute path to the tarball. You can use language-specific idioms (such as ``__dirname`` in Node.js) to create an absolute path based on the current script running directory.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CpuArchitecture
class CpuArchitectureDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CpuArchitecture'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CpuArchitectureDefConfig] = pydantic.Field(None)


class CpuArchitectureDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[models.aws_ecs.CpuArchitectureDefOfParams]] = pydantic.Field(None, description='Other cpu architecture.')

class CpuArchitectureDefOfParams(pydantic.BaseModel):
    cpu_architecture: str = pydantic.Field(..., description='cpu architecture.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-runtimeplatform.html#cfn-ecs-taskdefinition-runtimeplatform-cpuarchitecture for all available cpu architecture.\n')
    return_config: typing.Optional[list[models.aws_ecs.CpuArchitectureDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.CredentialSpec
class CredentialSpecDef(BaseClass):
    prefix_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    file_location: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Location or ARN from where to retrieve the CredSpec file.')
    _init_params: typing.ClassVar[list[str]] = ['prefix_id', 'file_location']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['arn_for_s3_object', 'arn_for_ssm_parameter']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CredentialSpec'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CredentialSpecDefConfig] = pydantic.Field(None)


class CredentialSpecDefConfig(pydantic.BaseModel):
    arn_for_s3_object: typing.Optional[list[models.aws_ecs.CredentialSpecDefArnForS3ObjectParams]] = pydantic.Field(None, description='Helper method to generate the ARN for a S3 object.\nUsed to avoid duplication of logic in derived classes.')
    arn_for_ssm_parameter: typing.Optional[list[models.aws_ecs.CredentialSpecDefArnForSsmParameterParams]] = pydantic.Field(None, description='Helper method to generate the ARN for a SSM parameter.\nUsed to avoid duplication of logic in derived classes.')
    bind: typing.Optional[list[models.aws_ecs.CredentialSpecDefBindParams]] = pydantic.Field(None, description='Called when the container is initialized to allow this object to bind to the stack.')

class CredentialSpecDefArnForS3ObjectParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='-\n')
    key: str = pydantic.Field(..., description='-')
    ...

class CredentialSpecDefArnForSsmParameterParams(pydantic.BaseModel):
    parameter: typing.Union[models.aws_ssm.StringListParameterDef, models.aws_ssm.StringParameterDef] = pydantic.Field(..., description='-')
    ...

class CredentialSpecDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.DomainJoinedCredentialSpec
class DomainJoinedCredentialSpecDef(BaseClass):
    file_location: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Location or ARN from where to retrieve the CredSpec file.')
    _init_params: typing.ClassVar[list[str]] = ['file_location']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['arn_for_s3_object', 'arn_for_ssm_parameter', 'from_s3_bucket', 'from_ssm_parameter']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.DomainJoinedCredentialSpec'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_s3_bucket', 'from_ssm_parameter']
    ...


    from_s3_bucket: typing.Optional[models.aws_ecs.DomainJoinedCredentialSpecDefFromS3BucketParams] = pydantic.Field(None, description='Loads the CredSpec from a S3 bucket object.')
    from_ssm_parameter: typing.Optional[models.aws_ecs.DomainJoinedCredentialSpecDefFromSsmParameterParams] = pydantic.Field(None, description='Loads the CredSpec from a SSM parameter.')
    resource_config: typing.Optional[models.aws_ecs.DomainJoinedCredentialSpecDefConfig] = pydantic.Field(None)


class DomainJoinedCredentialSpecDefConfig(pydantic.BaseModel):
    arn_for_s3_object: typing.Optional[list[models.aws_ecs.DomainJoinedCredentialSpecDefArnForS3ObjectParams]] = pydantic.Field(None, description='Helper method to generate the ARN for a S3 object.\nUsed to avoid duplication of logic in derived classes.')
    arn_for_ssm_parameter: typing.Optional[list[models.aws_ecs.DomainJoinedCredentialSpecDefArnForSsmParameterParams]] = pydantic.Field(None, description='Helper method to generate the ARN for a SSM parameter.\nUsed to avoid duplication of logic in derived classes.')
    bind: typing.Optional[list[models.aws_ecs.DomainJoinedCredentialSpecDefBindParams]] = pydantic.Field(None, description='Called when the container is initialized to allow this object to bind to the stack.')

class DomainJoinedCredentialSpecDefArnForS3ObjectParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='-\n')
    key: str = pydantic.Field(..., description='-')
    ...

class DomainJoinedCredentialSpecDefArnForSsmParameterParams(pydantic.BaseModel):
    parameter: typing.Union[models.aws_ssm.StringListParameterDef, models.aws_ssm.StringParameterDef] = pydantic.Field(..., description='-')
    ...

class DomainJoinedCredentialSpecDefBindParams(pydantic.BaseModel):
    ...

class DomainJoinedCredentialSpecDefFromS3BucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    ...

class DomainJoinedCredentialSpecDefFromSsmParameterParams(pydantic.BaseModel):
    parameter: typing.Union[models.aws_ssm.StringListParameterDef, models.aws_ssm.StringParameterDef] = pydantic.Field(..., description='The SSM parameter.\n')
    ...


#  autogenerated from aws_cdk.aws_ecs.DomainlessCredentialSpec
class DomainlessCredentialSpecDef(BaseClass):
    file_location: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Location or ARN from where to retrieve the CredSpec file.')
    _init_params: typing.ClassVar[list[str]] = ['file_location']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['arn_for_s3_object', 'arn_for_ssm_parameter', 'from_s3_bucket', 'from_ssm_parameter']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.DomainlessCredentialSpec'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_s3_bucket', 'from_ssm_parameter']
    ...


    from_s3_bucket: typing.Optional[models.aws_ecs.DomainlessCredentialSpecDefFromS3BucketParams] = pydantic.Field(None, description='Loads the CredSpec from a S3 bucket object.')
    from_ssm_parameter: typing.Optional[models.aws_ecs.DomainlessCredentialSpecDefFromSsmParameterParams] = pydantic.Field(None, description='Loads the CredSpec from a SSM parameter.')
    resource_config: typing.Optional[models.aws_ecs.DomainlessCredentialSpecDefConfig] = pydantic.Field(None)


class DomainlessCredentialSpecDefConfig(pydantic.BaseModel):
    arn_for_s3_object: typing.Optional[list[models.aws_ecs.DomainlessCredentialSpecDefArnForS3ObjectParams]] = pydantic.Field(None, description='Helper method to generate the ARN for a S3 object.\nUsed to avoid duplication of logic in derived classes.')
    arn_for_ssm_parameter: typing.Optional[list[models.aws_ecs.DomainlessCredentialSpecDefArnForSsmParameterParams]] = pydantic.Field(None, description='Helper method to generate the ARN for a SSM parameter.\nUsed to avoid duplication of logic in derived classes.')
    bind: typing.Optional[list[models.aws_ecs.DomainlessCredentialSpecDefBindParams]] = pydantic.Field(None, description='Called when the container is initialized to allow this object to bind to the stack.')

class DomainlessCredentialSpecDefArnForS3ObjectParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='-\n')
    key: str = pydantic.Field(..., description='-')
    ...

class DomainlessCredentialSpecDefArnForSsmParameterParams(pydantic.BaseModel):
    parameter: typing.Union[models.aws_ssm.StringListParameterDef, models.aws_ssm.StringParameterDef] = pydantic.Field(..., description='-')
    ...

class DomainlessCredentialSpecDefBindParams(pydantic.BaseModel):
    ...

class DomainlessCredentialSpecDefFromS3BucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    ...

class DomainlessCredentialSpecDefFromSsmParameterParams(pydantic.BaseModel):
    parameter: typing.Union[models.aws_ssm.StringListParameterDef, models.aws_ssm.StringParameterDef] = pydantic.Field(..., description='The SSM parameter.\n')
    ...


#  autogenerated from aws_cdk.aws_ecs.EcrImage
class EcrImageDef(BaseClass):
    repository: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-\n')
    tag_or_digest: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['repository', 'tag_or_digest']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EcrImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    ...


    from_asset: typing.Optional[models.aws_ecs.EcrImageDefFromAssetParams] = pydantic.Field(None, description="Reference an image that's constructed directly from sources on disk.\nIf you already have a ``DockerImageAsset`` instance, you can use the\n``ContainerImage.fromDockerImageAsset`` method instead.")
    from_docker_image_asset: typing.Optional[models.aws_ecs.EcrImageDefFromDockerImageAssetParams] = pydantic.Field(None, description='Use an existing ``DockerImageAsset`` for this container image.')
    from_ecr_repository: typing.Optional[models.aws_ecs.EcrImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='Reference an image in an ECR repository.')
    from_registry: typing.Optional[models.aws_ecs.EcrImageDefFromRegistryParams] = pydantic.Field(None, description='Reference an image on DockerHub or another online registry.')
    from_tarball: typing.Optional[models.aws_ecs.EcrImageDefFromTarballParams] = pydantic.Field(None, description='Use an existing tarball for this container image.\nUse this method if the container image has already been created by another process (e.g. jib)\nand you want to add it as a container image asset.')
    resource_config: typing.Optional[models.aws_ecs.EcrImageDefConfig] = pydantic.Field(None)


class EcrImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.EcrImageDefBindParams]] = pydantic.Field(None, description='Called when the image is used by a ContainerDefinition.')

class EcrImageDefBindParams(pydantic.BaseModel):
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-')
    ...

class EcrImageDefFromAssetParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='The directory containing the Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class EcrImageDefFromDockerImageAssetParams(pydantic.BaseModel):
    asset: models.aws_ecr_assets.DockerImageAssetDef = pydantic.Field(..., description='The ``DockerImageAsset`` to use for this container definition.')
    ...

class EcrImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='-\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='-')
    ...

class EcrImageDefFromRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret to expose to the container that contains the credentials for the image repository. The supported value is the full ARN of an AWS Secrets Manager secret.')
    ...

class EcrImageDefFromTarballParams(pydantic.BaseModel):
    tarball_file: str = pydantic.Field(..., description='Absolute path to the tarball. You can use language-specific idioms (such as ``__dirname`` in Node.js) to create an absolute path based on the current script running directory.')
    ...


#  autogenerated from aws_cdk.aws_ecs.EcsOptimizedImage
class EcsOptimizedImageDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['get_image']
    _classmethod_names: typing.ClassVar[list[str]] = ['amazon_linux', 'amazon_linux2', 'amazon_linux2023', 'windows']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EcsOptimizedImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.EcsOptimizedImageDefConfig] = pydantic.Field(None)


class EcsOptimizedImageDefConfig(pydantic.BaseModel):
    amazon_linux: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefAmazonLinuxParams]] = pydantic.Field(None, description='Construct an Amazon Linux AMI image from the latest ECS Optimized AMI published in SSM.')
    amazon_linux2: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefAmazonLinux2Params]] = pydantic.Field(None, description='Construct an Amazon Linux 2 image from the latest ECS Optimized AMI published in SSM.')
    amazon_linux2023: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefAmazonLinux2023Params]] = pydantic.Field(None, description='Construct an Amazon Linux 2023 image from the latest ECS Optimized AMI published in SSM.')
    get_image: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefGetImageParams]] = pydantic.Field(None, description='Return the correct image.')
    windows: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefWindowsParams]] = pydantic.Field(None, description='Construct a Windows image from the latest ECS Optimized AMI published in SSM.')

class EcsOptimizedImageDefAmazonLinuxParams(pydantic.BaseModel):
    cached_in_context: typing.Optional[bool] = pydantic.Field(None, description="Whether the AMI ID is cached to be stable between deployments. By default, the newest image is used on each deployment. This will cause instances to be replaced whenever a new version is released, and may cause downtime if there aren't enough running instances in the AutoScalingGroup to reschedule the tasks on. If set to true, the AMI ID will be cached in ``cdk.context.json`` and the same value will be used on future runs. Your instances will not be replaced but your AMI version will grow old over time. To refresh the AMI lookup, you will have to evict the value from the cache using the ``cdk context`` command. See https://docs.aws.amazon.com/cdk/latest/guide/context.html for more information. Can not be set to ``true`` in environment-agnostic stacks. Default: false")
    return_config: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefConfig]] = pydantic.Field(None)
    ...

class EcsOptimizedImageDefAmazonLinux2Params(pydantic.BaseModel):
    hardware_type: typing.Optional[aws_cdk.aws_ecs.AmiHardwareType] = pydantic.Field(None, description='ECS-optimized AMI variant to use.\n')
    cached_in_context: typing.Optional[bool] = pydantic.Field(None, description="Whether the AMI ID is cached to be stable between deployments. By default, the newest image is used on each deployment. This will cause instances to be replaced whenever a new version is released, and may cause downtime if there aren't enough running instances in the AutoScalingGroup to reschedule the tasks on. If set to true, the AMI ID will be cached in ``cdk.context.json`` and the same value will be used on future runs. Your instances will not be replaced but your AMI version will grow old over time. To refresh the AMI lookup, you will have to evict the value from the cache using the ``cdk context`` command. See https://docs.aws.amazon.com/cdk/latest/guide/context.html for more information. Can not be set to ``true`` in environment-agnostic stacks. Default: false")
    return_config: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefConfig]] = pydantic.Field(None)
    ...

class EcsOptimizedImageDefAmazonLinux2023Params(pydantic.BaseModel):
    hardware_type: typing.Optional[aws_cdk.aws_ecs.AmiHardwareType] = pydantic.Field(None, description='ECS-optimized AMI variant to use.\n')
    cached_in_context: typing.Optional[bool] = pydantic.Field(None, description="Whether the AMI ID is cached to be stable between deployments. By default, the newest image is used on each deployment. This will cause instances to be replaced whenever a new version is released, and may cause downtime if there aren't enough running instances in the AutoScalingGroup to reschedule the tasks on. If set to true, the AMI ID will be cached in ``cdk.context.json`` and the same value will be used on future runs. Your instances will not be replaced but your AMI version will grow old over time. To refresh the AMI lookup, you will have to evict the value from the cache using the ``cdk context`` command. See https://docs.aws.amazon.com/cdk/latest/guide/context.html for more information. Can not be set to ``true`` in environment-agnostic stacks. Default: false")
    return_config: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefConfig]] = pydantic.Field(None)
    ...

class EcsOptimizedImageDefGetImageParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    ...

class EcsOptimizedImageDefWindowsParams(pydantic.BaseModel):
    windows_version: aws_cdk.aws_ecs.WindowsOptimizedVersion = pydantic.Field(..., description='Windows Version to use.\n')
    cached_in_context: typing.Optional[bool] = pydantic.Field(None, description="Whether the AMI ID is cached to be stable between deployments. By default, the newest image is used on each deployment. This will cause instances to be replaced whenever a new version is released, and may cause downtime if there aren't enough running instances in the AutoScalingGroup to reschedule the tasks on. If set to true, the AMI ID will be cached in ``cdk.context.json`` and the same value will be used on future runs. Your instances will not be replaced but your AMI version will grow old over time. To refresh the AMI lookup, you will have to evict the value from the cache using the ``cdk context`` command. See https://docs.aws.amazon.com/cdk/latest/guide/context.html for more information. Can not be set to ``true`` in environment-agnostic stacks. Default: false")
    return_config: typing.Optional[list[models.aws_ecs.EcsOptimizedImageDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.EnvironmentFile
class EnvironmentFileDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_bucket']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EnvironmentFile'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_bucket']
    ...


    from_asset: typing.Optional[models.aws_ecs.EnvironmentFileDefFromAssetParams] = pydantic.Field(None, description='Loads the environment file from a local disk path.')
    from_bucket: typing.Optional[models.aws_ecs.EnvironmentFileDefFromBucketParams] = pydantic.Field(None, description='Loads the environment file from an S3 bucket.')
    resource_config: typing.Optional[models.aws_ecs.EnvironmentFileDefConfig] = pydantic.Field(None)


class EnvironmentFileDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.EnvironmentFileDefBindParams]] = pydantic.Field(None, description='Called when the container is initialized to allow this object to bind to the stack.')

class EnvironmentFileDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The binding scope.')
    ...

class EnvironmentFileDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Local disk path.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class EnvironmentFileDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.\n')
    ...


#  autogenerated from aws_cdk.aws_ecs.FireLensLogDriver
class FireLensLogDriverDef(BaseClass):
    options: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The configuration options to send to the log driver. Default: - the log driver options\n')
    secret_options: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secrets to pass to the log configuration. Default: - No secret options provided.\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    _init_params: typing.ClassVar[list[str]] = ['options', 'secret_options', 'env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FireLensLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.FireLensLogDriverDefConfig] = pydantic.Field(None)


class FireLensLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.FireLensLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.FireLensLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class FireLensLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class FireLensLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.FluentdLogDriver
class FluentdLogDriverDef(BaseClass):
    address: typing.Optional[str] = pydantic.Field(None, description='By default, the logging driver connects to localhost:24224. Supply the address option to connect to a different address. tcp(default) and unix sockets are supported. Default: - address not set.\n')
    async_connect: typing.Optional[bool] = pydantic.Field(None, description='Docker connects to Fluentd in the background. Messages are buffered until the connection is established. Default: - false\n')
    buffer_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of data to buffer before flushing to disk. Default: - The amount of RAM available to the container.\n')
    max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of retries. Default: - 4294967295 (2**32 - 1).\n')
    retry_wait: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How long to wait between retries. Default: - 1 second\n')
    sub_second_precision: typing.Optional[bool] = pydantic.Field(None, description='Generates event logs in nanosecond resolution. Default: - false\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    _init_params: typing.ClassVar[list[str]] = ['address', 'async_connect', 'buffer_limit', 'max_retries', 'retry_wait', 'sub_second_precision', 'env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FluentdLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.FluentdLogDriverDefConfig] = pydantic.Field(None)


class FluentdLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.FluentdLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.FluentdLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class FluentdLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class FluentdLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.GelfLogDriver
class GelfLogDriverDef(BaseClass):
    address: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The address of the GELF server. tcp and udp are the only supported URI specifier and you must specify the port.\n')
    compression_level: typing.Union[int, float, None] = pydantic.Field(None, description='UDP Only The level of compression when gzip or zlib is the gelf-compression-type. An integer in the range of -1 to 9 (BestCompression). Higher levels provide more compression at lower speed. Either -1 or 0 disables compression. Default: - 1\n')
    compression_type: typing.Optional[aws_cdk.aws_ecs.GelfCompressionType] = pydantic.Field(None, description='UDP Only The type of compression the GELF driver uses to compress each log message. Allowed values are gzip, zlib and none. Default: - gzip\n')
    tcp_max_reconnect: typing.Union[int, float, None] = pydantic.Field(None, description='TCP Only The maximum number of reconnection attempts when the connection drop. A positive integer. Default: - 3\n')
    tcp_reconnect_delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='TCP Only The number of seconds to wait between reconnection attempts. A positive integer. Default: - 1\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    _init_params: typing.ClassVar[list[str]] = ['address', 'compression_level', 'compression_type', 'tcp_max_reconnect', 'tcp_reconnect_delay', 'env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.GelfLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.GelfLogDriverDefConfig] = pydantic.Field(None)


class GelfLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.GelfLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.GelfLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class GelfLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class GelfLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.GenericLogDriver
class GenericLogDriverDef(BaseClass):
    log_driver: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log driver to use for the container. The valid values listed for this parameter are log drivers that the Amazon ECS container agent can communicate with by default. For tasks using the Fargate launch type, the supported log drivers are awslogs and splunk. For tasks using the EC2 launch type, the supported log drivers are awslogs, syslog, gelf, fluentd, splunk, journald, and json-file. For more information about using the awslogs log driver, see `Using the awslogs Log Driver <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html>`_ in the Amazon Elastic Container Service Developer Guide.\n')
    options: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The configuration options to send to the log driver. Default: - the log driver options.\n')
    secret_options: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secrets to pass to the log configuration. Default: - no secret options provided.')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.GenericLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.GenericLogDriverDefConfig] = pydantic.Field(None)


class GenericLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.GenericLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.GenericLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class GenericLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class GenericLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.JournaldLogDriver
class JournaldLogDriverDef(BaseClass):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.JournaldLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.JournaldLogDriverDefConfig] = pydantic.Field(None)


class JournaldLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.JournaldLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.JournaldLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class JournaldLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class JournaldLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.JsonFileLogDriver
class JsonFileLogDriverDef(BaseClass):
    compress: typing.Optional[bool] = pydantic.Field(None, description='Toggles compression for rotated logs. Default: - false\n')
    max_file: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed. Only effective when max-size is also set. A positive integer. Default: - 1\n')
    max_size: typing.Optional[str] = pydantic.Field(None, description='The maximum size of the log before it is rolled. A positive integer plus a modifier representing the unit of measure (k, m, or g). Default: - -1 (unlimited)\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    _init_params: typing.ClassVar[list[str]] = ['compress', 'max_file', 'max_size', 'env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.JsonFileLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.JsonFileLogDriverDefConfig] = pydantic.Field(None)


class JsonFileLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.JsonFileLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.JsonFileLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class JsonFileLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class JsonFileLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.ListenerConfig
class ListenerConfigDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['add_targets']
    _classmethod_names: typing.ClassVar[list[str]] = ['application_listener', 'network_listener']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ListenerConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ListenerConfigDefConfig] = pydantic.Field(None)


class ListenerConfigDefConfig(pydantic.BaseModel):
    add_targets: typing.Optional[list[models.aws_ecs.ListenerConfigDefAddTargetsParams]] = pydantic.Field(None, description='Create and attach a target group to listener.')
    application_listener: typing.Optional[list[models.aws_ecs.ListenerConfigDefApplicationListenerParams]] = pydantic.Field(None, description='Create a config for adding target group to ALB listener.')
    network_listener: typing.Optional[list[models.aws_ecs.ListenerConfigDefNetworkListenerParams]] = pydantic.Field(None, description='Create a config for adding target group to NLB listener.')

class ListenerConfigDefAddTargetsParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Union[models.aws_ecs.LoadBalancerTargetOptionsDef, dict[str, typing.Any]] = pydantic.Field(..., description='-\n')
    service: models.aws_ecs.BaseServiceDef = pydantic.Field(..., description='-')
    ...

class ListenerConfigDefApplicationListenerParams(pydantic.BaseModel):
    listener: models.aws_elasticloadbalancingv2.ApplicationListenerDef = pydantic.Field(..., description='-\n')
    deregistration_delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time for Elastic Load Balancing to wait before deregistering a target. The range is 0-3600 seconds. Default: Duration.minutes(5)\n')
    health_check: typing.Union[models.aws_elasticloadbalancingv2.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Health check configuration. Default: - The default value for each property in this configuration varies depending on the target.\n')
    load_balancing_algorithm_type: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.TargetGroupLoadBalancingAlgorithmType] = pydantic.Field(None, description='The load balancing algorithm to select targets for routing requests. Default: round_robin.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port on which the listener listens for requests. Default: Determined from protocol if known\n')
    protocol: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.ApplicationProtocol] = pydantic.Field(None, description='The protocol to use. Default: Determined from port if known\n')
    protocol_version: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.ApplicationProtocolVersion] = pydantic.Field(None, description='The protocol version to use. Default: ApplicationProtocolVersion.HTTP1\n')
    slow_start: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time period during which the load balancer sends a newly registered target a linearly increasing share of the traffic to the target group. The range is 30-900 seconds (15 minutes). Default: 0\n')
    stickiness_cookie_duration: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The stickiness cookie expiration period. Setting this value enables load balancer stickiness. After this period, the cookie is considered stale. The minimum value is 1 second and the maximum value is 7 days (604800 seconds). Default: Stickiness disabled\n')
    stickiness_cookie_name: typing.Optional[str] = pydantic.Field(None, description="The name of an application-based stickiness cookie. Names that start with the following prefixes are not allowed: AWSALB, AWSALBAPP, and AWSALBTG; they're reserved for use by the load balancer. Note: ``stickinessCookieName`` parameter depends on the presence of ``stickinessCookieDuration`` parameter. If ``stickinessCookieDuration`` is not set, ``stickinessCookieName`` will be omitted. Default: - If ``stickinessCookieDuration`` is set, a load-balancer generated cookie is used. Otherwise, no stickiness is defined.\n")
    target_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the target group. This name must be unique per region per account, can have a maximum of 32 characters, must contain only alphanumeric characters or hyphens, and must not begin or end with a hyphen. Default: Automatically generated\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_autoscaling.AutoScalingGroupDef, models.aws_ecs.BaseServiceDef, models.aws_ecs.Ec2ServiceDef, models.aws_ecs.ExternalServiceDef, models.aws_ecs.FargateServiceDef, models.aws_elasticloadbalancingv2_targets.InstanceIdTargetDef, models.aws_elasticloadbalancingv2_targets.InstanceTargetDef, models.aws_elasticloadbalancingv2_targets.IpTargetDef, models.aws_elasticloadbalancingv2_targets.LambdaTargetDef]]] = pydantic.Field(None, description='The targets to add to this target group. Can be ``Instance``, ``IPAddress``, or any self-registering load balancing target. All target must be of the same type.\n')
    conditions: typing.Optional[typing.Sequence[models.aws_elasticloadbalancingv2.ListenerConditionDef]] = pydantic.Field(None, description='Rule applies if matches the conditions. Default: - No conditions.\n')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='Priority of this target group. The rule with the lowest priority will be used for every request. If priority is not given, these target groups will be added as defaults, and must not have conditions. Priorities must be unique. Default: Target groups are used as defaults')
    return_config: typing.Optional[list[models.aws_ecs.ListenerConfigDefConfig]] = pydantic.Field(None)
    ...

class ListenerConfigDefNetworkListenerParams(pydantic.BaseModel):
    listener: models.aws_elasticloadbalancingv2.NetworkListenerDef = pydantic.Field(..., description='-\n')
    port: typing.Union[int, float] = pydantic.Field(..., description='The port on which the listener listens for requests. Default: Determined from protocol if known\n')
    deregistration_delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time for Elastic Load Balancing to wait before deregistering a target. The range is 0-3600 seconds. Default: Duration.minutes(5)\n')
    health_check: typing.Union[models.aws_elasticloadbalancingv2.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Health check configuration. Default: - The default value for each property in this configuration varies depending on the target.\n')
    preserve_client_ip: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether client IP preservation is enabled. Default: false if the target group type is IP address and the target group protocol is TCP or TLS. Otherwise, true.\n')
    protocol: typing.Optional[aws_cdk.aws_elasticloadbalancingv2.Protocol] = pydantic.Field(None, description='Protocol for target group, expects TCP, TLS, UDP, or TCP_UDP. Default: - inherits the protocol of the listener\n')
    proxy_protocol_v2: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether Proxy Protocol version 2 is enabled. Default: false\n')
    target_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the target group. This name must be unique per region per account, can have a maximum of 32 characters, must contain only alphanumeric characters or hyphens, and must not begin or end with a hyphen. Default: Automatically generated\n')
    targets: typing.Optional[typing.Sequence[typing.Union[models.aws_autoscaling.AutoScalingGroupDef, models.aws_ecs.BaseServiceDef, models.aws_ecs.Ec2ServiceDef, models.aws_ecs.ExternalServiceDef, models.aws_ecs.FargateServiceDef, models.aws_elasticloadbalancingv2_targets.AlbArnTargetDef, models.aws_elasticloadbalancingv2_targets.AlbTargetDef, models.aws_elasticloadbalancingv2_targets.InstanceIdTargetDef, models.aws_elasticloadbalancingv2_targets.InstanceTargetDef, models.aws_elasticloadbalancingv2_targets.IpTargetDef]]] = pydantic.Field(None, description='The targets to add to this target group. Can be ``Instance``, ``IPAddress``, or any self-registering load balancing target. If you use either ``Instance`` or ``IPAddress`` as targets, all target must be of the same type.')
    return_config: typing.Optional[list[models.aws_ecs.ListenerConfigDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.LogDriver
class LogDriverDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.LogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.LogDriverDefConfig] = pydantic.Field(None)


class LogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.LogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.LogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class LogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class LogDriverDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_ecs.LogDrivers
class LogDriversDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs', 'firelens', 'fluentd', 'gelf', 'journald', 'json_file', 'splunk', 'syslog']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.LogDrivers'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['aws_logs', 'firelens', 'fluentd', 'gelf', 'journald', 'json_file', 'splunk', 'syslog']
    ...


    aws_logs: typing.Optional[models.aws_ecs.LogDriversDefAwsLogsParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    firelens: typing.Optional[models.aws_ecs.LogDriversDefFirelensParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to firelens log router.\nFor detail configurations, please refer to Amazon ECS FireLens Examples:\nhttps://github.com/aws-samples/amazon-ecs-firelens-examples')
    fluentd: typing.Optional[models.aws_ecs.LogDriversDefFluentdParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to fluentd Logs.')
    gelf: typing.Optional[models.aws_ecs.LogDriversDefGelfParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to gelf Logs.')
    journald: typing.Optional[models.aws_ecs.LogDriversDefJournaldParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to journald Logs.')
    json_file: typing.Optional[models.aws_ecs.LogDriversDefJsonFileParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to json-file Logs.')
    splunk: typing.Optional[models.aws_ecs.LogDriversDefSplunkParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to splunk Logs.')
    syslog: typing.Optional[models.aws_ecs.LogDriversDefSyslogParams] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to syslog Logs.')

class LogDriversDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    ...

class LogDriversDefFirelensParams(pydantic.BaseModel):
    options: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The configuration options to send to the log driver. Default: - the log driver options\n')
    secret_options: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secrets to pass to the log configuration. Default: - No secret options provided.\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    ...

class LogDriversDefFluentdParams(pydantic.BaseModel):
    address: typing.Optional[str] = pydantic.Field(None, description='By default, the logging driver connects to localhost:24224. Supply the address option to connect to a different address. tcp(default) and unix sockets are supported. Default: - address not set.\n')
    async_connect: typing.Optional[bool] = pydantic.Field(None, description='Docker connects to Fluentd in the background. Messages are buffered until the connection is established. Default: - false\n')
    buffer_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of data to buffer before flushing to disk. Default: - The amount of RAM available to the container.\n')
    max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of retries. Default: - 4294967295 (2**32 - 1).\n')
    retry_wait: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How long to wait between retries. Default: - 1 second\n')
    sub_second_precision: typing.Optional[bool] = pydantic.Field(None, description='Generates event logs in nanosecond resolution. Default: - false\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    ...

class LogDriversDefGelfParams(pydantic.BaseModel):
    address: str = pydantic.Field(..., description='The address of the GELF server. tcp and udp are the only supported URI specifier and you must specify the port.\n')
    compression_level: typing.Union[int, float, None] = pydantic.Field(None, description='UDP Only The level of compression when gzip or zlib is the gelf-compression-type. An integer in the range of -1 to 9 (BestCompression). Higher levels provide more compression at lower speed. Either -1 or 0 disables compression. Default: - 1\n')
    compression_type: typing.Optional[aws_cdk.aws_ecs.GelfCompressionType] = pydantic.Field(None, description='UDP Only The type of compression the GELF driver uses to compress each log message. Allowed values are gzip, zlib and none. Default: - gzip\n')
    tcp_max_reconnect: typing.Union[int, float, None] = pydantic.Field(None, description='TCP Only The maximum number of reconnection attempts when the connection drop. A positive integer. Default: - 3\n')
    tcp_reconnect_delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='TCP Only The number of seconds to wait between reconnection attempts. A positive integer. Default: - 1\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    ...

class LogDriversDefJournaldParams(pydantic.BaseModel):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    ...

class LogDriversDefJsonFileParams(pydantic.BaseModel):
    compress: typing.Optional[bool] = pydantic.Field(None, description='Toggles compression for rotated logs. Default: - false\n')
    max_file: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed. Only effective when max-size is also set. A positive integer. Default: - 1\n')
    max_size: typing.Optional[str] = pydantic.Field(None, description='The maximum size of the log before it is rolled. A positive integer plus a modifier representing the unit of measure (k, m, or g). Default: - -1 (unlimited)\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    ...

class LogDriversDefSplunkParams(pydantic.BaseModel):
    secret_token: models.aws_ecs.SecretDef = pydantic.Field(..., description='Splunk HTTP Event Collector token (Secret). The splunk-token is added to the SecretOptions property of the Log Driver Configuration. So the secret value will not be resolved or viewable as plain text.\n')
    url: str = pydantic.Field(..., description='Path to your Splunk Enterprise, self-service Splunk Cloud instance, or Splunk Cloud managed cluster (including port and scheme used by HTTP Event Collector) in one of the following formats: https://your_splunk_instance:8088 or https://input-prd-p-XXXXXXX.cloud.splunk.com:8088 or https://http-inputs-XXXXXXXX.splunkcloud.com.\n')
    ca_name: typing.Optional[str] = pydantic.Field(None, description='Name to use for validating server certificate. Default: - The hostname of the splunk-url\n')
    ca_path: typing.Optional[str] = pydantic.Field(None, description='Path to root certificate. Default: - caPath not set.\n')
    format: typing.Optional[aws_cdk.aws_ecs.SplunkLogFormat] = pydantic.Field(None, description='Message format. Can be inline, json or raw. Default: - inline\n')
    gzip: typing.Optional[bool] = pydantic.Field(None, description='Enable/disable gzip compression to send events to Splunk Enterprise or Splunk Cloud instance. Default: - false\n')
    gzip_level: typing.Union[int, float, None] = pydantic.Field(None, description='Set compression level for gzip. Valid values are -1 (default), 0 (no compression), 1 (best speed) ... 9 (best compression). Default: - -1 (Default Compression)\n')
    index: typing.Optional[str] = pydantic.Field(None, description='Event index. Default: - index not set.\n')
    insecure_skip_verify: typing.Optional[str] = pydantic.Field(None, description='Ignore server certificate validation. Default: - insecureSkipVerify not set.\n')
    source: typing.Optional[str] = pydantic.Field(None, description='Event source. Default: - source not set.\n')
    source_type: typing.Optional[str] = pydantic.Field(None, description='Event source type. Default: - sourceType not set.\n')
    verify_connection: typing.Optional[bool] = pydantic.Field(None, description='Verify on start, that docker can connect to Splunk server. Default: - true\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    ...

class LogDriversDefSyslogParams(pydantic.BaseModel):
    address: typing.Optional[str] = pydantic.Field(None, description='The address of an external syslog server. The URI specifier may be [tcp|udp|tcp+tls]://host:port, unix://path, or unixgram://path. Default: - If the transport is tcp, udp, or tcp+tls, the default port is 514.\n')
    facility: typing.Optional[str] = pydantic.Field(None, description='The syslog facility to use. Can be the number or name for any valid syslog facility. See the syslog documentation: https://tools.ietf.org/html/rfc5424#section-6.2.1. Default: - facility not set\n')
    format: typing.Optional[str] = pydantic.Field(None, description='The syslog message format to use. If not specified the local UNIX syslog format is used, without a specified hostname. Specify rfc3164 for the RFC-3164 compatible format, rfc5424 for RFC-5424 compatible format, or rfc5424micro for RFC-5424 compatible format with microsecond timestamp resolution. Default: - format not set\n')
    tls_ca_cert: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the trust certificates signed by the CA. Ignored if the address protocol is not tcp+tls. Default: - tlsCaCert not set\n')
    tls_cert: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the TLS certificate file. Ignored if the address protocol is not tcp+tls. Default: - tlsCert not set\n')
    tls_key: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the TLS key file. Ignored if the address protocol is not tcp+tls. Default: - tlsKey not set\n')
    tls_skip_verify: typing.Optional[bool] = pydantic.Field(None, description='If set to true, TLS verification is skipped when connecting to the syslog daemon. Ignored if the address protocol is not tcp+tls. Default: - false\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    ...


#  autogenerated from aws_cdk.aws_ecs.OperatingSystemFamily
class OperatingSystemFamilyDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.OperatingSystemFamily'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.OperatingSystemFamilyDefConfig] = pydantic.Field(None)


class OperatingSystemFamilyDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[models.aws_ecs.OperatingSystemFamilyDefOfParams]] = pydantic.Field(None, description='Other operating system family.')

class OperatingSystemFamilyDefOfParams(pydantic.BaseModel):
    family: str = pydantic.Field(..., description='operating system family.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-runtimeplatform.html#cfn-ecs-taskdefinition-runtimeplatform-operatingsystemfamily for all available operating system family.\n')
    return_config: typing.Optional[list[models.aws_ecs.OperatingSystemFamilyDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.PlacementConstraint
class PlacementConstraintDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['distinct_instances', 'member_of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.PlacementConstraint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.PlacementConstraintDefConfig] = pydantic.Field(None)


class PlacementConstraintDefConfig(pydantic.BaseModel):
    distinct_instances: typing.Optional[list[models.aws_ecs.PlacementConstraintDefDistinctInstancesParams]] = pydantic.Field(None, description='Use distinctInstance to ensure that each task in a particular group is running on a different container instance.')
    member_of: typing.Optional[list[models.aws_ecs.PlacementConstraintDefMemberOfParams]] = pydantic.Field(None, description='Use memberOf to restrict the selection to a group of valid candidates specified by a query expression.\nMultiple expressions can be specified. For more information, see\n`Cluster Query Language <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html>`_.\n\nYou can specify multiple expressions in one call. The tasks will only be placed on instances matching all expressions.')

class PlacementConstraintDefDistinctInstancesParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_ecs.PlacementConstraintDefConfig]] = pydantic.Field(None)
    ...

class PlacementConstraintDefMemberOfParams(pydantic.BaseModel):
    expressions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_ecs.PlacementConstraintDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.PlacementStrategy
class PlacementStrategyDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['packed_by', 'packed_by_cpu', 'packed_by_memory', 'randomly', 'spread_across', 'spread_across_instances']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.PlacementStrategy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.PlacementStrategyDefConfig] = pydantic.Field(None)


class PlacementStrategyDefConfig(pydantic.BaseModel):
    packed_by: typing.Optional[list[models.aws_ecs.PlacementStrategyDefPackedByParams]] = pydantic.Field(None, description='Places tasks on the container instances with the least available capacity of the specified resource.')
    packed_by_cpu: typing.Optional[list[models.aws_ecs.PlacementStrategyDefPackedByCpuParams]] = pydantic.Field(None, description='Places tasks on container instances with the least available amount of CPU capacity.\nThis minimizes the number of instances in use.')
    packed_by_memory: typing.Optional[list[models.aws_ecs.PlacementStrategyDefPackedByMemoryParams]] = pydantic.Field(None, description='Places tasks on container instances with the least available amount of memory capacity.\nThis minimizes the number of instances in use.')
    randomly: typing.Optional[list[models.aws_ecs.PlacementStrategyDefRandomlyParams]] = pydantic.Field(None, description='Places tasks randomly.')
    spread_across: typing.Optional[list[models.aws_ecs.PlacementStrategyDefSpreadAcrossParams]] = pydantic.Field(None, description='Places tasks evenly based on the specified value.\nYou can use one of the built-in attributes found on ``BuiltInAttributes``\nor supply your own custom instance attributes. If more than one attribute\nis supplied, spreading is done in order.')
    spread_across_instances: typing.Optional[list[models.aws_ecs.PlacementStrategyDefSpreadAcrossInstancesParams]] = pydantic.Field(None, description='Places tasks evenly across all container instances in the cluster.')

class PlacementStrategyDefPackedByParams(pydantic.BaseModel):
    resource: aws_cdk.aws_ecs.BinPackResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_ecs.PlacementStrategyDefConfig]] = pydantic.Field(None)
    ...

class PlacementStrategyDefPackedByCpuParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_ecs.PlacementStrategyDefConfig]] = pydantic.Field(None)
    ...

class PlacementStrategyDefPackedByMemoryParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_ecs.PlacementStrategyDefConfig]] = pydantic.Field(None)
    ...

class PlacementStrategyDefRandomlyParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_ecs.PlacementStrategyDefConfig]] = pydantic.Field(None)
    ...

class PlacementStrategyDefSpreadAcrossParams(pydantic.BaseModel):
    fields: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_ecs.PlacementStrategyDefConfig]] = pydantic.Field(None)
    ...

class PlacementStrategyDefSpreadAcrossInstancesParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_ecs.PlacementStrategyDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.PortMap
class PortMapDef(BaseClass):
    networkmode: typing.Union[aws_cdk.aws_ecs.NetworkMode, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    container_port: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The port number on the container that is bound to the user-specified or automatically assigned host port. If you are using containers in a task with the awsvpc or host network mode, exposed ports should be specified using containerPort. If you are using containers in a task with the bridge network mode and you specify a container port and not a host port, your container automatically receives a host port in the ephemeral port range. For more information, see hostPort. Port mappings that are automatically assigned in this way do not count toward the 100 reserved ports limit of a container instance. If you want to expose a port range, you must specify ``CONTAINER_PORT_USE_RANGE`` as container port.\n')
    app_protocol: typing.Optional[models.aws_ecs.AppProtocolDef] = pydantic.Field(None, description='The protocol used by Service Connect. Valid values are AppProtocol.http, AppProtocol.http2, and AppProtocol.grpc. The protocol determines what telemetry will be shown in the ECS Console for Service Connect services using this port mapping. This field may only be set when the task definition uses Bridge or Awsvpc network modes. Default: - no app protocol\n')
    container_port_range: typing.Optional[str] = pydantic.Field(None, description="The port number range on the container that's bound to the dynamically mapped host port range. The following rules apply when you specify a ``containerPortRange``: - You must specify ``CONTAINER_PORT_USE_RANGE`` as ``containerPort`` - You must use either the ``bridge`` network mode or the ``awsvpc`` network mode. - The container instance must have at least version 1.67.0 of the container agent and at least version 1.67.0-1 of the ``ecs-init`` package - You can specify a maximum of 100 port ranges per container. - A port can only be included in one port mapping per container. - You cannot specify overlapping port ranges. - The first port in the range must be less than last port in the range. If you want to expose a single port, you must not set a range.\n")
    host_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number on the container instance to reserve for your container. If you are using containers in a task with the awsvpc or host network mode, the hostPort can either be left blank or set to the same value as the containerPort. If you are using containers in a task with the bridge network mode, you can specify a non-reserved host port for your container port mapping, or you can omit the hostPort (or set it to 0) while specifying a containerPort and your container automatically receives a port in the ephemeral port range for your container instance operating system and Docker version.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name to give the port mapping. Name is required in order to use the port mapping with ECS Service Connect. This field may only be set when the task definition uses Bridge or Awsvpc network modes. Default: - no port mapping name\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Valid values are Protocol.TCP and Protocol.UDP. Default: TCP')
    _init_params: typing.ClassVar[list[str]] = ['networkmode', 'container_port', 'app_protocol', 'container_port_range', 'host_port', 'name', 'protocol']
    _method_names: typing.ClassVar[list[str]] = ['validate']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.PortMap'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.PortMapDefConfig] = pydantic.Field(None)


class PortMapDefConfig(pydantic.BaseModel):
    validate_: typing.Optional[bool] = pydantic.Field(None, description='validate invalid portmapping and networkmode parameters.\nthrow Error when invalid parameters.', alias='validate')


#  autogenerated from aws_cdk.aws_ecs.ProxyConfiguration
class ProxyConfigurationDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ProxyConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ProxyConfigurationDefConfig] = pydantic.Field(None)


class ProxyConfigurationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.ProxyConfigurationDefBindParams]] = pydantic.Field(None, description='Called when the proxy configuration is configured on a task definition.')

class ProxyConfigurationDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.ProxyConfigurations
class ProxyConfigurationsDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['app_mesh_proxy_configuration']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ProxyConfigurations'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['app_mesh_proxy_configuration']
    ...


    app_mesh_proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationsDefAppMeshProxyConfigurationParams] = pydantic.Field(None, description='Constructs a new instance of the ProxyConfiguration class.')

class ProxyConfigurationsDefAppMeshProxyConfigurationParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='The name of the container that will serve as the App Mesh proxy.\n')
    properties: typing.Union[models.aws_ecs.AppMeshProxyConfigurationPropsDef, dict[str, typing.Any]] = pydantic.Field(..., description='The set of network configuration parameters to provide the Container Network Interface (CNI) plugin.')
    ...


#  autogenerated from aws_cdk.aws_ecs.RepositoryImage
class RepositoryImageDef(BaseClass):
    image_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-\n')
    credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret to expose to the container that contains the credentials for the image repository. The supported value is the full ARN of an AWS Secrets Manager secret.')
    _init_params: typing.ClassVar[list[str]] = ['image_name', 'credentials']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.RepositoryImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    ...


    from_asset: typing.Optional[models.aws_ecs.RepositoryImageDefFromAssetParams] = pydantic.Field(None, description="Reference an image that's constructed directly from sources on disk.\nIf you already have a ``DockerImageAsset`` instance, you can use the\n``ContainerImage.fromDockerImageAsset`` method instead.")
    from_docker_image_asset: typing.Optional[models.aws_ecs.RepositoryImageDefFromDockerImageAssetParams] = pydantic.Field(None, description='Use an existing ``DockerImageAsset`` for this container image.')
    from_ecr_repository: typing.Optional[models.aws_ecs.RepositoryImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='Reference an image in an ECR repository.')
    from_registry: typing.Optional[models.aws_ecs.RepositoryImageDefFromRegistryParams] = pydantic.Field(None, description='Reference an image on DockerHub or another online registry.')
    from_tarball: typing.Optional[models.aws_ecs.RepositoryImageDefFromTarballParams] = pydantic.Field(None, description='Use an existing tarball for this container image.\nUse this method if the container image has already been created by another process (e.g. jib)\nand you want to add it as a container image asset.')
    resource_config: typing.Optional[models.aws_ecs.RepositoryImageDefConfig] = pydantic.Field(None)


class RepositoryImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.RepositoryImageDefBindParams]] = pydantic.Field(None, description='Called when the image is used by a ContainerDefinition.')

class RepositoryImageDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-')
    ...

class RepositoryImageDefFromAssetParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='The directory containing the Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class RepositoryImageDefFromDockerImageAssetParams(pydantic.BaseModel):
    asset: models.aws_ecr_assets.DockerImageAssetDef = pydantic.Field(..., description='The ``DockerImageAsset`` to use for this container definition.')
    ...

class RepositoryImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='-\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='-')
    ...

class RepositoryImageDefFromRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret to expose to the container that contains the credentials for the image repository. The supported value is the full ARN of an AWS Secrets Manager secret.')
    ...

class RepositoryImageDefFromTarballParams(pydantic.BaseModel):
    tarball_file: str = pydantic.Field(..., description='Absolute path to the tarball. You can use language-specific idioms (such as ``__dirname`` in Node.js) to create an absolute path based on the current script running directory.')
    ...


#  autogenerated from aws_cdk.aws_ecs.S3EnvironmentFile
class S3EnvironmentFileDef(BaseClass):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'key', 'object_version']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_bucket']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.S3EnvironmentFile'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_bucket']
    ...


    from_asset: typing.Optional[models.aws_ecs.S3EnvironmentFileDefFromAssetParams] = pydantic.Field(None, description='Loads the environment file from a local disk path.')
    from_bucket: typing.Optional[models.aws_ecs.S3EnvironmentFileDefFromBucketParams] = pydantic.Field(None, description='Loads the environment file from an S3 bucket.')
    resource_config: typing.Optional[models.aws_ecs.S3EnvironmentFileDefConfig] = pydantic.Field(None)


class S3EnvironmentFileDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.S3EnvironmentFileDefBindParams]] = pydantic.Field(None, description='Called when the container is initialized to allow this object to bind to the stack.')

class S3EnvironmentFileDefBindParams(pydantic.BaseModel):
    ...

class S3EnvironmentFileDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Local disk path.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class S3EnvironmentFileDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.\n')
    ...


#  autogenerated from aws_cdk.aws_ecs.Secret
class SecretDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['grant_read']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_secrets_manager', 'from_secrets_manager_version', 'from_ssm_parameter']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Secret'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_secrets_manager', 'from_secrets_manager_version', 'from_ssm_parameter']
    ...


    from_secrets_manager: typing.Optional[models.aws_ecs.SecretDefFromSecretsManagerParams] = pydantic.Field(None, description='Creates a environment variable value from a secret stored in AWS Secrets Manager.')
    from_secrets_manager_version: typing.Optional[models.aws_ecs.SecretDefFromSecretsManagerVersionParams] = pydantic.Field(None, description='Creates a environment variable value from a secret stored in AWS Secrets Manager.')
    from_ssm_parameter: typing.Optional[models.aws_ecs.SecretDefFromSsmParameterParams] = pydantic.Field(None, description='Creates an environment variable value from a parameter stored in AWS Systems Manager Parameter Store.')
    resource_config: typing.Optional[models.aws_ecs.SecretDefConfig] = pydantic.Field(None)


class SecretDefConfig(pydantic.BaseModel):
    grant_read: typing.Optional[list[models.aws_ecs.SecretDefGrantReadParams]] = pydantic.Field(None, description='Grants reading the secret to a principal.')

class SecretDefFromSecretsManagerParams(pydantic.BaseModel):
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='the secret stored in AWS Secrets Manager.\n')
    field: typing.Optional[str] = pydantic.Field(None, description='the name of the field with the value that you want to set as the environment variable value. Only values in JSON format are supported. If you do not specify a JSON field, then the full content of the secret is used.')
    ...

class SecretDefFromSecretsManagerVersionParams(pydantic.BaseModel):
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='the secret stored in AWS Secrets Manager.\n')
    version_info: typing.Union[models.aws_ecs.SecretVersionInfoDef, dict[str, typing.Any]] = pydantic.Field(..., description='the version information to reference the secret.\n')
    field: typing.Optional[str] = pydantic.Field(None, description='the name of the field with the value that you want to set as the environment variable value. Only values in JSON format are supported. If you do not specify a JSON field, then the full content of the secret is used.')
    ...

class SecretDefFromSsmParameterParams(pydantic.BaseModel):
    parameter: typing.Union[models.aws_ssm.StringListParameterDef, models.aws_ssm.StringParameterDef] = pydantic.Field(..., description='-')
    ...

class SecretDefGrantReadParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.ServiceConnect
class ServiceConnectDef(BaseClass):
    networkmode: typing.Union[aws_cdk.aws_ecs.NetworkMode, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    container_port: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The port number on the container that is bound to the user-specified or automatically assigned host port. If you are using containers in a task with the awsvpc or host network mode, exposed ports should be specified using containerPort. If you are using containers in a task with the bridge network mode and you specify a container port and not a host port, your container automatically receives a host port in the ephemeral port range. For more information, see hostPort. Port mappings that are automatically assigned in this way do not count toward the 100 reserved ports limit of a container instance. If you want to expose a port range, you must specify ``CONTAINER_PORT_USE_RANGE`` as container port.\n')
    app_protocol: typing.Optional[models.aws_ecs.AppProtocolDef] = pydantic.Field(None, description='The protocol used by Service Connect. Valid values are AppProtocol.http, AppProtocol.http2, and AppProtocol.grpc. The protocol determines what telemetry will be shown in the ECS Console for Service Connect services using this port mapping. This field may only be set when the task definition uses Bridge or Awsvpc network modes. Default: - no app protocol\n')
    container_port_range: typing.Optional[str] = pydantic.Field(None, description="The port number range on the container that's bound to the dynamically mapped host port range. The following rules apply when you specify a ``containerPortRange``: - You must specify ``CONTAINER_PORT_USE_RANGE`` as ``containerPort`` - You must use either the ``bridge`` network mode or the ``awsvpc`` network mode. - The container instance must have at least version 1.67.0 of the container agent and at least version 1.67.0-1 of the ``ecs-init`` package - You can specify a maximum of 100 port ranges per container. - A port can only be included in one port mapping per container. - You cannot specify overlapping port ranges. - The first port in the range must be less than last port in the range. If you want to expose a single port, you must not set a range.\n")
    host_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number on the container instance to reserve for your container. If you are using containers in a task with the awsvpc or host network mode, the hostPort can either be left blank or set to the same value as the containerPort. If you are using containers in a task with the bridge network mode, you can specify a non-reserved host port for your container port mapping, or you can omit the hostPort (or set it to 0) while specifying a containerPort and your container automatically receives a port in the ephemeral port range for your container instance operating system and Docker version.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name to give the port mapping. Name is required in order to use the port mapping with ECS Service Connect. This field may only be set when the task definition uses Bridge or Awsvpc network modes. Default: - no port mapping name\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Valid values are Protocol.TCP and Protocol.UDP. Default: TCP')
    _init_params: typing.ClassVar[list[str]] = ['networkmode', 'container_port', 'app_protocol', 'container_port_range', 'host_port', 'name', 'protocol']
    _method_names: typing.ClassVar[list[str]] = ['validate']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ServiceConnect'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ServiceConnectDefConfig] = pydantic.Field(None)


class ServiceConnectDefConfig(pydantic.BaseModel):
    validate_: typing.Optional[bool] = pydantic.Field(None, description='Judge serviceconnect parametes are valid.\nIf invalid, throw Error.', alias='validate')


#  autogenerated from aws_cdk.aws_ecs.SplunkLogDriver
class SplunkLogDriverDef(BaseClass):
    secret_token: typing.Union[models.aws_ecs.SecretDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Splunk HTTP Event Collector token (Secret). The splunk-token is added to the SecretOptions property of the Log Driver Configuration. So the secret value will not be resolved or viewable as plain text.\n')
    url: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Path to your Splunk Enterprise, self-service Splunk Cloud instance, or Splunk Cloud managed cluster (including port and scheme used by HTTP Event Collector) in one of the following formats: https://your_splunk_instance:8088 or https://input-prd-p-XXXXXXX.cloud.splunk.com:8088 or https://http-inputs-XXXXXXXX.splunkcloud.com.\n')
    ca_name: typing.Optional[str] = pydantic.Field(None, description='Name to use for validating server certificate. Default: - The hostname of the splunk-url\n')
    ca_path: typing.Optional[str] = pydantic.Field(None, description='Path to root certificate. Default: - caPath not set.\n')
    format: typing.Optional[aws_cdk.aws_ecs.SplunkLogFormat] = pydantic.Field(None, description='Message format. Can be inline, json or raw. Default: - inline\n')
    gzip: typing.Optional[bool] = pydantic.Field(None, description='Enable/disable gzip compression to send events to Splunk Enterprise or Splunk Cloud instance. Default: - false\n')
    gzip_level: typing.Union[int, float, None] = pydantic.Field(None, description='Set compression level for gzip. Valid values are -1 (default), 0 (no compression), 1 (best speed) ... 9 (best compression). Default: - -1 (Default Compression)\n')
    index: typing.Optional[str] = pydantic.Field(None, description='Event index. Default: - index not set.\n')
    insecure_skip_verify: typing.Optional[str] = pydantic.Field(None, description='Ignore server certificate validation. Default: - insecureSkipVerify not set.\n')
    source: typing.Optional[str] = pydantic.Field(None, description='Event source. Default: - source not set.\n')
    source_type: typing.Optional[str] = pydantic.Field(None, description='Event source type. Default: - sourceType not set.\n')
    verify_connection: typing.Optional[bool] = pydantic.Field(None, description='Verify on start, that docker can connect to Splunk server. Default: - true\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    _init_params: typing.ClassVar[list[str]] = ['secret_token', 'url', 'ca_name', 'ca_path', 'format', 'gzip', 'gzip_level', 'index', 'insecure_skip_verify', 'source', 'source_type', 'verify_connection', 'env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.SplunkLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.SplunkLogDriverDefConfig] = pydantic.Field(None)


class SplunkLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.SplunkLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.SplunkLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class SplunkLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class SplunkLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.SyslogLogDriver
class SyslogLogDriverDef(BaseClass):
    address: typing.Optional[str] = pydantic.Field(None, description='The address of an external syslog server. The URI specifier may be [tcp|udp|tcp+tls]://host:port, unix://path, or unixgram://path. Default: - If the transport is tcp, udp, or tcp+tls, the default port is 514.\n')
    facility: typing.Optional[str] = pydantic.Field(None, description='The syslog facility to use. Can be the number or name for any valid syslog facility. See the syslog documentation: https://tools.ietf.org/html/rfc5424#section-6.2.1. Default: - facility not set\n')
    format: typing.Optional[str] = pydantic.Field(None, description='The syslog message format to use. If not specified the local UNIX syslog format is used, without a specified hostname. Specify rfc3164 for the RFC-3164 compatible format, rfc5424 for RFC-5424 compatible format, or rfc5424micro for RFC-5424 compatible format with microsecond timestamp resolution. Default: - format not set\n')
    tls_ca_cert: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the trust certificates signed by the CA. Ignored if the address protocol is not tcp+tls. Default: - tlsCaCert not set\n')
    tls_cert: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the TLS certificate file. Ignored if the address protocol is not tcp+tls. Default: - tlsCert not set\n')
    tls_key: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the TLS key file. Ignored if the address protocol is not tcp+tls. Default: - tlsKey not set\n')
    tls_skip_verify: typing.Optional[bool] = pydantic.Field(None, description='If set to true, TLS verification is skipped when connecting to the syslog daemon. Ignored if the address protocol is not tcp+tls. Default: - false\n')
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID')
    _init_params: typing.ClassVar[list[str]] = ['address', 'facility', 'format', 'tls_ca_cert', 'tls_cert', 'tls_key', 'tls_skip_verify', 'env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['aws_logs']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.SyslogLogDriver'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.SyslogLogDriverDefConfig] = pydantic.Field(None)


class SyslogLogDriverDefConfig(pydantic.BaseModel):
    aws_logs: typing.Optional[list[models.aws_ecs.SyslogLogDriverDefAwsLogsParams]] = pydantic.Field(None, description='Creates a log driver configuration that sends log information to CloudWatch Logs.')
    bind: typing.Optional[list[models.aws_ecs.SyslogLogDriverDefBindParams]] = pydantic.Field(None, description='Called when the log driver is configured on a container.')

class SyslogLogDriverDefAwsLogsParams(pydantic.BaseModel):
    stream_prefix: str = pydantic.Field(..., description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.')
    return_config: typing.Optional[list[models.aws_ecs.LogDriverDefConfig]] = pydantic.Field(None)
    ...

class SyslogLogDriverDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.TagParameterContainerImage
class TagParameterContainerImageDef(BaseClass):
    repository: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['repository']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.TagParameterContainerImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_docker_image_asset', 'from_ecr_repository', 'from_registry', 'from_tarball']
    ...


    from_asset: typing.Optional[models.aws_ecs.TagParameterContainerImageDefFromAssetParams] = pydantic.Field(None, description="Reference an image that's constructed directly from sources on disk.\nIf you already have a ``DockerImageAsset`` instance, you can use the\n``ContainerImage.fromDockerImageAsset`` method instead.")
    from_docker_image_asset: typing.Optional[models.aws_ecs.TagParameterContainerImageDefFromDockerImageAssetParams] = pydantic.Field(None, description='Use an existing ``DockerImageAsset`` for this container image.')
    from_ecr_repository: typing.Optional[models.aws_ecs.TagParameterContainerImageDefFromEcrRepositoryParams] = pydantic.Field(None, description='Reference an image in an ECR repository.')
    from_registry: typing.Optional[models.aws_ecs.TagParameterContainerImageDefFromRegistryParams] = pydantic.Field(None, description='Reference an image on DockerHub or another online registry.')
    from_tarball: typing.Optional[models.aws_ecs.TagParameterContainerImageDefFromTarballParams] = pydantic.Field(None, description='Use an existing tarball for this container image.\nUse this method if the container image has already been created by another process (e.g. jib)\nand you want to add it as a container image asset.')
    resource_config: typing.Optional[models.aws_ecs.TagParameterContainerImageDefConfig] = pydantic.Field(None)


class TagParameterContainerImageDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_ecs.TagParameterContainerImageDefBindParams]] = pydantic.Field(None, description='Called when the image is used by a ContainerDefinition.')

class TagParameterContainerImageDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    container_definition: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-')
    ...

class TagParameterContainerImageDefFromAssetParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='The directory containing the Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class TagParameterContainerImageDefFromDockerImageAssetParams(pydantic.BaseModel):
    asset: models.aws_ecr_assets.DockerImageAssetDef = pydantic.Field(..., description='The ``DockerImageAsset`` to use for this container definition.')
    ...

class TagParameterContainerImageDefFromEcrRepositoryParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='-\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='-')
    ...

class TagParameterContainerImageDefFromRegistryParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret to expose to the container that contains the credentials for the image repository. The supported value is the full ARN of an AWS Secrets Manager secret.')
    ...

class TagParameterContainerImageDefFromTarballParams(pydantic.BaseModel):
    tarball_file: str = pydantic.Field(..., description='Absolute path to the tarball. You can use language-specific idioms (such as ``__dirname`` in Node.js) to create an absolute path based on the current script running directory.')
    ...


#  autogenerated from aws_cdk.aws_ecs.TaskDefinitionRevision
class TaskDefinitionRevisionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.TaskDefinitionRevision'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDefConfig] = pydantic.Field(None)


class TaskDefinitionRevisionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[models.aws_ecs.TaskDefinitionRevisionDefOfParams]] = pydantic.Field(None, description='Specific revision of a task.')

class TaskDefinitionRevisionDefOfParams(pydantic.BaseModel):
    revision: typing.Union[int, float] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_ecs.TaskDefinitionRevisionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.AsgCapacityProvider
class AsgCapacityProviderDef(BaseConstruct):
    auto_scaling_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_autoscaling.AutoScalingGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The autoscaling group to add as a Capacity Provider. Warning: When passing an imported resource using ``AutoScalingGroup.fromAutoScalingGroupName`` along with ``enableManagedTerminationProtection: true``, the ``AsgCapacityProvider`` construct will not be able to enforce the option ``newInstancesProtectedFromScaleIn`` of the ``AutoScalingGroup``. In this case the constructor of ``AsgCapacityProvider`` will throw an exception.\n')
    capacity_provider_name: typing.Optional[str] = pydantic.Field(None, description='The name of the capacity provider. If a name is specified, it cannot start with ``aws``, ``ecs``, or ``fargate``. If no name is specified, a default name in the CFNStackName-CFNResourceName-RandomString format is used. If the stack name starts with ``aws``, ``ecs``, or ``fargate``, a unique resource name is generated that starts with ``cp-``. Default: CloudFormation-generated name\n')
    enable_managed_draining: typing.Optional[bool] = pydantic.Field(None, description='Managed instance draining facilitates graceful termination of Amazon ECS instances. This allows your service workloads to stop safely and be rescheduled to non-terminating instances. Infrastructure maintenance and updates are preformed without disruptions to workloads. To use managed instance draining, set enableManagedDraining to true. Default: true\n')
    enable_managed_scaling: typing.Optional[bool] = pydantic.Field(None, description="When enabled the scale-in and scale-out actions of the cluster's Auto Scaling Group will be managed for you. This means your cluster will automatically scale instances based on the load your tasks put on the cluster. For more information, see `Using Managed Scaling <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/asg-capacity-providers.html#asg-capacity-providers-managed-scaling>`_ in the ECS Developer Guide. Default: true\n")
    enable_managed_termination_protection: typing.Optional[bool] = pydantic.Field(None, description='When enabled the Auto Scaling Group will only terminate EC2 instances that no longer have running non-daemon tasks. Scale-in protection will be automatically enabled on instances. When all non-daemon tasks are stopped on an instance, ECS initiates the scale-in process and turns off scale-in protection for the instance. The Auto Scaling Group can then terminate the instance. For more information see `Managed termination protection <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-auto-scaling.html#managed-termination-protection>`_ in the ECS Developer Guide. Managed scaling must also be enabled. Default: true\n')
    instance_warmup_period: typing.Union[int, float, None] = pydantic.Field(None, description='The period of time, in seconds, after a newly launched Amazon EC2 instance can contribute to CloudWatch metrics for Auto Scaling group. Must be between 0 and 10000. Default: 300\n')
    maximum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum scaling step size. In most cases this should be left alone. Default: 1000\n')
    minimum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum scaling step size. In most cases this should be left alone. Default: 1\n')
    target_capacity_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Target capacity percent. In most cases this should be left alone. Default: 100\n')
    can_containers_access_instance_role: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the containers can access the container instance role. Default: false\n')
    machine_image_type: typing.Optional[aws_cdk.aws_ecs.MachineImageType] = pydantic.Field(None, description='What type of machine image this is. Depending on the setting, different UserData will automatically be added to the ``AutoScalingGroup`` to configure it properly for use with ECS. If you create an ``AutoScalingGroup`` yourself and are adding it via ``addAutoScalingGroup()``, you must specify this value. If you are adding an ``autoScalingGroup`` via ``addCapacity``, this value will be determined from the ``machineImage`` you pass. Default: - Automatically determined from ``machineImage``, if available, otherwise ``MachineImageType.AMAZON_LINUX_2``.\n')
    spot_instance_draining: typing.Optional[bool] = pydantic.Field(None, description='Specify whether to enable Automated Draining for Spot Instances running Amazon ECS Services. For more information, see `Using Spot Instances <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html>`_. Default: false\n')
    topic_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='If ``AddAutoScalingGroupCapacityOptions.taskDrainTime`` is non-zero, then the ECS cluster creates an SNS Topic to as part of a system to drain instances of tasks when the instance is being shut down. If this property is provided, then this key will be used to encrypt the contents of that SNS Topic. See `SNS Data Encryption <https://docs.aws.amazon.com/sns/latest/dg/sns-data-encryption.html>`_ for more information. Default: The SNS Topic will not be encrypted.')
    _init_params: typing.ClassVar[list[str]] = ['auto_scaling_group', 'capacity_provider_name', 'enable_managed_draining', 'enable_managed_scaling', 'enable_managed_termination_protection', 'instance_warmup_period', 'maximum_scaling_step_size', 'minimum_scaling_step_size', 'target_capacity_percent', 'can_containers_access_instance_role', 'machine_image_type', 'spot_instance_draining', 'topic_encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AsgCapacityProvider'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.AsgCapacityProviderDefConfig] = pydantic.Field(None)


class AsgCapacityProviderDefConfig(pydantic.BaseModel):
    auto_scaling_group_config: typing.Optional[models.aws_autoscaling.AutoScalingGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.Cluster
class ClusterDef(BaseConstruct):
    capacity: typing.Union[models.aws_ecs.AddCapacityOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ec2 capacity to add to the cluster. Default: - no EC2 capacity will be added, you can use ``addCapacity`` to add capacity later.\n')
    cluster_name: typing.Optional[str] = pydantic.Field(None, description='The name for the cluster. Default: CloudFormation-generated name\n')
    container_insights: typing.Optional[bool] = pydantic.Field(None, description='If true CloudWatch Container Insights will be enabled for the cluster. Default: - Container Insights will be disabled for this cluster.\n')
    default_cloud_map_namespace: typing.Union[models.aws_ecs.CloudMapNamespaceOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The service discovery namespace created in this cluster. Default: - no service discovery namespace created, you can use ``addDefaultCloudMapNamespace`` to add a default service discovery namespace later.\n')
    enable_fargate_capacity_providers: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Fargate Capacity Providers. Default: false\n')
    execute_command_configuration: typing.Union[models.aws_ecs.ExecuteCommandConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The execute command configuration for the cluster. Default: - no configuration will be provided.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where your ECS instances will be running or your ENIs will be deployed. Default: - creates a new VPC with two AZs')
    _init_params: typing.ClassVar[list[str]] = ['capacity', 'cluster_name', 'container_insights', 'default_cloud_map_namespace', 'enable_fargate_capacity_providers', 'execute_command_configuration', 'vpc']
    _method_names: typing.ClassVar[list[str]] = ['add_asg_capacity_provider', 'add_capacity', 'add_default_capacity_provider_strategy', 'add_default_cloud_map_namespace', 'apply_removal_policy', 'arn_for_tasks', 'enable_fargate_capacity_providers', 'grant_task_protection', 'metric', 'metric_cpu_reservation', 'metric_cpu_utilization', 'metric_memory_reservation', 'metric_memory_utilization']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_cluster_arn', 'from_cluster_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Cluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_cluster_arn', 'from_cluster_attributes']
    ...


    from_cluster_arn: typing.Optional[models.aws_ecs.ClusterDefFromClusterArnParams] = pydantic.Field(None, description='Import an existing cluster to the stack from the cluster ARN.\nThis does not provide access to the vpc, hasEc2Capacity, or connections -\nuse the ``fromClusterAttributes`` method to access those properties.')
    from_cluster_attributes: typing.Optional[models.aws_ecs.ClusterDefFromClusterAttributesParams] = pydantic.Field(None, description='Import an existing cluster to the stack from its attributes.')
    resource_config: typing.Optional[models.aws_ecs.ClusterDefConfig] = pydantic.Field(None)


class ClusterDefConfig(pydantic.BaseModel):
    add_asg_capacity_provider: typing.Optional[list[models.aws_ecs.ClusterDefAddAsgCapacityProviderParams]] = pydantic.Field(None, description='This method adds an Auto Scaling Group Capacity Provider to a cluster.')
    add_capacity: typing.Optional[list[models.aws_ecs.ClusterDefAddCapacityParams]] = pydantic.Field(None, description='It is highly recommended to use ``Cluster.addAsgCapacityProvider`` instead of this method.\nThis method adds compute capacity to a cluster by creating an AutoScalingGroup with the specified options.\n\nReturns the AutoScalingGroup so you can add autoscaling settings to it.')
    add_default_capacity_provider_strategy: typing.Optional[list[models.aws_ecs.ClusterDefAddDefaultCapacityProviderStrategyParams]] = pydantic.Field(None, description='Add default capacity provider strategy for this cluster.')
    add_default_cloud_map_namespace: typing.Optional[list[models.aws_ecs.ClusterDefAddDefaultCloudMapNamespaceParams]] = pydantic.Field(None, description='Add an AWS Cloud Map DNS namespace for this cluster.\nNOTE: HttpNamespaces are supported only for use cases involving Service Connect. For use cases involving both Service-\nDiscovery and Service Connect, customers should manage the HttpNamespace outside of the Cluster.addDefaultCloudMapNamespace method.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    arn_for_tasks: typing.Optional[list[models.aws_ecs.ClusterDefArnForTasksParams]] = pydantic.Field(None, description='Returns an ARN that represents all tasks within the cluster that match the task pattern specified.\nTo represent all tasks, specify ``"*"``.')
    enable_fargate_capacity_providers: typing.Optional[bool] = pydantic.Field(None, description='Enable the Fargate capacity providers for this cluster.')
    grant_task_protection: typing.Optional[list[models.aws_ecs.ClusterDefGrantTaskProtectionParams]] = pydantic.Field(None, description="Grants an ECS Task Protection API permission to the specified grantee.\nThis method provides a streamlined way to assign the 'ecs:UpdateTaskProtection'\npermission, enabling the grantee to manage task protection in the ECS cluster.")
    metric: typing.Optional[list[models.aws_ecs.ClusterDefMetricParams]] = pydantic.Field(None, description='This method returns the specifed CloudWatch metric for this cluster.')
    metric_cpu_reservation: typing.Optional[list[models.aws_ecs.ClusterDefMetricCpuReservationParams]] = pydantic.Field(None, description='This method returns the CloudWatch metric for this clusters CPU reservation.')
    metric_cpu_utilization: typing.Optional[list[models.aws_ecs.ClusterDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='This method returns the CloudWatch metric for this clusters CPU utilization.')
    metric_memory_reservation: typing.Optional[list[models.aws_ecs.ClusterDefMetricMemoryReservationParams]] = pydantic.Field(None, description='This method returns the CloudWatch metric for this clusters memory reservation.')
    metric_memory_utilization: typing.Optional[list[models.aws_ecs.ClusterDefMetricMemoryUtilizationParams]] = pydantic.Field(None, description='This method returns the CloudWatch metric for this clusters memory utilization.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)

class ClusterDefAddAsgCapacityProviderParams(pydantic.BaseModel):
    provider: models.aws_ecs.AsgCapacityProviderDef = pydantic.Field(..., description='the capacity provider to add to this cluster.\n')
    can_containers_access_instance_role: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the containers can access the container instance role. Default: false\n')
    machine_image_type: typing.Optional[aws_cdk.aws_ecs.MachineImageType] = pydantic.Field(None, description='What type of machine image this is. Depending on the setting, different UserData will automatically be added to the ``AutoScalingGroup`` to configure it properly for use with ECS. If you create an ``AutoScalingGroup`` yourself and are adding it via ``addAutoScalingGroup()``, you must specify this value. If you are adding an ``autoScalingGroup`` via ``addCapacity``, this value will be determined from the ``machineImage`` you pass. Default: - Automatically determined from ``machineImage``, if available, otherwise ``MachineImageType.AMAZON_LINUX_2``.\n')
    spot_instance_draining: typing.Optional[bool] = pydantic.Field(None, description='Specify whether to enable Automated Draining for Spot Instances running Amazon ECS Services. For more information, see `Using Spot Instances <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html>`_. Default: false\n')
    topic_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='If ``AddAutoScalingGroupCapacityOptions.taskDrainTime`` is non-zero, then the ECS cluster creates an SNS Topic to as part of a system to drain instances of tasks when the instance is being shut down. If this property is provided, then this key will be used to encrypt the contents of that SNS Topic. See `SNS Data Encryption <https://docs.aws.amazon.com/sns/latest/dg/sns-data-encryption.html>`_ for more information. Default: The SNS Topic will not be encrypted.')
    ...

class ClusterDefAddCapacityParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='The EC2 instance type to use when launching instances into the AutoScalingGroup.\n')
    machine_image: typing.Optional[typing.Union[models.aws_ec2.AmazonLinux2022ImageSsmParameterDef, models.aws_ec2.AmazonLinux2023ImageSsmParameterDef, models.aws_ec2.AmazonLinux2ImageSsmParameterDef, models.aws_ec2.AmazonLinuxImageDef, models.aws_ec2.AmazonLinuxImageSsmParameterBaseDef, models.aws_ec2.GenericLinuxImageDef, models.aws_ec2.GenericSSMParameterImageDef, models.aws_ec2.GenericWindowsImageDef, models.aws_ec2.LookupMachineImageDef, models.aws_ec2.NatInstanceImageDef, models.aws_ec2.ResolveSsmParameterAtLaunchImageDef, models.aws_ec2.WindowsImageDef, models.aws_ecs.BottleRocketImageDef, models.aws_ecs.EcsOptimizedImageDef, models.aws_eks.EksOptimizedImageDef]] = pydantic.Field(None, description='The ECS-optimized AMI variant to use. The default is to use an ECS-optimized AMI of Amazon Linux 2 which is automatically updated to the latest version on every deployment. This will replace the instances in the AutoScalingGroup. Make sure you have not disabled task draining, to avoid downtime when the AMI updates. To use an image that does not update on every deployment, pass:: const machineImage = ecs.EcsOptimizedImage.amazonLinux2(ecs.AmiHardwareType.STANDARD, { cachedInContext: true, }); For more information, see `Amazon ECS-optimized AMIs <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_. You must define either ``machineImage`` or ``machineImageType``, not both. Default: - Automatically updated, ECS-optimized Amazon Linux 2\n')
    can_containers_access_instance_role: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the containers can access the container instance role. Default: false\n')
    machine_image_type: typing.Optional[aws_cdk.aws_ecs.MachineImageType] = pydantic.Field(None, description='What type of machine image this is. Depending on the setting, different UserData will automatically be added to the ``AutoScalingGroup`` to configure it properly for use with ECS. If you create an ``AutoScalingGroup`` yourself and are adding it via ``addAutoScalingGroup()``, you must specify this value. If you are adding an ``autoScalingGroup`` via ``addCapacity``, this value will be determined from the ``machineImage`` you pass. Default: - Automatically determined from ``machineImage``, if available, otherwise ``MachineImageType.AMAZON_LINUX_2``.\n')
    spot_instance_draining: typing.Optional[bool] = pydantic.Field(None, description='Specify whether to enable Automated Draining for Spot Instances running Amazon ECS Services. For more information, see `Using Spot Instances <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html>`_. Default: false\n')
    topic_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='If ``AddAutoScalingGroupCapacityOptions.taskDrainTime`` is non-zero, then the ECS cluster creates an SNS Topic to as part of a system to drain instances of tasks when the instance is being shut down. If this property is provided, then this key will be used to encrypt the contents of that SNS Topic. See `SNS Data Encryption <https://docs.aws.amazon.com/sns/latest/dg/sns-data-encryption.html>`_ for more information. Default: The SNS Topic will not be encrypted.\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether the instances can initiate connections to anywhere by default. Default: true\n')
    associate_public_ip_address: typing.Optional[bool] = pydantic.Field(None, description='Whether instances in the Auto Scaling Group should have public IP addresses associated with them. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: - Use subnet setting.\n')
    auto_scaling_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the Auto Scaling group. This name must be unique per Region per account. Default: - Auto generated by CloudFormation\n')
    block_devices: typing.Optional[typing.Sequence[typing.Union[models.aws_autoscaling.BlockDeviceDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specifies how block devices are exposed to the instance. You can specify virtual devices and EBS volumes. Each instance that is launched has an associated root device volume, either an Amazon EBS volume or an instance store volume. You can use block device mappings to specify additional EBS volumes or instance store volumes to attach to an instance when it is launched. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: - Uses the block device mapping of the AMI\n')
    capacity_rebalance: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether Capacity Rebalancing is enabled. When you turn on Capacity Rebalancing, Amazon EC2 Auto Scaling attempts to launch a Spot Instance whenever Amazon EC2 notifies that a Spot Instance is at an elevated risk of interruption. After launching a new instance, it then terminates an old instance. Default: false\n')
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Default scaling cooldown for this AutoScalingGroup. Default: Duration.minutes(5)\n')
    default_instance_warmup: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time, in seconds, until a newly launched instance can contribute to the Amazon CloudWatch metrics. This delay lets an instance finish initializing before Amazon EC2 Auto Scaling aggregates instance metrics, resulting in more reliable usage data. Set this value equal to the amount of time that it takes for resource consumption to become stable after an instance reaches the InService state. To optimize the performance of scaling policies that scale continuously, such as target tracking and step scaling policies, we strongly recommend that you enable the default instance warmup, even if its value is set to 0 seconds Default instance warmup will not be added if no value is specified Default: None\n')
    desired_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Initial amount of instances in the fleet. If this is set to a number, every deployment will reset the amount of instances to this number. It is recommended to leave this value blank. Default: minCapacity, and leave unchanged during deployment\n')
    group_metrics: typing.Optional[typing.Sequence[models.aws_autoscaling.GroupMetricsDef]] = pydantic.Field(None, description='Enable monitoring for group metrics, these metrics describe the group rather than any of its instances. To report all group metrics use ``GroupMetrics.all()`` Group metrics are reported in a granularity of 1 minute at no additional charge. Default: - no group metrics will be reported\n')
    health_check: typing.Optional[models.aws_autoscaling.HealthCheckDef] = pydantic.Field(None, description='Configuration for health checks. Default: - HealthCheck.ec2 with no grace period\n')
    ignore_unmodified_size_properties: typing.Optional[bool] = pydantic.Field(None, description="If the ASG has scheduled actions, don't reset unchanged group sizes. Only used if the ASG has scheduled actions (which may scale your ASG up or down regardless of cdk deployments). If true, the size of the group will only be reset if it has been changed in the CDK app. If false, the sizes will always be changed back to what they were in the CDK app on deployment. Default: true\n")
    instance_monitoring: typing.Optional[aws_cdk.aws_autoscaling.Monitoring] = pydantic.Field(None, description='Controls whether instances in this group are launched with detailed or basic monitoring. When detailed monitoring is enabled, Amazon CloudWatch generates metrics every minute and your account is charged a fee. When you disable detailed monitoring, CloudWatch generates metrics every 5 minutes. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: - Monitoring.DETAILED\n')
    key_name: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Name of SSH keypair to grant access to instances. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified You can either specify ``keyPair`` or ``keyName``, not both. Default: - No SSH access will be possible.\n')
    key_pair: typing.Optional[typing.Union[models.aws_ec2.KeyPairDef]] = pydantic.Field(None, description='The SSH keypair to grant access to the instance. Feature flag ``AUTOSCALING_GENERATE_LAUNCH_TEMPLATE`` must be enabled to use this property. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified. You can either specify ``keyPair`` or ``keyName``, not both. Default: - No SSH access will be possible.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of instances in the fleet. Default: desiredCapacity\n')
    max_instance_lifetime: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time that an instance can be in service. The maximum duration applies to all current and future instances in the group. As an instance approaches its maximum duration, it is terminated and replaced, and cannot be used again. You must specify a value of at least 604,800 seconds (7 days). To clear a previously set value, leave this property undefined. Default: none\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum number of instances in the fleet. Default: 1\n')
    new_instances_protected_from_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Whether newly-launched instances are protected from termination by Amazon EC2 Auto Scaling when scaling in. By default, Auto Scaling can terminate an instance at any time after launch when scaling in an Auto Scaling Group, subject to the group's termination policy. However, you may wish to protect newly-launched instances from being scaled in if they are going to run critical applications that should not be prematurely terminated. This flag must be enabled if the Auto Scaling Group will be associated with an ECS Capacity Provider with managed termination protection. Default: false\n")
    notifications: typing.Optional[typing.Sequence[typing.Union[models.aws_autoscaling.NotificationConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configure autoscaling group to send notifications about fleet changes to an SNS topic(s). Default: - No fleet change notifications will be sent.\n')
    signals: typing.Optional[models.aws_autoscaling.SignalsDef] = pydantic.Field(None, description='Configure waiting for signals during deployment. Use this to pause the CloudFormation deployment to wait for the instances in the AutoScalingGroup to report successful startup during creation and updates. The UserData script needs to invoke ``cfn-signal`` with a success or failure code after it is done setting up the instance. Without waiting for signals, the CloudFormation deployment will proceed as soon as the AutoScalingGroup has been created or updated but before the instances in the group have been started. For example, to have instances wait for an Elastic Load Balancing health check before they signal success, add a health-check verification by using the cfn-init helper script. For an example, see the verify_instance_health command in the Auto Scaling rolling updates sample template: https://github.com/awslabs/aws-cloudformation-templates/blob/master/aws/services/AutoScaling/AutoScalingRollingUpdates.yaml Default: - Do not wait for signals\n')
    spot_price: typing.Optional[str] = pydantic.Field(None, description='The maximum hourly price (in USD) to be paid for any Spot Instance launched to fulfill the request. Spot Instances are launched when the price you specify exceeds the current Spot market price. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: none\n')
    ssm_session_permissions: typing.Optional[bool] = pydantic.Field(None, description='Add SSM session permissions to the instance role. Setting this to ``true`` adds the necessary permissions to connect to the instance using SSM Session Manager. You can do this from the AWS Console. NOTE: Setting this flag to ``true`` may not be enough by itself. You must also use an AMI that comes with the SSM Agent, or install the SSM Agent yourself. See `Working with SSM Agent <https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html>`_ in the SSM Developer Guide. Default: false\n')
    termination_policies: typing.Optional[typing.Sequence[aws_cdk.aws_autoscaling.TerminationPolicy]] = pydantic.Field(None, description='A policy or a list of policies that are used to select the instances to terminate. The policies are executed in the order that you list them. Default: - ``TerminationPolicy.DEFAULT``\n')
    termination_policy_custom_lambda_function_arn: typing.Optional[str] = pydantic.Field(None, description='A lambda function Arn that can be used as a custom termination policy to select the instances to terminate. This property must be specified if the TerminationPolicy.CUSTOM_LAMBDA_FUNCTION is used. Default: - No lambda function Arn will be supplied\n')
    update_policy: typing.Optional[models.aws_autoscaling.UpdatePolicyDef] = pydantic.Field(None, description="What to do when an AutoScalingGroup's instance configuration is changed. This is applied when any of the settings on the ASG are changed that affect how the instances should be created (VPC, instance type, startup scripts, etc.). It indicates how the existing instances should be replaced with new instances matching the new config. By default, nothing is done and only new instances are launched with the new config. Default: - ``UpdatePolicy.rollingUpdate()`` if using ``init``, ``UpdatePolicy.none()`` otherwise\n")
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place instances within the VPC. Default: - All Private subnets.')
    return_config: typing.Optional[list[models.aws_autoscaling.AutoScalingGroupDefConfig]] = pydantic.Field(None)
    ...

class ClusterDefAddDefaultCapacityProviderStrategyParams(pydantic.BaseModel):
    default_capacity_provider_strategy: typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]] = pydantic.Field(..., description="cluster default capacity provider strategy. This takes the form of a list of CapacityProviderStrategy objects. For example [ { capacityProvider: 'FARGATE', base: 10, weight: 50 } ]")
    ...

class ClusterDefAddDefaultCloudMapNamespaceParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='The name of the namespace, such as example.com.\n')
    type: typing.Optional[aws_cdk.aws_servicediscovery.NamespaceType] = pydantic.Field(None, description='The type of CloudMap Namespace to create. Default: PrivateDns\n')
    use_for_service_connect: typing.Optional[bool] = pydantic.Field(None, description='This property specifies whether to set the provided namespace as the service connect default in the cluster properties. Default: false\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC to associate the namespace with. This property is required for private DNS namespaces. Default: VPC of the cluster for Private DNS Namespace, otherwise none')
    return_config: typing.Optional[list[models._interface_methods.AwsServicediscoveryINamespaceDefConfig]] = pydantic.Field(None)
    ...

class ClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ClusterDefArnForTasksParams(pydantic.BaseModel):
    key_pattern: str = pydantic.Field(..., description='Task id pattern.')
    ...

class ClusterDefFromClusterArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    cluster_arn: str = pydantic.Field(..., description='-')
    ...

class ClusterDefFromClusterAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    cluster_name: str = pydantic.Field(..., description='The name of the cluster.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC associated with the cluster.\n')
    autoscaling_group: typing.Optional[typing.Union[models.aws_autoscaling.AutoScalingGroupDef]] = pydantic.Field(None, description='Autoscaling group added to the cluster if capacity is added. Default: - No default autoscaling group\n')
    cluster_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) that identifies the cluster. Default: Derived from clusterName\n')
    default_cloud_map_namespace: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='The AWS Cloud Map namespace to associate with the cluster. Default: - No default namespace\n')
    execute_command_configuration: typing.Union[models.aws_ecs.ExecuteCommandConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The execute command configuration for the cluster. Default: - none.\n')
    has_ec2_capacity: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the cluster has EC2 instance capacity. Default: true\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups associated with the container instances registered to the cluster. Default: - no security groups')
    ...

class ClusterDefGrantTaskProtectionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='The entity (e.g., IAM role or user) to grant the permissions to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class ClusterDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ClusterDefMetricCpuReservationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ClusterDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ClusterDefMetricMemoryReservationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ClusterDefMetricMemoryUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.ContainerDefinition
class ContainerDefinitionDef(BaseConstruct):
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the task definition that includes this container definition. [disable-awslint:ref-via-interface]\n')
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    _init_params: typing.ClassVar[list[str]] = ['task_definition', 'image', 'command', 'container_name', 'cpu', 'credential_specs', 'disable_networking', 'dns_search_domains', 'dns_servers', 'docker_labels', 'docker_security_options', 'entry_point', 'environment', 'environment_files', 'essential', 'extra_hosts', 'gpu_count', 'health_check', 'hostname', 'inference_accelerator_resources', 'interactive', 'linux_parameters', 'logging', 'memory_limit_mib', 'memory_reservation_mib', 'port_mappings', 'privileged', 'pseudo_terminal', 'readonly_root_filesystem', 'secrets', 'start_timeout', 'stop_timeout', 'system_controls', 'ulimits', 'user', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = ['add_container_dependencies', 'add_docker_label', 'add_environment', 'add_inference_accelerator_resource', 'add_link', 'add_mount_points', 'add_port_mappings', 'add_scratch', 'add_secret', 'add_to_execution_policy', 'add_ulimits', 'add_volumes_from', 'find_port_mapping', 'find_port_mapping_by_name', 'render_container_definition']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ContainerDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ContainerDefinitionDefConfig] = pydantic.Field(None)


class ContainerDefinitionDefConfig(pydantic.BaseModel):
    add_container_dependencies: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddContainerDependenciesParams]] = pydantic.Field(None, description='This method adds one or more container dependencies to the container.')
    add_docker_label: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddDockerLabelParams]] = pydantic.Field(None, description='This method adds a Docker label to the container.')
    add_environment: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddEnvironmentParams]] = pydantic.Field(None, description='This method adds an environment variable to the container.')
    add_inference_accelerator_resource: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddInferenceAcceleratorResourceParams]] = pydantic.Field(None, description='This method adds one or more resources to the container.')
    add_link: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddLinkParams]] = pydantic.Field(None, description='This method adds a link which allows containers to communicate with each other without the need for port mappings.\nThis parameter is only supported if the task definition is using the bridge network mode.\nWarning: The --link flag is a legacy feature of Docker. It may eventually be removed.')
    add_mount_points: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddMountPointsParams]] = pydantic.Field(None, description='This method adds one or more mount points for data volumes to the container.')
    add_port_mappings: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddPortMappingsParams]] = pydantic.Field(None, description='This method adds one or more port mappings to the container.')
    add_scratch: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddScratchParams]] = pydantic.Field(None, description='This method mounts temporary disk space to the container.\nThis adds the correct container mountPoint and task definition volume.')
    add_secret: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddSecretParams]] = pydantic.Field(None, description='This method adds a secret as environment variable to the container.')
    add_to_execution_policy: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddToExecutionPolicyParams]] = pydantic.Field(None, description='This method adds the specified statement to the IAM task execution policy in the task definition.')
    add_ulimits: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddUlimitsParams]] = pydantic.Field(None, description='This method adds one or more ulimits to the container.')
    add_volumes_from: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefAddVolumesFromParams]] = pydantic.Field(None, description='This method adds one or more volumes to the container.')
    find_port_mapping: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefFindPortMappingParams]] = pydantic.Field(None, description='Returns the host port for the requested container port if it exists.')
    find_port_mapping_by_name: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefFindPortMappingByNameParams]] = pydantic.Field(None, description='Returns the port mapping with the given name, if it exists.')
    render_container_definition: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefRenderContainerDefinitionParams]] = pydantic.Field(None, description='Render this container definition to a CloudFormation object.')

class ContainerDefinitionDefAddContainerDependenciesParams(pydantic.BaseModel):
    container_dependencies: list[models.aws_ecs.ContainerDependencyDef] = pydantic.Field(...)
    ...

class ContainerDefinitionDefAddDockerLabelParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    value: str = pydantic.Field(..., description='-')
    ...

class ContainerDefinitionDefAddEnvironmentParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    value: str = pydantic.Field(..., description='-')
    ...

class ContainerDefinitionDefAddInferenceAcceleratorResourceParams(pydantic.BaseModel):
    inference_accelerator_resources: list[str] = pydantic.Field(...)
    ...

class ContainerDefinitionDefAddLinkParams(pydantic.BaseModel):
    container: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-\n')
    alias: typing.Optional[str] = pydantic.Field(None, description='-')
    ...

class ContainerDefinitionDefAddMountPointsParams(pydantic.BaseModel):
    mount_points: list[models.aws_ecs.MountPointDef] = pydantic.Field(...)
    ...

class ContainerDefinitionDefAddPortMappingsParams(pydantic.BaseModel):
    port_mappings: list[models.aws_ecs.PortMappingDef] = pydantic.Field(...)
    ...

class ContainerDefinitionDefAddScratchParams(pydantic.BaseModel):
    container_path: str = pydantic.Field(..., description='The path on the container to mount the scratch volume at.\n')
    name: str = pydantic.Field(..., description='The name of the scratch volume to mount. Must be a volume name referenced in the name parameter of task definition volume.\n')
    read_only: bool = pydantic.Field(..., description='Specifies whether to give the container read-only access to the scratch volume. If this value is true, the container has read-only access to the scratch volume. If this value is false, then the container can write to the scratch volume.\n')
    source_path: str = pydantic.Field(..., description='')
    ...

class ContainerDefinitionDefAddSecretParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    secret: models.aws_ecs.SecretDef = pydantic.Field(..., description='-')
    ...

class ContainerDefinitionDefAddToExecutionPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class ContainerDefinitionDefAddUlimitsParams(pydantic.BaseModel):
    ulimits: list[models.aws_ecs.UlimitDef] = pydantic.Field(...)
    ...

class ContainerDefinitionDefAddVolumesFromParams(pydantic.BaseModel):
    volumes_from: list[models.aws_ecs.VolumeFromDef] = pydantic.Field(...)
    ...

class ContainerDefinitionDefFindPortMappingParams(pydantic.BaseModel):
    container_port: typing.Union[int, float] = pydantic.Field(..., description='-\n')
    protocol: aws_cdk.aws_ecs.Protocol = pydantic.Field(..., description='-')
    ...

class ContainerDefinitionDefFindPortMappingByNameParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-')
    ...

class ContainerDefinitionDefRenderContainerDefinitionParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.Ec2Service
class Ec2ServiceDef(BaseConstruct):
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task definition to use for tasks in the service. [disable-awslint:ref-via-interface]\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the task's elastic network interface receives a public IP address. If true, each task will receive a public IP address. This property is only used for tasks that use the awsvpc network mode. Default: false\n")
    daemon: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the service will use the daemon scheduling strategy. If true, the service scheduler deploys exactly one task on each container instance in your cluster. When you are using this strategy, do not specify a desired number of tasks or any task placement strategies. Default: false\n')
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='The placement constraints to use for tasks in the service. For more information, see `Amazon ECS Task Placement Constraints <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html>`_. Default: - No constraints.\n')
    placement_strategies: typing.Optional[typing.Sequence[models.aws_ecs.PlacementStrategyDef]] = pydantic.Field(None, description='The placement strategies to use for tasks in the service. For more information, see `Amazon ECS Task Placement Strategies <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html>`_. Default: - No strategies.\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to associate with the service. If you do not specify a security group, a new security group is created. This property is only used for tasks that use the awsvpc network mode. Default: - A new security group is created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets to associate with the service. This property is only used for tasks that use the awsvpc network mode. Default: - Public subnets if ``assignPublicIp`` is set, otherwise the first available one of Private, Isolated, Public, in that order.\n')
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined')
    _init_params: typing.ClassVar[list[str]] = ['task_definition', 'assign_public_ip', 'daemon', 'placement_constraints', 'placement_strategies', 'security_groups', 'vpc_subnets', 'cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations']
    _method_names: typing.ClassVar[list[str]] = ['add_placement_constraints', 'add_placement_strategies', 'add_volume', 'apply_removal_policy', 'associate_cloud_map_service', 'attach_to_application_target_group', 'attach_to_classic_lb', 'attach_to_network_target_group', 'auto_scale_task_count', 'enable_cloud_map', 'enable_deployment_alarms', 'enable_service_connect', 'load_balancer_target', 'metric', 'metric_cpu_utilization', 'metric_memory_utilization', 'register_load_balancer_targets']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_ec2_service_arn', 'from_ec2_service_attributes', 'from_service_arn_with_cluster']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Ec2Service'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_ec2_service_arn', 'from_ec2_service_attributes', 'from_service_arn_with_cluster']
    ...


    from_ec2_service_arn: typing.Optional[models.aws_ecs.Ec2ServiceDefFromEc2ServiceArnParams] = pydantic.Field(None, description='Imports from the specified service ARN.')
    from_ec2_service_attributes: typing.Optional[models.aws_ecs.Ec2ServiceDefFromEc2ServiceAttributesParams] = pydantic.Field(None, description='Imports from the specified service attributes.')
    from_service_arn_with_cluster: typing.Optional[models.aws_ecs.Ec2ServiceDefFromServiceArnWithClusterParams] = pydantic.Field(None, description='Import an existing ECS/Fargate Service using the service cluster format.\nThe format is the "new" format "arn:aws:ecs:region:aws_account_id:service/cluster-name/service-name".')
    resource_config: typing.Optional[models.aws_ecs.Ec2ServiceDefConfig] = pydantic.Field(None)


class Ec2ServiceDefConfig(pydantic.BaseModel):
    add_placement_constraints: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAddPlacementConstraintsParams]] = pydantic.Field(None, description='Adds one or more placement constraints to use for tasks in the service.\nFor more information, see\n`Amazon ECS Task Placement Constraints <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html>`_.')
    add_placement_strategies: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAddPlacementStrategiesParams]] = pydantic.Field(None, description='Adds one or more placement strategies to use for tasks in the service.\nFor more information, see\n`Amazon ECS Task Placement Strategies <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html>`_.')
    add_volume: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the Service.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    associate_cloud_map_service: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAssociateCloudMapServiceParams]] = pydantic.Field(None, description='Associates this service with a CloudMap service.')
    attach_to_application_target_group: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAttachToApplicationTargetGroupParams]] = pydantic.Field(None, description="This method is called to attach this service to an Application Load Balancer.\nDon't call this function directly. Instead, call ``listener.addTargets()``\nto add this service to a load balancer.")
    attach_to_classic_lb: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAttachToClassicLbParams]] = pydantic.Field(None, description="Registers the service as a target of a Classic Load Balancer (CLB).\nDon't call this. Call ``loadBalancer.addTarget()`` instead.")
    attach_to_network_target_group: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAttachToNetworkTargetGroupParams]] = pydantic.Field(None, description="This method is called to attach this service to a Network Load Balancer.\nDon't call this function directly. Instead, call ``listener.addTargets()``\nto add this service to a load balancer.")
    auto_scale_task_count: typing.Optional[list[models.aws_ecs.Ec2ServiceDefAutoScaleTaskCountParams]] = pydantic.Field(None, description='An attribute representing the minimum and maximum task count for an AutoScalingGroup.')
    enable_cloud_map: typing.Optional[list[models.aws_ecs.Ec2ServiceDefEnableCloudMapParams]] = pydantic.Field(None, description='Enable CloudMap service discovery for the service.')
    enable_deployment_alarms: typing.Optional[list[models.aws_ecs.Ec2ServiceDefEnableDeploymentAlarmsParams]] = pydantic.Field(None, description="Enable Deployment Alarms which take advantage of arbitrary alarms and configure them after service initialization.\nIf you have already enabled deployment alarms, this function can be used to tell ECS about additional alarms that\nshould interrupt a deployment.\n\nNew alarms specified in subsequent calls of this function will be appended to the existing list of alarms.\n\nThe same Alarm Behavior must be used on all deployment alarms. If you specify different AlarmBehavior values in\nmultiple calls to this function, or the Alarm Behavior used here doesn't match the one used in the service\nconstructor, an error will be thrown.\n\nIf the alarm's metric references the service, you cannot pass ``Alarm.alarmName`` here. That will cause a circular\ndependency between the service and its deployment alarm. See this package's README for options to alarm on service\nmetrics, and avoid this circular dependency.")
    enable_service_connect: typing.Optional[list[models.aws_ecs.Ec2ServiceDefEnableServiceConnectParams]] = pydantic.Field(None, description='Enable Service Connect on this service.')
    load_balancer_target: typing.Optional[list[models.aws_ecs.Ec2ServiceDefLoadBalancerTargetParams]] = pydantic.Field(None, description='Return a load balancing target for a specific container and port.\nUse this function to create a load balancer target if you want to load balance to\nanother container than the first essential container or the first mapped port on\nthe container.\n\nUse the return value of this function where you would normally use a load balancer\ntarget, instead of the ``Service`` object itself.')
    metric: typing.Optional[list[models.aws_ecs.Ec2ServiceDefMetricParams]] = pydantic.Field(None, description='This method returns the specified CloudWatch metric name for this service.')
    metric_cpu_utilization: typing.Optional[list[models.aws_ecs.Ec2ServiceDefMetricCpuUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's CPU utilization.")
    metric_memory_utilization: typing.Optional[list[models.aws_ecs.Ec2ServiceDefMetricMemoryUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's memory utilization.")
    register_load_balancer_targets: typing.Optional[list[models.aws_ecs.Ec2ServiceDefRegisterLoadBalancerTargetsParams]] = pydantic.Field(None, description='Use this function to create all load balancer targets to be registered in this service, add them to target groups, and attach target groups to listeners accordingly.\nAlternatively, you can use ``listener.addTargets()`` to create targets and add them to target groups.')
    cluster_config: typing.Optional[models._interface_methods.AwsEcsIClusterDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    task_definition_config: typing.Optional[models.aws_ecs.TaskDefinitionDefConfig] = pydantic.Field(None)

class Ec2ServiceDefAddPlacementConstraintsParams(pydantic.BaseModel):
    constraints: list[models.aws_ecs.PlacementConstraintDef] = pydantic.Field(...)
    ...

class Ec2ServiceDefAddPlacementStrategiesParams(pydantic.BaseModel):
    strategies: list[models.aws_ecs.PlacementStrategyDef] = pydantic.Field(...)
    ...

class Ec2ServiceDefAddVolumeParams(pydantic.BaseModel):
    volume: models.aws_ecs.ServiceManagedVolumeDef = pydantic.Field(..., description='-')
    ...

class Ec2ServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class Ec2ServiceDefAssociateCloudMapServiceParams(pydantic.BaseModel):
    service: typing.Union[models.aws_servicediscovery.ServiceDef] = pydantic.Field(..., description='The cloudmap service to register with.\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container")
    ...

class Ec2ServiceDefAttachToApplicationTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef] = pydantic.Field(..., description='-')
    ...

class Ec2ServiceDefAttachToClassicLbParams(pydantic.BaseModel):
    load_balancer: models.aws_elasticloadbalancing.LoadBalancerDef = pydantic.Field(..., description='-')
    ...

class Ec2ServiceDefAttachToNetworkTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.NetworkTargetGroupDef] = pydantic.Field(..., description='-')
    ...

class Ec2ServiceDefAutoScaleTaskCountParams(pydantic.BaseModel):
    max_capacity: typing.Union[int, float] = pydantic.Field(..., description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1')
    return_config: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefConfig]] = pydantic.Field(None)
    ...

class Ec2ServiceDefEnableCloudMapParams(pydantic.BaseModel):
    cloud_map_namespace: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='The service discovery namespace for the Cloud Map service to attach to the ECS service. Default: - the defaultCloudMapNamespace associated to the cluster\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container\n")
    dns_record_type: typing.Optional[aws_cdk.aws_servicediscovery.DnsRecordType] = pydantic.Field(None, description='The DNS record type that you want AWS Cloud Map to create. The supported record types are A or SRV. Default: - DnsRecordType.A if TaskDefinition.networkMode = AWS_VPC, otherwise DnsRecordType.SRV\n')
    dns_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time that you want DNS resolvers to cache the settings for this record. Default: Duration.minutes(1)\n')
    failure_threshold: typing.Union[int, float, None] = pydantic.Field(None, description='The number of 30-second intervals that you want Cloud Map to wait after receiving an UpdateInstanceCustomHealthStatus request before it changes the health status of a service instance. NOTE: This is used for HealthCheckCustomConfig\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the Cloud Map service to attach to the ECS service. Default: CloudFormation-generated name\n')
    return_config: typing.Optional[list[models.aws_servicediscovery.ServiceDefConfig]] = pydantic.Field(None)
    ...

class Ec2ServiceDefEnableDeploymentAlarmsParams(pydantic.BaseModel):
    alarm_names: typing.Sequence[str] = pydantic.Field(..., description='-\n')
    behavior: typing.Optional[aws_cdk.aws_ecs.AlarmBehavior] = pydantic.Field(None, description='Default rollback on alarm. Default: AlarmBehavior.ROLLBACK_ON_ALARM')
    ...

class Ec2ServiceDefEnableServiceConnectParams(pydantic.BaseModel):
    log_driver: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log driver configuration to use for the Service Connect agent logs. Default: - none\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description='The cloudmap namespace to register this service into. Default: the cloudmap namespace specified on the cluster.\n')
    services: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.ServiceConnectServiceDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of Services, including a port mapping, terse client alias, and optional intermediate DNS name. This property may be left blank if the current ECS service does not need to advertise any ports via Service Connect. Default: none')
    ...

class Ec2ServiceDefFromEc2ServiceArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    ec2_service_arn: str = pydantic.Field(..., description='-')
    ...

class Ec2ServiceDefFromEc2ServiceAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    cluster: typing.Union[models.aws_ecs.ClusterDef] = pydantic.Field(..., description='The cluster that hosts the service.\n')
    service_arn: typing.Optional[str] = pydantic.Field(None, description='The service ARN. Default: - either this, or ``serviceName``, is required\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - either this, or ``serviceArn``, is required')
    ...

class Ec2ServiceDefFromServiceArnWithClusterParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    service_arn: str = pydantic.Field(..., description='-\n\n:see: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-account-settings.html#ecs-resource-ids\n')
    ...

class Ec2ServiceDefLoadBalancerTargetParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='The name of the container.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number of the container. Only applicable when using application/network load balancers. Default: - Container port of the first added port mapping.\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Only applicable when using application load balancers. Default: Protocol.TCP\n\nExample::\n\n    # listener: elbv2.ApplicationListener\n    # service: ecs.BaseService\n\n    listener.add_targets("ECS",\n        port=80,\n        targets=[service.load_balancer_target(\n            container_name="MyContainer",\n            container_port=1234\n        )]\n    )\n')
    return_config: typing.Optional[list[models._interface_methods.AwsEcsIEcsLoadBalancerTargetDefConfig]] = pydantic.Field(None)
    ...

class Ec2ServiceDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class Ec2ServiceDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class Ec2ServiceDefMetricMemoryUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class Ec2ServiceDefRegisterLoadBalancerTargetsParams(pydantic.BaseModel):
    targets: list[models.aws_ecs.EcsTargetDef] = pydantic.Field(...)
    ...


#  autogenerated from aws_cdk.aws_ecs.Ec2TaskDefinition
class Ec2TaskDefinitionDef(BaseConstruct):
    inference_accelerators: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.InferenceAcceleratorDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The inference accelerators to use for the containers in the task. Not supported in Fargate. Default: - No inference accelerators.\n')
    ipc_mode: typing.Optional[aws_cdk.aws_ecs.IpcMode] = pydantic.Field(None, description='The IPC resource namespace to use for the containers in the task. Not supported in Fargate and Windows containers. Default: - IpcMode used by the task is not specified\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The Docker networking mode to use for the containers in the task. The valid values are NONE, BRIDGE, AWS_VPC, and HOST. Default: - NetworkMode.BRIDGE for EC2 tasks, AWS_VPC for Fargate tasks.\n')
    pid_mode: typing.Optional[aws_cdk.aws_ecs.PidMode] = pydantic.Field(None, description='The process namespace to use for the containers in the task. Not supported in Windows containers. Default: - PidMode used by the task is not specified\n')
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='An array of placement constraint objects to use for the task. You can specify a maximum of 10 constraints per task (this limit includes constraints in the task definition and those specified at run time). Default: - No placement constraints.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.')
    _init_params: typing.ClassVar[list[str]] = ['inference_accelerators', 'ipc_mode', 'network_mode', 'pid_mode', 'placement_constraints', 'execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['add_container', 'add_extension', 'add_firelens_log_router', 'add_inference_accelerator', 'add_placement_constraint', 'add_to_execution_role_policy', 'add_to_task_role_policy', 'add_volume', 'apply_removal_policy', 'find_container', 'find_port_mapping_by_name', 'grant_run', 'obtain_execution_role']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_ec2_task_definition_arn', 'from_ec2_task_definition_attributes', 'from_task_definition_arn', 'from_task_definition_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Ec2TaskDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_ec2_task_definition_arn', 'from_ec2_task_definition_attributes', 'from_task_definition_arn', 'from_task_definition_attributes']
    ...


    from_ec2_task_definition_arn: typing.Optional[models.aws_ecs.Ec2TaskDefinitionDefFromEc2TaskDefinitionArnParams] = pydantic.Field(None, description='Imports a task definition from the specified task definition ARN.')
    from_ec2_task_definition_attributes: typing.Optional[models.aws_ecs.Ec2TaskDefinitionDefFromEc2TaskDefinitionAttributesParams] = pydantic.Field(None, description='Imports an existing Ec2 task definition from its attributes.')
    from_task_definition_arn: typing.Optional[models.aws_ecs.Ec2TaskDefinitionDefFromTaskDefinitionArnParams] = pydantic.Field(None, description='Imports a task definition from the specified task definition ARN.\nThe task will have a compatibility of EC2+Fargate.')
    from_task_definition_attributes: typing.Optional[models.aws_ecs.Ec2TaskDefinitionDefFromTaskDefinitionAttributesParams] = pydantic.Field(None, description='Create a task definition from a task definition reference.')
    resource_config: typing.Optional[models.aws_ecs.Ec2TaskDefinitionDefConfig] = pydantic.Field(None)


class Ec2TaskDefinitionDefConfig(pydantic.BaseModel):
    add_container: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddContainerParams]] = pydantic.Field(None, description='Tasks running in AWSVPC networking mode requires an additional environment variable for the region to be sourced.\nThis override adds in the additional environment variable as required')
    add_extension: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddExtensionParams]] = pydantic.Field(None, description='Adds the specified extension to the task definition.\nExtension can be used to apply a packaged modification to\na task definition.')
    add_firelens_log_router: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddFirelensLogRouterParams]] = pydantic.Field(None, description='Adds a firelens log router to the task definition.')
    add_inference_accelerator: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddInferenceAcceleratorParams]] = pydantic.Field(None, description='Adds an inference accelerator to the task definition.')
    add_placement_constraint: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddPlacementConstraintParams]] = pydantic.Field(None, description='Adds the specified placement constraint to the task definition.')
    add_to_execution_role_policy: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddToExecutionRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task execution IAM role.')
    add_to_task_role_policy: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddToTaskRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task IAM role.')
    add_volume: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the task definition.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    find_container: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefFindContainerParams]] = pydantic.Field(None, description='Returns the container that match the provided containerName.')
    find_port_mapping_by_name: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefFindPortMappingByNameParams]] = pydantic.Field(None, description='Determine the existing port mapping for the provided name.')
    grant_run: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefGrantRunParams]] = pydantic.Field(None, description='Grants permissions to run this task definition.\nThis will grant the following permissions:\n\n- ecs:RunTask\n- iam:PassRole')
    obtain_execution_role: typing.Optional[list[models.aws_ecs.Ec2TaskDefinitionDefObtainExecutionRoleParams]] = pydantic.Field(None, description="Creates the task execution IAM role if it doesn't already exist.")
    task_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class Ec2TaskDefinitionDefAddContainerParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefConfig]] = pydantic.Field(None)
    ...

class Ec2TaskDefinitionDefAddExtensionParams(pydantic.BaseModel):
    extension: models.UnsupportedResource = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefAddFirelensLogRouterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    firelens_config: typing.Union[models.aws_ecs.FirelensConfigDef, dict[str, typing.Any]] = pydantic.Field(..., description='Firelens configuration.\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefConfig]] = pydantic.Field(None)
    ...

class Ec2TaskDefinitionDefAddInferenceAcceleratorParams(pydantic.BaseModel):
    device_name: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator device name. Default: - empty\n')
    device_type: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator type to use. The allowed values are: eia2.medium, eia2.large and eia2.xlarge. Default: - empty')
    ...

class Ec2TaskDefinitionDefAddPlacementConstraintParams(pydantic.BaseModel):
    constraint: models.aws_ecs.PlacementConstraintDef = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefAddToExecutionRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefAddToTaskRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefAddVolumeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='The name of the volume. Up to 255 letters (uppercase and lowercase), numbers, and hyphens are allowed. This name is referenced in the sourceVolume parameter of container definition mountPoints.\n')
    configured_at_launch: typing.Optional[bool] = pydantic.Field(None, description='Indicates if the volume should be configured at launch. Default: false\n')
    docker_volume_configuration: typing.Union[models.aws_ecs.DockerVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using Docker volumes. Docker volumes are only supported when you are using the EC2 launch type. Windows containers only support the use of the local driver. To use bind mounts, specify a host instead.\n')
    efs_volume_configuration: typing.Union[models.aws_ecs.EfsVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This property is specified when you are using Amazon EFS. When specifying Amazon EFS volumes in tasks using the Fargate launch type, Fargate creates a supervisor container that is responsible for managing the Amazon EFS volume. The supervisor container uses a small amount of the task's memory. The supervisor container is visible when querying the task metadata version 4 endpoint, but is not visible in CloudWatch Container Insights. Default: No Elastic FileSystem is setup\n")
    host: typing.Union[models.aws_ecs.HostDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using bind mount host volumes. Bind mount host volumes are supported when you are using either the EC2 or Fargate launch types. The contents of the host parameter determine whether your bind mount host volume persists on the host container instance and where it is stored. If the host parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data is not guaranteed to persist after the containers associated with it stop running.')
    ...

class Ec2TaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefFindContainerParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefFindPortMappingByNameParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description=': port mapping name.\n')
    ...

class Ec2TaskDefinitionDefFromEc2TaskDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    ec2_task_definition_arn: str = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefFromEc2TaskDefinitionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    task_definition_arn: str = pydantic.Field(..., description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.')
    ...

class Ec2TaskDefinitionDefFromTaskDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    task_definition_arn: str = pydantic.Field(..., description='-')
    ...

class Ec2TaskDefinitionDefFromTaskDefinitionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    compatibility: typing.Optional[aws_cdk.aws_ecs.Compatibility] = pydantic.Field(None, description='What launch types this task definition should be compatible with. Default: Compatibility.EC2_AND_FARGATE\n')
    task_definition_arn: str = pydantic.Field(..., description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.')
    ...

class Ec2TaskDefinitionDefGrantRunParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='Principal to grant consume rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class Ec2TaskDefinitionDefObtainExecutionRoleParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models._interface_methods.AwsIamIRoleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.ExternalService
class ExternalServiceDef(BaseConstruct):
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task definition to use for tasks in the service. [disable-awslint:ref-via-interface]\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to associate with the service. If you do not specify a security group, a new security group is created. Default: - A new security group is created.\n')
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined')
    _init_params: typing.ClassVar[list[str]] = ['task_definition', 'security_groups', 'cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations']
    _method_names: typing.ClassVar[list[str]] = ['add_volume', 'apply_removal_policy', 'associate_cloud_map_service', 'attach_to_application_target_group', 'attach_to_classic_lb', 'attach_to_network_target_group', 'auto_scale_task_count', 'enable_cloud_map', 'enable_deployment_alarms', 'enable_service_connect', 'load_balancer_target', 'metric', 'metric_cpu_utilization', 'metric_memory_utilization', 'register_load_balancer_targets']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_external_service_arn', 'from_external_service_attributes', 'from_service_arn_with_cluster']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExternalService'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_external_service_arn', 'from_external_service_attributes', 'from_service_arn_with_cluster']
    ...


    from_external_service_arn: typing.Optional[models.aws_ecs.ExternalServiceDefFromExternalServiceArnParams] = pydantic.Field(None, description='Imports from the specified service ARN.')
    from_external_service_attributes: typing.Optional[models.aws_ecs.ExternalServiceDefFromExternalServiceAttributesParams] = pydantic.Field(None, description='Imports from the specified service attributes.')
    from_service_arn_with_cluster: typing.Optional[models.aws_ecs.ExternalServiceDefFromServiceArnWithClusterParams] = pydantic.Field(None, description='Import an existing ECS/Fargate Service using the service cluster format.\nThe format is the "new" format "arn:aws:ecs:region:aws_account_id:service/cluster-name/service-name".')
    resource_config: typing.Optional[models.aws_ecs.ExternalServiceDefConfig] = pydantic.Field(None)


class ExternalServiceDefConfig(pydantic.BaseModel):
    add_volume: typing.Optional[list[models.aws_ecs.ExternalServiceDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the Service.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    associate_cloud_map_service: typing.Optional[list[models.aws_ecs.ExternalServiceDefAssociateCloudMapServiceParams]] = pydantic.Field(None, description='Overriden method to throw error as ``associateCloudMapService`` is not supported for external service.')
    attach_to_application_target_group: typing.Optional[list[models.aws_ecs.ExternalServiceDefAttachToApplicationTargetGroupParams]] = pydantic.Field(None, description='Overriden method to throw error as ``attachToApplicationTargetGroup`` is not supported for external service.')
    attach_to_classic_lb: typing.Optional[list[models.aws_ecs.ExternalServiceDefAttachToClassicLbParams]] = pydantic.Field(None, description="Registers the service as a target of a Classic Load Balancer (CLB).\nDon't call this. Call ``loadBalancer.addTarget()`` instead.")
    attach_to_network_target_group: typing.Optional[list[models.aws_ecs.ExternalServiceDefAttachToNetworkTargetGroupParams]] = pydantic.Field(None, description="This method is called to attach this service to a Network Load Balancer.\nDon't call this function directly. Instead, call ``listener.addTargets()``\nto add this service to a load balancer.")
    auto_scale_task_count: typing.Optional[list[models.aws_ecs.ExternalServiceDefAutoScaleTaskCountParams]] = pydantic.Field(None, description='Overriden method to throw error as ``autoScaleTaskCount`` is not supported for external service.')
    enable_cloud_map: typing.Optional[list[models.aws_ecs.ExternalServiceDefEnableCloudMapParams]] = pydantic.Field(None, description='Overriden method to throw error as ``enableCloudMap`` is not supported for external service.')
    enable_deployment_alarms: typing.Optional[list[models.aws_ecs.ExternalServiceDefEnableDeploymentAlarmsParams]] = pydantic.Field(None, description="Enable Deployment Alarms which take advantage of arbitrary alarms and configure them after service initialization.\nIf you have already enabled deployment alarms, this function can be used to tell ECS about additional alarms that\nshould interrupt a deployment.\n\nNew alarms specified in subsequent calls of this function will be appended to the existing list of alarms.\n\nThe same Alarm Behavior must be used on all deployment alarms. If you specify different AlarmBehavior values in\nmultiple calls to this function, or the Alarm Behavior used here doesn't match the one used in the service\nconstructor, an error will be thrown.\n\nIf the alarm's metric references the service, you cannot pass ``Alarm.alarmName`` here. That will cause a circular\ndependency between the service and its deployment alarm. See this package's README for options to alarm on service\nmetrics, and avoid this circular dependency.")
    enable_service_connect: typing.Optional[list[models.aws_ecs.ExternalServiceDefEnableServiceConnectParams]] = pydantic.Field(None, description='Enable Service Connect on this service.')
    load_balancer_target: typing.Optional[list[models.aws_ecs.ExternalServiceDefLoadBalancerTargetParams]] = pydantic.Field(None, description='Overriden method to throw error as ``loadBalancerTarget`` is not supported for external service.')
    metric: typing.Optional[list[models.aws_ecs.ExternalServiceDefMetricParams]] = pydantic.Field(None, description='This method returns the specified CloudWatch metric name for this service.')
    metric_cpu_utilization: typing.Optional[list[models.aws_ecs.ExternalServiceDefMetricCpuUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's CPU utilization.")
    metric_memory_utilization: typing.Optional[list[models.aws_ecs.ExternalServiceDefMetricMemoryUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's memory utilization.")
    register_load_balancer_targets: typing.Optional[bool] = pydantic.Field(None, description='Overriden method to throw error as ``registerLoadBalancerTargets`` is not supported for external service.')
    cluster_config: typing.Optional[models._interface_methods.AwsEcsIClusterDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    task_definition_config: typing.Optional[models.aws_ecs.TaskDefinitionDefConfig] = pydantic.Field(None)

class ExternalServiceDefAddVolumeParams(pydantic.BaseModel):
    volume: models.aws_ecs.ServiceManagedVolumeDef = pydantic.Field(..., description='-')
    ...

class ExternalServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ExternalServiceDefAssociateCloudMapServiceParams(pydantic.BaseModel):
    service: typing.Union[models.aws_servicediscovery.ServiceDef] = pydantic.Field(..., description='The cloudmap service to register with.\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container")
    ...

class ExternalServiceDefAttachToApplicationTargetGroupParams(pydantic.BaseModel):
    ...

class ExternalServiceDefAttachToClassicLbParams(pydantic.BaseModel):
    load_balancer: models.aws_elasticloadbalancing.LoadBalancerDef = pydantic.Field(..., description='-')
    ...

class ExternalServiceDefAttachToNetworkTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.NetworkTargetGroupDef] = pydantic.Field(..., description='-')
    ...

class ExternalServiceDefAutoScaleTaskCountParams(pydantic.BaseModel):
    max_capacity: typing.Union[int, float] = pydantic.Field(..., description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1')
    return_config: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefConfig]] = pydantic.Field(None)
    ...

class ExternalServiceDefEnableCloudMapParams(pydantic.BaseModel):
    cloud_map_namespace: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='The service discovery namespace for the Cloud Map service to attach to the ECS service. Default: - the defaultCloudMapNamespace associated to the cluster\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container\n")
    dns_record_type: typing.Optional[aws_cdk.aws_servicediscovery.DnsRecordType] = pydantic.Field(None, description='The DNS record type that you want AWS Cloud Map to create. The supported record types are A or SRV. Default: - DnsRecordType.A if TaskDefinition.networkMode = AWS_VPC, otherwise DnsRecordType.SRV\n')
    dns_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time that you want DNS resolvers to cache the settings for this record. Default: Duration.minutes(1)\n')
    failure_threshold: typing.Union[int, float, None] = pydantic.Field(None, description='The number of 30-second intervals that you want Cloud Map to wait after receiving an UpdateInstanceCustomHealthStatus request before it changes the health status of a service instance. NOTE: This is used for HealthCheckCustomConfig\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the Cloud Map service to attach to the ECS service. Default: CloudFormation-generated name')
    return_config: typing.Optional[list[models.aws_servicediscovery.ServiceDefConfig]] = pydantic.Field(None)
    ...

class ExternalServiceDefEnableDeploymentAlarmsParams(pydantic.BaseModel):
    alarm_names: typing.Sequence[str] = pydantic.Field(..., description='-\n')
    behavior: typing.Optional[aws_cdk.aws_ecs.AlarmBehavior] = pydantic.Field(None, description='Default rollback on alarm. Default: AlarmBehavior.ROLLBACK_ON_ALARM')
    ...

class ExternalServiceDefEnableServiceConnectParams(pydantic.BaseModel):
    log_driver: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log driver configuration to use for the Service Connect agent logs. Default: - none\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description='The cloudmap namespace to register this service into. Default: the cloudmap namespace specified on the cluster.\n')
    services: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.ServiceConnectServiceDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of Services, including a port mapping, terse client alias, and optional intermediate DNS name. This property may be left blank if the current ECS service does not need to advertise any ports via Service Connect. Default: none')
    ...

class ExternalServiceDefFromExternalServiceArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    external_service_arn: str = pydantic.Field(..., description='-')
    ...

class ExternalServiceDefFromExternalServiceAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    cluster: typing.Union[models.aws_ecs.ClusterDef] = pydantic.Field(..., description='The cluster that hosts the service.\n')
    service_arn: typing.Optional[str] = pydantic.Field(None, description='The service ARN. Default: - either this, or ``serviceName``, is required\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - either this, or ``serviceArn``, is required')
    ...

class ExternalServiceDefFromServiceArnWithClusterParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    service_arn: str = pydantic.Field(..., description='-\n\n:see: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-account-settings.html#ecs-resource-ids\n')
    ...

class ExternalServiceDefLoadBalancerTargetParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='The name of the container.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number of the container. Only applicable when using application/network load balancers. Default: - Container port of the first added port mapping.\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Only applicable when using application load balancers. Default: Protocol.TCP')
    return_config: typing.Optional[list[models._interface_methods.AwsEcsIEcsLoadBalancerTargetDefConfig]] = pydantic.Field(None)
    ...

class ExternalServiceDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ExternalServiceDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class ExternalServiceDefMetricMemoryUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.ExternalTaskDefinition
class ExternalTaskDefinitionDef(BaseConstruct):
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. With ECS Anywhere, supported modes are bridge, host and none. Default: NetworkMode.BRIDGE\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.')
    _init_params: typing.ClassVar[list[str]] = ['network_mode', 'execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['add_container', 'add_extension', 'add_firelens_log_router', 'add_inference_accelerator', 'add_placement_constraint', 'add_to_execution_role_policy', 'add_to_task_role_policy', 'add_volume', 'apply_removal_policy', 'find_container', 'find_port_mapping_by_name', 'grant_run', 'obtain_execution_role']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_ec2_task_definition_arn', 'from_external_task_definition_attributes', 'from_task_definition_arn', 'from_task_definition_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExternalTaskDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_ec2_task_definition_arn', 'from_external_task_definition_attributes', 'from_task_definition_arn', 'from_task_definition_attributes']
    ...


    from_ec2_task_definition_arn: typing.Optional[models.aws_ecs.ExternalTaskDefinitionDefFromEc2TaskDefinitionArnParams] = pydantic.Field(None, description='Imports a task definition from the specified task definition ARN.')
    from_external_task_definition_attributes: typing.Optional[models.aws_ecs.ExternalTaskDefinitionDefFromExternalTaskDefinitionAttributesParams] = pydantic.Field(None, description='Imports an existing External task definition from its attributes.')
    from_task_definition_arn: typing.Optional[models.aws_ecs.ExternalTaskDefinitionDefFromTaskDefinitionArnParams] = pydantic.Field(None, description='Imports a task definition from the specified task definition ARN.\nThe task will have a compatibility of EC2+Fargate.')
    from_task_definition_attributes: typing.Optional[models.aws_ecs.ExternalTaskDefinitionDefFromTaskDefinitionAttributesParams] = pydantic.Field(None, description='Create a task definition from a task definition reference.')
    resource_config: typing.Optional[models.aws_ecs.ExternalTaskDefinitionDefConfig] = pydantic.Field(None)


class ExternalTaskDefinitionDefConfig(pydantic.BaseModel):
    add_container: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddContainerParams]] = pydantic.Field(None, description='Adds a new container to the task definition.')
    add_extension: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddExtensionParams]] = pydantic.Field(None, description='Adds the specified extension to the task definition.\nExtension can be used to apply a packaged modification to\na task definition.')
    add_firelens_log_router: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddFirelensLogRouterParams]] = pydantic.Field(None, description='Adds a firelens log router to the task definition.')
    add_inference_accelerator: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddInferenceAcceleratorParams]] = pydantic.Field(None, description='Overriden method to throw error as interface accelerators are not supported for external tasks.')
    add_placement_constraint: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddPlacementConstraintParams]] = pydantic.Field(None, description='Adds the specified placement constraint to the task definition.')
    add_to_execution_role_policy: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddToExecutionRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task execution IAM role.')
    add_to_task_role_policy: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddToTaskRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task IAM role.')
    add_volume: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the task definition.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    find_container: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefFindContainerParams]] = pydantic.Field(None, description='Returns the container that match the provided containerName.')
    find_port_mapping_by_name: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefFindPortMappingByNameParams]] = pydantic.Field(None, description='Determine the existing port mapping for the provided name.')
    grant_run: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefGrantRunParams]] = pydantic.Field(None, description='Grants permissions to run this task definition.\nThis will grant the following permissions:\n\n- ecs:RunTask\n- iam:PassRole')
    obtain_execution_role: typing.Optional[list[models.aws_ecs.ExternalTaskDefinitionDefObtainExecutionRoleParams]] = pydantic.Field(None, description="Creates the task execution IAM role if it doesn't already exist.")
    task_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class ExternalTaskDefinitionDefAddContainerParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefConfig]] = pydantic.Field(None)
    ...

class ExternalTaskDefinitionDefAddExtensionParams(pydantic.BaseModel):
    extension: models.UnsupportedResource = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefAddFirelensLogRouterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    firelens_config: typing.Union[models.aws_ecs.FirelensConfigDef, dict[str, typing.Any]] = pydantic.Field(..., description='Firelens configuration.\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefConfig]] = pydantic.Field(None)
    ...

class ExternalTaskDefinitionDefAddInferenceAcceleratorParams(pydantic.BaseModel):
    device_name: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator device name. Default: - empty\n')
    device_type: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator type to use. The allowed values are: eia2.medium, eia2.large and eia2.xlarge. Default: - empty')
    ...

class ExternalTaskDefinitionDefAddPlacementConstraintParams(pydantic.BaseModel):
    constraint: models.aws_ecs.PlacementConstraintDef = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefAddToExecutionRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefAddToTaskRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefAddVolumeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='The name of the volume. Up to 255 letters (uppercase and lowercase), numbers, and hyphens are allowed. This name is referenced in the sourceVolume parameter of container definition mountPoints.\n')
    configured_at_launch: typing.Optional[bool] = pydantic.Field(None, description='Indicates if the volume should be configured at launch. Default: false\n')
    docker_volume_configuration: typing.Union[models.aws_ecs.DockerVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using Docker volumes. Docker volumes are only supported when you are using the EC2 launch type. Windows containers only support the use of the local driver. To use bind mounts, specify a host instead.\n')
    efs_volume_configuration: typing.Union[models.aws_ecs.EfsVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This property is specified when you are using Amazon EFS. When specifying Amazon EFS volumes in tasks using the Fargate launch type, Fargate creates a supervisor container that is responsible for managing the Amazon EFS volume. The supervisor container uses a small amount of the task's memory. The supervisor container is visible when querying the task metadata version 4 endpoint, but is not visible in CloudWatch Container Insights. Default: No Elastic FileSystem is setup\n")
    host: typing.Union[models.aws_ecs.HostDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using bind mount host volumes. Bind mount host volumes are supported when you are using either the EC2 or Fargate launch types. The contents of the host parameter determine whether your bind mount host volume persists on the host container instance and where it is stored. If the host parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data is not guaranteed to persist after the containers associated with it stop running.')
    ...

class ExternalTaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefFindContainerParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefFindPortMappingByNameParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description=': port mapping name.\n')
    ...

class ExternalTaskDefinitionDefFromEc2TaskDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    external_task_definition_arn: str = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefFromExternalTaskDefinitionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    task_definition_arn: str = pydantic.Field(..., description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.')
    ...

class ExternalTaskDefinitionDefFromTaskDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    task_definition_arn: str = pydantic.Field(..., description='-')
    ...

class ExternalTaskDefinitionDefFromTaskDefinitionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    compatibility: typing.Optional[aws_cdk.aws_ecs.Compatibility] = pydantic.Field(None, description='What launch types this task definition should be compatible with. Default: Compatibility.EC2_AND_FARGATE\n')
    task_definition_arn: str = pydantic.Field(..., description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.')
    ...

class ExternalTaskDefinitionDefGrantRunParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='Principal to grant consume rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class ExternalTaskDefinitionDefObtainExecutionRoleParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models._interface_methods.AwsIamIRoleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.FargateService
class FargateServiceDef(BaseConstruct):
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task definition to use for tasks in the service. [disable-awslint:ref-via-interface]\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the task's elastic network interface receives a public IP address. If true, each task will receive a public IP address. Default: false\n")
    platform_version: typing.Optional[aws_cdk.aws_ecs.FargatePlatformVersion] = pydantic.Field(None, description='The platform version on which to run your service. If one is not specified, the LATEST platform version is used by default. For more information, see `AWS Fargate Platform Versions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html>`_ in the Amazon Elastic Container Service Developer Guide. Default: Latest\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to associate with the service. If you do not specify a security group, a new security group is created. Default: - A new security group is created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets to associate with the service. Default: - Public subnets if ``assignPublicIp`` is set, otherwise the first available one of Private, Isolated, Public, in that order.\n')
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined')
    _init_params: typing.ClassVar[list[str]] = ['task_definition', 'assign_public_ip', 'platform_version', 'security_groups', 'vpc_subnets', 'cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations']
    _method_names: typing.ClassVar[list[str]] = ['add_volume', 'apply_removal_policy', 'associate_cloud_map_service', 'attach_to_application_target_group', 'attach_to_classic_lb', 'attach_to_network_target_group', 'auto_scale_task_count', 'enable_cloud_map', 'enable_deployment_alarms', 'enable_service_connect', 'load_balancer_target', 'metric', 'metric_cpu_utilization', 'metric_memory_utilization', 'register_load_balancer_targets']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_fargate_service_arn', 'from_fargate_service_attributes', 'from_service_arn_with_cluster']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FargateService'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_fargate_service_arn', 'from_fargate_service_attributes', 'from_service_arn_with_cluster']
    ...


    from_fargate_service_arn: typing.Optional[models.aws_ecs.FargateServiceDefFromFargateServiceArnParams] = pydantic.Field(None, description='Imports from the specified service ARN.')
    from_fargate_service_attributes: typing.Optional[models.aws_ecs.FargateServiceDefFromFargateServiceAttributesParams] = pydantic.Field(None, description='Imports from the specified service attributes.')
    from_service_arn_with_cluster: typing.Optional[models.aws_ecs.FargateServiceDefFromServiceArnWithClusterParams] = pydantic.Field(None, description='Import an existing ECS/Fargate Service using the service cluster format.\nThe format is the "new" format "arn:aws:ecs:region:aws_account_id:service/cluster-name/service-name".')
    resource_config: typing.Optional[models.aws_ecs.FargateServiceDefConfig] = pydantic.Field(None)


class FargateServiceDefConfig(pydantic.BaseModel):
    add_volume: typing.Optional[list[models.aws_ecs.FargateServiceDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the Service.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    associate_cloud_map_service: typing.Optional[list[models.aws_ecs.FargateServiceDefAssociateCloudMapServiceParams]] = pydantic.Field(None, description='Associates this service with a CloudMap service.')
    attach_to_application_target_group: typing.Optional[list[models.aws_ecs.FargateServiceDefAttachToApplicationTargetGroupParams]] = pydantic.Field(None, description="This method is called to attach this service to an Application Load Balancer.\nDon't call this function directly. Instead, call ``listener.addTargets()``\nto add this service to a load balancer.")
    attach_to_classic_lb: typing.Optional[list[models.aws_ecs.FargateServiceDefAttachToClassicLbParams]] = pydantic.Field(None, description="Registers the service as a target of a Classic Load Balancer (CLB).\nDon't call this. Call ``loadBalancer.addTarget()`` instead.")
    attach_to_network_target_group: typing.Optional[list[models.aws_ecs.FargateServiceDefAttachToNetworkTargetGroupParams]] = pydantic.Field(None, description="This method is called to attach this service to a Network Load Balancer.\nDon't call this function directly. Instead, call ``listener.addTargets()``\nto add this service to a load balancer.")
    auto_scale_task_count: typing.Optional[list[models.aws_ecs.FargateServiceDefAutoScaleTaskCountParams]] = pydantic.Field(None, description='An attribute representing the minimum and maximum task count for an AutoScalingGroup.')
    enable_cloud_map: typing.Optional[list[models.aws_ecs.FargateServiceDefEnableCloudMapParams]] = pydantic.Field(None, description='Enable CloudMap service discovery for the service.')
    enable_deployment_alarms: typing.Optional[list[models.aws_ecs.FargateServiceDefEnableDeploymentAlarmsParams]] = pydantic.Field(None, description="Enable Deployment Alarms which take advantage of arbitrary alarms and configure them after service initialization.\nIf you have already enabled deployment alarms, this function can be used to tell ECS about additional alarms that\nshould interrupt a deployment.\n\nNew alarms specified in subsequent calls of this function will be appended to the existing list of alarms.\n\nThe same Alarm Behavior must be used on all deployment alarms. If you specify different AlarmBehavior values in\nmultiple calls to this function, or the Alarm Behavior used here doesn't match the one used in the service\nconstructor, an error will be thrown.\n\nIf the alarm's metric references the service, you cannot pass ``Alarm.alarmName`` here. That will cause a circular\ndependency between the service and its deployment alarm. See this package's README for options to alarm on service\nmetrics, and avoid this circular dependency.")
    enable_service_connect: typing.Optional[list[models.aws_ecs.FargateServiceDefEnableServiceConnectParams]] = pydantic.Field(None, description='Enable Service Connect on this service.')
    load_balancer_target: typing.Optional[list[models.aws_ecs.FargateServiceDefLoadBalancerTargetParams]] = pydantic.Field(None, description='Return a load balancing target for a specific container and port.\nUse this function to create a load balancer target if you want to load balance to\nanother container than the first essential container or the first mapped port on\nthe container.\n\nUse the return value of this function where you would normally use a load balancer\ntarget, instead of the ``Service`` object itself.')
    metric: typing.Optional[list[models.aws_ecs.FargateServiceDefMetricParams]] = pydantic.Field(None, description='This method returns the specified CloudWatch metric name for this service.')
    metric_cpu_utilization: typing.Optional[list[models.aws_ecs.FargateServiceDefMetricCpuUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's CPU utilization.")
    metric_memory_utilization: typing.Optional[list[models.aws_ecs.FargateServiceDefMetricMemoryUtilizationParams]] = pydantic.Field(None, description="This method returns the CloudWatch metric for this service's memory utilization.")
    register_load_balancer_targets: typing.Optional[list[models.aws_ecs.FargateServiceDefRegisterLoadBalancerTargetsParams]] = pydantic.Field(None, description='Use this function to create all load balancer targets to be registered in this service, add them to target groups, and attach target groups to listeners accordingly.\nAlternatively, you can use ``listener.addTargets()`` to create targets and add them to target groups.')
    cluster_config: typing.Optional[models._interface_methods.AwsEcsIClusterDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    task_definition_config: typing.Optional[models.aws_ecs.TaskDefinitionDefConfig] = pydantic.Field(None)

class FargateServiceDefAddVolumeParams(pydantic.BaseModel):
    volume: models.aws_ecs.ServiceManagedVolumeDef = pydantic.Field(..., description='-')
    ...

class FargateServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class FargateServiceDefAssociateCloudMapServiceParams(pydantic.BaseModel):
    service: typing.Union[models.aws_servicediscovery.ServiceDef] = pydantic.Field(..., description='The cloudmap service to register with.\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container")
    ...

class FargateServiceDefAttachToApplicationTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef] = pydantic.Field(..., description='-')
    ...

class FargateServiceDefAttachToClassicLbParams(pydantic.BaseModel):
    load_balancer: models.aws_elasticloadbalancing.LoadBalancerDef = pydantic.Field(..., description='-')
    ...

class FargateServiceDefAttachToNetworkTargetGroupParams(pydantic.BaseModel):
    target_group: typing.Union[models.aws_elasticloadbalancingv2.NetworkTargetGroupDef] = pydantic.Field(..., description='-')
    ...

class FargateServiceDefAutoScaleTaskCountParams(pydantic.BaseModel):
    max_capacity: typing.Union[int, float] = pydantic.Field(..., description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1')
    return_config: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefConfig]] = pydantic.Field(None)
    ...

class FargateServiceDefEnableCloudMapParams(pydantic.BaseModel):
    cloud_map_namespace: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='The service discovery namespace for the Cloud Map service to attach to the ECS service. Default: - the defaultCloudMapNamespace associated to the cluster\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container\n")
    dns_record_type: typing.Optional[aws_cdk.aws_servicediscovery.DnsRecordType] = pydantic.Field(None, description='The DNS record type that you want AWS Cloud Map to create. The supported record types are A or SRV. Default: - DnsRecordType.A if TaskDefinition.networkMode = AWS_VPC, otherwise DnsRecordType.SRV\n')
    dns_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time that you want DNS resolvers to cache the settings for this record. Default: Duration.minutes(1)\n')
    failure_threshold: typing.Union[int, float, None] = pydantic.Field(None, description='The number of 30-second intervals that you want Cloud Map to wait after receiving an UpdateInstanceCustomHealthStatus request before it changes the health status of a service instance. NOTE: This is used for HealthCheckCustomConfig\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the Cloud Map service to attach to the ECS service. Default: CloudFormation-generated name\n')
    return_config: typing.Optional[list[models.aws_servicediscovery.ServiceDefConfig]] = pydantic.Field(None)
    ...

class FargateServiceDefEnableDeploymentAlarmsParams(pydantic.BaseModel):
    alarm_names: typing.Sequence[str] = pydantic.Field(..., description='-\n')
    behavior: typing.Optional[aws_cdk.aws_ecs.AlarmBehavior] = pydantic.Field(None, description='Default rollback on alarm. Default: AlarmBehavior.ROLLBACK_ON_ALARM')
    ...

class FargateServiceDefEnableServiceConnectParams(pydantic.BaseModel):
    log_driver: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log driver configuration to use for the Service Connect agent logs. Default: - none\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description='The cloudmap namespace to register this service into. Default: the cloudmap namespace specified on the cluster.\n')
    services: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.ServiceConnectServiceDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of Services, including a port mapping, terse client alias, and optional intermediate DNS name. This property may be left blank if the current ECS service does not need to advertise any ports via Service Connect. Default: none')
    ...

class FargateServiceDefFromFargateServiceArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    fargate_service_arn: str = pydantic.Field(..., description='-')
    ...

class FargateServiceDefFromFargateServiceAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    cluster: typing.Union[models.aws_ecs.ClusterDef] = pydantic.Field(..., description='The cluster that hosts the service.\n')
    service_arn: typing.Optional[str] = pydantic.Field(None, description='The service ARN. Default: - either this, or ``serviceName``, is required\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - either this, or ``serviceArn``, is required')
    ...

class FargateServiceDefFromServiceArnWithClusterParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    service_arn: str = pydantic.Field(..., description='-\n\n:see: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-account-settings.html#ecs-resource-ids\n')
    ...

class FargateServiceDefLoadBalancerTargetParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='The name of the container.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number of the container. Only applicable when using application/network load balancers. Default: - Container port of the first added port mapping.\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Only applicable when using application load balancers. Default: Protocol.TCP\n\nExample::\n\n    # listener: elbv2.ApplicationListener\n    # service: ecs.BaseService\n\n    listener.add_targets("ECS",\n        port=80,\n        targets=[service.load_balancer_target(\n            container_name="MyContainer",\n            container_port=1234\n        )]\n    )\n')
    return_config: typing.Optional[list[models._interface_methods.AwsEcsIEcsLoadBalancerTargetDefConfig]] = pydantic.Field(None)
    ...

class FargateServiceDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FargateServiceDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FargateServiceDefMetricMemoryUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FargateServiceDefRegisterLoadBalancerTargetsParams(pydantic.BaseModel):
    targets: list[models.aws_ecs.EcsTargetDef] = pydantic.Field(...)
    ...


#  autogenerated from aws_cdk.aws_ecs.FargateTaskDefinition
class FargateTaskDefinitionDef(BaseConstruct):
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The number of cpu units used by the task. For tasks using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the memory parameter: 256 (.25 vCPU) - Available memory values: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) 512 (.5 vCPU) - Available memory values: 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) 1024 (1 vCPU) - Available memory values: 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) 2048 (2 vCPU) - Available memory values: Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) 4096 (4 vCPU) - Available memory values: Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) 8192 (8 vCPU) - Available memory values: Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) 16384 (16 vCPU) - Available memory values: Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) Default: 256\n')
    ephemeral_storage_gib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in GiB) of ephemeral storage to be allocated to the task. The maximum supported value is 200 GiB. NOTE: This parameter is only supported for tasks hosted on AWS Fargate using platform version 1.4.0 or later. Default: 20\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory used by the task. For tasks using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the cpu parameter: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - Available cpu values: 256 (.25 vCPU) 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - Available cpu values: 512 (.5 vCPU) 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - Available cpu values: 1024 (1 vCPU) Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - Available cpu values: 2048 (2 vCPU) Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - Available cpu values: 4096 (4 vCPU) Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) - Available cpu values: 8192 (8 vCPU) Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) - Available cpu values: 16384 (16 vCPU) Default: 512\n')
    pid_mode: typing.Optional[aws_cdk.aws_ecs.PidMode] = pydantic.Field(None, description='The process namespace to use for the containers in the task. Only supported for tasks that are hosted on AWS Fargate if the tasks are using platform version 1.4.0 or later (Linux). Only the TASK option is supported for Linux-based Fargate containers. Not supported in Windows containers. If pidMode is specified for a Fargate task, then runtimePlatform.operatingSystemFamily must also be specified. For more information, see `Task Definition Parameters <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#task_definition_pidmode>`_. Default: - PidMode used by the task is not specified\n')
    runtime_platform: typing.Union[models.aws_ecs.RuntimePlatformDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The operating system that your task definitions are running on. A runtimePlatform is supported only for tasks using the Fargate launch type. Default: - Undefined.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.')
    _init_params: typing.ClassVar[list[str]] = ['cpu', 'ephemeral_storage_gib', 'memory_limit_mib', 'pid_mode', 'runtime_platform', 'execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['add_container', 'add_extension', 'add_firelens_log_router', 'add_inference_accelerator', 'add_placement_constraint', 'add_to_execution_role_policy', 'add_to_task_role_policy', 'add_volume', 'apply_removal_policy', 'find_container', 'find_port_mapping_by_name', 'grant_run', 'obtain_execution_role']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_fargate_task_definition_arn', 'from_fargate_task_definition_attributes', 'from_task_definition_arn', 'from_task_definition_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FargateTaskDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_fargate_task_definition_arn', 'from_fargate_task_definition_attributes', 'from_task_definition_arn', 'from_task_definition_attributes']
    ...


    from_fargate_task_definition_arn: typing.Optional[models.aws_ecs.FargateTaskDefinitionDefFromFargateTaskDefinitionArnParams] = pydantic.Field(None, description='Imports a task definition from the specified task definition ARN.')
    from_fargate_task_definition_attributes: typing.Optional[models.aws_ecs.FargateTaskDefinitionDefFromFargateTaskDefinitionAttributesParams] = pydantic.Field(None, description='Import an existing Fargate task definition from its attributes.')
    from_task_definition_arn: typing.Optional[models.aws_ecs.FargateTaskDefinitionDefFromTaskDefinitionArnParams] = pydantic.Field(None, description='Imports a task definition from the specified task definition ARN.\nThe task will have a compatibility of EC2+Fargate.')
    from_task_definition_attributes: typing.Optional[models.aws_ecs.FargateTaskDefinitionDefFromTaskDefinitionAttributesParams] = pydantic.Field(None, description='Create a task definition from a task definition reference.')
    resource_config: typing.Optional[models.aws_ecs.FargateTaskDefinitionDefConfig] = pydantic.Field(None)


class FargateTaskDefinitionDefConfig(pydantic.BaseModel):
    add_container: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddContainerParams]] = pydantic.Field(None, description='Adds a new container to the task definition.')
    add_extension: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddExtensionParams]] = pydantic.Field(None, description='Adds the specified extension to the task definition.\nExtension can be used to apply a packaged modification to\na task definition.')
    add_firelens_log_router: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddFirelensLogRouterParams]] = pydantic.Field(None, description='Adds a firelens log router to the task definition.')
    add_inference_accelerator: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddInferenceAcceleratorParams]] = pydantic.Field(None, description='Adds an inference accelerator to the task definition.')
    add_placement_constraint: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddPlacementConstraintParams]] = pydantic.Field(None, description='Adds the specified placement constraint to the task definition.')
    add_to_execution_role_policy: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddToExecutionRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task execution IAM role.')
    add_to_task_role_policy: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddToTaskRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task IAM role.')
    add_volume: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the task definition.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    find_container: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefFindContainerParams]] = pydantic.Field(None, description='Returns the container that match the provided containerName.')
    find_port_mapping_by_name: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefFindPortMappingByNameParams]] = pydantic.Field(None, description='Determine the existing port mapping for the provided name.')
    grant_run: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefGrantRunParams]] = pydantic.Field(None, description='Grants permissions to run this task definition.\nThis will grant the following permissions:\n\n- ecs:RunTask\n- iam:PassRole')
    obtain_execution_role: typing.Optional[list[models.aws_ecs.FargateTaskDefinitionDefObtainExecutionRoleParams]] = pydantic.Field(None, description="Creates the task execution IAM role if it doesn't already exist.")
    task_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class FargateTaskDefinitionDefAddContainerParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefConfig]] = pydantic.Field(None)
    ...

class FargateTaskDefinitionDefAddExtensionParams(pydantic.BaseModel):
    extension: models.UnsupportedResource = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefAddFirelensLogRouterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    firelens_config: typing.Union[models.aws_ecs.FirelensConfigDef, dict[str, typing.Any]] = pydantic.Field(..., description='Firelens configuration.\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefConfig]] = pydantic.Field(None)
    ...

class FargateTaskDefinitionDefAddInferenceAcceleratorParams(pydantic.BaseModel):
    device_name: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator device name. Default: - empty\n')
    device_type: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator type to use. The allowed values are: eia2.medium, eia2.large and eia2.xlarge. Default: - empty')
    ...

class FargateTaskDefinitionDefAddPlacementConstraintParams(pydantic.BaseModel):
    constraint: models.aws_ecs.PlacementConstraintDef = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefAddToExecutionRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefAddToTaskRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefAddVolumeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='The name of the volume. Up to 255 letters (uppercase and lowercase), numbers, and hyphens are allowed. This name is referenced in the sourceVolume parameter of container definition mountPoints.\n')
    configured_at_launch: typing.Optional[bool] = pydantic.Field(None, description='Indicates if the volume should be configured at launch. Default: false\n')
    docker_volume_configuration: typing.Union[models.aws_ecs.DockerVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using Docker volumes. Docker volumes are only supported when you are using the EC2 launch type. Windows containers only support the use of the local driver. To use bind mounts, specify a host instead.\n')
    efs_volume_configuration: typing.Union[models.aws_ecs.EfsVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This property is specified when you are using Amazon EFS. When specifying Amazon EFS volumes in tasks using the Fargate launch type, Fargate creates a supervisor container that is responsible for managing the Amazon EFS volume. The supervisor container uses a small amount of the task's memory. The supervisor container is visible when querying the task metadata version 4 endpoint, but is not visible in CloudWatch Container Insights. Default: No Elastic FileSystem is setup\n")
    host: typing.Union[models.aws_ecs.HostDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using bind mount host volumes. Bind mount host volumes are supported when you are using either the EC2 or Fargate launch types. The contents of the host parameter determine whether your bind mount host volume persists on the host container instance and where it is stored. If the host parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data is not guaranteed to persist after the containers associated with it stop running.')
    ...

class FargateTaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefFindContainerParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefFindPortMappingByNameParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description=': port mapping name.\n')
    ...

class FargateTaskDefinitionDefFromFargateTaskDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    fargate_task_definition_arn: str = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefFromFargateTaskDefinitionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    task_definition_arn: str = pydantic.Field(..., description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.')
    ...

class FargateTaskDefinitionDefFromTaskDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    task_definition_arn: str = pydantic.Field(..., description='-')
    ...

class FargateTaskDefinitionDefFromTaskDefinitionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    compatibility: typing.Optional[aws_cdk.aws_ecs.Compatibility] = pydantic.Field(None, description='What launch types this task definition should be compatible with. Default: Compatibility.EC2_AND_FARGATE\n')
    task_definition_arn: str = pydantic.Field(..., description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.')
    ...

class FargateTaskDefinitionDefGrantRunParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='Principal to grant consume rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FargateTaskDefinitionDefObtainExecutionRoleParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models._interface_methods.AwsIamIRoleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.FirelensLogRouter
class FirelensLogRouterDef(BaseConstruct):
    firelens_config: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.FirelensConfigDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Firelens configuration.\n')
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the task definition that includes this container definition. [disable-awslint:ref-via-interface]\n')
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    _init_params: typing.ClassVar[list[str]] = ['firelens_config', 'task_definition', 'image', 'command', 'container_name', 'cpu', 'credential_specs', 'disable_networking', 'dns_search_domains', 'dns_servers', 'docker_labels', 'docker_security_options', 'entry_point', 'environment', 'environment_files', 'essential', 'extra_hosts', 'gpu_count', 'health_check', 'hostname', 'inference_accelerator_resources', 'interactive', 'linux_parameters', 'logging', 'memory_limit_mib', 'memory_reservation_mib', 'port_mappings', 'privileged', 'pseudo_terminal', 'readonly_root_filesystem', 'secrets', 'start_timeout', 'stop_timeout', 'system_controls', 'ulimits', 'user', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = ['add_container_dependencies', 'add_docker_label', 'add_environment', 'add_inference_accelerator_resource', 'add_link', 'add_mount_points', 'add_port_mappings', 'add_scratch', 'add_secret', 'add_to_execution_policy', 'add_ulimits', 'add_volumes_from', 'find_port_mapping', 'find_port_mapping_by_name', 'render_container_definition']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FirelensLogRouter'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.FirelensLogRouterDefConfig] = pydantic.Field(None)


class FirelensLogRouterDefConfig(pydantic.BaseModel):
    add_container_dependencies: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddContainerDependenciesParams]] = pydantic.Field(None, description='This method adds one or more container dependencies to the container.')
    add_docker_label: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddDockerLabelParams]] = pydantic.Field(None, description='This method adds a Docker label to the container.')
    add_environment: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddEnvironmentParams]] = pydantic.Field(None, description='This method adds an environment variable to the container.')
    add_inference_accelerator_resource: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddInferenceAcceleratorResourceParams]] = pydantic.Field(None, description='This method adds one or more resources to the container.')
    add_link: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddLinkParams]] = pydantic.Field(None, description='This method adds a link which allows containers to communicate with each other without the need for port mappings.\nThis parameter is only supported if the task definition is using the bridge network mode.\nWarning: The --link flag is a legacy feature of Docker. It may eventually be removed.')
    add_mount_points: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddMountPointsParams]] = pydantic.Field(None, description='This method adds one or more mount points for data volumes to the container.')
    add_port_mappings: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddPortMappingsParams]] = pydantic.Field(None, description='This method adds one or more port mappings to the container.')
    add_scratch: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddScratchParams]] = pydantic.Field(None, description='This method mounts temporary disk space to the container.\nThis adds the correct container mountPoint and task definition volume.')
    add_secret: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddSecretParams]] = pydantic.Field(None, description='This method adds a secret as environment variable to the container.')
    add_to_execution_policy: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddToExecutionPolicyParams]] = pydantic.Field(None, description='This method adds the specified statement to the IAM task execution policy in the task definition.')
    add_ulimits: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddUlimitsParams]] = pydantic.Field(None, description='This method adds one or more ulimits to the container.')
    add_volumes_from: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefAddVolumesFromParams]] = pydantic.Field(None, description='This method adds one or more volumes to the container.')
    find_port_mapping: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefFindPortMappingParams]] = pydantic.Field(None, description='Returns the host port for the requested container port if it exists.')
    find_port_mapping_by_name: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefFindPortMappingByNameParams]] = pydantic.Field(None, description='Returns the port mapping with the given name, if it exists.')
    render_container_definition: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefRenderContainerDefinitionParams]] = pydantic.Field(None, description='Render this container definition to a CloudFormation object.')

class FirelensLogRouterDefAddContainerDependenciesParams(pydantic.BaseModel):
    container_dependencies: list[models.aws_ecs.ContainerDependencyDef] = pydantic.Field(...)
    ...

class FirelensLogRouterDefAddDockerLabelParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    value: str = pydantic.Field(..., description='-')
    ...

class FirelensLogRouterDefAddEnvironmentParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    value: str = pydantic.Field(..., description='-')
    ...

class FirelensLogRouterDefAddInferenceAcceleratorResourceParams(pydantic.BaseModel):
    inference_accelerator_resources: list[str] = pydantic.Field(...)
    ...

class FirelensLogRouterDefAddLinkParams(pydantic.BaseModel):
    container: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='-\n')
    alias: typing.Optional[str] = pydantic.Field(None, description='-')
    ...

class FirelensLogRouterDefAddMountPointsParams(pydantic.BaseModel):
    mount_points: list[models.aws_ecs.MountPointDef] = pydantic.Field(...)
    ...

class FirelensLogRouterDefAddPortMappingsParams(pydantic.BaseModel):
    port_mappings: list[models.aws_ecs.PortMappingDef] = pydantic.Field(...)
    ...

class FirelensLogRouterDefAddScratchParams(pydantic.BaseModel):
    container_path: str = pydantic.Field(..., description='The path on the container to mount the scratch volume at.\n')
    name: str = pydantic.Field(..., description='The name of the scratch volume to mount. Must be a volume name referenced in the name parameter of task definition volume.\n')
    read_only: bool = pydantic.Field(..., description='Specifies whether to give the container read-only access to the scratch volume. If this value is true, the container has read-only access to the scratch volume. If this value is false, then the container can write to the scratch volume.\n')
    source_path: str = pydantic.Field(..., description='')
    ...

class FirelensLogRouterDefAddSecretParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    secret: models.aws_ecs.SecretDef = pydantic.Field(..., description='-')
    ...

class FirelensLogRouterDefAddToExecutionPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class FirelensLogRouterDefAddUlimitsParams(pydantic.BaseModel):
    ulimits: list[models.aws_ecs.UlimitDef] = pydantic.Field(...)
    ...

class FirelensLogRouterDefAddVolumesFromParams(pydantic.BaseModel):
    volumes_from: list[models.aws_ecs.VolumeFromDef] = pydantic.Field(...)
    ...

class FirelensLogRouterDefFindPortMappingParams(pydantic.BaseModel):
    container_port: typing.Union[int, float] = pydantic.Field(..., description='-\n')
    protocol: aws_cdk.aws_ecs.Protocol = pydantic.Field(..., description='-')
    ...

class FirelensLogRouterDefFindPortMappingByNameParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-')
    ...

class FirelensLogRouterDefRenderContainerDefinitionParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.LinuxParameters
class LinuxParametersDef(BaseConstruct):
    init_process_enabled: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to run an init process inside the container that forwards signals and reaps processes. Default: false\n')
    max_swap: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The total amount of swap memory a container can use. This parameter will be translated to the --memory-swap option to docker run. This parameter is only supported when you are using the EC2 launch type. Accepted values are positive integers. Default: No swap.\n')
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The value for the size of the /dev/shm volume. Default: No shared memory.\n')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description="This allows you to tune a container's memory swappiness behavior. This parameter maps to the --memory-swappiness option to docker run. The swappiness relates to the kernel's tendency to swap memory. A value of 0 will cause swapping to not happen unless absolutely necessary. A value of 100 will cause pages to be swapped very aggressively. This parameter is only supported when you are using the EC2 launch type. Accepted values are whole numbers between 0 and 100. If a value is not specified for maxSwap then this parameter is ignored. Default: 60")
    _init_params: typing.ClassVar[list[str]] = ['init_process_enabled', 'max_swap', 'shared_memory_size', 'swappiness']
    _method_names: typing.ClassVar[list[str]] = ['add_capabilities', 'add_devices', 'add_tmpfs', 'drop_capabilities', 'render_linux_parameters']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.LinuxParameters'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.LinuxParametersDefConfig] = pydantic.Field(None)


class LinuxParametersDefConfig(pydantic.BaseModel):
    add_capabilities: typing.Optional[list[models.aws_ecs.LinuxParametersDefAddCapabilitiesParams]] = pydantic.Field(None, description="Adds one or more Linux capabilities to the Docker configuration of a container.\nTasks launched on Fargate only support adding the 'SYS_PTRACE' kernel capability.")
    add_devices: typing.Optional[list[models.aws_ecs.LinuxParametersDefAddDevicesParams]] = pydantic.Field(None, description='Adds one or more host devices to a container.')
    add_tmpfs: typing.Optional[list[models.aws_ecs.LinuxParametersDefAddTmpfsParams]] = pydantic.Field(None, description='Specifies the container path, mount options, and size (in MiB) of the tmpfs mount for a container.\nOnly works with EC2 launch type.')
    drop_capabilities: typing.Optional[list[models.aws_ecs.LinuxParametersDefDropCapabilitiesParams]] = pydantic.Field(None, description='Removes one or more Linux capabilities to the Docker configuration of a container.')
    render_linux_parameters: typing.Optional[list[models.aws_ecs.LinuxParametersDefRenderLinuxParametersParams]] = pydantic.Field(None, description='Renders the Linux parameters to a CloudFormation object.')

class LinuxParametersDefAddCapabilitiesParams(pydantic.BaseModel):
    cap: list[aws_cdk.aws_ecs.Capability] = pydantic.Field(...)
    ...

class LinuxParametersDefAddDevicesParams(pydantic.BaseModel):
    device: list[models.aws_ecs.DeviceDef] = pydantic.Field(...)
    ...

class LinuxParametersDefAddTmpfsParams(pydantic.BaseModel):
    tmpfs: list[models.aws_ecs.TmpfsDef] = pydantic.Field(...)
    ...

class LinuxParametersDefDropCapabilitiesParams(pydantic.BaseModel):
    cap: list[aws_cdk.aws_ecs.Capability] = pydantic.Field(...)
    ...

class LinuxParametersDefRenderLinuxParametersParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_ecs.ScalableTaskCount
class ScalableTaskCountDef(BaseConstruct):
    dimension: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Scalable dimension of the attribute.\n')
    resource_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Resource ID of the attribute.\n')
    role: typing.Union[_REQUIRED_INIT_PARAM, models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Role to use for scaling.\n')
    service_namespace: typing.Union[aws_cdk.aws_applicationautoscaling.ServiceNamespace, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Service namespace of the scalable attribute.\n')
    max_capacity: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1')
    _init_params: typing.ClassVar[list[str]] = ['dimension', 'resource_id', 'role', 'service_namespace', 'max_capacity', 'min_capacity']
    _method_names: typing.ClassVar[list[str]] = ['scale_on_cpu_utilization', 'scale_on_memory_utilization', 'scale_on_metric', 'scale_on_request_count', 'scale_on_schedule', 'scale_to_track_custom_metric']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ScalableTaskCount'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ScalableTaskCountDefConfig] = pydantic.Field(None)


class ScalableTaskCountDefConfig(pydantic.BaseModel):
    scale_on_cpu_utilization: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefScaleOnCpuUtilizationParams]] = pydantic.Field(None, description='Scales in or out to achieve a target CPU utilization.')
    scale_on_memory_utilization: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefScaleOnMemoryUtilizationParams]] = pydantic.Field(None, description='Scales in or out to achieve a target memory utilization.')
    scale_on_metric: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefScaleOnMetricParams]] = pydantic.Field(None, description='Scales in or out based on a specified metric value.')
    scale_on_request_count: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefScaleOnRequestCountParams]] = pydantic.Field(None, description='Scales in or out to achieve a target Application Load Balancer request count per target.')
    scale_on_schedule: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefScaleOnScheduleParams]] = pydantic.Field(None, description='Scales in or out based on a specified scheduled time.')
    scale_to_track_custom_metric: typing.Optional[list[models.aws_ecs.ScalableTaskCountDefScaleToTrackCustomMetricParams]] = pydantic.Field(None, description='Scales in or out to achieve a target on a custom metric.')

class ScalableTaskCountDefScaleOnCpuUtilizationParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target_utilization_percent: typing.Union[int, float] = pydantic.Field(..., description='The target value for CPU utilization across all tasks in the service.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency')
    ...

class ScalableTaskCountDefScaleOnMemoryUtilizationParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target_utilization_percent: typing.Union[int, float] = pydantic.Field(..., description='The target value for memory utilization across all tasks in the service.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency')
    ...

class ScalableTaskCountDefScaleOnMetricParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    metric: typing.Union[models.aws_cloudwatch.MathExpressionDef, models.aws_cloudwatch.MetricDef] = pydantic.Field(..., description='Metric to scale on.\n')
    scaling_steps: typing.Sequence[typing.Union[models.aws_applicationautoscaling.ScalingIntervalDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The intervals for scaling. Maps a range of metric values to a particular scaling behavior. Must be between 2 and 40 steps.\n')
    adjustment_type: typing.Optional[aws_cdk.aws_applicationautoscaling.AdjustmentType] = pydantic.Field(None, description="How the adjustment numbers inside 'intervals' are interpreted. Default: ChangeInCapacity\n")
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Grace period after scaling activity. Subsequent scale outs during the cooldown period are squashed so that only the biggest scale out happens. Subsequent scale ins during the cooldown period are ignored. Default: No cooldown period\n')
    datapoints_to_alarm: typing.Union[int, float, None] = pydantic.Field(None, description='The number of data points out of the evaluation periods that must be breaching to trigger a scaling action. Creates an "M out of N" alarm, where this property is the M and the value set for ``evaluationPeriods`` is the N value. Only has meaning if ``evaluationPeriods != 1``. Default: - Same as ``evaluationPeriods``\n')
    evaluation_periods: typing.Union[int, float, None] = pydantic.Field(None, description='How many evaluation periods of the metric to wait before triggering a scaling action. Raising this value can be used to smooth out the metric, at the expense of slower response times. If ``datapointsToAlarm`` is not set, then all data points in the evaluation period must meet the criteria to trigger a scaling action. Default: 1\n')
    metric_aggregation_type: typing.Optional[aws_cdk.aws_applicationautoscaling.MetricAggregationType] = pydantic.Field(None, description='Aggregation to apply to all data points over the evaluation periods. Only has meaning if ``evaluationPeriods != 1``. Default: - The statistic from the metric if applicable (MIN, MAX, AVERAGE), otherwise AVERAGE.\n')
    min_adjustment_magnitude: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum absolute number to adjust capacity with as result of percentage scaling. Only when using AdjustmentType = PercentChangeInCapacity, this number controls the minimum absolute effect size. Default: No minimum scaling effect')
    ...

class ScalableTaskCountDefScaleOnRequestCountParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    requests_per_target: typing.Union[int, float] = pydantic.Field(..., description='The number of ALB requests per target.\n')
    target_group: models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef = pydantic.Field(..., description='The ALB target group name.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency')
    ...

class ScalableTaskCountDefScaleOnScheduleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    schedule: models.aws_applicationautoscaling.ScheduleDef = pydantic.Field(..., description='When to perform this action.\n')
    end_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action expires. Default: The rule never expires.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new maximum capacity. During the scheduled time, the current capacity is above the maximum capacity, Application Auto Scaling scales in to the maximum capacity. At least one of maxCapacity and minCapacity must be supplied. Default: No new maximum capacity\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The new minimum capacity. During the scheduled time, if the current capacity is below the minimum capacity, Application Auto Scaling scales out to the minimum capacity. At least one of maxCapacity and minCapacity must be supplied. Default: No new minimum capacity\n')
    start_time: typing.Optional[datetime.datetime] = pydantic.Field(None, description='When this scheduled action becomes active. Default: The rule is activate immediately\n')
    time_zone: typing.Optional[models.TimeZoneDef] = pydantic.Field(None, description='The time zone used when referring to the date and time of a scheduled action, when the scheduled action uses an at or cron expression. Default: - UTC')
    ...

class ScalableTaskCountDefScaleToTrackCustomMetricParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    metric: typing.Union[models.aws_cloudwatch.MathExpressionDef, models.aws_cloudwatch.MetricDef] = pydantic.Field(..., description='The custom CloudWatch metric to track. The metric must represent utilization; that is, you will always get the following behavior: - metric > targetValue => scale out - metric < targetValue => scale in\n')
    target_value: typing.Union[int, float] = pydantic.Field(..., description='The target value for the custom CloudWatch metric.\n')
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency')
    ...


#  autogenerated from aws_cdk.aws_ecs.ServiceManagedVolume
class ServiceManagedVolumeDef(BaseConstruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume. This corresponds to the name provided in the ECS TaskDefinition.\n')
    managed_ebs_volume: typing.Union[models.aws_ecs.ServiceManagedEBSVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for an Amazon Elastic Block Store (EBS) volume managed by ECS. Default: - undefined')
    _init_params: typing.ClassVar[list[str]] = ['name', 'managed_ebs_volume']
    _method_names: typing.ClassVar[list[str]] = ['mount_in']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ServiceManagedVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ServiceManagedVolumeDefConfig] = pydantic.Field(None)


class ServiceManagedVolumeDefConfig(pydantic.BaseModel):
    mount_in: typing.Optional[list[models.aws_ecs.ServiceManagedVolumeDefMountInParams]] = pydantic.Field(None, description='Mounts the service managed volume to a specified container at a defined mount point.')
    role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class ServiceManagedVolumeDefMountInParams(pydantic.BaseModel):
    container: models.aws_ecs.ContainerDefinitionDef = pydantic.Field(..., description='The container to mount the volume on.\n')
    container_path: str = pydantic.Field(..., description='The path on the container to mount the host volume at.\n')
    read_only: bool = pydantic.Field(..., description='Specifies whether to give the container read-only access to the volume. If this value is true, the container has read-only access to the volume. If this value is false, then the container can write to the volume.')
    ...


#  autogenerated from aws_cdk.aws_ecs.TaskDefinition
class TaskDefinitionDef(BaseConstruct):
    compatibility: typing.Union[aws_cdk.aws_ecs.Compatibility, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task launch type compatiblity requirement.\n')
    cpu: typing.Optional[str] = pydantic.Field(None, description='The number of cpu units used by the task. If you are using the EC2 launch type, this field is optional and any value can be used. If you are using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the memory parameter: 256 (.25 vCPU) - Available memory values: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) 512 (.5 vCPU) - Available memory values: 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) 1024 (1 vCPU) - Available memory values: 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) 2048 (2 vCPU) - Available memory values: Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) 4096 (4 vCPU) - Available memory values: Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) 8192 (8 vCPU) - Available memory values: Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) 16384 (16 vCPU) - Available memory values: Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) Default: - CPU units are not specified.\n')
    ephemeral_storage_gib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in GiB) of ephemeral storage to be allocated to the task. Only supported in Fargate platform version 1.4.0 or later. Default: - Undefined, in which case, the task will receive 20GiB ephemeral storage.\n')
    inference_accelerators: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.InferenceAcceleratorDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The inference accelerators to use for the containers in the task. Not supported in Fargate. Default: - No inference accelerators.\n')
    ipc_mode: typing.Optional[aws_cdk.aws_ecs.IpcMode] = pydantic.Field(None, description='The IPC resource namespace to use for the containers in the task. Not supported in Fargate and Windows containers. Default: - IpcMode used by the task is not specified\n')
    memory_mib: typing.Optional[str] = pydantic.Field(None, description='The amount (in MiB) of memory used by the task. If using the EC2 launch type, this field is optional and any value can be used. If using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the cpu parameter: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - Available cpu values: 256 (.25 vCPU) 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - Available cpu values: 512 (.5 vCPU) 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - Available cpu values: 1024 (1 vCPU) Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - Available cpu values: 2048 (2 vCPU) Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - Available cpu values: 4096 (4 vCPU) Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) - Available cpu values: 8192 (8 vCPU) Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) - Available cpu values: 16384 (16 vCPU) Default: - Memory used by task is not specified.\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. On Fargate, the only supported networking mode is AwsVpc. Default: - NetworkMode.Bridge for EC2 & External tasks, AwsVpc for Fargate tasks.\n')
    pid_mode: typing.Optional[aws_cdk.aws_ecs.PidMode] = pydantic.Field(None, description='The process namespace to use for the containers in the task. Only supported for tasks that are hosted on AWS Fargate if the tasks are using platform version 1.4.0 or later (Linux). Only the TASK option is supported for Linux-based Fargate containers. Not supported in Windows containers. If pidMode is specified for a Fargate task, then runtimePlatform.operatingSystemFamily must also be specified. For more information, see `Task Definition Parameters <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#task_definition_pidmode>`_. Default: - PidMode used by the task is not specified\n')
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='The placement constraints to use for tasks in the service. You can specify a maximum of 10 constraints per task (this limit includes constraints in the task definition and those specified at run time). Not supported in Fargate. Default: - No placement constraints.\n')
    runtime_platform: typing.Union[models.aws_ecs.RuntimePlatformDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The operating system that your task definitions are running on. A runtimePlatform is supported only for tasks using the Fargate launch type. Default: - Undefined.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.')
    _init_params: typing.ClassVar[list[str]] = ['compatibility', 'cpu', 'ephemeral_storage_gib', 'inference_accelerators', 'ipc_mode', 'memory_mib', 'network_mode', 'pid_mode', 'placement_constraints', 'runtime_platform', 'execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['add_container', 'add_extension', 'add_firelens_log_router', 'add_inference_accelerator', 'add_placement_constraint', 'add_to_execution_role_policy', 'add_to_task_role_policy', 'add_volume', 'apply_removal_policy', 'find_container', 'find_port_mapping_by_name', 'grant_run', 'obtain_execution_role']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_task_definition_arn', 'from_task_definition_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.TaskDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_task_definition_arn', 'from_task_definition_attributes']
    ...


    from_task_definition_arn: typing.Optional[models.aws_ecs.TaskDefinitionDefFromTaskDefinitionArnParams] = pydantic.Field(None, description='Imports a task definition from the specified task definition ARN.\nThe task will have a compatibility of EC2+Fargate.')
    from_task_definition_attributes: typing.Optional[models.aws_ecs.TaskDefinitionDefFromTaskDefinitionAttributesParams] = pydantic.Field(None, description='Create a task definition from a task definition reference.')
    resource_config: typing.Optional[models.aws_ecs.TaskDefinitionDefConfig] = pydantic.Field(None)


class TaskDefinitionDefConfig(pydantic.BaseModel):
    add_container: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddContainerParams]] = pydantic.Field(None, description='Adds a new container to the task definition.')
    add_extension: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddExtensionParams]] = pydantic.Field(None, description='Adds the specified extension to the task definition.\nExtension can be used to apply a packaged modification to\na task definition.')
    add_firelens_log_router: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddFirelensLogRouterParams]] = pydantic.Field(None, description='Adds a firelens log router to the task definition.')
    add_inference_accelerator: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddInferenceAcceleratorParams]] = pydantic.Field(None, description='Adds an inference accelerator to the task definition.')
    add_placement_constraint: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddPlacementConstraintParams]] = pydantic.Field(None, description='Adds the specified placement constraint to the task definition.')
    add_to_execution_role_policy: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddToExecutionRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task execution IAM role.')
    add_to_task_role_policy: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddToTaskRolePolicyParams]] = pydantic.Field(None, description='Adds a policy statement to the task IAM role.')
    add_volume: typing.Optional[list[models.aws_ecs.TaskDefinitionDefAddVolumeParams]] = pydantic.Field(None, description='Adds a volume to the task definition.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    find_container: typing.Optional[list[models.aws_ecs.TaskDefinitionDefFindContainerParams]] = pydantic.Field(None, description='Returns the container that match the provided containerName.')
    find_port_mapping_by_name: typing.Optional[list[models.aws_ecs.TaskDefinitionDefFindPortMappingByNameParams]] = pydantic.Field(None, description='Determine the existing port mapping for the provided name.')
    grant_run: typing.Optional[list[models.aws_ecs.TaskDefinitionDefGrantRunParams]] = pydantic.Field(None, description='Grants permissions to run this task definition.\nThis will grant the following permissions:\n\n- ecs:RunTask\n- iam:PassRole')
    obtain_execution_role: typing.Optional[list[models.aws_ecs.TaskDefinitionDefObtainExecutionRoleParams]] = pydantic.Field(None, description="Creates the task execution IAM role if it doesn't already exist.")
    task_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)

class TaskDefinitionDefAddContainerParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.ContainerDefinitionDefConfig]] = pydantic.Field(None)
    ...

class TaskDefinitionDefAddExtensionParams(pydantic.BaseModel):
    extension: models.UnsupportedResource = pydantic.Field(..., description='-')
    ...

class TaskDefinitionDefAddFirelensLogRouterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    firelens_config: typing.Union[models.aws_ecs.FirelensConfigDef, dict[str, typing.Any]] = pydantic.Field(..., description='Firelens configuration.\n')
    image: models.aws_ecs.ContainerImageDef = pydantic.Field(..., description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /')
    return_config: typing.Optional[list[models.aws_ecs.FirelensLogRouterDefConfig]] = pydantic.Field(None)
    ...

class TaskDefinitionDefAddInferenceAcceleratorParams(pydantic.BaseModel):
    device_name: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator device name. Default: - empty\n')
    device_type: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator type to use. The allowed values are: eia2.medium, eia2.large and eia2.xlarge. Default: - empty')
    ...

class TaskDefinitionDefAddPlacementConstraintParams(pydantic.BaseModel):
    constraint: models.aws_ecs.PlacementConstraintDef = pydantic.Field(..., description='-')
    ...

class TaskDefinitionDefAddToExecutionRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class TaskDefinitionDefAddToTaskRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class TaskDefinitionDefAddVolumeParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='The name of the volume. Up to 255 letters (uppercase and lowercase), numbers, and hyphens are allowed. This name is referenced in the sourceVolume parameter of container definition mountPoints.\n')
    configured_at_launch: typing.Optional[bool] = pydantic.Field(None, description='Indicates if the volume should be configured at launch. Default: false\n')
    docker_volume_configuration: typing.Union[models.aws_ecs.DockerVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using Docker volumes. Docker volumes are only supported when you are using the EC2 launch type. Windows containers only support the use of the local driver. To use bind mounts, specify a host instead.\n')
    efs_volume_configuration: typing.Union[models.aws_ecs.EfsVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This property is specified when you are using Amazon EFS. When specifying Amazon EFS volumes in tasks using the Fargate launch type, Fargate creates a supervisor container that is responsible for managing the Amazon EFS volume. The supervisor container uses a small amount of the task's memory. The supervisor container is visible when querying the task metadata version 4 endpoint, but is not visible in CloudWatch Container Insights. Default: No Elastic FileSystem is setup\n")
    host: typing.Union[models.aws_ecs.HostDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using bind mount host volumes. Bind mount host volumes are supported when you are using either the EC2 or Fargate launch types. The contents of the host parameter determine whether your bind mount host volume persists on the host container instance and where it is stored. If the host parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data is not guaranteed to persist after the containers associated with it stop running.')
    ...

class TaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class TaskDefinitionDefFindContainerParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='-')
    ...

class TaskDefinitionDefFindPortMappingByNameParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description=': port mapping name.\n')
    ...

class TaskDefinitionDefFromTaskDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    task_definition_arn: str = pydantic.Field(..., description='-')
    ...

class TaskDefinitionDefFromTaskDefinitionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    compatibility: typing.Optional[aws_cdk.aws_ecs.Compatibility] = pydantic.Field(None, description='What launch types this task definition should be compatible with. Default: Compatibility.EC2_AND_FARGATE\n')
    task_definition_arn: str = pydantic.Field(..., description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.')
    ...

class TaskDefinitionDefGrantRunParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='Principal to grant consume rights to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class TaskDefinitionDefObtainExecutionRoleParams(pydantic.BaseModel):
    return_config: typing.Optional[list[models._interface_methods.AwsIamIRoleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_ecs.AddAutoScalingGroupCapacityOptions
class AddAutoScalingGroupCapacityOptionsDef(BaseStruct):
    can_containers_access_instance_role: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the containers can access the container instance role. Default: false\n')
    machine_image_type: typing.Optional[aws_cdk.aws_ecs.MachineImageType] = pydantic.Field(None, description='What type of machine image this is. Depending on the setting, different UserData will automatically be added to the ``AutoScalingGroup`` to configure it properly for use with ECS. If you create an ``AutoScalingGroup`` yourself and are adding it via ``addAutoScalingGroup()``, you must specify this value. If you are adding an ``autoScalingGroup`` via ``addCapacity``, this value will be determined from the ``machineImage`` you pass. Default: - Automatically determined from ``machineImage``, if available, otherwise ``MachineImageType.AMAZON_LINUX_2``.\n')
    spot_instance_draining: typing.Optional[bool] = pydantic.Field(None, description='Specify whether to enable Automated Draining for Spot Instances running Amazon ECS Services. For more information, see `Using Spot Instances <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html>`_. Default: false\n')
    topic_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='If ``AddAutoScalingGroupCapacityOptions.taskDrainTime`` is non-zero, then the ECS cluster creates an SNS Topic to as part of a system to drain instances of tasks when the instance is being shut down. If this property is provided, then this key will be used to encrypt the contents of that SNS Topic. See `SNS Data Encryption <https://docs.aws.amazon.com/sns/latest/dg/sns-data-encryption.html>`_ for more information. Default: The SNS Topic will not be encrypted.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_kms as kms\n\n    # key: kms.Key\n\n    add_auto_scaling_group_capacity_options = ecs.AddAutoScalingGroupCapacityOptions(\n        can_containers_access_instance_role=False,\n        machine_image_type=ecs.MachineImageType.AMAZON_LINUX_2,\n        spot_instance_draining=False,\n        topic_encryption_key=key\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['can_containers_access_instance_role', 'machine_image_type', 'spot_instance_draining', 'topic_encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AddAutoScalingGroupCapacityOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.AddCapacityOptions
class AddCapacityOptionsDef(BaseStruct):
    can_containers_access_instance_role: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the containers can access the container instance role. Default: false\n')
    machine_image_type: typing.Optional[aws_cdk.aws_ecs.MachineImageType] = pydantic.Field(None, description='What type of machine image this is. Depending on the setting, different UserData will automatically be added to the ``AutoScalingGroup`` to configure it properly for use with ECS. If you create an ``AutoScalingGroup`` yourself and are adding it via ``addAutoScalingGroup()``, you must specify this value. If you are adding an ``autoScalingGroup`` via ``addCapacity``, this value will be determined from the ``machineImage`` you pass. Default: - Automatically determined from ``machineImage``, if available, otherwise ``MachineImageType.AMAZON_LINUX_2``.\n')
    spot_instance_draining: typing.Optional[bool] = pydantic.Field(None, description='Specify whether to enable Automated Draining for Spot Instances running Amazon ECS Services. For more information, see `Using Spot Instances <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html>`_. Default: false\n')
    topic_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='If ``AddAutoScalingGroupCapacityOptions.taskDrainTime`` is non-zero, then the ECS cluster creates an SNS Topic to as part of a system to drain instances of tasks when the instance is being shut down. If this property is provided, then this key will be used to encrypt the contents of that SNS Topic. See `SNS Data Encryption <https://docs.aws.amazon.com/sns/latest/dg/sns-data-encryption.html>`_ for more information. Default: The SNS Topic will not be encrypted.\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether the instances can initiate connections to anywhere by default. Default: true\n')
    associate_public_ip_address: typing.Optional[bool] = pydantic.Field(None, description='Whether instances in the Auto Scaling Group should have public IP addresses associated with them. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: - Use subnet setting.\n')
    auto_scaling_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the Auto Scaling group. This name must be unique per Region per account. Default: - Auto generated by CloudFormation\n')
    block_devices: typing.Optional[typing.Sequence[typing.Union[models.aws_autoscaling.BlockDeviceDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specifies how block devices are exposed to the instance. You can specify virtual devices and EBS volumes. Each instance that is launched has an associated root device volume, either an Amazon EBS volume or an instance store volume. You can use block device mappings to specify additional EBS volumes or instance store volumes to attach to an instance when it is launched. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: - Uses the block device mapping of the AMI\n')
    capacity_rebalance: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether Capacity Rebalancing is enabled. When you turn on Capacity Rebalancing, Amazon EC2 Auto Scaling attempts to launch a Spot Instance whenever Amazon EC2 notifies that a Spot Instance is at an elevated risk of interruption. After launching a new instance, it then terminates an old instance. Default: false\n')
    cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Default scaling cooldown for this AutoScalingGroup. Default: Duration.minutes(5)\n')
    default_instance_warmup: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time, in seconds, until a newly launched instance can contribute to the Amazon CloudWatch metrics. This delay lets an instance finish initializing before Amazon EC2 Auto Scaling aggregates instance metrics, resulting in more reliable usage data. Set this value equal to the amount of time that it takes for resource consumption to become stable after an instance reaches the InService state. To optimize the performance of scaling policies that scale continuously, such as target tracking and step scaling policies, we strongly recommend that you enable the default instance warmup, even if its value is set to 0 seconds Default instance warmup will not be added if no value is specified Default: None\n')
    desired_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Initial amount of instances in the fleet. If this is set to a number, every deployment will reset the amount of instances to this number. It is recommended to leave this value blank. Default: minCapacity, and leave unchanged during deployment\n')
    group_metrics: typing.Optional[typing.Sequence[models.aws_autoscaling.GroupMetricsDef]] = pydantic.Field(None, description='Enable monitoring for group metrics, these metrics describe the group rather than any of its instances. To report all group metrics use ``GroupMetrics.all()`` Group metrics are reported in a granularity of 1 minute at no additional charge. Default: - no group metrics will be reported\n')
    health_check: typing.Optional[models.aws_autoscaling.HealthCheckDef] = pydantic.Field(None, description='Configuration for health checks. Default: - HealthCheck.ec2 with no grace period\n')
    ignore_unmodified_size_properties: typing.Optional[bool] = pydantic.Field(None, description="If the ASG has scheduled actions, don't reset unchanged group sizes. Only used if the ASG has scheduled actions (which may scale your ASG up or down regardless of cdk deployments). If true, the size of the group will only be reset if it has been changed in the CDK app. If false, the sizes will always be changed back to what they were in the CDK app on deployment. Default: true\n")
    instance_monitoring: typing.Optional[aws_cdk.aws_autoscaling.Monitoring] = pydantic.Field(None, description='Controls whether instances in this group are launched with detailed or basic monitoring. When detailed monitoring is enabled, Amazon CloudWatch generates metrics every minute and your account is charged a fee. When you disable detailed monitoring, CloudWatch generates metrics every 5 minutes. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: - Monitoring.DETAILED\n')
    key_name: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Name of SSH keypair to grant access to instances. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified You can either specify ``keyPair`` or ``keyName``, not both. Default: - No SSH access will be possible.\n')
    key_pair: typing.Optional[typing.Union[models.aws_ec2.KeyPairDef]] = pydantic.Field(None, description='The SSH keypair to grant access to the instance. Feature flag ``AUTOSCALING_GENERATE_LAUNCH_TEMPLATE`` must be enabled to use this property. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified. You can either specify ``keyPair`` or ``keyName``, not both. Default: - No SSH access will be possible.\n')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of instances in the fleet. Default: desiredCapacity\n')
    max_instance_lifetime: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time that an instance can be in service. The maximum duration applies to all current and future instances in the group. As an instance approaches its maximum duration, it is terminated and replaced, and cannot be used again. You must specify a value of at least 604,800 seconds (7 days). To clear a previously set value, leave this property undefined. Default: none\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum number of instances in the fleet. Default: 1\n')
    new_instances_protected_from_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Whether newly-launched instances are protected from termination by Amazon EC2 Auto Scaling when scaling in. By default, Auto Scaling can terminate an instance at any time after launch when scaling in an Auto Scaling Group, subject to the group's termination policy. However, you may wish to protect newly-launched instances from being scaled in if they are going to run critical applications that should not be prematurely terminated. This flag must be enabled if the Auto Scaling Group will be associated with an ECS Capacity Provider with managed termination protection. Default: false\n")
    notifications: typing.Optional[typing.Sequence[typing.Union[models.aws_autoscaling.NotificationConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configure autoscaling group to send notifications about fleet changes to an SNS topic(s). Default: - No fleet change notifications will be sent.\n')
    signals: typing.Optional[models.aws_autoscaling.SignalsDef] = pydantic.Field(None, description='Configure waiting for signals during deployment. Use this to pause the CloudFormation deployment to wait for the instances in the AutoScalingGroup to report successful startup during creation and updates. The UserData script needs to invoke ``cfn-signal`` with a success or failure code after it is done setting up the instance. Without waiting for signals, the CloudFormation deployment will proceed as soon as the AutoScalingGroup has been created or updated but before the instances in the group have been started. For example, to have instances wait for an Elastic Load Balancing health check before they signal success, add a health-check verification by using the cfn-init helper script. For an example, see the verify_instance_health command in the Auto Scaling rolling updates sample template: https://github.com/awslabs/aws-cloudformation-templates/blob/master/aws/services/AutoScaling/AutoScalingRollingUpdates.yaml Default: - Do not wait for signals\n')
    spot_price: typing.Optional[str] = pydantic.Field(None, description='The maximum hourly price (in USD) to be paid for any Spot Instance launched to fulfill the request. Spot Instances are launched when the price you specify exceeds the current Spot market price. ``launchTemplate`` and ``mixedInstancesPolicy`` must not be specified when this property is specified Default: none\n')
    ssm_session_permissions: typing.Optional[bool] = pydantic.Field(None, description='Add SSM session permissions to the instance role. Setting this to ``true`` adds the necessary permissions to connect to the instance using SSM Session Manager. You can do this from the AWS Console. NOTE: Setting this flag to ``true`` may not be enough by itself. You must also use an AMI that comes with the SSM Agent, or install the SSM Agent yourself. See `Working with SSM Agent <https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html>`_ in the SSM Developer Guide. Default: false\n')
    termination_policies: typing.Optional[typing.Sequence[aws_cdk.aws_autoscaling.TerminationPolicy]] = pydantic.Field(None, description='A policy or a list of policies that are used to select the instances to terminate. The policies are executed in the order that you list them. Default: - ``TerminationPolicy.DEFAULT``\n')
    termination_policy_custom_lambda_function_arn: typing.Optional[str] = pydantic.Field(None, description='A lambda function Arn that can be used as a custom termination policy to select the instances to terminate. This property must be specified if the TerminationPolicy.CUSTOM_LAMBDA_FUNCTION is used. Default: - No lambda function Arn will be supplied\n')
    update_policy: typing.Optional[models.aws_autoscaling.UpdatePolicyDef] = pydantic.Field(None, description="What to do when an AutoScalingGroup's instance configuration is changed. This is applied when any of the settings on the ASG are changed that affect how the instances should be created (VPC, instance type, startup scripts, etc.). It indicates how the existing instances should be replaced with new instances matching the new config. By default, nothing is done and only new instances are launched with the new config. Default: - ``UpdatePolicy.rollingUpdate()`` if using ``init``, ``UpdatePolicy.none()`` otherwise\n")
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place instances within the VPC. Default: - All Private subnets.\n')
    instance_type: typing.Union[models.aws_ec2.InstanceTypeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The EC2 instance type to use when launching instances into the AutoScalingGroup.\n')
    machine_image: typing.Optional[typing.Union[models.aws_ec2.AmazonLinux2022ImageSsmParameterDef, models.aws_ec2.AmazonLinux2023ImageSsmParameterDef, models.aws_ec2.AmazonLinux2ImageSsmParameterDef, models.aws_ec2.AmazonLinuxImageDef, models.aws_ec2.AmazonLinuxImageSsmParameterBaseDef, models.aws_ec2.GenericLinuxImageDef, models.aws_ec2.GenericSSMParameterImageDef, models.aws_ec2.GenericWindowsImageDef, models.aws_ec2.LookupMachineImageDef, models.aws_ec2.NatInstanceImageDef, models.aws_ec2.ResolveSsmParameterAtLaunchImageDef, models.aws_ec2.WindowsImageDef, models.aws_ecs.BottleRocketImageDef, models.aws_ecs.EcsOptimizedImageDef, models.aws_eks.EksOptimizedImageDef]] = pydantic.Field(None, description='The ECS-optimized AMI variant to use. The default is to use an ECS-optimized AMI of Amazon Linux 2 which is automatically updated to the latest version on every deployment. This will replace the instances in the AutoScalingGroup. Make sure you have not disabled task draining, to avoid downtime when the AMI updates. To use an image that does not update on every deployment, pass:: const machineImage = ecs.EcsOptimizedImage.amazonLinux2(ecs.AmiHardwareType.STANDARD, { cachedInContext: true, }); For more information, see `Amazon ECS-optimized AMIs <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_. You must define either ``machineImage`` or ``machineImageType``, not both. Default: - Automatically updated, ECS-optimized Amazon Linux 2\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n\n\n    cluster.add_capacity("graviton-cluster",\n        min_capacity=2,\n        instance_type=ec2.InstanceType("c6g.large"),\n        machine_image=ecs.EcsOptimizedImage.amazon_linux2(ecs.AmiHardwareType.ARM)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['can_containers_access_instance_role', 'machine_image_type', 'spot_instance_draining', 'topic_encryption_key', 'allow_all_outbound', 'associate_public_ip_address', 'auto_scaling_group_name', 'block_devices', 'capacity_rebalance', 'cooldown', 'default_instance_warmup', 'desired_capacity', 'group_metrics', 'health_check', 'ignore_unmodified_size_properties', 'instance_monitoring', 'key_name', 'key_pair', 'max_capacity', 'max_instance_lifetime', 'min_capacity', 'new_instances_protected_from_scale_in', 'notifications', 'signals', 'spot_price', 'ssm_session_permissions', 'termination_policies', 'termination_policy_custom_lambda_function_arn', 'update_policy', 'vpc_subnets', 'instance_type', 'machine_image']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AddCapacityOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.AddCapacityOptionsDefConfig] = pydantic.Field(None)


class AddCapacityOptionsDefConfig(pydantic.BaseModel):
    instance_type_config: typing.Optional[models.aws_ec2.InstanceTypeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.AppMeshProxyConfigurationConfigProps
class AppMeshProxyConfigurationConfigPropsDef(BaseStruct):
    container_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the container that will serve as the App Mesh proxy.\n')
    properties: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.AppMeshProxyConfigurationPropsDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The set of network configuration parameters to provide the Container Network Interface (CNI) plugin.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    app_mesh_proxy_configuration_config_props = ecs.AppMeshProxyConfigurationConfigProps(\n        container_name="containerName",\n        properties=ecs.AppMeshProxyConfigurationProps(\n            app_ports=[123],\n            proxy_egress_port=123,\n            proxy_ingress_port=123,\n\n            # the properties below are optional\n            egress_ignored_iPs=["egressIgnoredIPs"],\n            egress_ignored_ports=[123],\n            ignored_gID=123,\n            ignored_uID=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'properties']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AppMeshProxyConfigurationConfigProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.AppMeshProxyConfigurationProps
class AppMeshProxyConfigurationPropsDef(BaseStruct):
    app_ports: typing.Union[typing.Sequence[typing.Union[int, float]], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The list of ports that the application uses. Network traffic to these ports is forwarded to the ProxyIngressPort and ProxyEgressPort.\n')
    proxy_egress_port: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the port that outgoing traffic from the AppPorts is directed to.\n')
    proxy_ingress_port: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the port that incoming traffic to the AppPorts is directed to.\n')
    egress_ignored_i_ps: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The egress traffic going to these specified IP addresses is ignored and not redirected to the ProxyEgressPort. It can be an empty list.\n')
    egress_ignored_ports: typing.Optional[typing.Sequence[typing.Union[int, float]]] = pydantic.Field(None, description='The egress traffic going to these specified ports is ignored and not redirected to the ProxyEgressPort. It can be an empty list.\n')
    ignored_gid: typing.Union[int, float, None] = pydantic.Field(None, description='The group ID (GID) of the proxy container as defined by the user parameter in a container definition. This is used to ensure the proxy ignores its own traffic. If IgnoredUID is specified, this field can be empty.\n')
    ignored_uid: typing.Union[int, float, None] = pydantic.Field(None, description='The user ID (UID) of the proxy container as defined by the user parameter in a container definition. This is used to ensure the proxy ignores its own traffic. If IgnoredGID is specified, this field can be empty.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    app_mesh_proxy_configuration_props = ecs.AppMeshProxyConfigurationProps(\n        app_ports=[123],\n        proxy_egress_port=123,\n        proxy_ingress_port=123,\n\n        # the properties below are optional\n        egress_ignored_iPs=["egressIgnoredIPs"],\n        egress_ignored_ports=[123],\n        ignored_gID=123,\n        ignored_uID=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['app_ports', 'proxy_egress_port', 'proxy_ingress_port', 'egress_ignored_i_ps', 'egress_ignored_ports', 'ignored_gid', 'ignored_uid']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AppMeshProxyConfigurationProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.AsgCapacityProviderProps
class AsgCapacityProviderPropsDef(BaseStruct):
    can_containers_access_instance_role: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the containers can access the container instance role. Default: false\n')
    machine_image_type: typing.Optional[aws_cdk.aws_ecs.MachineImageType] = pydantic.Field(None, description='What type of machine image this is. Depending on the setting, different UserData will automatically be added to the ``AutoScalingGroup`` to configure it properly for use with ECS. If you create an ``AutoScalingGroup`` yourself and are adding it via ``addAutoScalingGroup()``, you must specify this value. If you are adding an ``autoScalingGroup`` via ``addCapacity``, this value will be determined from the ``machineImage`` you pass. Default: - Automatically determined from ``machineImage``, if available, otherwise ``MachineImageType.AMAZON_LINUX_2``.\n')
    spot_instance_draining: typing.Optional[bool] = pydantic.Field(None, description='Specify whether to enable Automated Draining for Spot Instances running Amazon ECS Services. For more information, see `Using Spot Instances <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html>`_. Default: false\n')
    topic_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='If ``AddAutoScalingGroupCapacityOptions.taskDrainTime`` is non-zero, then the ECS cluster creates an SNS Topic to as part of a system to drain instances of tasks when the instance is being shut down. If this property is provided, then this key will be used to encrypt the contents of that SNS Topic. See `SNS Data Encryption <https://docs.aws.amazon.com/sns/latest/dg/sns-data-encryption.html>`_ for more information. Default: The SNS Topic will not be encrypted.\n')
    auto_scaling_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_autoscaling.AutoScalingGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The autoscaling group to add as a Capacity Provider. Warning: When passing an imported resource using ``AutoScalingGroup.fromAutoScalingGroupName`` along with ``enableManagedTerminationProtection: true``, the ``AsgCapacityProvider`` construct will not be able to enforce the option ``newInstancesProtectedFromScaleIn`` of the ``AutoScalingGroup``. In this case the constructor of ``AsgCapacityProvider`` will throw an exception.\n')
    capacity_provider_name: typing.Optional[str] = pydantic.Field(None, description='The name of the capacity provider. If a name is specified, it cannot start with ``aws``, ``ecs``, or ``fargate``. If no name is specified, a default name in the CFNStackName-CFNResourceName-RandomString format is used. If the stack name starts with ``aws``, ``ecs``, or ``fargate``, a unique resource name is generated that starts with ``cp-``. Default: CloudFormation-generated name\n')
    enable_managed_draining: typing.Optional[bool] = pydantic.Field(None, description='Managed instance draining facilitates graceful termination of Amazon ECS instances. This allows your service workloads to stop safely and be rescheduled to non-terminating instances. Infrastructure maintenance and updates are preformed without disruptions to workloads. To use managed instance draining, set enableManagedDraining to true. Default: true\n')
    enable_managed_scaling: typing.Optional[bool] = pydantic.Field(None, description="When enabled the scale-in and scale-out actions of the cluster's Auto Scaling Group will be managed for you. This means your cluster will automatically scale instances based on the load your tasks put on the cluster. For more information, see `Using Managed Scaling <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/asg-capacity-providers.html#asg-capacity-providers-managed-scaling>`_ in the ECS Developer Guide. Default: true\n")
    enable_managed_termination_protection: typing.Optional[bool] = pydantic.Field(None, description='When enabled the Auto Scaling Group will only terminate EC2 instances that no longer have running non-daemon tasks. Scale-in protection will be automatically enabled on instances. When all non-daemon tasks are stopped on an instance, ECS initiates the scale-in process and turns off scale-in protection for the instance. The Auto Scaling Group can then terminate the instance. For more information see `Managed termination protection <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-auto-scaling.html#managed-termination-protection>`_ in the ECS Developer Guide. Managed scaling must also be enabled. Default: true\n')
    instance_warmup_period: typing.Union[int, float, None] = pydantic.Field(None, description='The period of time, in seconds, after a newly launched Amazon EC2 instance can contribute to CloudWatch metrics for Auto Scaling group. Must be between 0 and 10000. Default: 300\n')
    maximum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum scaling step size. In most cases this should be left alone. Default: 1000\n')
    minimum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum scaling step size. In most cases this should be left alone. Default: 1\n')
    target_capacity_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Target capacity percent. In most cases this should be left alone. Default: 100\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    launch_template = ec2.LaunchTemplate(self, "ASG-LaunchTemplate",\n        instance_type=ec2.InstanceType("t3.medium"),\n        machine_image=ecs.EcsOptimizedImage.amazon_linux2(),\n        user_data=ec2.UserData.for_linux()\n    )\n\n    auto_scaling_group = autoscaling.AutoScalingGroup(self, "ASG",\n        vpc=vpc,\n        mixed_instances_policy=autoscaling.MixedInstancesPolicy(\n            instances_distribution=autoscaling.InstancesDistribution(\n                on_demand_percentage_above_base_capacity=50\n            ),\n            launch_template=launch_template\n        )\n    )\n\n    cluster = ecs.Cluster(self, "Cluster", vpc=vpc)\n\n    capacity_provider = ecs.AsgCapacityProvider(self, "AsgCapacityProvider",\n        auto_scaling_group=auto_scaling_group,\n        machine_image_type=ecs.MachineImageType.AMAZON_LINUX_2\n    )\n\n    cluster.add_asg_capacity_provider(capacity_provider)\n')
    _init_params: typing.ClassVar[list[str]] = ['can_containers_access_instance_role', 'machine_image_type', 'spot_instance_draining', 'topic_encryption_key', 'auto_scaling_group', 'capacity_provider_name', 'enable_managed_draining', 'enable_managed_scaling', 'enable_managed_termination_protection', 'instance_warmup_period', 'maximum_scaling_step_size', 'minimum_scaling_step_size', 'target_capacity_percent']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AsgCapacityProviderProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.AsgCapacityProviderPropsDefConfig] = pydantic.Field(None)


class AsgCapacityProviderPropsDefConfig(pydantic.BaseModel):
    auto_scaling_group_config: typing.Optional[models._interface_methods.AwsAutoscalingIAutoScalingGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.AssetImageProps
class AssetImagePropsDef(BaseStruct):
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecr_assets as ecr_assets\n    from aws_cdk import aws_ecs as ecs\n\n    # network_mode: ecr_assets.NetworkMode\n    # platform: ecr_assets.Platform\n\n    asset_image_props = ecs.AssetImageProps(\n        asset_name="assetName",\n        build_args={\n            "build_args_key": "buildArgs"\n        },\n        build_secrets={\n            "build_secrets_key": "buildSecrets"\n        },\n        build_ssh="buildSsh",\n        cache_disabled=False,\n        cache_from=[ecr_assets.DockerCacheOption(\n            type="type",\n\n            # the properties below are optional\n            params={\n                "params_key": "params"\n            }\n        )],\n        cache_to=ecr_assets.DockerCacheOption(\n            type="type",\n\n            # the properties below are optional\n            params={\n                "params_key": "params"\n            }\n        ),\n        exclude=["exclude"],\n        extra_hash="extraHash",\n        file="file",\n        follow_symlinks=cdk.SymlinkFollowMode.NEVER,\n        ignore_mode=cdk.IgnoreMode.GLOB,\n        invalidation=ecr_assets.DockerImageAssetInvalidationOptions(\n            build_args=False,\n            build_secrets=False,\n            build_ssh=False,\n            extra_hash=False,\n            file=False,\n            network_mode=False,\n            outputs=False,\n            platform=False,\n            repository_name=False,\n            target=False\n        ),\n        network_mode=network_mode,\n        outputs=["outputs"],\n        platform=platform,\n        target="target"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['exclude', 'follow_symlinks', 'ignore_mode', 'extra_hash', 'asset_name', 'build_args', 'build_secrets', 'build_ssh', 'cache_disabled', 'cache_from', 'cache_to', 'file', 'invalidation', 'network_mode', 'outputs', 'platform', 'target']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AssetImageProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.AssociateCloudMapServiceOptions
class AssociateCloudMapServiceOptionsDef(BaseStruct):
    service: typing.Union[_REQUIRED_INIT_PARAM, models.aws_servicediscovery.ServiceDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cloudmap service to register with.\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container\n\n:exampleMetadata: infused\n\nExample::\n\n    # cloud_map_service: cloudmap.Service\n    # ecs_service: ecs.FargateService\n\n\n    ecs_service.associate_cloud_map_service(\n        service=cloud_map_service\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['service', 'container', 'container_port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AssociateCloudMapServiceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.AssociateCloudMapServiceOptionsDefConfig] = pydantic.Field(None)


class AssociateCloudMapServiceOptionsDefConfig(pydantic.BaseModel):
    service_config: typing.Optional[models._interface_methods.AwsServicediscoveryIServiceDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.AuthorizationConfig
class AuthorizationConfigDef(BaseStruct):
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The access point ID to use. If an access point is specified, the root directory value will be relative to the directory set for the access point. If specified, transit encryption must be enabled in the EFSVolumeConfiguration. Default: No id\n')
    iam: typing.Optional[str] = pydantic.Field(None, description='Whether or not to use the Amazon ECS task IAM role defined in a task definition when mounting the Amazon EFS file system. If enabled, transit encryption must be enabled in the EFSVolumeConfiguration. Valid values: ENABLED | DISABLED Default: If this parameter is omitted, the default value of DISABLED is used.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    authorization_config = ecs.AuthorizationConfig(\n        access_point_id="accessPointId",\n        iam="iam"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_point_id', 'iam']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AuthorizationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.AwsLogDriverProps
class AwsLogDriverPropsDef(BaseStruct):
    stream_prefix: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Prefix for the log streams. The awslogs-stream-prefix option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs. If you specify a prefix with this option, then the log stream takes the following format:: prefix-name/container-name/ecs-task-id\n')
    datetime_format: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. Default: - No multiline matching.\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group to log to. Default: - A log group is automatically created.\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='The number of days log events are kept in CloudWatch Logs when the log group is automatically created by this construct. Default: - Logs never expire.\n')
    max_buffer_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='When AwsLogDriverMode.NON_BLOCKING is configured, this parameter controls the size of the non-blocking buffer used to temporarily store messages. This parameter is not valid with AwsLogDriverMode.BLOCKING. Default: - 1 megabyte if driver mode is non-blocking, otherwise this property is not set\n')
    mode: typing.Optional[aws_cdk.aws_ecs.AwsLogDriverMode] = pydantic.Field(None, description='The delivery mode of log messages from the container to awslogs. Default: - AwsLogDriverMode.BLOCKING\n')
    multiline_pattern: typing.Optional[str] = pydantic.Field(None, description='This option defines a multiline start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern. Thus the matched line is the delimiter between log messages. This option is ignored if datetimeFormat is also configured. Default: - No multiline matching.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Create a Task Definition for the Windows container to start\n    task_definition = ecs.FargateTaskDefinition(self, "TaskDef",\n        runtime_platform=ecs.RuntimePlatform(\n            operating_system_family=ecs.OperatingSystemFamily.WINDOWS_SERVER_2019_CORE,\n            cpu_architecture=ecs.CpuArchitecture.X86_64\n        ),\n        cpu=1024,\n        memory_limit_mi_b=2048\n    )\n\n    task_definition.add_container("windowsservercore",\n        logging=ecs.LogDriver.aws_logs(stream_prefix="win-iis-on-fargate"),\n        port_mappings=[ecs.PortMapping(container_port=80)],\n        image=ecs.ContainerImage.from_registry("mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['stream_prefix', 'datetime_format', 'log_group', 'log_retention', 'max_buffer_size', 'mode', 'multiline_pattern']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.AwsLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.BaseLogDriverProps
class BaseLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    base_log_driver_props = ecs.BaseLogDriverProps(\n        env=["env"],\n        env_regex="envRegex",\n        labels=["labels"],\n        tag="tag"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BaseLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.BaseMountPoint
class BaseMountPointDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path on the container to mount the host volume at.\n')
    read_only: typing.Union[bool, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether to give the container read-only access to the volume. If this value is true, the container has read-only access to the volume. If this value is false, then the container can write to the volume.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    base_mount_point = ecs.BaseMountPoint(\n        container_path="containerPath",\n        read_only=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'read_only']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BaseMountPoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.BaseServiceOptions
class BaseServiceOptionsDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_servicediscovery as servicediscovery\n\n    # cluster: ecs.Cluster\n    # container_definition: ecs.ContainerDefinition\n    # log_driver: ecs.LogDriver\n    # namespace: servicediscovery.INamespace\n    # service_managed_volume: ecs.ServiceManagedVolume\n    # task_definition_revision: ecs.TaskDefinitionRevision\n\n    base_service_options = ecs.BaseServiceOptions(\n        cluster=cluster,\n\n        # the properties below are optional\n        capacity_provider_strategies=[ecs.CapacityProviderStrategy(\n            capacity_provider="capacityProvider",\n\n            # the properties below are optional\n            base=123,\n            weight=123\n        )],\n        circuit_breaker=ecs.DeploymentCircuitBreaker(\n            enable=False,\n            rollback=False\n        ),\n        cloud_map_options=ecs.CloudMapOptions(\n            cloud_map_namespace=namespace,\n            container=container_definition,\n            container_port=123,\n            dns_record_type=servicediscovery.DnsRecordType.A,\n            dns_ttl=cdk.Duration.minutes(30),\n            failure_threshold=123,\n            name="name"\n        ),\n        deployment_alarms=ecs.DeploymentAlarmConfig(\n            alarm_names=["alarmNames"],\n\n            # the properties below are optional\n            behavior=ecs.AlarmBehavior.ROLLBACK_ON_ALARM\n        ),\n        deployment_controller=ecs.DeploymentController(\n            type=ecs.DeploymentControllerType.ECS\n        ),\n        desired_count=123,\n        enable_eCSManaged_tags=False,\n        enable_execute_command=False,\n        health_check_grace_period=cdk.Duration.minutes(30),\n        max_healthy_percent=123,\n        min_healthy_percent=123,\n        propagate_tags=ecs.PropagatedTagSource.SERVICE,\n        service_connect_configuration=ecs.ServiceConnectProps(\n            log_driver=log_driver,\n            namespace="namespace",\n            services=[ecs.ServiceConnectService(\n                port_mapping_name="portMappingName",\n\n                # the properties below are optional\n                discovery_name="discoveryName",\n                dns_name="dnsName",\n                idle_timeout=cdk.Duration.minutes(30),\n                ingress_port_override=123,\n                per_request_timeout=cdk.Duration.minutes(30),\n                port=123\n            )]\n        ),\n        service_name="serviceName",\n        task_definition_revision=task_definition_revision,\n        volume_configurations=[service_managed_volume]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BaseServiceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.BaseServiceProps
class BaseServicePropsDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined\n')
    launch_type: typing.Union[aws_cdk.aws_ecs.LaunchType, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The launch type on which to run your service. LaunchType will be omitted if capacity provider strategies are specified on the service.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_servicediscovery as servicediscovery\n\n    # cluster: ecs.Cluster\n    # container_definition: ecs.ContainerDefinition\n    # log_driver: ecs.LogDriver\n    # namespace: servicediscovery.INamespace\n    # service_managed_volume: ecs.ServiceManagedVolume\n    # task_definition_revision: ecs.TaskDefinitionRevision\n\n    base_service_props = ecs.BaseServiceProps(\n        cluster=cluster,\n        launch_type=ecs.LaunchType.EC2,\n\n        # the properties below are optional\n        capacity_provider_strategies=[ecs.CapacityProviderStrategy(\n            capacity_provider="capacityProvider",\n\n            # the properties below are optional\n            base=123,\n            weight=123\n        )],\n        circuit_breaker=ecs.DeploymentCircuitBreaker(\n            enable=False,\n            rollback=False\n        ),\n        cloud_map_options=ecs.CloudMapOptions(\n            cloud_map_namespace=namespace,\n            container=container_definition,\n            container_port=123,\n            dns_record_type=servicediscovery.DnsRecordType.A,\n            dns_ttl=cdk.Duration.minutes(30),\n            failure_threshold=123,\n            name="name"\n        ),\n        deployment_alarms=ecs.DeploymentAlarmConfig(\n            alarm_names=["alarmNames"],\n\n            # the properties below are optional\n            behavior=ecs.AlarmBehavior.ROLLBACK_ON_ALARM\n        ),\n        deployment_controller=ecs.DeploymentController(\n            type=ecs.DeploymentControllerType.ECS\n        ),\n        desired_count=123,\n        enable_eCSManaged_tags=False,\n        enable_execute_command=False,\n        health_check_grace_period=cdk.Duration.minutes(30),\n        max_healthy_percent=123,\n        min_healthy_percent=123,\n        propagate_tags=ecs.PropagatedTagSource.SERVICE,\n        service_connect_configuration=ecs.ServiceConnectProps(\n            log_driver=log_driver,\n            namespace="namespace",\n            services=[ecs.ServiceConnectService(\n                port_mapping_name="portMappingName",\n\n                # the properties below are optional\n                discovery_name="discoveryName",\n                dns_name="dnsName",\n                idle_timeout=cdk.Duration.minutes(30),\n                ingress_port_override=123,\n                per_request_timeout=cdk.Duration.minutes(30),\n                port=123\n            )]\n        ),\n        service_name="serviceName",\n        task_definition_revision=task_definition_revision,\n        volume_configurations=[service_managed_volume]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations', 'launch_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BaseServiceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.BottleRocketImageProps
class BottleRocketImagePropsDef(BaseStruct):
    architecture: typing.Optional[aws_cdk.aws_ec2.InstanceArchitecture] = pydantic.Field(None, description='The CPU architecture. Default: - x86_64\n')
    cached_in_context: typing.Optional[bool] = pydantic.Field(None, description="Whether the AMI ID is cached to be stable between deployments. By default, the newest image is used on each deployment. This will cause instances to be replaced whenever a new version is released, and may cause downtime if there aren't enough running instances in the AutoScalingGroup to reschedule the tasks on. If set to true, the AMI ID will be cached in ``cdk.context.json`` and the same value will be used on future runs. Your instances will not be replaced but your AMI version will grow old over time. To refresh the AMI lookup, you will have to evict the value from the cache using the ``cdk context`` command. See https://docs.aws.amazon.com/cdk/latest/guide/context.html for more information. Can not be set to ``true`` in environment-agnostic stacks. Default: false\n")
    variant: typing.Optional[aws_cdk.aws_ecs.BottlerocketEcsVariant] = pydantic.Field(None, description='The Amazon ECS variant to use. Default: - BottlerocketEcsVariant.AWS_ECS_1\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n\n\n    cluster.add_capacity("bottlerocket-asg",\n        instance_type=ec2.InstanceType("p3.2xlarge"),\n        machine_image=ecs.BottleRocketImage(\n            variant=ecs.BottlerocketEcsVariant.AWS_ECS_2_NVIDIA\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['architecture', 'cached_in_context', 'variant']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.BottleRocketImageProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CapacityProviderStrategy
class CapacityProviderStrategyDef(BaseStruct):
    capacity_provider: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the capacity provider.\n')
    base: typing.Union[int, float, None] = pydantic.Field(None, description='The base value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a base defined. If no value is specified, the default value of 0 is used. Default: - none\n')
    weight: typing.Union[int, float, None] = pydantic.Field(None, description='The weight value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The weight value is taken into consideration after the base value, if defined, is satisfied. Default: - 0\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    capacity_provider_strategy = ecs.CapacityProviderStrategy(\n        capacity_provider="capacityProvider",\n\n        # the properties below are optional\n        base=123,\n        weight=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capacity_provider', 'base', 'weight']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CapacityProviderStrategy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCapacityProvider.AutoScalingGroupProviderProperty
class CfnCapacityProvider_AutoScalingGroupProviderPropertyDef(BaseStruct):
    auto_scaling_group_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) that identifies the Auto Scaling group, or the Auto Scaling group name.\n')
    managed_draining: typing.Optional[str] = pydantic.Field(None, description='The managed draining option for the Auto Scaling group capacity provider. When you enable this, Amazon ECS manages and gracefully drains the EC2 container instances that are in the Auto Scaling group capacity provider.\n')
    managed_scaling: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCapacityProvider_ManagedScalingPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The managed scaling settings for the Auto Scaling group capacity provider.\n')
    managed_termination_protection: typing.Optional[str] = pydantic.Field(None, description='The managed termination protection setting to use for the Auto Scaling group capacity provider. This determines whether the Auto Scaling group has managed termination protection. The default is off. .. epigraph:: When using managed termination protection, managed scaling must also be used otherwise managed termination protection doesn\'t work. When managed termination protection is on, Amazon ECS prevents the Amazon EC2 instances in an Auto Scaling group that contain tasks from being terminated during a scale-in action. The Auto Scaling group and each instance in the Auto Scaling group must have instance protection from scale-in actions on as well. For more information, see `Instance Protection <https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#instance-protection>`_ in the *AWS Auto Scaling User Guide* . When managed termination protection is off, your Amazon EC2 instances aren\'t protected from termination when the Auto Scaling group scales in.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-capacityprovider-autoscalinggroupprovider.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    auto_scaling_group_provider_property = ecs.CfnCapacityProvider.AutoScalingGroupProviderProperty(\n        auto_scaling_group_arn="autoScalingGroupArn",\n\n        # the properties below are optional\n        managed_draining="managedDraining",\n        managed_scaling=ecs.CfnCapacityProvider.ManagedScalingProperty(\n            instance_warmup_period=123,\n            maximum_scaling_step_size=123,\n            minimum_scaling_step_size=123,\n            status="status",\n            target_capacity=123\n        ),\n        managed_termination_protection="managedTerminationProtection"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_scaling_group_arn', 'managed_draining', 'managed_scaling', 'managed_termination_protection']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCapacityProvider.AutoScalingGroupProviderProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCapacityProvider.ManagedScalingProperty
class CfnCapacityProvider_ManagedScalingPropertyDef(BaseStruct):
    instance_warmup_period: typing.Union[int, float, None] = pydantic.Field(None, description='The period of time, in seconds, after a newly launched Amazon EC2 instance can contribute to CloudWatch metrics for Auto Scaling group. If this parameter is omitted, the default value of ``300`` seconds is used.\n')
    maximum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of Amazon EC2 instances that Amazon ECS will scale out at one time. The scale in process is not affected by this parameter. If this parameter is omitted, the default value of ``10000`` is used.\n')
    minimum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of Amazon EC2 instances that Amazon ECS will scale out at one time. The scale in process is not affected by this parameter If this parameter is omitted, the default value of ``1`` is used. When additional capacity is required, Amazon ECS will scale up the minimum scaling step size even if the actual demand is less than the minimum scaling step size. If you use a capacity provider with an Auto Scaling group configured with more than one Amazon EC2 instance type or Availability Zone, Amazon ECS will scale up by the exact minimum scaling step size value and will ignore both the maximum scaling step size as well as the capacity demand.\n')
    status: typing.Optional[str] = pydantic.Field(None, description='Determines whether to use managed scaling for the capacity provider.\n')
    target_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The target capacity utilization as a percentage for the capacity provider. The specified value must be greater than ``0`` and less than or equal to ``100`` . For example, if you want the capacity provider to maintain 10% spare capacity, then that means the utilization is 90%, so use a ``targetCapacity`` of ``90`` . The default value of ``100`` percent results in the Amazon EC2 instances in your Auto Scaling group being completely used.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-capacityprovider-managedscaling.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    managed_scaling_property = ecs.CfnCapacityProvider.ManagedScalingProperty(\n        instance_warmup_period=123,\n        maximum_scaling_step_size=123,\n        minimum_scaling_step_size=123,\n        status="status",\n        target_capacity=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_warmup_period', 'maximum_scaling_step_size', 'minimum_scaling_step_size', 'status', 'target_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCapacityProvider.ManagedScalingProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCluster.CapacityProviderStrategyItemProperty
class CfnCluster_CapacityProviderStrategyItemPropertyDef(BaseStruct):
    base: typing.Union[int, float, None] = pydantic.Field(None, description='The *base* value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a *base* defined. If no value is specified, the default value of ``0`` is used.\n')
    capacity_provider: typing.Optional[str] = pydantic.Field(None, description='The short name of the capacity provider.\n')
    weight: typing.Union[int, float, None] = pydantic.Field(None, description='The *weight* value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The ``weight`` value is taken into consideration after the ``base`` value, if defined, is satisfied. If no ``weight`` value is specified, the default value of ``0`` is used. When multiple capacity providers are specified within a capacity provider strategy, at least one of the capacity providers must have a weight value greater than zero and any capacity providers with a weight of ``0`` can\'t be used to place tasks. If you specify multiple capacity providers in a strategy that all have a weight of ``0`` , any ``RunTask`` or ``CreateService`` actions using the capacity provider strategy will fail. An example scenario for using weights is defining a strategy that contains two capacity providers and both have a weight of ``1`` , then when the ``base`` is satisfied, the tasks will be split evenly across the two capacity providers. Using that same logic, if you specify a weight of ``1`` for *capacityProviderA* and a weight of ``4`` for *capacityProviderB* , then for every one task that\'s run using *capacityProviderA* , four tasks would use *capacityProviderB* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-cluster-capacityproviderstrategyitem.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    capacity_provider_strategy_item_property = ecs.CfnCluster.CapacityProviderStrategyItemProperty(\n        base=123,\n        capacity_provider="capacityProvider",\n        weight=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['base', 'capacity_provider', 'weight']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster.CapacityProviderStrategyItemProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCluster.ClusterConfigurationProperty
class CfnCluster_ClusterConfigurationPropertyDef(BaseStruct):
    execute_command_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ExecuteCommandConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The details of the execute command configuration.\n')
    managed_storage_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ManagedStorageConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The details of the managed storage configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-cluster-clusterconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cluster_configuration_property = ecs.CfnCluster.ClusterConfigurationProperty(\n        execute_command_configuration=ecs.CfnCluster.ExecuteCommandConfigurationProperty(\n            kms_key_id="kmsKeyId",\n            log_configuration=ecs.CfnCluster.ExecuteCommandLogConfigurationProperty(\n                cloud_watch_encryption_enabled=False,\n                cloud_watch_log_group_name="cloudWatchLogGroupName",\n                s3_bucket_name="s3BucketName",\n                s3_encryption_enabled=False,\n                s3_key_prefix="s3KeyPrefix"\n            ),\n            logging="logging"\n        ),\n        managed_storage_configuration=ecs.CfnCluster.ManagedStorageConfigurationProperty(\n            fargate_ephemeral_storage_kms_key_id="fargateEphemeralStorageKmsKeyId",\n            kms_key_id="kmsKeyId"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execute_command_configuration', 'managed_storage_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster.ClusterConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCluster.ClusterSettingsProperty
class CfnCluster_ClusterSettingsPropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the cluster setting. The value is ``containerInsights`` .\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value to set for the cluster setting. The supported values are ``enabled`` and ``disabled`` . If you set ``name`` to ``containerInsights`` and ``value`` to ``enabled`` , CloudWatch Container Insights will be on for the cluster, otherwise it will be off unless the ``containerInsights`` account setting is turned on. If a cluster value is specified, it will override the ``containerInsights`` value set with `PutAccountSetting <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PutAccountSetting.html>`_ or `PutAccountSettingDefault <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PutAccountSettingDefault.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-cluster-clustersettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cluster_settings_property = ecs.CfnCluster.ClusterSettingsProperty(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster.ClusterSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCluster.ExecuteCommandConfigurationProperty
class CfnCluster_ExecuteCommandConfigurationPropertyDef(BaseStruct):
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='Specify an AWS Key Management Service key ID to encrypt the data between the local client and the container.\n')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ExecuteCommandLogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The log configuration for the results of the execute command actions. The logs can be sent to CloudWatch Logs or an Amazon S3 bucket. When ``logging=OVERRIDE`` is specified, a ``logConfiguration`` must be provided.\n')
    logging: typing.Optional[str] = pydantic.Field(None, description='The log setting to use for redirecting logs for your execute command results. The following log settings are available. - ``NONE`` : The execute command session is not logged. - ``DEFAULT`` : The ``awslogs`` configuration in the task definition is used. If no logging parameter is specified, it defaults to this value. If no ``awslogs`` log driver is configured in the task definition, the output won\'t be logged. - ``OVERRIDE`` : Specify the logging details as a part of ``logConfiguration`` . If the ``OVERRIDE`` logging option is specified, the ``logConfiguration`` is required.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-cluster-executecommandconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    execute_command_configuration_property = ecs.CfnCluster.ExecuteCommandConfigurationProperty(\n        kms_key_id="kmsKeyId",\n        log_configuration=ecs.CfnCluster.ExecuteCommandLogConfigurationProperty(\n            cloud_watch_encryption_enabled=False,\n            cloud_watch_log_group_name="cloudWatchLogGroupName",\n            s3_bucket_name="s3BucketName",\n            s3_encryption_enabled=False,\n            s3_key_prefix="s3KeyPrefix"\n        ),\n        logging="logging"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['kms_key_id', 'log_configuration', 'logging']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster.ExecuteCommandConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCluster.ExecuteCommandLogConfigurationProperty
class CfnCluster_ExecuteCommandLogConfigurationPropertyDef(BaseStruct):
    cloud_watch_encryption_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Determines whether to use encryption on the CloudWatch logs. If not specified, encryption will be off.\n')
    cloud_watch_log_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudWatch log group to send logs to. .. epigraph:: The CloudWatch log group must already be created.\n')
    s3_bucket_name: typing.Optional[str] = pydantic.Field(None, description='The name of the S3 bucket to send logs to. .. epigraph:: The S3 bucket must already be created.\n')
    s3_encryption_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Determines whether to use encryption on the S3 logs. If not specified, encryption is not used.\n')
    s3_key_prefix: typing.Optional[str] = pydantic.Field(None, description='An optional folder in the S3 bucket to place logs in.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-cluster-executecommandlogconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    execute_command_log_configuration_property = ecs.CfnCluster.ExecuteCommandLogConfigurationProperty(\n        cloud_watch_encryption_enabled=False,\n        cloud_watch_log_group_name="cloudWatchLogGroupName",\n        s3_bucket_name="s3BucketName",\n        s3_encryption_enabled=False,\n        s3_key_prefix="s3KeyPrefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_watch_encryption_enabled', 'cloud_watch_log_group_name', 's3_bucket_name', 's3_encryption_enabled', 's3_key_prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster.ExecuteCommandLogConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCluster.ManagedStorageConfigurationProperty
class CfnCluster_ManagedStorageConfigurationPropertyDef(BaseStruct):
    fargate_ephemeral_storage_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='Specify the AWS Key Management Service key ID for the Fargate ephemeral storage.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='Specify a AWS Key Management Service key ID to encrypt the managed storage.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-cluster-managedstorageconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    managed_storage_configuration_property = ecs.CfnCluster.ManagedStorageConfigurationProperty(\n        fargate_ephemeral_storage_kms_key_id="fargateEphemeralStorageKmsKeyId",\n        kms_key_id="kmsKeyId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['fargate_ephemeral_storage_kms_key_id', 'kms_key_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster.ManagedStorageConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnCluster.ServiceConnectDefaultsProperty
class CfnCluster_ServiceConnectDefaultsPropertyDef(BaseStruct):
    namespace: typing.Optional[str] = pydantic.Field(None, description='The namespace name or full Amazon Resource Name (ARN) of the AWS Cloud Map namespace that\'s used when you create a service and don\'t specify a Service Connect configuration. The namespace name can include up to 1024 characters. The name is case-sensitive. The name can\'t include hyphens (-), tilde (~), greater than (>), less than (<), or slash (/). If you enter an existing namespace name or ARN, then that namespace will be used. Any namespace type is supported. The namespace must be in this account and this AWS Region. If you enter a new name, a AWS Cloud Map namespace will be created. Amazon ECS creates a AWS Cloud Map namespace with the "API calls" method of instance discovery only. This instance discovery method is the "HTTP" namespace type in the AWS Command Line Interface . Other types of instance discovery aren\'t used by Service Connect. If you update the cluster with an empty string ``""`` for the namespace name, the cluster configuration for Service Connect is removed. Note that the namespace will remain in AWS Cloud Map and must be deleted separately. For more information about AWS Cloud Map , see `Working with Services <https://docs.aws.amazon.com/cloud-map/latest/dg/working-with-services.html>`_ in the *AWS Cloud Map Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-cluster-serviceconnectdefaults.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_connect_defaults_property = ecs.CfnCluster.ServiceConnectDefaultsProperty(\n        namespace="namespace"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['namespace']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster.ServiceConnectDefaultsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnClusterCapacityProviderAssociations.CapacityProviderStrategyProperty
class CfnClusterCapacityProviderAssociations_CapacityProviderStrategyPropertyDef(BaseStruct):
    capacity_provider: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name of the capacity provider.\n')
    base: typing.Union[int, float, None] = pydantic.Field(None, description='The *base* value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a *base* defined. If no value is specified, the default value of ``0`` is used.\n')
    weight: typing.Union[int, float, None] = pydantic.Field(None, description='The *weight* value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The ``weight`` value is taken into consideration after the ``base`` value, if defined, is satisfied. If no ``weight`` value is specified, the default value of ``0`` is used. When multiple capacity providers are specified within a capacity provider strategy, at least one of the capacity providers must have a weight value greater than zero and any capacity providers with a weight of ``0`` can\'t be used to place tasks. If you specify multiple capacity providers in a strategy that all have a weight of ``0`` , any ``RunTask`` or ``CreateService`` actions using the capacity provider strategy will fail. An example scenario for using weights is defining a strategy that contains two capacity providers and both have a weight of ``1`` , then when the ``base`` is satisfied, the tasks will be split evenly across the two capacity providers. Using that same logic, if you specify a weight of ``1`` for *capacityProviderA* and a weight of ``4`` for *capacityProviderB* , then for every one task that\'s run using *capacityProviderA* , four tasks would use *capacityProviderB* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-clustercapacityproviderassociations-capacityproviderstrategy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    capacity_provider_strategy_property = ecs.CfnClusterCapacityProviderAssociations.CapacityProviderStrategyProperty(\n        capacity_provider="capacityProvider",\n\n        # the properties below are optional\n        base=123,\n        weight=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capacity_provider', 'base', 'weight']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnClusterCapacityProviderAssociations.CapacityProviderStrategyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.AwsVpcConfigurationProperty
class CfnService_AwsVpcConfigurationPropertyDef(BaseStruct):
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description="Whether the task's elastic network interface receives a public IP address. The default value is ``DISABLED`` .\n")
    security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The IDs of the security groups associated with the task or service. If you don't specify a security group, the default security group for the VPC is used. There's a limit of 5 security groups that can be specified per ``AwsVpcConfiguration`` . .. epigraph:: All specified security groups must be from the same VPC.\n")
    subnets: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The IDs of the subnets associated with the task or service. There\'s a limit of 16 subnets that can be specified per ``AwsVpcConfiguration`` . .. epigraph:: All specified subnets must be from the same VPC.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-awsvpcconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    aws_vpc_configuration_property = ecs.CfnService.AwsVpcConfigurationProperty(\n        assign_public_ip="assignPublicIp",\n        security_groups=["securityGroups"],\n        subnets=["subnets"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['assign_public_ip', 'security_groups', 'subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.AwsVpcConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.CapacityProviderStrategyItemProperty
class CfnService_CapacityProviderStrategyItemPropertyDef(BaseStruct):
    base: typing.Union[int, float, None] = pydantic.Field(None, description='The *base* value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a *base* defined. If no value is specified, the default value of ``0`` is used.\n')
    capacity_provider: typing.Optional[str] = pydantic.Field(None, description='The short name of the capacity provider.\n')
    weight: typing.Union[int, float, None] = pydantic.Field(None, description='The *weight* value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The ``weight`` value is taken into consideration after the ``base`` value, if defined, is satisfied. If no ``weight`` value is specified, the default value of ``0`` is used. When multiple capacity providers are specified within a capacity provider strategy, at least one of the capacity providers must have a weight value greater than zero and any capacity providers with a weight of ``0`` can\'t be used to place tasks. If you specify multiple capacity providers in a strategy that all have a weight of ``0`` , any ``RunTask`` or ``CreateService`` actions using the capacity provider strategy will fail. An example scenario for using weights is defining a strategy that contains two capacity providers and both have a weight of ``1`` , then when the ``base`` is satisfied, the tasks will be split evenly across the two capacity providers. Using that same logic, if you specify a weight of ``1`` for *capacityProviderA* and a weight of ``4`` for *capacityProviderB* , then for every one task that\'s run using *capacityProviderA* , four tasks would use *capacityProviderB* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-capacityproviderstrategyitem.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    capacity_provider_strategy_item_property = ecs.CfnService.CapacityProviderStrategyItemProperty(\n        base=123,\n        capacity_provider="capacityProvider",\n        weight=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['base', 'capacity_provider', 'weight']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.CapacityProviderStrategyItemProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.DeploymentAlarmsProperty
class CfnService_DeploymentAlarmsPropertyDef(BaseStruct):
    alarm_names: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='One or more CloudWatch alarm names. Use a "," to separate the alarms.\n')
    enable: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Determines whether to use the CloudWatch alarm option in the service deployment process.\n')
    rollback: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Determines whether to configure Amazon ECS to roll back the service if a service deployment fails. If rollback is used, when a service deployment fails, the service is rolled back to the last deployment that completed successfully.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-deploymentalarms.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    deployment_alarms_property = ecs.CfnService.DeploymentAlarmsProperty(\n        alarm_names=["alarmNames"],\n        enable=False,\n        rollback=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['alarm_names', 'enable', 'rollback']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.DeploymentAlarmsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.DeploymentCircuitBreakerProperty
class CfnService_DeploymentCircuitBreakerPropertyDef(BaseStruct):
    enable: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Determines whether to use the deployment circuit breaker logic for the service.\n')
    rollback: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Determines whether to configure Amazon ECS to roll back the service if a service deployment fails. If rollback is on, when a service deployment fails, the service is rolled back to the last deployment that completed successfully.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-deploymentcircuitbreaker.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    deployment_circuit_breaker_property = ecs.CfnService.DeploymentCircuitBreakerProperty(\n        enable=False,\n        rollback=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['enable', 'rollback']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.DeploymentCircuitBreakerProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.DeploymentConfigurationProperty
class CfnService_DeploymentConfigurationPropertyDef(BaseStruct):
    alarms: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentAlarmsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Information about the CloudWatch alarms.\n')
    deployment_circuit_breaker: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentCircuitBreakerPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description=".. epigraph:: The deployment circuit breaker can only be used for services using the rolling update ( ``ECS`` ) deployment type. The *deployment circuit breaker* determines whether a service deployment will fail if the service can't reach a steady state. If you use the deployment circuit breaker, a service deployment will transition to a failed state and stop launching new tasks. If you use the rollback option, when a service deployment fails, the service is rolled back to the last deployment that completed successfully. For more information, see `Rolling update <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html>`_ in the *Amazon Elastic Container Service Developer Guide*\n")
    maximum_percent: typing.Union[int, float, None] = pydantic.Field(None, description="If a service is using the rolling update ( ``ECS`` ) deployment type, the ``maximumPercent`` parameter represents an upper limit on the number of your service's tasks that are allowed in the ``RUNNING`` or ``PENDING`` state during a deployment, as a percentage of the ``desiredCount`` (rounded down to the nearest integer). This parameter enables you to define the deployment batch size. For example, if your service is using the ``REPLICA`` service scheduler and has a ``desiredCount`` of four tasks and a ``maximumPercent`` value of 200%, the scheduler may start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default ``maximumPercent`` value for a service using the ``REPLICA`` service scheduler is 200%. If a service is using either the blue/green ( ``CODE_DEPLOY`` ) or ``EXTERNAL`` deployment types and tasks that use the EC2 launch type, the *maximum percent* value is set to the default value and is used to define the upper limit on the number of the tasks in the service that remain in the ``RUNNING`` state while the container instances are in the ``DRAINING`` state. If the tasks in the service use the Fargate launch type, the maximum percent value is not used, although it is returned when describing your service.\n")
    minimum_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description='If a service is using the rolling update ( ``ECS`` ) deployment type, the ``minimumHealthyPercent`` represents a lower limit on the number of your service\'s tasks that must remain in the ``RUNNING`` state during a deployment, as a percentage of the ``desiredCount`` (rounded up to the nearest integer). This parameter enables you to deploy without using additional cluster capacity. For example, if your service has a ``desiredCount`` of four tasks and a ``minimumHealthyPercent`` of 50%, the service scheduler may stop two existing tasks to free up cluster capacity before starting two new tasks. For services that *do not* use a load balancer, the following should be noted: - A service is considered healthy if all essential containers within the tasks in the service pass their health checks. - If a task has no essential containers with a health check defined, the service scheduler will wait for 40 seconds after a task reaches a ``RUNNING`` state before the task is counted towards the minimum healthy percent total. - If a task has one or more essential containers with a health check defined, the service scheduler will wait for the task to reach a healthy status before counting it towards the minimum healthy percent total. A task is considered healthy when all essential containers within the task have passed their health checks. The amount of time the service scheduler can wait for is determined by the container health check settings. For services that *do* use a load balancer, the following should be noted: - If a task has no essential containers with a health check defined, the service scheduler will wait for the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total. - If a task has an essential container with a health check defined, the service scheduler will wait for both the task to reach a healthy status and the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total. The default value for a replica service for ``minimumHealthyPercent`` is 100%. The default ``minimumHealthyPercent`` value for a service using the ``DAEMON`` service schedule is 0% for the AWS CLI , the AWS SDKs, and the APIs and 50% for the AWS Management Console. The minimum number of healthy tasks during a deployment is the ``desiredCount`` multiplied by the ``minimumHealthyPercent`` /100, rounded up to the nearest integer value. If a service is using either the blue/green ( ``CODE_DEPLOY`` ) or ``EXTERNAL`` deployment types and is running tasks that use the EC2 launch type, the *minimum healthy percent* value is set to the default value and is used to define the lower limit on the number of the tasks in the service that remain in the ``RUNNING`` state while the container instances are in the ``DRAINING`` state. If a service is using either the blue/green ( ``CODE_DEPLOY`` ) or ``EXTERNAL`` deployment types and is running tasks that use the Fargate launch type, the minimum healthy percent value is not used, although it is returned when describing your service.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-deploymentconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    deployment_configuration_property = ecs.CfnService.DeploymentConfigurationProperty(\n        alarms=ecs.CfnService.DeploymentAlarmsProperty(\n            alarm_names=["alarmNames"],\n            enable=False,\n            rollback=False\n        ),\n        deployment_circuit_breaker=ecs.CfnService.DeploymentCircuitBreakerProperty(\n            enable=False,\n            rollback=False\n        ),\n        maximum_percent=123,\n        minimum_healthy_percent=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['alarms', 'deployment_circuit_breaker', 'maximum_percent', 'minimum_healthy_percent']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.DeploymentConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.DeploymentControllerProperty
class CfnService_DeploymentControllerPropertyDef(BaseStruct):
    type: typing.Optional[str] = pydantic.Field(None, description='The deployment controller type to use. There are three deployment controller types available:. - **ECS** - The rolling update ( ``ECS`` ) deployment type involves replacing the current running version of the container with the latest version. The number of containers Amazon ECS adds or removes from the service during a rolling update is controlled by adjusting the minimum and maximum number of healthy tasks allowed during a service deployment, as specified in the `DeploymentConfiguration <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_DeploymentConfiguration.html>`_ . - **CODE_DEPLOY** - The blue/green ( ``CODE_DEPLOY`` ) deployment type uses the blue/green deployment model powered by AWS CodeDeploy , which allows you to verify a new deployment of a service before sending production traffic to it. - **EXTERNAL** - The external ( ``EXTERNAL`` ) deployment type enables you to use any third-party deployment controller for full control over the deployment process for an Amazon ECS service.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-deploymentcontroller.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    deployment_controller_property = ecs.CfnService.DeploymentControllerProperty(\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.DeploymentControllerProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.EBSTagSpecificationProperty
class CfnService_EBSTagSpecificationPropertyDef(BaseStruct):
    resource_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of volume resource.\n')
    propagate_tags: typing.Optional[str] = pydantic.Field(None, description="Determines whether to propagate the tags from the task definition to the Amazon EBS volume. Tags can only propagate to a ``SERVICE`` specified in ``ServiceVolumeConfiguration`` . If no value is specified, the tags aren't propagated.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The tags applied to this Amazon EBS volume. ``AmazonECSCreated`` and ``AmazonECSManaged`` are reserved tags that can\'t be used.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-ebstagspecification.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    e_bSTag_specification_property = ecs.CfnService.EBSTagSpecificationProperty(\n        resource_type="resourceType",\n\n        # the properties below are optional\n        propagate_tags="propagateTags",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['resource_type', 'propagate_tags', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.EBSTagSpecificationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.LoadBalancerProperty
class CfnService_LoadBalancerPropertyDef(BaseStruct):
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container (as it appears in a container definition) to associate with the load balancer. You need to specify the container name when configuring the target group for an Amazon ECS load balancer.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port on the container to associate with the load balancer. This port must correspond to a ``containerPort`` in the task definition the tasks in the service are using. For tasks that use the EC2 launch type, the container instance they're launched on must allow ingress traffic on the ``hostPort`` of the port mapping.\n")
    load_balancer_name: typing.Optional[str] = pydantic.Field(None, description='The name of the load balancer to associate with the Amazon ECS service or task set. If you are using an Application Load Balancer or a Network Load Balancer the load balancer name parameter should be omitted.\n')
    target_group_arn: typing.Optional[str] = pydantic.Field(None, description='The full Amazon Resource Name (ARN) of the Elastic Load Balancing target group or groups associated with a service or task set. A target group ARN is only specified when using an Application Load Balancer or Network Load Balancer. For services using the ``ECS`` deployment controller, you can specify one or multiple target groups. For more information, see `Registering multiple target groups with a service <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/register-multiple-targetgroups.html>`_ in the *Amazon Elastic Container Service Developer Guide* . For services using the ``CODE_DEPLOY`` deployment controller, you\'re required to define two target groups for the load balancer. For more information, see `Blue/green deployment with CodeDeploy <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html>`_ in the *Amazon Elastic Container Service Developer Guide* . .. epigraph:: If your service\'s task definition uses the ``awsvpc`` network mode, you must choose ``ip`` as the target type, not ``instance`` . Do this when creating your target groups because tasks that use the ``awsvpc`` network mode are associated with an elastic network interface, not an Amazon EC2 instance. This network mode is required for the Fargate launch type.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-loadbalancer.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    load_balancer_property = ecs.CfnService.LoadBalancerProperty(\n        container_name="containerName",\n        container_port=123,\n        load_balancer_name="loadBalancerName",\n        target_group_arn="targetGroupArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'container_port', 'load_balancer_name', 'target_group_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.LoadBalancerProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.LogConfigurationProperty
class CfnService_LogConfigurationPropertyDef(BaseStruct):
    log_driver: typing.Optional[str] = pydantic.Field(None, description="The log driver to use for the container. For tasks on AWS Fargate , the supported log drivers are ``awslogs`` , ``splunk`` , and ``awsfirelens`` . For tasks hosted on Amazon EC2 instances, the supported log drivers are ``awslogs`` , ``fluentd`` , ``gelf`` , ``json-file`` , ``journald`` , ``logentries`` , ``syslog`` , ``splunk`` , and ``awsfirelens`` . For more information about using the ``awslogs`` log driver, see `Send Amazon ECS logs to CloudWatch <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html>`_ in the *Amazon Elastic Container Service Developer Guide* . For more information about using the ``awsfirelens`` log driver, see `Send Amazon ECS logs to an AWS service or AWS Partner <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_firelens.html>`_ . .. epigraph:: If you have a custom driver that isn't listed, you can fork the Amazon ECS container agent project that's `available on GitHub <https://docs.aws.amazon.com/https://github.com/aws/amazon-ecs-agent>`_ and customize it to work with that driver. We encourage you to submit pull requests for changes that you would like to have included. However, we don't currently provide support for running modified copies of this software.\n")
    options: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description="The configuration options to send to the log driver. This parameter requires version 1.19 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version --format '{{.Server.APIVersion}}'``\n")
    secret_options: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The secrets to pass to the log configuration. For more information, see `Specifying sensitive data <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-logconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    log_configuration_property = ecs.CfnService.LogConfigurationProperty(\n        log_driver="logDriver",\n        options={\n            "options_key": "options"\n        },\n        secret_options=[ecs.CfnService.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.LogConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.NetworkConfigurationProperty
class CfnService_NetworkConfigurationPropertyDef(BaseStruct):
    awsvpc_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_AwsVpcConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC subnets and security groups that are associated with a task. .. epigraph:: All specified subnets and security groups must be from the same VPC.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-networkconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    network_configuration_property = ecs.CfnService.NetworkConfigurationProperty(\n        awsvpc_configuration=ecs.CfnService.AwsVpcConfigurationProperty(\n            assign_public_ip="assignPublicIp",\n            security_groups=["securityGroups"],\n            subnets=["subnets"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['awsvpc_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.NetworkConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.PlacementConstraintProperty
class CfnService_PlacementConstraintPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of constraint. Use ``distinctInstance`` to ensure that each task in a particular group is running on a different container instance. Use ``memberOf`` to restrict the selection to a group of valid candidates.\n')
    expression: typing.Optional[str] = pydantic.Field(None, description='A cluster query language expression to apply to the constraint. The expression can have a maximum length of 2000 characters. You can\'t specify an expression if the constraint type is ``distinctInstance`` . For more information, see `Cluster query language <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-placementconstraint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    placement_constraint_property = ecs.CfnService.PlacementConstraintProperty(\n        type="type",\n\n        # the properties below are optional\n        expression="expression"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'expression']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.PlacementConstraintProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.PlacementStrategyProperty
class CfnService_PlacementStrategyPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The type of placement strategy. The ``random`` placement strategy randomly places tasks on available candidates. The ``spread`` placement strategy spreads placement across available candidates evenly based on the ``field`` parameter. The ``binpack`` strategy places tasks on available candidates that have the least available amount of the resource that's specified with the ``field`` parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory but still enough to run the task.\n")
    field: typing.Optional[str] = pydantic.Field(None, description='The field to apply the placement strategy against. For the ``spread`` placement strategy, valid values are ``instanceId`` (or ``host`` , which has the same effect), or any platform or custom attribute that\'s applied to a container instance, such as ``attribute:ecs.availability-zone`` . For the ``binpack`` placement strategy, valid values are ``cpu`` and ``memory`` . For the ``random`` placement strategy, this field is not used.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-placementstrategy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    placement_strategy_property = ecs.CfnService.PlacementStrategyProperty(\n        type="type",\n\n        # the properties below are optional\n        field="field"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'field']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.PlacementStrategyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.SecretProperty
class CfnService_SecretPropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the secret.\n')
    value_from: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The secret to expose to the container. The supported values are either the full ARN of the AWS Secrets Manager secret or the full ARN of the parameter in the SSM Parameter Store. For information about the require AWS Identity and Access Management permissions, see `Required IAM permissions for Amazon ECS secrets <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-secrets.html#secrets-iam>`_ (for Secrets Manager) or `Required IAM permissions for Amazon ECS secrets <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-parameters.html>`_ (for Systems Manager Parameter store) in the *Amazon Elastic Container Service Developer Guide* . .. epigraph:: If the SSM Parameter Store parameter exists in the same Region as the task you\'re launching, then you can use either the full ARN or name of the parameter. If the parameter exists in a different Region, then the full ARN must be specified.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-secret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    secret_property = ecs.CfnService.SecretProperty(\n        name="name",\n        value_from="valueFrom"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value_from']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.SecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceConnectClientAliasProperty
class CfnService_ServiceConnectClientAliasPropertyDef(BaseStruct):
    port: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The listening port number for the Service Connect proxy. This port is available inside of all of the tasks within the same namespace. To avoid changing your applications in client Amazon ECS services, set this to the same port that the client application uses by default. For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    dns_name: typing.Optional[str] = pydantic.Field(None, description='The ``dnsName`` is the name that you use in the applications of client tasks to connect to this service. The name must be a valid DNS name but doesn\'t need to be fully-qualified. The name can include up to 127 characters. The name can include lowercase letters, numbers, underscores (_), hyphens (-), and periods (.). The name can\'t start with a hyphen. If this parameter isn\'t specified, the default value of ``discoveryName.namespace`` is used. If the ``discoveryName`` isn\'t specified, the port mapping name from the task definition is used in ``portName.namespace`` . To avoid changing your applications in client Amazon ECS services, set this to the same name that the client application uses by default. For example, a few common names are ``database`` , ``db`` , or the lowercase name of a database, such as ``mysql`` or ``redis`` . For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-serviceconnectclientalias.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_connect_client_alias_property = ecs.CfnService.ServiceConnectClientAliasProperty(\n        port=123,\n\n        # the properties below are optional\n        dns_name="dnsName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['port', 'dns_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceConnectClientAliasProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceConnectConfigurationProperty
class CfnService_ServiceConnectConfigurationPropertyDef(BaseStruct):
    enabled: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether to use Service Connect with this service.\n')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The log configuration for the container. This parameter maps to ``LogConfig`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--log-driver`` option to ```docker run`` <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/commandline/run/>`_ . By default, containers use the same logging driver that the Docker daemon uses. However, the container might use a different logging driver than the Docker daemon by specifying a log driver configuration in the container definition. For more information about the options for different supported log drivers, see `Configure logging drivers <https://docs.aws.amazon.com/https://docs.docker.com/engine/admin/logging/overview/>`_ in the Docker documentation. Understand the following when specifying a log configuration for your containers. - Amazon ECS currently supports a subset of the logging drivers available to the Docker daemon. Additional log drivers may be available in future releases of the Amazon ECS container agent. For tasks on AWS Fargate , the supported log drivers are ``awslogs`` , ``splunk`` , and ``awsfirelens`` . For tasks hosted on Amazon EC2 instances, the supported log drivers are ``awslogs`` , ``fluentd`` , ``gelf`` , ``json-file`` , ``journald`` , ``syslog`` , ``splunk`` , and ``awsfirelens`` . - This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. - For tasks that are hosted on Amazon EC2 instances, the Amazon ECS container agent must register the available logging drivers with the ``ECS_AVAILABLE_LOGGING_DRIVERS`` environment variable before containers placed on that instance can use these log configuration options. For more information, see `Amazon ECS container agent configuration <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html>`_ in the *Amazon Elastic Container Service Developer Guide* . - For tasks that are on AWS Fargate , because you don't have access to the underlying infrastructure your tasks are hosted on, any additional software needed must be installed outside of the task. For example, the Fluentd output aggregators or a remote host running Logstash to send Gelf logs to.\n")
    namespace: typing.Optional[str] = pydantic.Field(None, description="The namespace name or full Amazon Resource Name (ARN) of the AWS Cloud Map namespace for use with Service Connect. The namespace must be in the same AWS Region as the Amazon ECS service and cluster. The type of namespace doesn't affect Service Connect. For more information about AWS Cloud Map , see `Working with Services <https://docs.aws.amazon.com/cloud-map/latest/dg/working-with-services.html>`_ in the *AWS Cloud Map Developer Guide* .\n")
    services: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectServicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The list of Service Connect service objects. These are names and aliases (also known as endpoints) that are used by other Amazon ECS services to connect to this service. This field is not required for a "client" Amazon ECS service that\'s a member of a namespace only to connect to other services within the namespace. An example of this would be a frontend application that accepts incoming requests from either a load balancer that\'s attached to the service or by other means. An object selects a port from the task definition, assigns a name for the AWS Cloud Map service, and a list of aliases (endpoints) and ports for client applications to refer to this service.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-serviceconnectconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_connect_configuration_property = ecs.CfnService.ServiceConnectConfigurationProperty(\n        enabled=False,\n\n        # the properties below are optional\n        log_configuration=ecs.CfnService.LogConfigurationProperty(\n            log_driver="logDriver",\n            options={\n                "options_key": "options"\n            },\n            secret_options=[ecs.CfnService.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )]\n        ),\n        namespace="namespace",\n        services=[ecs.CfnService.ServiceConnectServiceProperty(\n            port_name="portName",\n\n            # the properties below are optional\n            client_aliases=[ecs.CfnService.ServiceConnectClientAliasProperty(\n                port=123,\n\n                # the properties below are optional\n                dns_name="dnsName"\n            )],\n            discovery_name="discoveryName",\n            ingress_port_override=123,\n            timeout=ecs.CfnService.TimeoutConfigurationProperty(\n                idle_timeout_seconds=123,\n                per_request_timeout_seconds=123\n            ),\n            tls=ecs.CfnService.ServiceConnectTlsConfigurationProperty(\n                issuer_certificate_authority=ecs.CfnService.ServiceConnectTlsCertificateAuthorityProperty(\n                    aws_pca_authority_arn="awsPcaAuthorityArn"\n                ),\n\n                # the properties below are optional\n                kms_key="kmsKey",\n                role_arn="roleArn"\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['enabled', 'log_configuration', 'namespace', 'services']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceConnectConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceConnectServiceProperty
class CfnService_ServiceConnectServicePropertyDef(BaseStruct):
    port_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ``portName`` must match the name of one of the ``portMappings`` from all the containers in the task definition of this Amazon ECS service.\n')
    client_aliases: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectClientAliasPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The list of client aliases for this Service Connect service. You use these to assign names that can be used by client applications. The maximum number of client aliases that you can have in this list is 1. Each alias ("endpoint") is a fully-qualified name and port number that other Amazon ECS tasks ("clients") can use to connect to this service. Each name and port mapping must be unique within the namespace. For each ``ServiceConnectService`` , you must provide at least one ``clientAlias`` with one ``port`` .\n')
    discovery_name: typing.Optional[str] = pydantic.Field(None, description="The ``discoveryName`` is the name of the new AWS Cloud Map service that Amazon ECS creates for this Amazon ECS service. This must be unique within the AWS Cloud Map namespace. The name can contain up to 64 characters. The name can include lowercase letters, numbers, underscores (_), and hyphens (-). The name can't start with a hyphen. If the ``discoveryName`` isn't specified, the port mapping name from the task definition is used in ``portName.namespace`` .\n")
    ingress_port_override: typing.Union[int, float, None] = pydantic.Field(None, description='The port number for the Service Connect proxy to listen on. Use the value of this field to bypass the proxy for traffic on the port number specified in the named ``portMapping`` in the task definition of this application, and then use it in your VPC security groups to allow traffic into the proxy for this Amazon ECS service. In ``awsvpc`` mode and Fargate, the default value is the container port number. The container port number is in the ``portMapping`` in the task definition. In bridge mode, the default value is the ephemeral port of the Service Connect proxy.\n')
    timeout: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_TimeoutConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A reference to an object that represents the configured timeouts for Service Connect.\n')
    tls: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectTlsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A reference to an object that represents a Transport Layer Security (TLS) configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-serviceconnectservice.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_connect_service_property = ecs.CfnService.ServiceConnectServiceProperty(\n        port_name="portName",\n\n        # the properties below are optional\n        client_aliases=[ecs.CfnService.ServiceConnectClientAliasProperty(\n            port=123,\n\n            # the properties below are optional\n            dns_name="dnsName"\n        )],\n        discovery_name="discoveryName",\n        ingress_port_override=123,\n        timeout=ecs.CfnService.TimeoutConfigurationProperty(\n            idle_timeout_seconds=123,\n            per_request_timeout_seconds=123\n        ),\n        tls=ecs.CfnService.ServiceConnectTlsConfigurationProperty(\n            issuer_certificate_authority=ecs.CfnService.ServiceConnectTlsCertificateAuthorityProperty(\n                aws_pca_authority_arn="awsPcaAuthorityArn"\n            ),\n\n            # the properties below are optional\n            kms_key="kmsKey",\n            role_arn="roleArn"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['port_name', 'client_aliases', 'discovery_name', 'ingress_port_override', 'timeout', 'tls']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceConnectServiceProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceConnectTlsCertificateAuthorityProperty
class CfnService_ServiceConnectTlsCertificateAuthorityPropertyDef(BaseStruct):
    aws_pca_authority_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of the AWS Private Certificate Authority certificate.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-serviceconnecttlscertificateauthority.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_connect_tls_certificate_authority_property = ecs.CfnService.ServiceConnectTlsCertificateAuthorityProperty(\n        aws_pca_authority_arn="awsPcaAuthorityArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['aws_pca_authority_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceConnectTlsCertificateAuthorityProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceConnectTlsConfigurationProperty
class CfnService_ServiceConnectTlsConfigurationPropertyDef(BaseStruct):
    issuer_certificate_authority: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectTlsCertificateAuthorityPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The signer certificate authority.\n')
    kms_key: typing.Optional[str] = pydantic.Field(None, description='The AWS Key Management Service key.\n')
    role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the IAM role that\'s associated with the Service Connect TLS.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-serviceconnecttlsconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_connect_tls_configuration_property = ecs.CfnService.ServiceConnectTlsConfigurationProperty(\n        issuer_certificate_authority=ecs.CfnService.ServiceConnectTlsCertificateAuthorityProperty(\n            aws_pca_authority_arn="awsPcaAuthorityArn"\n        ),\n\n        # the properties below are optional\n        kms_key="kmsKey",\n        role_arn="roleArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['issuer_certificate_authority', 'kms_key', 'role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceConnectTlsConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceManagedEBSVolumeConfigurationProperty
class CfnService_ServiceManagedEBSVolumeConfigurationPropertyDef(BaseStruct):
    role_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of the IAM role to associate with this volume. This is the Amazon ECS infrastructure IAM role that is used to manage your AWS infrastructure. We recommend using the Amazon ECS-managed ``AmazonECSInfrastructureRolePolicyForVolumes`` IAM policy with this role. For more information, see `Amazon ECS infrastructure IAM role <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/infrastructure_IAM_role.html>`_ in the *Amazon ECS Developer Guide* .\n')
    encrypted: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether the volume should be encrypted. If no value is specified, encryption is turned on by default. This parameter maps 1:1 with the ``Encrypted`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* .\n')
    filesystem_type: typing.Optional[str] = pydantic.Field(None, description='The Linux filesystem type for the volume. For volumes created from a snapshot, you must specify the same filesystem type that the volume was using when the snapshot was created. If there is a filesystem type mismatch, the task will fail to start. The available filesystem types are ``ext3`` , ``ext4`` , and ``xfs`` . If no value is specified, the ``xfs`` filesystem type is used by default.\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS). For ``gp3`` , ``io1`` , and ``io2`` volumes, this represents the number of IOPS that are provisioned for the volume. For ``gp2`` volumes, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. The following are the supported values for each volume type. - ``gp3`` : 3,000 - 16,000 IOPS - ``io1`` : 100 - 64,000 IOPS - ``io2`` : 100 - 256,000 IOPS This parameter is required for ``io1`` and ``io2`` volume types. The default for ``gp3`` volumes is ``3,000 IOPS`` . This parameter is not supported for ``st1`` , ``sc1`` , or ``standard`` volume types. This parameter maps 1:1 with the ``Iops`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* .\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) identifier of the AWS Key Management Service key to use for Amazon EBS encryption. When encryption is turned on and no AWS Key Management Service key is specified, the default AWS managed key for Amazon EBS volumes is used. This parameter maps 1:1 with the ``KmsKeyId`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* . .. epigraph:: AWS authenticates the AWS Key Management Service key asynchronously. Therefore, if you specify an ID, alias, or ARN that is invalid, the action can appear to complete, but eventually fails.\n')
    size_in_gib: typing.Union[int, float, None] = pydantic.Field(None, description='The size of the volume in GiB. You must specify either a volume size or a snapshot ID. If you specify a snapshot ID, the snapshot size is used for the volume size by default. You can optionally specify a volume size greater than or equal to the snapshot size. This parameter maps 1:1 with the ``Size`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* . The following are the supported volume size values for each volume type. - ``gp2`` and ``gp3`` : 1-16,384 - ``io1`` and ``io2`` : 4-16,384 - ``st1`` and ``sc1`` : 125-16,384 - ``standard`` : 1-1,024\n')
    snapshot_id: typing.Optional[str] = pydantic.Field(None, description='The snapshot that Amazon ECS uses to create the volume. You must specify either a snapshot ID or a volume size. This parameter maps 1:1 with the ``SnapshotId`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* .\n')
    tag_specifications: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_EBSTagSpecificationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The tags to apply to the volume. Amazon ECS applies service-managed tags by default. This parameter maps 1:1 with the ``TagSpecifications.N`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* .\n')
    throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The throughput to provision for a volume, in MiB/s, with a maximum of 1,000 MiB/s. This parameter maps 1:1 with the ``Throughput`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* . .. epigraph:: This parameter is only supported for the ``gp3`` volume type.\n')
    volume_type: typing.Optional[str] = pydantic.Field(None, description='The volume type. This parameter maps 1:1 with the ``VolumeType`` parameter of the `CreateVolume API <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVolume.html>`_ in the *Amazon EC2 API Reference* . For more information, see `Amazon EBS volume types <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html>`_ in the *Amazon EC2 User Guide* . The following are the supported volume types. - General Purpose SSD: ``gp2`` | ``gp3`` - Provisioned IOPS SSD: ``io1`` | ``io2`` - Throughput Optimized HDD: ``st1`` - Cold HDD: ``sc1`` - Magnetic: ``standard`` .. epigraph:: The magnetic volume type is not supported on Fargate.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-servicemanagedebsvolumeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_managed_eBSVolume_configuration_property = ecs.CfnService.ServiceManagedEBSVolumeConfigurationProperty(\n        role_arn="roleArn",\n\n        # the properties below are optional\n        encrypted=False,\n        filesystem_type="filesystemType",\n        iops=123,\n        kms_key_id="kmsKeyId",\n        size_in_gi_b=123,\n        snapshot_id="snapshotId",\n        tag_specifications=[ecs.CfnService.EBSTagSpecificationProperty(\n            resource_type="resourceType",\n\n            # the properties below are optional\n            propagate_tags="propagateTags",\n            tags=[CfnTag(\n                key="key",\n                value="value"\n            )]\n        )],\n        throughput=123,\n        volume_type="volumeType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['role_arn', 'encrypted', 'filesystem_type', 'iops', 'kms_key_id', 'size_in_gib', 'snapshot_id', 'tag_specifications', 'throughput', 'volume_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceManagedEBSVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceRegistryProperty
class CfnService_ServiceRegistryPropertyDef(BaseStruct):
    container_name: typing.Optional[str] = pydantic.Field(None, description="The container name value to be used for your service discovery service. It's already specified in the task definition. If the task definition that your service task specifies uses the ``bridge`` or ``host`` network mode, you must specify a ``containerName`` and ``containerPort`` combination from the task definition. If the task definition that your service task specifies uses the ``awsvpc`` network mode and a type SRV DNS record is used, you must specify either a ``containerName`` and ``containerPort`` combination or a ``port`` value. However, you can't specify both.\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port value to be used for your service discovery service. It's already specified in the task definition. If the task definition your service task specifies uses the ``bridge`` or ``host`` network mode, you must specify a ``containerName`` and ``containerPort`` combination from the task definition. If the task definition your service task specifies uses the ``awsvpc`` network mode and a type SRV DNS record is used, you must specify either a ``containerName`` and ``containerPort`` combination or a ``port`` value. However, you can't specify both.\n")
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port value used if your service discovery service specified an SRV record. This field might be used if both the ``awsvpc`` network mode and SRV records are used.\n')
    registry_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the service registry. The currently supported service registry is AWS Cloud Map . For more information, see `CreateService <https://docs.aws.amazon.com/cloud-map/latest/api/API_CreateService.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-serviceregistry.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_registry_property = ecs.CfnService.ServiceRegistryProperty(\n        container_name="containerName",\n        container_port=123,\n        port=123,\n        registry_arn="registryArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'container_port', 'port', 'registry_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceRegistryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.ServiceVolumeConfigurationProperty
class CfnService_ServiceVolumeConfigurationPropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume. This value must match the volume name from the ``Volume`` object in the task definition.\n')
    managed_ebs_volume: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceManagedEBSVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration for the Amazon EBS volume that Amazon ECS creates and manages on your behalf. These settings are used to create each Amazon EBS volume, with one volume created for each task in the service. The Amazon EBS volumes are visible in your account in the Amazon EC2 console once they are created.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-servicevolumeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_volume_configuration_property = ecs.CfnService.ServiceVolumeConfigurationProperty(\n        name="name",\n\n        # the properties below are optional\n        managed_ebs_volume=ecs.CfnService.ServiceManagedEBSVolumeConfigurationProperty(\n            role_arn="roleArn",\n\n            # the properties below are optional\n            encrypted=False,\n            filesystem_type="filesystemType",\n            iops=123,\n            kms_key_id="kmsKeyId",\n            size_in_gi_b=123,\n            snapshot_id="snapshotId",\n            tag_specifications=[ecs.CfnService.EBSTagSpecificationProperty(\n                resource_type="resourceType",\n\n                # the properties below are optional\n                propagate_tags="propagateTags",\n                tags=[CfnTag(\n                    key="key",\n                    value="value"\n                )]\n            )],\n            throughput=123,\n            volume_type="volumeType"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'managed_ebs_volume']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.ServiceVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnService.TimeoutConfigurationProperty
class CfnService_TimeoutConfigurationPropertyDef(BaseStruct):
    idle_timeout_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time in seconds a connection will stay active while idle. A value of ``0`` can be set to disable ``idleTimeout`` . The ``idleTimeout`` default for ``HTTP`` / ``HTTP2`` / ``GRPC`` is 5 minutes. The ``idleTimeout`` default for ``TCP`` is 1 hour.\n')
    per_request_timeout_seconds: typing.Union[int, float, None] = pydantic.Field(None, description="The amount of time waiting for the upstream to respond with a complete response per request. A value of ``0`` can be set to disable ``perRequestTimeout`` . ``perRequestTimeout`` can only be set if Service Connect ``appProtocol`` isn't ``TCP`` . Only ``idleTimeout`` is allowed for ``TCP`` ``appProtocol`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-service-timeoutconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    timeout_configuration_property = ecs.CfnService.TimeoutConfigurationProperty(\n        idle_timeout_seconds=123,\n        per_request_timeout_seconds=123\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['idle_timeout_seconds', 'per_request_timeout_seconds']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService.TimeoutConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.AuthorizationConfigProperty
class CfnTaskDefinition_AuthorizationConfigPropertyDef(BaseStruct):
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, the root directory value specified in the ``EFSVolumeConfiguration`` must either be omitted or set to ``/`` which will enforce the path set on the EFS access point. If an access point is used, transit encryption must be on in the ``EFSVolumeConfiguration`` . For more information, see `Working with Amazon EFS access points <https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html>`_ in the *Amazon Elastic File System User Guide* .\n')
    iam: typing.Optional[str] = pydantic.Field(None, description='Determines whether to use the Amazon ECS task role defined in a task definition when mounting the Amazon EFS file system. If it is turned on, transit encryption must be turned on in the ``EFSVolumeConfiguration`` . If this parameter is omitted, the default value of ``DISABLED`` is used. For more information, see `Using Amazon EFS access points <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/efs-volumes.html#efs-volume-accesspoints>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-authorizationconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    authorization_config_property = ecs.CfnTaskDefinition.AuthorizationConfigProperty(\n        access_point_id="accessPointId",\n        iam="iam"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_point_id', 'iam']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.AuthorizationConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.ContainerDefinitionProperty
class CfnTaskDefinition_ContainerDefinitionPropertyDef(BaseStruct):
    image: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The image used to start a container. This string is passed directly to the Docker daemon. By default, images in the Docker Hub registry are available. Other repositories are specified with either ``*repository-url* / *image* : *tag*`` or ``*repository-url* / *image* @ *digest*`` . Up to 255 letters (uppercase and lowercase), numbers, hyphens, underscores, colons, periods, forward slashes, and number signs are allowed. This parameter maps to ``Image`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``IMAGE`` parameter of `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . - When a new task starts, the Amazon ECS container agent pulls the latest version of the specified image and tag for the container to use. However, subsequent updates to a repository image aren't propagated to already running tasks. - Images in Amazon ECR repositories can be specified by either using the full ``registry/repository:tag`` or ``registry/repository@digest`` . For example, ``012345678910.dkr.ecr.<region-name>.amazonaws.com/<repository-name>:latest`` or ``012345678910.dkr.ecr.<region-name>.amazonaws.com/<repository-name>@sha256:94afd1f2e64d908bc90dbca0035a5b567EXAMPLE`` . - Images in official repositories on Docker Hub use a single name (for example, ``ubuntu`` or ``mongo`` ). - Images in other repositories on Docker Hub are qualified with an organization name (for example, ``amazon/amazon-ecs-agent`` ). - Images in other online repositories are qualified further by a domain name (for example, ``quay.io/assemblyline/ubuntu`` ).\n")
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The name of a container. If you're linking multiple containers together in a task definition, the ``name`` of one container can be entered in the ``links`` of another container to connect the containers. Up to 255 letters (uppercase and lowercase), numbers, underscores, and hyphens are allowed. This parameter maps to ``name`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--name`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ .\n")
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. This parameter maps to ``Cmd`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``COMMAND`` parameter to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . For more information, see `https://docs.docker.com/engine/reference/builder/#cmd <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/builder/#cmd>`_ . If there are multiple arguments, each argument is a separated string in the array.\n")
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description="The number of ``cpu`` units reserved for the container. This parameter maps to ``CpuShares`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--cpu-shares`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . This field is optional for tasks using the Fargate launch type, and the only requirement is that the total amount of CPU reserved for all containers within a task be lower than the task-level ``cpu`` value. .. epigraph:: You can determine the number of CPU units that are available per EC2 instance type by multiplying the vCPUs listed for that instance type on the `Amazon EC2 Instances <https://docs.aws.amazon.com/ec2/instance-types/>`_ detail page by 1,024. Linux containers share unallocated CPU units with other containers on the container instance with the same ratio as their allocated amount. For example, if you run a single-container task on a single-core instance type with 512 CPU units specified for that container, and that's the only task running on the container instance, that container could use the full 1,024 CPU unit share at any given time. However, if you launched another copy of the same task on that container instance, each task is guaranteed a minimum of 512 CPU units when needed. Moreover, each container could float to higher CPU usage if the other container was not using it. If both tasks were 100% active all of the time, they would be limited to 512 CPU units. On Linux container instances, the Docker daemon on the container instance uses the CPU value to calculate the relative CPU share ratios for running containers. For more information, see `CPU share constraint <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#cpu-share-constraint>`_ in the Docker documentation. The minimum valid CPU share value that the Linux kernel allows is 2, and the maximum valid CPU share value that the Linux kernel allows is 262144. However, the CPU parameter isn't required, and you can use CPU values below 2 or above 262144 in your container definitions. For CPU values below 2 (including null) or above 262144, the behavior varies based on your Amazon ECS container agent version: - *Agent versions less than or equal to 1.1.0:* Null and zero CPU values are passed to Docker as 0, which Docker then converts to 1,024 CPU shares. CPU values of 1 are passed to Docker as 1, which the Linux kernel converts to two CPU shares. - *Agent versions greater than or equal to 1.2.0:* Null, zero, and CPU values of 1 are passed to Docker as 2. - *Agent versions greater than or equal to 1.84.0:* CPU values greater than 256 vCPU are passed to Docker as 256, which is equivalent to 262144 CPU shares. On Windows container instances, the CPU limit is enforced as an absolute limit, or a quota. Windows containers only have access to the specified amount of CPU that's described in the task definition. A null or zero CPU value is passed to Docker as ``0`` , which Windows interprets as 1% of one CPU.\n")
    credential_specs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of ARNs in SSM or Amazon S3 to a credential spec ( ``CredSpec`` ) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions`` . The maximum number of ARNs is 1. There are two formats for each ARN. - **credentialspecdomainless:MyARN** - You use ``credentialspecdomainless:MyARN`` to provide a ``CredSpec`` with an additional section for a secret in AWS Secrets Manager . You provide the login credentials to the domain in the secret. Each task that runs on any container instance can join different domains. You can use this format without joining the container instance to a domain. - **credentialspec:MyARN** - You use ``credentialspec:MyARN`` to provide a ``CredSpec`` for a single domain. You must join the container instance to the domain before you start any tasks that use this task definition. In both formats, replace ``MyARN`` with the ARN in SSM or Amazon S3. If you provide a ``credentialspecdomainless:MyARN`` , the ``credspec`` must provide a ARN in AWS Secrets Manager for a secret containing the username, password, and the domain to connect to. For better security, the instance isn't joined to the domain for domainless authentication. Other applications on the instance can't use the domainless credentials. You can use this parameter to run tasks on the same instance, even it the tasks need to join different domains. For more information, see `Using gMSAs for Windows Containers <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/windows-gmsa.html>`_ and `Using gMSAs for Linux Containers <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/linux-gmsa.html>`_ .\n")
    depends_on: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ContainerDependencyPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The dependencies defined for container startup and shutdown. A container can contain multiple dependencies. When a dependency is defined for container startup, for container shutdown it is reversed. For tasks using the EC2 launch type, the container instances require at least version 1.26.0 of the container agent to turn on container dependencies. However, we recommend using the latest container agent version. For information about checking your agent version and updating to the latest version, see `Updating the Amazon ECS Container Agent <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-update.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If you're using an Amazon ECS-optimized Linux AMI, your instance needs at least version 1.26.0-1 of the ``ecs-init`` package. If your container instances are launched from version ``20190301`` or later, then they contain the required versions of the container agent and ``ecs-init`` . For more information, see `Amazon ECS-optimized Linux AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_ in the *Amazon Elastic Container Service Developer Guide* . For tasks using the Fargate launch type, the task or service requires the following platforms: - Linux platform version ``1.3.0`` or later. - Windows platform version ``1.0.0`` or later. If the task definition is used in a blue/green deployment that uses `AWS::CodeDeploy::DeploymentGroup BlueGreenDeploymentConfiguration <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codedeploy-deploymentgroup-bluegreendeploymentconfiguration.html>`_ , the ``dependsOn`` parameter is not supported. For more information see `Issue #680 <https://docs.aws.amazon.com/https://github.com/aws-cloudformation/cloudformation-coverage-roadmap/issues/680>`_ on the on the GitHub website.\n")
    disable_networking: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this parameter is true, networking is off within the container. This parameter maps to ``NetworkDisabled`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ . .. epigraph:: This parameter is not supported for Windows containers.\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. This parameter maps to ``DnsSearch`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--dns-search`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: This parameter is not supported for Windows containers.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. This parameter maps to ``Dns`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--dns`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: This parameter is not supported for Windows containers.\n')
    docker_labels: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description="A key/value map of labels to add to the container. This parameter maps to ``Labels`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--label`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version --format '{{.Server.APIVersion}}'``\n")
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom configuration for multiple security systems. For more information about valid values, see `Docker Run Security Configuration <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . This field isn\'t valid for containers in tasks using the Fargate launch type. For Linux tasks on EC2, this parameter can be used to reference custom labels for SELinux and AppArmor multi-level security systems. For any tasks on EC2, this parameter can be used to reference a credential spec file that configures a container for Active Directory authentication. For more information, see `Using gMSAs for Windows Containers <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/windows-gmsa.html>`_ and `Using gMSAs for Linux Containers <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/linux-gmsa.html>`_ in the *Amazon Elastic Container Service Developer Guide* . This parameter maps to ``SecurityOpt`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--security-opt`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: The Amazon ECS container agent running on a container instance must register with the ``ECS_SELINUX_CAPABLE=true`` or ``ECS_APPARMOR_CAPABLE=true`` environment variables before containers placed on that instance can use these security options. For more information, see `Amazon ECS Container Agent Configuration <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html>`_ in the *Amazon Elastic Container Service Developer Guide* . For more information about valid values, see `Docker Run Security Configuration <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . Valid values: "no-new-privileges" | "apparmor:PROFILE" | "label:value" | "credentialspec:CredentialSpecFilePath"\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description=".. epigraph:: Early versions of the Amazon ECS container agent don't properly handle ``entryPoint`` parameters. If you have problems using ``entryPoint`` , update your container agent or enter your commands and arguments as ``command`` array items instead. The entry point that's passed to the container. This parameter maps to ``Entrypoint`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--entrypoint`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . For more information, see `https://docs.docker.com/engine/reference/builder/#entrypoint <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/builder/#entrypoint>`_ .\n")
    environment: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_KeyValuePairPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The environment variables to pass to a container. This parameter maps to ``Env`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--env`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: We don't recommend that you use plaintext environment variables for sensitive information, such as credential data.\n")
    environment_files: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_EnvironmentFilePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="A list of files containing the environment variables to pass to a container. This parameter maps to the ``--env-file`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . You can specify up to ten environment files. The file must have a ``.env`` file extension. Each line in an environment file contains an environment variable in ``VARIABLE=VALUE`` format. Lines beginning with ``#`` are treated as comments and are ignored. For more information about the environment variable file syntax, see `Declare default environment variables in file <https://docs.aws.amazon.com/https://docs.docker.com/compose/env-file/>`_ . If there are environment variables specified using the ``environment`` parameter in a container definition, they take precedence over the variables contained within an environment file. If multiple environment files are specified that contain the same variable, they're processed from the top down. We recommend that you use unique variable names. For more information, see `Specifying Environment Variables <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n")
    essential: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="If the ``essential`` parameter of a container is marked as ``true`` , and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the ``essential`` parameter of a container is marked as ``false`` , its failure doesn't affect the rest of the containers in a task. If this parameter is omitted, a container is assumed to be essential. All tasks must have at least one essential container. If you have an application that's composed of multiple containers, group containers that are used for a common purpose into components, and separate the different components into multiple task definitions. For more information, see `Application Architecture <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n")
    extra_hosts: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_HostEntryPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="A list of hostnames and IP address mappings to append to the ``/etc/hosts`` file on the container. This parameter maps to ``ExtraHosts`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--add-host`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: This parameter isn't supported for Windows containers or tasks that use the ``awsvpc`` network mode.\n")
    firelens_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_FirelensConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The FireLens configuration for the container. This is used to specify and configure a log router for container logs. For more information, see `Custom Log Routing <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_firelens.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    health_check: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_HealthCheckPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The container health check command and associated configuration parameters for the container. This parameter maps to ``HealthCheck`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``HEALTHCHECK`` parameter of `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ .\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description="The hostname to use for your container. This parameter maps to ``Hostname`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--hostname`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: The ``hostname`` parameter is not supported if you're using the ``awsvpc`` network mode.\n")
    interactive: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this parameter is ``true`` , you can deploy containerized applications that require ``stdin`` or a ``tty`` to be allocated. This parameter maps to ``OpenStdin`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--interactive`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ .\n')
    links: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ``links`` parameter allows containers to communicate with each other without the need for port mappings. This parameter is only supported if the network mode of a task definition is ``bridge`` . The ``name:internalName`` construct is analogous to ``name:alias`` in Docker links. Up to 255 letters (uppercase and lowercase), numbers, underscores, and hyphens are allowed. For more information about linking Docker containers, go to `Legacy container links <https://docs.aws.amazon.com/https://docs.docker.com/network/links/>`_ in the Docker documentation. This parameter maps to ``Links`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--link`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: This parameter is not supported for Windows containers. > Containers that are collocated on a single container instance may be able to communicate with each other without requiring links or host port mappings. Network isolation is achieved on the container instance using security groups and VPC settings.\n')
    linux_parameters: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_LinuxParametersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_ . .. epigraph:: This parameter is not supported for Windows containers.\n')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The log configuration specification for the container. This parameter maps to ``LogConfig`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--log-driver`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . By default, containers use the same logging driver that the Docker daemon uses. However, the container may use a different logging driver than the Docker daemon by specifying a log driver with this parameter in the container definition. To use a different logging driver for a container, the log system must be configured properly on the container instance (or on a different log server for remote logging options). For more information on the options for different supported log drivers, see `Configure logging drivers <https://docs.aws.amazon.com/https://docs.docker.com/engine/admin/logging/overview/>`_ in the Docker documentation. .. epigraph:: Amazon ECS currently supports a subset of the logging drivers available to the Docker daemon (shown in the `LogConfiguration <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_LogConfiguration.html>`_ data type). Additional log drivers may be available in future releases of the Amazon ECS container agent. This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version --format '{{.Server.APIVersion}}'`` .. epigraph:: The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with the ``ECS_AVAILABLE_LOGGING_DRIVERS`` environment variable before containers placed on that instance can use these log configuration options. For more information, see `Amazon ECS Container Agent Configuration <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n")
    memory: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the memory specified here, the container is killed. The total amount of memory reserved for all containers within a task must be lower than the task ``memory`` value, if one is specified. This parameter maps to ``Memory`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--memory`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . If using the Fargate launch type, this parameter is optional. If using the EC2 launch type, you must specify either a task-level memory value or a container-level memory value. If you specify both a container-level ``memory`` and ``memoryReservation`` value, ``memory`` must be greater than ``memoryReservation`` . If you specify ``memoryReservation`` , then that value is subtracted from the available memory resources for the container instance where the container is placed. Otherwise, the value of ``memory`` is used. The Docker 20.10.0 or later daemon reserves a minimum of 6 MiB of memory for a container, so you should not specify fewer than 6 MiB of memory for your containers. The Docker 19.03.13-ce or earlier daemon reserves a minimum of 4 MiB of memory for a container, so you should not specify fewer than 4 MiB of memory for your containers.\n')
    memory_reservation: typing.Union[int, float, None] = pydantic.Field(None, description="The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the ``memory`` parameter (if applicable), or all of the available memory on the container instance, whichever comes first. This parameter maps to ``MemoryReservation`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--memory-reservation`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . If a task-level memory value is not specified, you must specify a non-zero integer for one or both of ``memory`` or ``memoryReservation`` in a container definition. If you specify both, ``memory`` must be greater than ``memoryReservation`` . If you specify ``memoryReservation`` , then that value is subtracted from the available memory resources for the container instance where the container is placed. Otherwise, the value of ``memory`` is used. For example, if your container normally uses 128 MiB of memory, but occasionally bursts to 256 MiB of memory for short periods of time, you can set a ``memoryReservation`` of 128 MiB, and a ``memory`` hard limit of 300 MiB. This configuration would allow the container to only reserve 128 MiB of memory from the remaining resources on the container instance, but also allow the container to consume more memory resources when needed. The Docker 20.10.0 or later daemon reserves a minimum of 6 MiB of memory for a container. So, don't specify less than 6 MiB of memory for your containers. The Docker 19.03.13-ce or earlier daemon reserves a minimum of 4 MiB of memory for a container. So, don't specify less than 4 MiB of memory for your containers.\n")
    mount_points: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_MountPointPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The mount points for data volumes in your container. This parameter maps to ``Volumes`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--volume`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . Windows containers can mount whole directories on the same drive as ``$env:ProgramData`` . Windows containers can't mount directories on a different drive, and mount point can't be across drives.\n")
    port_mappings: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_PortMappingPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The list of port mappings for the container. Port mappings allow containers to access ports on the host container instance to send or receive traffic. For task definitions that use the ``awsvpc`` network mode, you should only specify the ``containerPort`` . The ``hostPort`` can be left blank or it must be the same value as the ``containerPort`` . Port mappings on Windows use the ``NetNAT`` gateway address rather than ``localhost`` . There is no loopback for port mappings on Windows, so you cannot access a container's mapped port from the host itself. This parameter maps to ``PortBindings`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--publish`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . If the network mode of a task definition is set to ``none`` , then you can't specify port mappings. If the network mode of a task definition is set to ``host`` , then host ports must either be undefined or they must match the container port in the port mapping. .. epigraph:: After a task reaches the ``RUNNING`` status, manual and automatic host and container port assignments are visible in the *Network Bindings* section of a container description for a selected task in the Amazon ECS console. The assignments are also visible in the ``networkBindings`` section `DescribeTasks <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_DescribeTasks.html>`_ responses.\n")
    privileged: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this parameter is true, the container is given elevated privileges on the host container instance (similar to the ``root`` user). This parameter maps to ``Privileged`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--privileged`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: This parameter is not supported for Windows containers or tasks run on AWS Fargate .\n')
    pseudo_terminal: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this parameter is ``true`` , a TTY is allocated. This parameter maps to ``Tty`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--tty`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ .\n')
    readonly_root_filesystem: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. This parameter maps to ``ReadonlyRootfs`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--read-only`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: This parameter is not supported for Windows containers.\n')
    repository_credentials: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_RepositoryCredentialsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The private repository authentication credentials to use.\n')
    resource_requirements: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ResourceRequirementPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The type and amount of a resource to assign to a container. The only supported resource is a GPU.\n')
    secrets: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The secrets to pass to the container. For more information, see `Specifying Sensitive Data <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    start_timeout: typing.Union[int, float, None] = pydantic.Field(None, description="Time duration (in seconds) to wait before giving up on resolving dependencies for a container. For example, you specify two containers in a task definition with containerA having a dependency on containerB reaching a ``COMPLETE`` , ``SUCCESS`` , or ``HEALTHY`` status. If a ``startTimeout`` value is specified for containerB and it doesn't reach the desired status within that time then containerA gives up and not start. This results in the task transitioning to a ``STOPPED`` state. .. epigraph:: When the ``ECS_CONTAINER_START_TIMEOUT`` container agent configuration variable is used, it's enforced independently from this start timeout value. For tasks using the Fargate launch type, the task or service requires the following platforms: - Linux platform version ``1.3.0`` or later. - Windows platform version ``1.0.0`` or later. For tasks using the EC2 launch type, your container instances require at least version ``1.26.0`` of the container agent to use a container start timeout value. However, we recommend using the latest container agent version. For information about checking your agent version and updating to the latest version, see `Updating the Amazon ECS Container Agent <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-update.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If you're using an Amazon ECS-optimized Linux AMI, your instance needs at least version ``1.26.0-1`` of the ``ecs-init`` package. If your container instances are launched from version ``20190301`` or later, then they contain the required versions of the container agent and ``ecs-init`` . For more information, see `Amazon ECS-optimized Linux AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_ in the *Amazon Elastic Container Service Developer Guide* . The valid values are 2-120 seconds.\n")
    stop_timeout: typing.Union[int, float, None] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. For tasks using the Fargate launch type, the task or service requires the following platforms: - Linux platform version ``1.3.0`` or later. - Windows platform version ``1.0.0`` or later. The max stop timeout value is 120 seconds and if the parameter is not specified, the default value of 30 seconds is used. For tasks that use the EC2 launch type, if the ``stopTimeout`` parameter isn't specified, the value set for the Amazon ECS container agent configuration variable ``ECS_CONTAINER_STOP_TIMEOUT`` is used. If neither the ``stopTimeout`` parameter or the ``ECS_CONTAINER_STOP_TIMEOUT`` agent configuration variable are set, then the default values of 30 seconds for Linux containers and 30 seconds on Windows containers are used. Your container instances require at least version 1.26.0 of the container agent to use a container stop timeout value. However, we recommend using the latest container agent version. For information about checking your agent version and updating to the latest version, see `Updating the Amazon ECS Container Agent <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-update.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If you're using an Amazon ECS-optimized Linux AMI, your instance needs at least version 1.26.0-1 of the ``ecs-init`` package. If your container instances are launched from version ``20190301`` or later, then they contain the required versions of the container agent and ``ecs-init`` . For more information, see `Amazon ECS-optimized Linux AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_ in the *Amazon Elastic Container Service Developer Guide* . The valid values are 2-120 seconds.\n")
    system_controls: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_SystemControlPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. This parameter maps to ``Sysctls`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--sysctl`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . For example, you can configure ``net.ipv4.tcp_keepalive_time`` setting to maintain longer lived connections.\n')
    ulimits: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_UlimitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="A list of ``ulimits`` to set in the container. This parameter maps to ``Ulimits`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--ulimit`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . Valid naming values are displayed in the `Ulimit <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_Ulimit.html>`_ data type. This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version --format '{{.Server.APIVersion}}'`` .. epigraph:: This parameter is not supported for Windows containers.\n")
    user: typing.Optional[str] = pydantic.Field(None, description="The user to use inside the container. This parameter maps to ``User`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--user`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: When running tasks using the ``host`` network mode, don't run containers using the root user (UID 0). We recommend using a non-root user for better security. You can specify the ``user`` using the following formats. If specifying a UID or GID, you must specify it as a positive integer. - ``user`` - ``user:group`` - ``uid`` - ``uid:gid`` - ``user:gid`` - ``uid:group`` .. epigraph:: This parameter is not supported for Windows containers.\n")
    volumes_from: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_VolumeFromPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Data volumes to mount from another container. This parameter maps to ``VolumesFrom`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--volumes-from`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ .\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory to run commands inside the container in. This parameter maps to ``WorkingDir`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--workdir`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-containerdefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    container_definition_property = ecs.CfnTaskDefinition.ContainerDefinitionProperty(\n        image="image",\n        name="name",\n\n        # the properties below are optional\n        command=["command"],\n        cpu=123,\n        credential_specs=["credentialSpecs"],\n        depends_on=[ecs.CfnTaskDefinition.ContainerDependencyProperty(\n            condition="condition",\n            container_name="containerName"\n        )],\n        disable_networking=False,\n        dns_search_domains=["dnsSearchDomains"],\n        dns_servers=["dnsServers"],\n        docker_labels={\n            "docker_labels_key": "dockerLabels"\n        },\n        docker_security_options=["dockerSecurityOptions"],\n        entry_point=["entryPoint"],\n        environment=[ecs.CfnTaskDefinition.KeyValuePairProperty(\n            name="name",\n            value="value"\n        )],\n        environment_files=[ecs.CfnTaskDefinition.EnvironmentFileProperty(\n            type="type",\n            value="value"\n        )],\n        essential=False,\n        extra_hosts=[ecs.CfnTaskDefinition.HostEntryProperty(\n            hostname="hostname",\n            ip_address="ipAddress"\n        )],\n        firelens_configuration=ecs.CfnTaskDefinition.FirelensConfigurationProperty(\n            options={\n                "options_key": "options"\n            },\n            type="type"\n        ),\n        health_check=ecs.CfnTaskDefinition.HealthCheckProperty(\n            command=["command"],\n            interval=123,\n            retries=123,\n            start_period=123,\n            timeout=123\n        ),\n        hostname="hostname",\n        interactive=False,\n        links=["links"],\n        linux_parameters=ecs.CfnTaskDefinition.LinuxParametersProperty(\n            capabilities=ecs.CfnTaskDefinition.KernelCapabilitiesProperty(\n                add=["add"],\n                drop=["drop"]\n            ),\n            devices=[ecs.CfnTaskDefinition.DeviceProperty(\n                container_path="containerPath",\n                host_path="hostPath",\n                permissions=["permissions"]\n            )],\n            init_process_enabled=False,\n            max_swap=123,\n            shared_memory_size=123,\n            swappiness=123,\n            tmpfs=[ecs.CfnTaskDefinition.TmpfsProperty(\n                size=123,\n\n                # the properties below are optional\n                container_path="containerPath",\n                mount_options=["mountOptions"]\n            )]\n        ),\n        log_configuration=ecs.CfnTaskDefinition.LogConfigurationProperty(\n            log_driver="logDriver",\n\n            # the properties below are optional\n            options={\n                "options_key": "options"\n            },\n            secret_options=[ecs.CfnTaskDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )]\n        ),\n        memory=123,\n        memory_reservation=123,\n        mount_points=[ecs.CfnTaskDefinition.MountPointProperty(\n            container_path="containerPath",\n            read_only=False,\n            source_volume="sourceVolume"\n        )],\n        port_mappings=[ecs.CfnTaskDefinition.PortMappingProperty(\n            app_protocol="appProtocol",\n            container_port=123,\n            container_port_range="containerPortRange",\n            host_port=123,\n            name="name",\n            protocol="protocol"\n        )],\n        privileged=False,\n        pseudo_terminal=False,\n        readonly_root_filesystem=False,\n        repository_credentials=ecs.CfnTaskDefinition.RepositoryCredentialsProperty(\n            credentials_parameter="credentialsParameter"\n        ),\n        resource_requirements=[ecs.CfnTaskDefinition.ResourceRequirementProperty(\n            type="type",\n            value="value"\n        )],\n        secrets=[ecs.CfnTaskDefinition.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )],\n        start_timeout=123,\n        stop_timeout=123,\n        system_controls=[ecs.CfnTaskDefinition.SystemControlProperty(\n            namespace="namespace",\n            value="value"\n        )],\n        ulimits=[ecs.CfnTaskDefinition.UlimitProperty(\n            hard_limit=123,\n            name="name",\n            soft_limit=123\n        )],\n        user="user",\n        volumes_from=[ecs.CfnTaskDefinition.VolumeFromProperty(\n            read_only=False,\n            source_container="sourceContainer"\n        )],\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'name', 'command', 'cpu', 'credential_specs', 'depends_on', 'disable_networking', 'dns_search_domains', 'dns_servers', 'docker_labels', 'docker_security_options', 'entry_point', 'environment', 'environment_files', 'essential', 'extra_hosts', 'firelens_configuration', 'health_check', 'hostname', 'interactive', 'links', 'linux_parameters', 'log_configuration', 'memory', 'memory_reservation', 'mount_points', 'port_mappings', 'privileged', 'pseudo_terminal', 'readonly_root_filesystem', 'repository_credentials', 'resource_requirements', 'secrets', 'start_timeout', 'stop_timeout', 'system_controls', 'ulimits', 'user', 'volumes_from', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.ContainerDefinitionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.ContainerDependencyProperty
class CfnTaskDefinition_ContainerDependencyPropertyDef(BaseStruct):
    condition: typing.Optional[str] = pydantic.Field(None, description="The dependency condition of the container. The following are the available conditions and their behavior:. - ``START`` - This condition emulates the behavior of links and volumes today. It validates that a dependent container is started before permitting other containers to start. - ``COMPLETE`` - This condition validates that a dependent container runs to completion (exits) before permitting other containers to start. This can be useful for nonessential containers that run a script and then exit. This condition can't be set on an essential container. - ``SUCCESS`` - This condition is the same as ``COMPLETE`` , but it also requires that the container exits with a ``zero`` status. This condition can't be set on an essential container. - ``HEALTHY`` - This condition validates that the dependent container passes its Docker health check before permitting other containers to start. This requires that the dependent container has health checks configured. This condition is confirmed only at task startup.\n")
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of a container.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-containerdependency.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    container_dependency_property = ecs.CfnTaskDefinition.ContainerDependencyProperty(\n        condition="condition",\n        container_name="containerName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['condition', 'container_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.ContainerDependencyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.DeviceProperty
class CfnTaskDefinition_DevicePropertyDef(BaseStruct):
    container_path: typing.Optional[str] = pydantic.Field(None, description='The path inside the container at which to expose the host device.\n')
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path for the device on the host container instance.\n')
    permissions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The explicit permissions to provide to the container for the device. By default, the container has permissions for ``read`` , ``write`` , and ``mknod`` for the device.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-device.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    device_property = ecs.CfnTaskDefinition.DeviceProperty(\n        container_path="containerPath",\n        host_path="hostPath",\n        permissions=["permissions"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'host_path', 'permissions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.DeviceProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.DockerVolumeConfigurationProperty
class CfnTaskDefinition_DockerVolumeConfigurationPropertyDef(BaseStruct):
    autoprovision: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="If this value is ``true`` , the Docker volume is created if it doesn't already exist. .. epigraph:: This field is only used if the ``scope`` is ``shared`` .\n")
    driver: typing.Optional[str] = pydantic.Field(None, description='The Docker volume driver to use. The driver value must match the driver name provided by Docker because it is used for task placement. If the driver was installed using the Docker plugin CLI, use ``docker plugin ls`` to retrieve the driver name from your container instance. If the driver was installed using another method, use Docker plugin discovery to retrieve the driver name. For more information, see `Docker plugin discovery <https://docs.aws.amazon.com/https://docs.docker.com/engine/extend/plugin_api/#plugin-discovery>`_ . This parameter maps to ``Driver`` in the `Create a volume <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/VolumeCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``xxdriver`` option to `docker volume create <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/commandline/volume_create/>`_ .\n')
    driver_opts: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='A map of Docker driver-specific options passed through. This parameter maps to ``DriverOpts`` in the `Create a volume <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/VolumeCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``xxopt`` option to `docker volume create <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/commandline/volume_create/>`_ .\n')
    labels: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='Custom metadata to add to your Docker volume. This parameter maps to ``Labels`` in the `Create a volume <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/VolumeCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``xxlabel`` option to `docker volume create <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/commandline/volume_create/>`_ .\n')
    _init_params: typing.ClassVar[list[str]] = ['autoprovision', 'driver', 'driver_opts', 'labels']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.DockerVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.EFSVolumeConfigurationProperty
class CfnTaskDefinition_EFSVolumeConfigurationPropertyDef(BaseStruct):
    filesystem_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon EFS file system ID to use.\n')
    authorization_config: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_AuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The authorization configuration details for the Amazon EFS file system.\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume will be used. Specifying ``/`` will have the same effect as omitting this parameter. .. epigraph:: If an EFS access point is specified in the ``authorizationConfig`` , the root directory parameter must either be omitted or set to ``/`` which will enforce the path set on the EFS access point.\n')
    transit_encryption: typing.Optional[str] = pydantic.Field(None, description='Determines whether to use encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Transit encryption must be turned on if Amazon EFS IAM authorization is used. If this parameter is omitted, the default value of ``DISABLED`` is used. For more information, see `Encrypting data in transit <https://docs.aws.amazon.com/efs/latest/ug/encryption-in-transit.html>`_ in the *Amazon Elastic File System User Guide* .\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. If you do not specify a transit encryption port, it will use the port selection strategy that the Amazon EFS mount helper uses. For more information, see `EFS mount helper <https://docs.aws.amazon.com/efs/latest/ug/efs-mount-helper.html>`_ in the *Amazon Elastic File System User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-efsvolumeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    e_fSVolume_configuration_property = ecs.CfnTaskDefinition.EFSVolumeConfigurationProperty(\n        filesystem_id="filesystemId",\n\n        # the properties below are optional\n        authorization_config=ecs.CfnTaskDefinition.AuthorizationConfigProperty(\n            access_point_id="accessPointId",\n            iam="iam"\n        ),\n        root_directory="rootDirectory",\n        transit_encryption="transitEncryption",\n        transit_encryption_port=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['filesystem_id', 'authorization_config', 'root_directory', 'transit_encryption', 'transit_encryption_port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.EFSVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.EnvironmentFileProperty
class CfnTaskDefinition_EnvironmentFilePropertyDef(BaseStruct):
    type: typing.Optional[str] = pydantic.Field(None, description='The file type to use. Environment files are objects in Amazon S3. The only supported value is ``s3`` .\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the Amazon S3 object containing the environment variable file.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-environmentfile.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    environment_file_property = ecs.CfnTaskDefinition.EnvironmentFileProperty(\n        type="type",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.EnvironmentFileProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.EphemeralStorageProperty
class CfnTaskDefinition_EphemeralStoragePropertyDef(BaseStruct):
    size_in_gib: typing.Union[int, float, None] = pydantic.Field(None, description='The total amount, in GiB, of ephemeral storage to set for the task. The minimum supported value is ``20`` GiB and the maximum supported value is ``200`` GiB.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-ephemeralstorage.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    ephemeral_storage_property = ecs.CfnTaskDefinition.EphemeralStorageProperty(\n        size_in_gi_b=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['size_in_gib']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.EphemeralStorageProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.FirelensConfigurationProperty
class CfnTaskDefinition_FirelensConfigurationPropertyDef(BaseStruct):
    options: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='The options to use when configuring the log router. This field is optional and can be used to add additional metadata, such as the task, task definition, cluster, and container instance details to the log event. If specified, valid option keys are: - ``enable-ecs-log-metadata`` , which can be ``true`` or ``false`` - ``config-file-type`` , which can be ``s3`` or ``file`` - ``config-file-value`` , which is either an S3 ARN or a file path\n')
    type: typing.Optional[str] = pydantic.Field(None, description='The log router to use. The valid values are ``fluentd`` or ``fluentbit`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-firelensconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    firelens_configuration_property = ecs.CfnTaskDefinition.FirelensConfigurationProperty(\n        options={\n            "options_key": "options"\n        },\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['options', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.FirelensConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.FSxAuthorizationConfigProperty
class CfnTaskDefinition_FSxAuthorizationConfigPropertyDef(BaseStruct):
    credentials_parameter: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    domain: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    _init_params: typing.ClassVar[list[str]] = ['credentials_parameter', 'domain']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.FSxAuthorizationConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.FSxWindowsFileServerVolumeConfigurationProperty
class CfnTaskDefinition_FSxWindowsFileServerVolumeConfigurationPropertyDef(BaseStruct):
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon FSx for Windows File Server file system ID to use.\n')
    root_directory: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The directory within the Amazon FSx for Windows File Server file system to mount as the root directory inside the host.\n')
    authorization_config: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_FSxAuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The authorization configuration details for the Amazon FSx for Windows File Server file system.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-fsxwindowsfileservervolumeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    f_sx_windows_file_server_volume_configuration_property = ecs.CfnTaskDefinition.FSxWindowsFileServerVolumeConfigurationProperty(\n        file_system_id="fileSystemId",\n        root_directory="rootDirectory",\n\n        # the properties below are optional\n        authorization_config=ecs.CfnTaskDefinition.FSxAuthorizationConfigProperty(\n            credentials_parameter="credentialsParameter",\n            domain="domain"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_system_id', 'root_directory', 'authorization_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.FSxWindowsFileServerVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.HealthCheckProperty
class CfnTaskDefinition_HealthCheckPropertyDef(BaseStruct):
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A string array representing the command that the container runs to determine if it is healthy. The string array must start with ``CMD`` to run the command arguments directly, or ``CMD-SHELL`` to run the command with the container\'s default shell. When you use the AWS Management Console JSON panel, the AWS Command Line Interface , or the APIs, enclose the list of commands in double quotes and brackets. ``[ "CMD-SHELL", "curl -f http://localhost/ || exit 1" ]`` You don\'t include the double quotes and brackets when you use the AWS Management Console. ``CMD-SHELL, curl -f http://localhost/ || exit 1`` An exit code of 0 indicates success, and non-zero exit code indicates failure. For more information, see ``HealthCheck`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ .\n')
    interval: typing.Union[int, float, None] = pydantic.Field(None, description='The time period in seconds between each health check execution. You may specify between 5 and 300 seconds. The default value is 30 seconds.\n')
    retries: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a failed health check before the container is considered unhealthy. You may specify between 1 and 10 retries. The default value is 3.\n')
    start_period: typing.Union[int, float, None] = pydantic.Field(None, description='The optional grace period to provide containers time to bootstrap before failed health checks count towards the maximum number of retries. You can specify between 0 and 300 seconds. By default, the ``startPeriod`` is off. .. epigraph:: If a health check succeeds within the ``startPeriod`` , then the container is considered healthy and any subsequent failures count toward the maximum number of retries.\n')
    timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The time period in seconds to wait for a health check to succeed before it is considered a failure. You may specify between 2 and 60 seconds. The default value is 5.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-healthcheck.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    health_check_property = ecs.CfnTaskDefinition.HealthCheckProperty(\n        command=["command"],\n        interval=123,\n        retries=123,\n        start_period=123,\n        timeout=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['command', 'interval', 'retries', 'start_period', 'timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.HealthCheckProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.HostEntryProperty
class CfnTaskDefinition_HostEntryPropertyDef(BaseStruct):
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use in the ``/etc/hosts`` entry.\n')
    ip_address: typing.Optional[str] = pydantic.Field(None, description='The IP address to use in the ``/etc/hosts`` entry.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-hostentry.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    host_entry_property = ecs.CfnTaskDefinition.HostEntryProperty(\n        hostname="hostname",\n        ip_address="ipAddress"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['hostname', 'ip_address']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.HostEntryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.HostVolumePropertiesProperty
class CfnTaskDefinition_HostVolumePropertiesPropertyDef(BaseStruct):
    source_path: typing.Optional[str] = pydantic.Field(None, description='When the ``host`` parameter is used, specify a ``sourcePath`` to declare the path on the host container instance that\'s presented to the container. If this parameter is empty, then the Docker daemon has assigned a host path for you. If the ``host`` parameter contains a ``sourcePath`` file location, then the data volume persists at the specified location on the host container instance until you delete it manually. If the ``sourcePath`` value doesn\'t exist on the host container instance, the Docker daemon creates it. If the location does exist, the contents of the source path folder are exported. If you\'re using the Fargate launch type, the ``sourcePath`` parameter is not supported.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-hostvolumeproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    host_volume_properties_property = ecs.CfnTaskDefinition.HostVolumePropertiesProperty(\n        source_path="sourcePath"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['source_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.HostVolumePropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.InferenceAcceleratorProperty
class CfnTaskDefinition_InferenceAcceleratorPropertyDef(BaseStruct):
    device_name: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator device name. The ``deviceName`` must also be referenced in a container definition as a `ResourceRequirement <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ResourceRequirement.html>`_ .\n')
    device_type: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator type to use.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-inferenceaccelerator.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    inference_accelerator_property = ecs.CfnTaskDefinition.InferenceAcceleratorProperty(\n        device_name="deviceName",\n        device_type="deviceType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['device_name', 'device_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.InferenceAcceleratorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.KernelCapabilitiesProperty
class CfnTaskDefinition_KernelCapabilitiesPropertyDef(BaseStruct):
    add: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The Linux capabilities for the container that have been added to the default configuration provided by Docker. This parameter maps to ``CapAdd`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--cap-add`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: Tasks launched on AWS Fargate only support adding the ``SYS_PTRACE`` kernel capability. Valid values: ``"ALL" | "AUDIT_CONTROL" | "AUDIT_WRITE" | "BLOCK_SUSPEND" | "CHOWN" | "DAC_OVERRIDE" | "DAC_READ_SEARCH" | "FOWNER" | "FSETID" | "IPC_LOCK" | "IPC_OWNER" | "KILL" | "LEASE" | "LINUX_IMMUTABLE" | "MAC_ADMIN" | "MAC_OVERRIDE" | "MKNOD" | "NET_ADMIN" | "NET_BIND_SERVICE" | "NET_BROADCAST" | "NET_RAW" | "SETFCAP" | "SETGID" | "SETPCAP" | "SETUID" | "SYS_ADMIN" | "SYS_BOOT" | "SYS_CHROOT" | "SYS_MODULE" | "SYS_NICE" | "SYS_PACCT" | "SYS_PTRACE" | "SYS_RAWIO" | "SYS_RESOURCE" | "SYS_TIME" | "SYS_TTY_CONFIG" | "SYSLOG" | "WAKE_ALARM"``\n')
    drop: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The Linux capabilities for the container that have been removed from the default configuration provided by Docker. This parameter maps to ``CapDrop`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--cap-drop`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . Valid values: ``"ALL" | "AUDIT_CONTROL" | "AUDIT_WRITE" | "BLOCK_SUSPEND" | "CHOWN" | "DAC_OVERRIDE" | "DAC_READ_SEARCH" | "FOWNER" | "FSETID" | "IPC_LOCK" | "IPC_OWNER" | "KILL" | "LEASE" | "LINUX_IMMUTABLE" | "MAC_ADMIN" | "MAC_OVERRIDE" | "MKNOD" | "NET_ADMIN" | "NET_BIND_SERVICE" | "NET_BROADCAST" | "NET_RAW" | "SETFCAP" | "SETGID" | "SETPCAP" | "SETUID" | "SYS_ADMIN" | "SYS_BOOT" | "SYS_CHROOT" | "SYS_MODULE" | "SYS_NICE" | "SYS_PACCT" | "SYS_PTRACE" | "SYS_RAWIO" | "SYS_RESOURCE" | "SYS_TIME" | "SYS_TTY_CONFIG" | "SYSLOG" | "WAKE_ALARM"``\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-kernelcapabilities.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    kernel_capabilities_property = ecs.CfnTaskDefinition.KernelCapabilitiesProperty(\n        add=["add"],\n        drop=["drop"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['add', 'drop']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.KernelCapabilitiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.KeyValuePairProperty
class CfnTaskDefinition_KeyValuePairPropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the key-value pair. For environment variables, this is the name of the environment variable.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value of the key-value pair. For environment variables, this is the value of the environment variable.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-keyvaluepair.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    key_value_pair_property = ecs.CfnTaskDefinition.KeyValuePairProperty(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.KeyValuePairProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.LinuxParametersProperty
class CfnTaskDefinition_LinuxParametersPropertyDef(BaseStruct):
    capabilities: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_KernelCapabilitiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The Linux capabilities for the container that are added to or dropped from the default configuration provided by Docker. .. epigraph:: For tasks that use the Fargate launch type, ``capabilities`` is supported for all platform versions but the ``add`` parameter is only supported if using platform version 1.4.0 or later.\n')
    devices: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_DevicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Any host devices to expose to the container. This parameter maps to ``Devices`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.35/>`_ and the ``--device`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: If you're using tasks that use the Fargate launch type, the ``devices`` parameter isn't supported.\n")
    init_process_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Run an ``init`` process inside the container that forwards signals and reaps processes. This parameter maps to the ``--init`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . This parameter requires version 1.25 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version --format '{{.Server.APIVersion}}'``\n")
    max_swap: typing.Union[int, float, None] = pydantic.Field(None, description="The total amount of swap memory (in MiB) a container can use. This parameter will be translated to the ``--memory-swap`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ where the value would be the sum of the container memory plus the ``maxSwap`` value. If a ``maxSwap`` value of ``0`` is specified, the container will not use swap. Accepted values are ``0`` or any positive integer. If the ``maxSwap`` parameter is omitted, the container will use the swap configuration for the container instance it is running on. A ``maxSwap`` value must be set for the ``swappiness`` parameter to be used. .. epigraph:: If you're using tasks that use the Fargate launch type, the ``maxSwap`` parameter isn't supported. If you're using tasks on Amazon Linux 2023 the ``swappiness`` parameter isn't supported.\n")
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The value for the size (in MiB) of the ``/dev/shm`` volume. This parameter maps to the ``--shm-size`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: If you are using tasks that use the Fargate launch type, the ``sharedMemorySize`` parameter is not supported.\n')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description="This allows you to tune a container's memory swappiness behavior. A ``swappiness`` value of ``0`` will cause swapping to not happen unless absolutely necessary. A ``swappiness`` value of ``100`` will cause pages to be swapped very aggressively. Accepted values are whole numbers between ``0`` and ``100`` . If the ``swappiness`` parameter is not specified, a default value of ``60`` is used. If a value is not specified for ``maxSwap`` then this parameter is ignored. This parameter maps to the ``--memory-swappiness`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: If you're using tasks that use the Fargate launch type, the ``swappiness`` parameter isn't supported. If you're using tasks on Amazon Linux 2023 the ``swappiness`` parameter isn't supported.\n")
    tmpfs: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_TmpfsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The container path, mount options, and size (in MiB) of the tmpfs mount. This parameter maps to the ``--tmpfs`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#security-configuration>`_ . .. epigraph:: If you\'re using tasks that use the Fargate launch type, the ``tmpfs`` parameter isn\'t supported.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-linuxparameters.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    linux_parameters_property = ecs.CfnTaskDefinition.LinuxParametersProperty(\n        capabilities=ecs.CfnTaskDefinition.KernelCapabilitiesProperty(\n            add=["add"],\n            drop=["drop"]\n        ),\n        devices=[ecs.CfnTaskDefinition.DeviceProperty(\n            container_path="containerPath",\n            host_path="hostPath",\n            permissions=["permissions"]\n        )],\n        init_process_enabled=False,\n        max_swap=123,\n        shared_memory_size=123,\n        swappiness=123,\n        tmpfs=[ecs.CfnTaskDefinition.TmpfsProperty(\n            size=123,\n\n            # the properties below are optional\n            container_path="containerPath",\n            mount_options=["mountOptions"]\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capabilities', 'devices', 'init_process_enabled', 'max_swap', 'shared_memory_size', 'swappiness', 'tmpfs']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.LinuxParametersProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.LogConfigurationProperty
class CfnTaskDefinition_LogConfigurationPropertyDef(BaseStruct):
    log_driver: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The log driver to use for the container. For tasks on AWS Fargate , the supported log drivers are ``awslogs`` , ``splunk`` , and ``awsfirelens`` . For tasks hosted on Amazon EC2 instances, the supported log drivers are ``awslogs`` , ``fluentd`` , ``gelf`` , ``json-file`` , ``journald`` , ``logentries`` , ``syslog`` , ``splunk`` , and ``awsfirelens`` . For more information about using the ``awslogs`` log driver, see `Send Amazon ECS logs to CloudWatch <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html>`_ in the *Amazon Elastic Container Service Developer Guide* . For more information about using the ``awsfirelens`` log driver, see `Send Amazon ECS logs to an AWS service or AWS Partner <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_firelens.html>`_ . .. epigraph:: If you have a custom driver that isn't listed, you can fork the Amazon ECS container agent project that's `available on GitHub <https://docs.aws.amazon.com/https://github.com/aws/amazon-ecs-agent>`_ and customize it to work with that driver. We encourage you to submit pull requests for changes that you would like to have included. However, we don't currently provide support for running modified copies of this software.\n")
    options: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description="The configuration options to send to the log driver. This parameter requires version 1.19 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version --format '{{.Server.APIVersion}}'``\n")
    secret_options: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The secrets to pass to the log configuration. For more information, see `Specifying sensitive data <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-logconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    log_configuration_property = ecs.CfnTaskDefinition.LogConfigurationProperty(\n        log_driver="logDriver",\n\n        # the properties below are optional\n        options={\n            "options_key": "options"\n        },\n        secret_options=[ecs.CfnTaskDefinition.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.LogConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.MountPointProperty
class CfnTaskDefinition_MountPointPropertyDef(BaseStruct):
    container_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container to mount the host volume at.\n')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If this value is ``true`` , the container has read-only access to the volume. If this value is ``false`` , then the container can write to the volume. The default value is ``false`` .\n')
    source_volume: typing.Optional[str] = pydantic.Field(None, description='The name of the volume to mount. Must be a volume name referenced in the ``name`` parameter of task definition ``volume`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-mountpoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    mount_point_property = ecs.CfnTaskDefinition.MountPointProperty(\n        container_path="containerPath",\n        read_only=False,\n        source_volume="sourceVolume"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'read_only', 'source_volume']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.MountPointProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.PortMappingProperty
class CfnTaskDefinition_PortMappingPropertyDef(BaseStruct):
    app_protocol: typing.Optional[str] = pydantic.Field(None, description="The application protocol that's used for the port mapping. This parameter only applies to Service Connect. We recommend that you set this parameter to be consistent with the protocol that your application uses. If you set this parameter, Amazon ECS adds protocol-specific connection handling to the Service Connect proxy. If you set this parameter, Amazon ECS adds protocol-specific telemetry in the Amazon ECS console and CloudWatch. If you don't set a value for this parameter, then TCP is used. However, Amazon ECS doesn't add protocol-specific telemetry for TCP. ``appProtocol`` is immutable in a Service Connect service. Updating this field requires a service deletion and redeployment. Tasks that run in a namespace can use short names to connect to services in the namespace. Tasks can connect to services across all of the clusters in the namespace. Tasks connect through a managed proxy container that collects logs and metrics for increased visibility. Only the tasks that Amazon ECS services create are supported with Service Connect. For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port number on the container that's bound to the user-specified or automatically assigned host port. If you use containers in a task with the ``awsvpc`` or ``host`` network mode, specify the exposed ports using ``containerPort`` . If you use containers in a task with the ``bridge`` network mode and you specify a container port and not a host port, your container automatically receives a host port in the ephemeral port range. For more information, see ``hostPort`` . Port mappings that are automatically assigned in this way do not count toward the 100 reserved ports limit of a container instance.\n")
    container_port_range: typing.Optional[str] = pydantic.Field(None, description="The port number range on the container that's bound to the dynamically mapped host port range. The following rules apply when you specify a ``containerPortRange`` : - You must use either the ``bridge`` network mode or the ``awsvpc`` network mode. - This parameter is available for both the EC2 and AWS Fargate launch types. - This parameter is available for both the Linux and Windows operating systems. - The container instance must have at least version 1.67.0 of the container agent and at least version 1.67.0-1 of the ``ecs-init`` package - You can specify a maximum of 100 port ranges per container. - You do not specify a ``hostPortRange`` . The value of the ``hostPortRange`` is set as follows: - For containers in a task with the ``awsvpc`` network mode, the ``hostPortRange`` is set to the same value as the ``containerPortRange`` . This is a static mapping strategy. - For containers in a task with the ``bridge`` network mode, the Amazon ECS agent finds open host ports from the default ephemeral range and passes it to docker to bind them to the container ports. - The ``containerPortRange`` valid values are between 1 and 65535. - A port can only be included in one port mapping per container. - You cannot specify overlapping port ranges. - The first port in the range must be less than last port in the range. - Docker recommends that you turn off the docker-proxy in the Docker daemon config file when you have a large number of ports. For more information, see `Issue #11185 <https://docs.aws.amazon.com/https://github.com/moby/moby/issues/11185>`_ on the Github website. For information about how to turn off the docker-proxy in the Docker daemon config file, see `Docker daemon <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html#bootstrap_docker_daemon>`_ in the *Amazon ECS Developer Guide* . You can call ```DescribeTasks`` <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_DescribeTasks.html>`_ to view the ``hostPortRange`` which are the host ports that are bound to the container ports.\n")
    host_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port number on the container instance to reserve for your container. If you specify a ``containerPortRange`` , leave this field empty and the value of the ``hostPort`` is set as follows: - For containers in a task with the ``awsvpc`` network mode, the ``hostPort`` is set to the same value as the ``containerPort`` . This is a static mapping strategy. - For containers in a task with the ``bridge`` network mode, the Amazon ECS agent finds open ports on the host and automatically binds them to the container ports. This is a dynamic mapping strategy. If you use containers in a task with the ``awsvpc`` or ``host`` network mode, the ``hostPort`` can either be left blank or set to the same value as the ``containerPort`` . If you use containers in a task with the ``bridge`` network mode, you can specify a non-reserved host port for your container port mapping, or you can omit the ``hostPort`` (or set it to ``0`` ) while specifying a ``containerPort`` and your container automatically receives a port in the ephemeral port range for your container instance operating system and Docker version. The default ephemeral port range for Docker version 1.6.0 and later is listed on the instance under ``/proc/sys/net/ipv4/ip_local_port_range`` . If this kernel parameter is unavailable, the default ephemeral port range from 49153 through 65535 (Linux) or 49152 through 65535 (Windows) is used. Do not attempt to specify a host port in the ephemeral port range as these are reserved for automatic assignment. In general, ports below 32768 are outside of the ephemeral port range. The default reserved ports are 22 for SSH, the Docker ports 2375 and 2376, and the Amazon ECS container agent ports 51678-51680. Any host port that was previously specified in a running task is also reserved while the task is running. That is, after a task stops, the host port is released. The current reserved ports are displayed in the ``remainingResources`` of `DescribeContainerInstances <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_DescribeContainerInstances.html>`_ output. A container instance can have up to 100 reserved ports at a time. This number includes the default reserved ports. Automatically assigned ports aren't included in the 100 reserved ports quota.\n")
    name: typing.Optional[str] = pydantic.Field(None, description="The name that's used for the port mapping. This parameter only applies to Service Connect. This parameter is the name that you use in the ``serviceConnectConfiguration`` of a service. The name can include up to 64 characters. The characters can include lowercase letters, numbers, underscores (_), and hyphens (-). The name can't start with a hyphen. For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n")
    protocol: typing.Optional[str] = pydantic.Field(None, description='The protocol used for the port mapping. Valid values are ``tcp`` and ``udp`` . The default is ``tcp`` . ``protocol`` is immutable in a Service Connect service. Updating this field requires a service deletion and redeployment.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-portmapping.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    port_mapping_property = ecs.CfnTaskDefinition.PortMappingProperty(\n        app_protocol="appProtocol",\n        container_port=123,\n        container_port_range="containerPortRange",\n        host_port=123,\n        name="name",\n        protocol="protocol"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['app_protocol', 'container_port', 'container_port_range', 'host_port', 'name', 'protocol']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.PortMappingProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.ProxyConfigurationProperty
class CfnTaskDefinition_ProxyConfigurationPropertyDef(BaseStruct):
    container_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the container that will serve as the App Mesh proxy.\n')
    proxy_configuration_properties: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_KeyValuePairPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The set of network configuration parameters to provide the Container Network Interface (CNI) plugin, specified as key-value pairs. - ``IgnoredUID`` - (Required) The user ID (UID) of the proxy container as defined by the ``user`` parameter in a container definition. This is used to ensure the proxy ignores its own traffic. If ``IgnoredGID`` is specified, this field can be empty. - ``IgnoredGID`` - (Required) The group ID (GID) of the proxy container as defined by the ``user`` parameter in a container definition. This is used to ensure the proxy ignores its own traffic. If ``IgnoredUID`` is specified, this field can be empty. - ``AppPorts`` - (Required) The list of ports that the application uses. Network traffic to these ports is forwarded to the ``ProxyIngressPort`` and ``ProxyEgressPort`` . - ``ProxyIngressPort`` - (Required) Specifies the port that incoming traffic to the ``AppPorts`` is directed to. - ``ProxyEgressPort`` - (Required) Specifies the port that outgoing traffic from the ``AppPorts`` is directed to. - ``EgressIgnoredPorts`` - (Required) The egress traffic going to the specified ports is ignored and not redirected to the ``ProxyEgressPort`` . It can be an empty list. - ``EgressIgnoredIPs`` - (Required) The egress traffic going to the specified IP addresses is ignored and not redirected to the ``ProxyEgressPort`` . It can be an empty list.\n')
    type: typing.Optional[str] = pydantic.Field(None, description='The proxy type. The only supported value is ``APPMESH`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-proxyconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    proxy_configuration_property = ecs.CfnTaskDefinition.ProxyConfigurationProperty(\n        container_name="containerName",\n\n        # the properties below are optional\n        proxy_configuration_properties=[ecs.CfnTaskDefinition.KeyValuePairProperty(\n            name="name",\n            value="value"\n        )],\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'proxy_configuration_properties', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.ProxyConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.RepositoryCredentialsProperty
class CfnTaskDefinition_RepositoryCredentialsPropertyDef(BaseStruct):
    credentials_parameter: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the secret containing the private repository credentials. .. epigraph:: When you use the Amazon ECS API, AWS CLI , or AWS SDK, if the secret exists in the same Region as the task that you\'re launching then you can use either the full ARN or the name of the secret. When you use the AWS Management Console, you must specify the full ARN of the secret.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-repositorycredentials.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    repository_credentials_property = ecs.CfnTaskDefinition.RepositoryCredentialsProperty(\n        credentials_parameter="credentialsParameter"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['credentials_parameter']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.RepositoryCredentialsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.ResourceRequirementProperty
class CfnTaskDefinition_ResourceRequirementPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of resource to assign to a container.\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The value for the specified resource type. When the type is ``GPU`` , the value is the number of physical ``GPUs`` the Amazon ECS container agent reserves for the container. The number of GPUs that\'s reserved for all containers in a task can\'t exceed the number of available GPUs on the container instance that the task is launched on. When the type is ``InferenceAccelerator`` , the ``value`` matches the ``deviceName`` for an `InferenceAccelerator <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_InferenceAccelerator.html>`_ specified in a task definition.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-resourcerequirement.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    resource_requirement_property = ecs.CfnTaskDefinition.ResourceRequirementProperty(\n        type="type",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.ResourceRequirementProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.RuntimePlatformProperty
class CfnTaskDefinition_RuntimePlatformPropertyDef(BaseStruct):
    cpu_architecture: typing.Optional[str] = pydantic.Field(None, description='The CPU architecture. You can run your Linux tasks on an ARM-based platform by setting the value to ``ARM64`` . This option is available for tasks that run on Linux Amazon EC2 instance or Linux containers on Fargate.\n')
    operating_system_family: typing.Optional[str] = pydantic.Field(None, description='The operating system.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-runtimeplatform.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    runtime_platform_property = ecs.CfnTaskDefinition.RuntimePlatformProperty(\n        cpu_architecture="cpuArchitecture",\n        operating_system_family="operatingSystemFamily"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu_architecture', 'operating_system_family']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.RuntimePlatformProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.SecretProperty
class CfnTaskDefinition_SecretPropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the secret.\n')
    value_from: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The secret to expose to the container. The supported values are either the full ARN of the AWS Secrets Manager secret or the full ARN of the parameter in the SSM Parameter Store. For information about the require AWS Identity and Access Management permissions, see `Required IAM permissions for Amazon ECS secrets <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-secrets.html#secrets-iam>`_ (for Secrets Manager) or `Required IAM permissions for Amazon ECS secrets <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-parameters.html>`_ (for Systems Manager Parameter store) in the *Amazon Elastic Container Service Developer Guide* . .. epigraph:: If the SSM Parameter Store parameter exists in the same Region as the task you\'re launching, then you can use either the full ARN or name of the parameter. If the parameter exists in a different Region, then the full ARN must be specified.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-secret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    secret_property = ecs.CfnTaskDefinition.SecretProperty(\n        name="name",\n        value_from="valueFrom"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value_from']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.SecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.SystemControlProperty
class CfnTaskDefinition_SystemControlPropertyDef(BaseStruct):
    namespace: typing.Optional[str] = pydantic.Field(None, description='The namespaced kernel parameter to set a ``value`` for.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The namespaced kernel parameter to set a ``value`` for. Valid IPC namespace values: ``"kernel.msgmax" | "kernel.msgmnb" | "kernel.msgmni" | "kernel.sem" | "kernel.shmall" | "kernel.shmmax" | "kernel.shmmni" | "kernel.shm_rmid_forced"`` , and ``Sysctls`` that start with ``"fs.mqueue.*"`` Valid network namespace values: ``Sysctls`` that start with ``"net.*"`` All of these values are supported by Fargate.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-systemcontrol.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    system_control_property = ecs.CfnTaskDefinition.SystemControlProperty(\n        namespace="namespace",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['namespace', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.SystemControlProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.TaskDefinitionPlacementConstraintProperty
class CfnTaskDefinition_TaskDefinitionPlacementConstraintPropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of constraint. The ``MemberOf`` constraint restricts selection to be from a group of valid candidates.\n')
    expression: typing.Optional[str] = pydantic.Field(None, description='A cluster query language expression to apply to the constraint. For more information, see `Cluster query language <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-taskdefinitionplacementconstraint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    task_definition_placement_constraint_property = ecs.CfnTaskDefinition.TaskDefinitionPlacementConstraintProperty(\n        type="type",\n\n        # the properties below are optional\n        expression="expression"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'expression']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.TaskDefinitionPlacementConstraintProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.TmpfsProperty
class CfnTaskDefinition_TmpfsPropertyDef(BaseStruct):
    size: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The maximum size (in MiB) of the tmpfs volume.\n')
    container_path: typing.Optional[str] = pydantic.Field(None, description='The absolute file path where the tmpfs volume is to be mounted.\n')
    mount_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of tmpfs volume mount options. Valid values: ``"defaults" | "ro" | "rw" | "suid" | "nosuid" | "dev" | "nodev" | "exec" | "noexec" | "sync" | "async" | "dirsync" | "remount" | "mand" | "nomand" | "atime" | "noatime" | "diratime" | "nodiratime" | "bind" | "rbind" | "unbindable" | "runbindable" | "private" | "rprivate" | "shared" | "rshared" | "slave" | "rslave" | "relatime" | "norelatime" | "strictatime" | "nostrictatime" | "mode" | "uid" | "gid" | "nr_inodes" | "nr_blocks" | "mpol"``\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-tmpfs.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    tmpfs_property = ecs.CfnTaskDefinition.TmpfsProperty(\n        size=123,\n\n        # the properties below are optional\n        container_path="containerPath",\n        mount_options=["mountOptions"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['size', 'container_path', 'mount_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.TmpfsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.UlimitProperty
class CfnTaskDefinition_UlimitPropertyDef(BaseStruct):
    hard_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The hard limit for the ``ulimit`` type.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ``type`` of the ``ulimit`` .\n')
    soft_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The soft limit for the ``ulimit`` type.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-ulimit.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    ulimit_property = ecs.CfnTaskDefinition.UlimitProperty(\n        hard_limit=123,\n        name="name",\n        soft_limit=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['hard_limit', 'name', 'soft_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.UlimitProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.VolumeFromProperty
class CfnTaskDefinition_VolumeFromPropertyDef(BaseStruct):
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If this value is ``true`` , the container has read-only access to the volume. If this value is ``false`` , then the container can write to the volume. The default value is ``false`` .\n')
    source_container: typing.Optional[str] = pydantic.Field(None, description='The name of another container within the same task definition to mount volumes from.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-volumefrom.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    volume_from_property = ecs.CfnTaskDefinition.VolumeFromProperty(\n        read_only=False,\n        source_container="sourceContainer"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['read_only', 'source_container']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.VolumeFromProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition.VolumeProperty
class CfnTaskDefinition_VolumePropertyDef(BaseStruct):
    configured_at_launch: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Indicates whether the volume should be configured at launch time. This is used to create Amazon EBS volumes for standalone tasks or tasks created as part of a service. Each task definition revision may only have one volume configured at launch in the volume configuration. To configure a volume at launch time, use this task definition revision and specify a ``volumeConfigurations`` object when calling the ``CreateService`` , ``UpdateService`` , ``RunTask`` or ``StartTask`` APIs.\n')
    docker_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_DockerVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This parameter is specified when you use Docker volumes. Windows containers only support the use of the ``local`` driver. To use bind mounts, specify the ``host`` parameter instead. .. epigraph:: Docker volumes aren't supported by tasks run on AWS Fargate .\n")
    efs_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_EFSVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This parameter is specified when you use an Amazon Elastic File System file system for task storage.\n')
    f_sx_windows_file_server_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_FSxWindowsFileServerVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This parameter is specified when you use Amazon FSx for Windows File Server file system for task storage.\n')
    host: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_HostVolumePropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This parameter is specified when you use bind mount host volumes. The contents of the ``host`` parameter determine whether your bind mount host volume persists on the host container instance and where it's stored. If the ``host`` parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data isn't guaranteed to persist after the containers that are associated with it stop running. Windows containers can mount whole directories on the same drive as ``$env:ProgramData`` . Windows containers can't mount directories on a different drive, and mount point can't be across drives. For example, you can mount ``C:\\my\\path:C:\\my\\path`` and ``D:\\:D:\\`` , but not ``D:\\my\\path:C:\\my\\path`` or ``D:\\:C:\\my\\path`` .\n")
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the volume. Up to 255 letters (uppercase and lowercase), numbers, underscores, and hyphens are allowed. When using a volume configured at launch, the ``name`` is required and must also be specified as the volume name in the ``ServiceVolumeConfiguration`` or ``TaskVolumeConfiguration`` parameter when creating your service or standalone task. For all other types of volumes, this name is referenced in the ``sourceVolume`` parameter of the ``mountPoints`` object in the container definition. When a volume is using the ``efsVolumeConfiguration`` , the name is required.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-volume.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    volume_property = ecs.CfnTaskDefinition.VolumeProperty(\n        configured_at_launch=False,\n        docker_volume_configuration=ecs.CfnTaskDefinition.DockerVolumeConfigurationProperty(\n            autoprovision=False,\n            driver="driver",\n            driver_opts={\n                "driver_opts_key": "driverOpts"\n            },\n            labels={\n                "labels_key": "labels"\n            },\n            scope="scope"\n        ),\n        efs_volume_configuration=ecs.CfnTaskDefinition.EFSVolumeConfigurationProperty(\n            filesystem_id="filesystemId",\n\n            # the properties below are optional\n            authorization_config=ecs.CfnTaskDefinition.AuthorizationConfigProperty(\n                access_point_id="accessPointId",\n                iam="iam"\n            ),\n            root_directory="rootDirectory",\n            transit_encryption="transitEncryption",\n            transit_encryption_port=123\n        ),\n        f_sx_windows_file_server_volume_configuration=ecs.CfnTaskDefinition.FSxWindowsFileServerVolumeConfigurationProperty(\n            file_system_id="fileSystemId",\n            root_directory="rootDirectory",\n\n            # the properties below are optional\n            authorization_config=ecs.CfnTaskDefinition.FSxAuthorizationConfigProperty(\n                credentials_parameter="credentialsParameter",\n                domain="domain"\n            )\n        ),\n        host=ecs.CfnTaskDefinition.HostVolumePropertiesProperty(\n            source_path="sourcePath"\n        ),\n        name="name"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['configured_at_launch', 'docker_volume_configuration', 'efs_volume_configuration', 'f_sx_windows_file_server_volume_configuration', 'host', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition.VolumeProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskSet.AwsVpcConfigurationProperty
class CfnTaskSet_AwsVpcConfigurationPropertyDef(BaseStruct):
    subnets: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The IDs of the subnets associated with the task or service. There's a limit of 16 subnets that can be specified per ``AwsVpcConfiguration`` . .. epigraph:: All specified subnets must be from the same VPC.\n")
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description="Whether the task's elastic network interface receives a public IP address. The default value is ``DISABLED`` .\n")
    security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The IDs of the security groups associated with the task or service. If you don\'t specify a security group, the default security group for the VPC is used. There\'s a limit of 5 security groups that can be specified per ``AwsVpcConfiguration`` . .. epigraph:: All specified security groups must be from the same VPC.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskset-awsvpcconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    aws_vpc_configuration_property = ecs.CfnTaskSet.AwsVpcConfigurationProperty(\n        subnets=["subnets"],\n\n        # the properties below are optional\n        assign_public_ip="assignPublicIp",\n        security_groups=["securityGroups"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['subnets', 'assign_public_ip', 'security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskSet.AwsVpcConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskSet.LoadBalancerProperty
class CfnTaskSet_LoadBalancerPropertyDef(BaseStruct):
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container (as it appears in a container definition) to associate with the load balancer. You need to specify the container name when configuring the target group for an Amazon ECS load balancer.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port on the container to associate with the load balancer. This port must correspond to a ``containerPort`` in the task definition the tasks in the service are using. For tasks that use the EC2 launch type, the container instance they're launched on must allow ingress traffic on the ``hostPort`` of the port mapping.\n")
    target_group_arn: typing.Optional[str] = pydantic.Field(None, description='The full Amazon Resource Name (ARN) of the Elastic Load Balancing target group or groups associated with a service or task set. A target group ARN is only specified when using an Application Load Balancer or Network Load Balancer. For services using the ``ECS`` deployment controller, you can specify one or multiple target groups. For more information, see `Registering multiple target groups with a service <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/register-multiple-targetgroups.html>`_ in the *Amazon Elastic Container Service Developer Guide* . For services using the ``CODE_DEPLOY`` deployment controller, you\'re required to define two target groups for the load balancer. For more information, see `Blue/green deployment with CodeDeploy <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html>`_ in the *Amazon Elastic Container Service Developer Guide* . .. epigraph:: If your service\'s task definition uses the ``awsvpc`` network mode, you must choose ``ip`` as the target type, not ``instance`` . Do this when creating your target groups because tasks that use the ``awsvpc`` network mode are associated with an elastic network interface, not an Amazon EC2 instance. This network mode is required for the Fargate launch type.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskset-loadbalancer.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    load_balancer_property = ecs.CfnTaskSet.LoadBalancerProperty(\n        container_name="containerName",\n        container_port=123,\n        target_group_arn="targetGroupArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'container_port', 'target_group_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskSet.LoadBalancerProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskSet.NetworkConfigurationProperty
class CfnTaskSet_NetworkConfigurationPropertyDef(BaseStruct):
    aws_vpc_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_AwsVpcConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC subnets and security groups that are associated with a task. .. epigraph:: All specified subnets and security groups must be from the same VPC.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskset-networkconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    network_configuration_property = ecs.CfnTaskSet.NetworkConfigurationProperty(\n        aws_vpc_configuration=ecs.CfnTaskSet.AwsVpcConfigurationProperty(\n            subnets=["subnets"],\n\n            # the properties below are optional\n            assign_public_ip="assignPublicIp",\n            security_groups=["securityGroups"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['aws_vpc_configuration']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskSet.NetworkConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskSet.ScaleProperty
class CfnTaskSet_ScalePropertyDef(BaseStruct):
    unit: typing.Optional[str] = pydantic.Field(None, description='The unit of measure for the scale value.\n')
    value: typing.Union[int, float, None] = pydantic.Field(None, description='The value, specified as a percent total of a service\'s ``desiredCount`` , to scale the task set. Accepted values are numbers between 0 and 100.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskset-scale.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    scale_property = ecs.CfnTaskSet.ScaleProperty(\n        unit="unit",\n        value=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['unit', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskSet.ScaleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskSet.ServiceRegistryProperty
class CfnTaskSet_ServiceRegistryPropertyDef(BaseStruct):
    container_name: typing.Optional[str] = pydantic.Field(None, description="The container name value to be used for your service discovery service. It's already specified in the task definition. If the task definition that your service task specifies uses the ``bridge`` or ``host`` network mode, you must specify a ``containerName`` and ``containerPort`` combination from the task definition. If the task definition that your service task specifies uses the ``awsvpc`` network mode and a type SRV DNS record is used, you must specify either a ``containerName`` and ``containerPort`` combination or a ``port`` value. However, you can't specify both.\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port value to be used for your service discovery service. It's already specified in the task definition. If the task definition your service task specifies uses the ``bridge`` or ``host`` network mode, you must specify a ``containerName`` and ``containerPort`` combination from the task definition. If the task definition your service task specifies uses the ``awsvpc`` network mode and a type SRV DNS record is used, you must specify either a ``containerName`` and ``containerPort`` combination or a ``port`` value. However, you can't specify both.\n")
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port value used if your service discovery service specified an SRV record. This field might be used if both the ``awsvpc`` network mode and SRV records are used.\n')
    registry_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the service registry. The currently supported service registry is AWS Cloud Map . For more information, see `CreateService <https://docs.aws.amazon.com/cloud-map/latest/api/API_CreateService.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskset-serviceregistry.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    service_registry_property = ecs.CfnTaskSet.ServiceRegistryProperty(\n        container_name="containerName",\n        container_port=123,\n        port=123,\n        registry_arn="registryArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'container_port', 'port', 'registry_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskSet.ServiceRegistryProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CloudMapNamespaceOptions
class CloudMapNamespaceOptionsDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the namespace, such as example.com.\n')
    type: typing.Optional[aws_cdk.aws_servicediscovery.NamespaceType] = pydantic.Field(None, description='The type of CloudMap Namespace to create. Default: PrivateDns\n')
    use_for_service_connect: typing.Optional[bool] = pydantic.Field(None, description='This property specifies whether to set the provided namespace as the service connect default in the cluster properties. Default: false\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC to associate the namespace with. This property is required for private DNS namespaces. Default: VPC of the cluster for Private DNS Namespace, otherwise none\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n    # container_options: ecs.ContainerDefinitionOptions\n\n\n    container = task_definition.add_container("MyContainer", container_options)\n\n    container.add_port_mappings(\n        name="api",\n        container_port=8080\n    )\n\n    cluster.add_default_cloud_map_namespace(\n        name="local"\n    )\n\n    service = ecs.FargateService(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        service_connect_configuration=ecs.ServiceConnectProps(\n            services=[ecs.ServiceConnectService(\n                port_mapping_name="api",\n                dns_name="http-api",\n                port=80\n            )\n            ]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'type', 'use_for_service_connect', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CloudMapNamespaceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CloudMapOptions
class CloudMapOptionsDef(BaseStruct):
    cloud_map_namespace: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='The service discovery namespace for the Cloud Map service to attach to the ECS service. Default: - the defaultCloudMapNamespace associated to the cluster\n')
    container: typing.Optional[models.aws_ecs.ContainerDefinitionDef] = pydantic.Field(None, description="The container to point to for a SRV record. Default: - the task definition's default container\n")
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description="The port to point to for a SRV record. Default: - the default port of the task definition's default container\n")
    dns_record_type: typing.Optional[aws_cdk.aws_servicediscovery.DnsRecordType] = pydantic.Field(None, description='The DNS record type that you want AWS Cloud Map to create. The supported record types are A or SRV. Default: - DnsRecordType.A if TaskDefinition.networkMode = AWS_VPC, otherwise DnsRecordType.SRV\n')
    dns_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time that you want DNS resolvers to cache the settings for this record. Default: Duration.minutes(1)\n')
    failure_threshold: typing.Union[int, float, None] = pydantic.Field(None, description='The number of 30-second intervals that you want Cloud Map to wait after receiving an UpdateInstanceCustomHealthStatus request before it changes the health status of a service instance. NOTE: This is used for HealthCheckCustomConfig\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the Cloud Map service to attach to the ECS service. Default: CloudFormation-generated name\n\n:exampleMetadata: infused\n\nExample::\n\n    # task_definition: ecs.TaskDefinition\n    # cluster: ecs.Cluster\n\n\n    service = ecs.Ec2Service(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        cloud_map_options=ecs.CloudMapOptions(\n            # Create A records - useful for AWSVPC network mode.\n            dns_record_type=cloudmap.DnsRecordType.A\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_map_namespace', 'container', 'container_port', 'dns_record_type', 'dns_ttl', 'failure_threshold', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CloudMapOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ClusterAttributes
class ClusterAttributesDef(BaseStruct):
    cluster_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster.\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The VPC associated with the cluster.\n')
    autoscaling_group: typing.Optional[typing.Union[models.aws_autoscaling.AutoScalingGroupDef]] = pydantic.Field(None, description='Autoscaling group added to the cluster if capacity is added. Default: - No default autoscaling group\n')
    cluster_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) that identifies the cluster. Default: Derived from clusterName\n')
    default_cloud_map_namespace: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='The AWS Cloud Map namespace to associate with the cluster. Default: - No default namespace\n')
    execute_command_configuration: typing.Union[models.aws_ecs.ExecuteCommandConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The execute command configuration for the cluster. Default: - none.\n')
    has_ec2_capacity: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the cluster has EC2 instance capacity. Default: true\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups associated with the container instances registered to the cluster. Default: - no security groups\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_autoscaling as autoscaling\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_kms as kms\n    from aws_cdk import aws_logs as logs\n    from aws_cdk import aws_s3 as s3\n    from aws_cdk import aws_servicediscovery as servicediscovery\n\n    # auto_scaling_group: autoscaling.AutoScalingGroup\n    # bucket: s3.Bucket\n    # key: kms.Key\n    # log_group: logs.LogGroup\n    # namespace: servicediscovery.INamespace\n    # security_group: ec2.SecurityGroup\n    # vpc: ec2.Vpc\n\n    cluster_attributes = ecs.ClusterAttributes(\n        cluster_name="clusterName",\n        vpc=vpc,\n\n        # the properties below are optional\n        autoscaling_group=auto_scaling_group,\n        cluster_arn="clusterArn",\n        default_cloud_map_namespace=namespace,\n        execute_command_configuration=ecs.ExecuteCommandConfiguration(\n            kms_key=key,\n            log_configuration=ecs.ExecuteCommandLogConfiguration(\n                cloud_watch_encryption_enabled=False,\n                cloud_watch_log_group=log_group,\n                s3_bucket=bucket,\n                s3_encryption_enabled=False,\n                s3_key_prefix="s3KeyPrefix"\n            ),\n            logging=ecs.ExecuteCommandLogging.NONE\n        ),\n        has_ec2_capacity=False,\n        security_groups=[security_group]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster_name', 'vpc', 'autoscaling_group', 'cluster_arn', 'default_cloud_map_namespace', 'execute_command_configuration', 'has_ec2_capacity', 'security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ClusterAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ClusterAttributesDefConfig] = pydantic.Field(None)


class ClusterAttributesDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.ClusterProps
class ClusterPropsDef(BaseStruct):
    capacity: typing.Union[models.aws_ecs.AddCapacityOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ec2 capacity to add to the cluster. Default: - no EC2 capacity will be added, you can use ``addCapacity`` to add capacity later.\n')
    cluster_name: typing.Optional[str] = pydantic.Field(None, description='The name for the cluster. Default: CloudFormation-generated name\n')
    container_insights: typing.Optional[bool] = pydantic.Field(None, description='If true CloudWatch Container Insights will be enabled for the cluster. Default: - Container Insights will be disabled for this cluster.\n')
    default_cloud_map_namespace: typing.Union[models.aws_ecs.CloudMapNamespaceOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The service discovery namespace created in this cluster. Default: - no service discovery namespace created, you can use ``addDefaultCloudMapNamespace`` to add a default service discovery namespace later.\n')
    enable_fargate_capacity_providers: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Fargate Capacity Providers. Default: false\n')
    execute_command_configuration: typing.Union[models.aws_ecs.ExecuteCommandConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The execute command configuration for the cluster. Default: - no configuration will be provided.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where your ECS instances will be running or your ENIs will be deployed. Default: - creates a new VPC with two AZs\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n\n    cluster = ecs.Cluster(self, "Cluster",\n        vpc=vpc\n    )\n\n    auto_scaling_group = autoscaling.AutoScalingGroup(self, "ASG",\n        vpc=vpc,\n        instance_type=ec2.InstanceType("t2.micro"),\n        machine_image=ecs.EcsOptimizedImage.amazon_linux2(),\n        min_capacity=0,\n        max_capacity=100\n    )\n\n    capacity_provider = ecs.AsgCapacityProvider(self, "AsgCapacityProvider",\n        auto_scaling_group=auto_scaling_group,\n        instance_warmup_period=300\n    )\n    cluster.add_asg_capacity_provider(capacity_provider)\n\n    task_definition = ecs.Ec2TaskDefinition(self, "TaskDef")\n\n    task_definition.add_container("web",\n        image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n        memory_reservation_mi_b=256\n    )\n\n    ecs.Ec2Service(self, "EC2Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        capacity_provider_strategies=[ecs.CapacityProviderStrategy(\n            capacity_provider=capacity_provider.capacity_provider_name,\n            weight=1\n        )\n        ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capacity', 'cluster_name', 'container_insights', 'default_cloud_map_namespace', 'enable_fargate_capacity_providers', 'execute_command_configuration', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CommonTaskDefinitionAttributes
class CommonTaskDefinitionAttributesDef(BaseStruct):
    task_definition_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    common_task_definition_attributes = ecs.CommonTaskDefinitionAttributes(\n        task_definition_arn="taskDefinitionArn",\n\n        # the properties below are optional\n        execution_role=role,\n        network_mode=ecs.NetworkMode.NONE,\n        task_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['task_definition_arn', 'execution_role', 'network_mode', 'task_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CommonTaskDefinitionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CommonTaskDefinitionProps
class CommonTaskDefinitionPropsDef(BaseStruct):
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # proxy_configuration: ecs.ProxyConfiguration\n    # role: iam.Role\n\n    common_task_definition_props = ecs.CommonTaskDefinitionProps(\n        execution_role=role,\n        family="family",\n        proxy_configuration=proxy_configuration,\n        task_role=role,\n        volumes=[ecs.Volume(\n            name="name",\n\n            # the properties below are optional\n            configured_at_launch=False,\n            docker_volume_configuration=ecs.DockerVolumeConfiguration(\n                driver="driver",\n                scope=ecs.Scope.TASK,\n\n                # the properties below are optional\n                autoprovision=False,\n                driver_opts={\n                    "driver_opts_key": "driverOpts"\n                },\n                labels={\n                    "labels_key": "labels"\n                }\n            ),\n            efs_volume_configuration=ecs.EfsVolumeConfiguration(\n                file_system_id="fileSystemId",\n\n                # the properties below are optional\n                authorization_config=ecs.AuthorizationConfig(\n                    access_point_id="accessPointId",\n                    iam="iam"\n                ),\n                root_directory="rootDirectory",\n                transit_encryption="transitEncryption",\n                transit_encryption_port=123\n            ),\n            host=ecs.Host(\n                source_path="sourcePath"\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CommonTaskDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ContainerDefinitionOptions
class ContainerDefinitionOptionsDef(BaseStruct):
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /\n\n:exampleMetadata: infused\n\nExample::\n\n    # task_definition: ecs.TaskDefinition\n    # cluster: ecs.Cluster\n\n\n    # Add a container to the task definition\n    specific_container = task_definition.add_container("Container",\n        image=ecs.ContainerImage.from_registry("/aws/aws-example-app"),\n        memory_limit_mi_b=2048\n    )\n\n    # Add a port mapping\n    specific_container.add_port_mappings(\n        container_port=7600,\n        protocol=ecs.Protocol.TCP\n    )\n\n    ecs.Ec2Service(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        cloud_map_options=ecs.CloudMapOptions(\n            # Create SRV records - useful for bridge networking\n            dns_record_type=cloudmap.DnsRecordType.SRV,\n            # Targets port TCP port 7600 `specificContainer`\n            container=specific_container,\n            container_port=7600\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'container_name', 'cpu', 'credential_specs', 'disable_networking', 'dns_search_domains', 'dns_servers', 'docker_labels', 'docker_security_options', 'entry_point', 'environment', 'environment_files', 'essential', 'extra_hosts', 'gpu_count', 'health_check', 'hostname', 'inference_accelerator_resources', 'interactive', 'linux_parameters', 'logging', 'memory_limit_mib', 'memory_reservation_mib', 'port_mappings', 'privileged', 'pseudo_terminal', 'readonly_root_filesystem', 'secrets', 'start_timeout', 'stop_timeout', 'system_controls', 'ulimits', 'user', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ContainerDefinitionOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ContainerDefinitionProps
class ContainerDefinitionPropsDef(BaseStruct):
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /\n')
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the task definition that includes this container definition. [disable-awslint:ref-via-interface]\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecs as ecs\n\n    # app_protocol: ecs.AppProtocol\n    # container_image: ecs.ContainerImage\n    # credential_spec: ecs.CredentialSpec\n    # environment_file: ecs.EnvironmentFile\n    # linux_parameters: ecs.LinuxParameters\n    # log_driver: ecs.LogDriver\n    # secret: ecs.Secret\n    # task_definition: ecs.TaskDefinition\n\n    container_definition_props = ecs.ContainerDefinitionProps(\n        image=container_image,\n        task_definition=task_definition,\n\n        # the properties below are optional\n        command=["command"],\n        container_name="containerName",\n        cpu=123,\n        credential_specs=[credential_spec],\n        disable_networking=False,\n        dns_search_domains=["dnsSearchDomains"],\n        dns_servers=["dnsServers"],\n        docker_labels={\n            "docker_labels_key": "dockerLabels"\n        },\n        docker_security_options=["dockerSecurityOptions"],\n        entry_point=["entryPoint"],\n        environment={\n            "environment_key": "environment"\n        },\n        environment_files=[environment_file],\n        essential=False,\n        extra_hosts={\n            "extra_hosts_key": "extraHosts"\n        },\n        gpu_count=123,\n        health_check=ecs.HealthCheck(\n            command=["command"],\n\n            # the properties below are optional\n            interval=cdk.Duration.minutes(30),\n            retries=123,\n            start_period=cdk.Duration.minutes(30),\n            timeout=cdk.Duration.minutes(30)\n        ),\n        hostname="hostname",\n        inference_accelerator_resources=["inferenceAcceleratorResources"],\n        interactive=False,\n        linux_parameters=linux_parameters,\n        logging=log_driver,\n        memory_limit_mi_b=123,\n        memory_reservation_mi_b=123,\n        port_mappings=[ecs.PortMapping(\n            container_port=123,\n\n            # the properties below are optional\n            app_protocol=app_protocol,\n            container_port_range="containerPortRange",\n            host_port=123,\n            name="name",\n            protocol=ecs.Protocol.TCP\n        )],\n        privileged=False,\n        pseudo_terminal=False,\n        readonly_root_filesystem=False,\n        secrets={\n            "secrets_key": secret\n        },\n        start_timeout=cdk.Duration.minutes(30),\n        stop_timeout=cdk.Duration.minutes(30),\n        system_controls=[ecs.SystemControl(\n            namespace="namespace",\n            value="value"\n        )],\n        ulimits=[ecs.Ulimit(\n            hard_limit=123,\n            name=ecs.UlimitName.CORE,\n            soft_limit=123\n        )],\n        user="user",\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'container_name', 'cpu', 'credential_specs', 'disable_networking', 'dns_search_domains', 'dns_servers', 'docker_labels', 'docker_security_options', 'entry_point', 'environment', 'environment_files', 'essential', 'extra_hosts', 'gpu_count', 'health_check', 'hostname', 'inference_accelerator_resources', 'interactive', 'linux_parameters', 'logging', 'memory_limit_mib', 'memory_reservation_mib', 'port_mappings', 'privileged', 'pseudo_terminal', 'readonly_root_filesystem', 'secrets', 'start_timeout', 'stop_timeout', 'system_controls', 'ulimits', 'user', 'working_directory', 'task_definition']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ContainerDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ContainerDependency
class ContainerDependencyDef(BaseStruct):
    container: typing.Union[models.aws_ecs.ContainerDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The container to depend on.\n')
    condition: typing.Optional[aws_cdk.aws_ecs.ContainerDependencyCondition] = pydantic.Field(None, description='The state the container needs to be in to satisfy the dependency and proceed with startup. Valid values are ContainerDependencyCondition.START, ContainerDependencyCondition.COMPLETE, ContainerDependencyCondition.SUCCESS and ContainerDependencyCondition.HEALTHY. Default: ContainerDependencyCondition.HEALTHY\n\n:see: https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ContainerDependency.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    # container_definition: ecs.ContainerDefinition\n\n    container_dependency = ecs.ContainerDependency(\n        container=container_definition,\n\n        # the properties below are optional\n        condition=ecs.ContainerDependencyCondition.START\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container', 'condition']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ContainerDependency'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ContainerDependencyDefConfig] = pydantic.Field(None)


class ContainerDependencyDefConfig(pydantic.BaseModel):
    container_config: typing.Optional[models.aws_ecs.ContainerDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.ContainerImageConfig
class ContainerImageConfigDef(BaseStruct):
    image_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the name of the container image.\n')
    repository_credentials: typing.Union[models.aws_ecs.CfnTaskDefinition_RepositoryCredentialsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the credentials used to access the image repository.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    container_image_config = ecs.ContainerImageConfig(\n        image_name="imageName",\n\n        # the properties below are optional\n        repository_credentials=ecs.CfnTaskDefinition.RepositoryCredentialsProperty(\n            credentials_parameter="credentialsParameter"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image_name', 'repository_credentials']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ContainerImageConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ContainerMountPoint
class ContainerMountPointDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path on the container to mount the host volume at.\n')
    read_only: typing.Union[bool, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether to give the container read-only access to the volume. If this value is true, the container has read-only access to the volume. If this value is false, then the container can write to the volume.\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n\n    task_definition = ecs.FargateTaskDefinition(self, "TaskDef")\n\n    container = task_definition.add_container("web",\n        image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n        port_mappings=[ecs.PortMapping(\n            container_port=80,\n            protocol=ecs.Protocol.TCP\n        )]\n    )\n\n    volume = ecs.ServiceManagedVolume(self, "EBSVolume",\n        name="ebs1",\n        managed_eBSVolume=ecs.ServiceManagedEBSVolumeConfiguration(\n            size=Size.gibibytes(15),\n            volume_type=ec2.EbsDeviceVolumeType.GP3,\n            file_system_type=ecs.FileSystemType.XFS,\n            tag_specifications=[ecs.EBSTagSpecification(\n                tags={\n                    "purpose": "production"\n                },\n                propagate_tags=ecs.EbsPropagatedTagSource.SERVICE\n            )]\n        )\n    )\n\n    volume.mount_in(container,\n        container_path="/var/lib",\n        read_only=False\n    )\n\n    task_definition.add_volume(volume)\n\n    service = ecs.FargateService(self, "FargateService",\n        cluster=cluster,\n        task_definition=task_definition\n    )\n\n    service.add_volume(volume)\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'read_only']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ContainerMountPoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CpuUtilizationScalingProps
class CpuUtilizationScalingPropsDef(BaseStruct):
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    target_utilization_percent: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The target value for CPU utilization across all tasks in the service.\n\n:exampleMetadata: infused\n\nExample::\n\n    # target: elbv2.ApplicationTargetGroup\n    # service: ecs.BaseService\n\n    scaling = service.auto_scale_task_count(max_capacity=10)\n    scaling.scale_on_cpu_utilization("CpuScaling",\n        target_utilization_percent=50\n    )\n\n    scaling.scale_on_request_count("RequestScaling",\n        requests_per_target=10000,\n        target_group=target\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['disable_scale_in', 'policy_name', 'scale_in_cooldown', 'scale_out_cooldown', 'target_utilization_percent']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CpuUtilizationScalingProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CredentialSpecConfig
class CredentialSpecConfigDef(BaseStruct):
    location: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Location of the CredSpec file.\n')
    type_prefix: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Prefix used for the CredSpec string.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    credential_spec_config = ecs.CredentialSpecConfig(\n        location="location",\n        type_prefix="typePrefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['location', 'type_prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CredentialSpecConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.DeploymentAlarmConfig
class DeploymentAlarmConfigDef(BaseStruct):
    behavior: typing.Optional[aws_cdk.aws_ecs.AlarmBehavior] = pydantic.Field(None, description='Default rollback on alarm. Default: AlarmBehavior.ROLLBACK_ON_ALARM\n')
    alarm_names: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='List of alarm names to monitor during deployments.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_cloudwatch as cw\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n    # elb_alarm: cw.Alarm\n\n\n    service = ecs.FargateService(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        deployment_alarms=ecs.DeploymentAlarmConfig(\n            alarm_names=[elb_alarm.alarm_name],\n            behavior=ecs.AlarmBehavior.ROLLBACK_ON_ALARM\n        )\n    )\n\n    # Defining a deployment alarm after the service has been created\n    cpu_alarm_name = "MyCpuMetricAlarm"\n    cw.Alarm(self, "CPUAlarm",\n        alarm_name=cpu_alarm_name,\n        metric=service.metric_cpu_utilization(),\n        evaluation_periods=2,\n        threshold=80\n    )\n    service.enable_deployment_alarms([cpu_alarm_name],\n        behavior=ecs.AlarmBehavior.FAIL_ON_ALARM\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['behavior', 'alarm_names']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.DeploymentAlarmConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.DeploymentAlarmOptions
class DeploymentAlarmOptionsDef(BaseStruct):
    behavior: typing.Optional[aws_cdk.aws_ecs.AlarmBehavior] = pydantic.Field(None, description='Default rollback on alarm. Default: AlarmBehavior.ROLLBACK_ON_ALARM\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_cloudwatch as cw\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n    # elb_alarm: cw.Alarm\n\n\n    service = ecs.FargateService(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        deployment_alarms=ecs.DeploymentAlarmConfig(\n            alarm_names=[elb_alarm.alarm_name],\n            behavior=ecs.AlarmBehavior.ROLLBACK_ON_ALARM\n        )\n    )\n\n    # Defining a deployment alarm after the service has been created\n    cpu_alarm_name = "MyCpuMetricAlarm"\n    cw.Alarm(self, "CPUAlarm",\n        alarm_name=cpu_alarm_name,\n        metric=service.metric_cpu_utilization(),\n        evaluation_periods=2,\n        threshold=80\n    )\n    service.enable_deployment_alarms([cpu_alarm_name],\n        behavior=ecs.AlarmBehavior.FAIL_ON_ALARM\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['behavior']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.DeploymentAlarmOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.DeploymentCircuitBreaker
class DeploymentCircuitBreakerDef(BaseStruct):
    enable: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker logic. Default: true\n')
    rollback: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable rollback on deployment failure. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n\n    service = ecs.FargateService(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        circuit_breaker=ecs.DeploymentCircuitBreaker(\n            enable=True,\n            rollback=True\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['enable', 'rollback']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.DeploymentCircuitBreaker'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.DeploymentController
class DeploymentControllerDef(BaseStruct):
    type: typing.Optional[aws_cdk.aws_ecs.DeploymentControllerType] = pydantic.Field(None, description='The deployment controller type to use. Default: DeploymentControllerType.ECS\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_application: codedeploy.EcsApplication\n    # cluster: ecs.Cluster\n    # task_definition: ecs.FargateTaskDefinition\n    # blue_target_group: elbv2.ITargetGroup\n    # green_target_group: elbv2.ITargetGroup\n    # listener: elbv2.IApplicationListener\n\n\n    service = ecs.FargateService(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        deployment_controller=ecs.DeploymentController(\n            type=ecs.DeploymentControllerType.CODE_DEPLOY\n        )\n    )\n\n    codedeploy.EcsDeploymentGroup(self, "BlueGreenDG",\n        service=service,\n        blue_green_deployment_config=codedeploy.EcsBlueGreenDeploymentConfig(\n            blue_target_group=blue_target_group,\n            green_target_group=green_target_group,\n            listener=listener\n        ),\n        deployment_config=codedeploy.EcsDeploymentConfig.CANARY_10PERCENT_5MINUTES\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.DeploymentController'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Device
class DeviceDef(BaseStruct):
    host_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path for the device on the host container instance.\n')
    container_path: typing.Optional[str] = pydantic.Field(None, description='The path inside the container at which to expose the host device. Default: Same path as the host\n')
    permissions: typing.Optional[typing.Sequence[aws_cdk.aws_ecs.DevicePermission]] = pydantic.Field(None, description='The explicit permissions to provide to the container for the device. By default, the container has permissions for read, write, and mknod for the device. Default: Readonly\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    device = ecs.Device(\n        host_path="hostPath",\n\n        # the properties below are optional\n        container_path="containerPath",\n        permissions=[ecs.DevicePermission.READ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['host_path', 'container_path', 'permissions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Device'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.DockerVolumeConfiguration
class DockerVolumeConfigurationDef(BaseStruct):
    driver: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Docker volume driver to use.\n')
    autoprovision: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the Docker volume should be created if it does not already exist. If true is specified, the Docker volume will be created for you. Default: false\n')
    driver_opts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A map of Docker driver-specific options passed through. Default: No options\n')
    labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Custom metadata to add to your Docker volume. Default: No labels\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    docker_volume_configuration = ecs.DockerVolumeConfiguration(\n        driver="driver",\n        scope=ecs.Scope.TASK,\n\n        # the properties below are optional\n        autoprovision=False,\n        driver_opts={\n            "driver_opts_key": "driverOpts"\n        },\n        labels={\n            "labels_key": "labels"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['driver', 'autoprovision', 'driver_opts', 'labels']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.DockerVolumeConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.EBSTagSpecification
class EBSTagSpecificationDef(BaseStruct):
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.EbsPropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the task. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION Default: - undefined\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags to apply to the volume. Default: - No tags\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    e_bSTag_specification = ecs.EBSTagSpecification(\n        propagate_tags=ecs.EbsPropagatedTagSource.SERVICE,\n        tags={\n            "tags_key": "tags"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['propagate_tags', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EBSTagSpecification'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Ec2ServiceAttributes
class Ec2ServiceAttributesDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cluster that hosts the service.\n')
    service_arn: typing.Optional[str] = pydantic.Field(None, description='The service ARN. Default: - either this, or ``serviceName``, is required\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - either this, or ``serviceArn``, is required\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    # cluster: ecs.Cluster\n\n    ec2_service_attributes = ecs.Ec2ServiceAttributes(\n        cluster=cluster,\n\n        # the properties below are optional\n        service_arn="serviceArn",\n        service_name="serviceName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'service_arn', 'service_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Ec2ServiceAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Ec2ServiceProps
class Ec2ServicePropsDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined\n')
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task definition to use for tasks in the service. [disable-awslint:ref-via-interface]\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the task's elastic network interface receives a public IP address. If true, each task will receive a public IP address. This property is only used for tasks that use the awsvpc network mode. Default: false\n")
    daemon: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the service will use the daemon scheduling strategy. If true, the service scheduler deploys exactly one task on each container instance in your cluster. When you are using this strategy, do not specify a desired number of tasks or any task placement strategies. Default: false\n')
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='The placement constraints to use for tasks in the service. For more information, see `Amazon ECS Task Placement Constraints <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html>`_. Default: - No constraints.\n')
    placement_strategies: typing.Optional[typing.Sequence[models.aws_ecs.PlacementStrategyDef]] = pydantic.Field(None, description='The placement strategies to use for tasks in the service. For more information, see `Amazon ECS Task Placement Strategies <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html>`_. Default: - No strategies.\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to associate with the service. If you do not specify a security group, a new security group is created. This property is only used for tasks that use the awsvpc network mode. Default: - A new security group is created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets to associate with the service. This property is only used for tasks that use the awsvpc network mode. Default: - Public subnets if ``assignPublicIp`` is set, otherwise the first available one of Private, Isolated, Public, in that order.\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n    # vpc: ec2.Vpc\n\n    service = ecs.Ec2Service(self, "Service", cluster=cluster, task_definition=task_definition)\n\n    lb = elb.LoadBalancer(self, "LB", vpc=vpc)\n    lb.add_listener(external_port=80)\n    lb.add_target(service.load_balancer_target(\n        container_name="MyContainer",\n        container_port=80\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations', 'task_definition', 'assign_public_ip', 'daemon', 'placement_constraints', 'placement_strategies', 'security_groups', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Ec2ServiceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Ec2TaskDefinitionAttributes
class Ec2TaskDefinitionAttributesDef(BaseStruct):
    task_definition_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    ec2_task_definition_attributes = ecs.Ec2TaskDefinitionAttributes(\n        task_definition_arn="taskDefinitionArn",\n\n        # the properties below are optional\n        execution_role=role,\n        network_mode=ecs.NetworkMode.NONE,\n        task_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['task_definition_arn', 'execution_role', 'network_mode', 'task_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Ec2TaskDefinitionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Ec2TaskDefinitionProps
class Ec2TaskDefinitionPropsDef(BaseStruct):
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.\n')
    inference_accelerators: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.InferenceAcceleratorDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The inference accelerators to use for the containers in the task. Not supported in Fargate. Default: - No inference accelerators.\n')
    ipc_mode: typing.Optional[aws_cdk.aws_ecs.IpcMode] = pydantic.Field(None, description='The IPC resource namespace to use for the containers in the task. Not supported in Fargate and Windows containers. Default: - IpcMode used by the task is not specified\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The Docker networking mode to use for the containers in the task. The valid values are NONE, BRIDGE, AWS_VPC, and HOST. Default: - NetworkMode.BRIDGE for EC2 tasks, AWS_VPC for Fargate tasks.\n')
    pid_mode: typing.Optional[aws_cdk.aws_ecs.PidMode] = pydantic.Field(None, description='The process namespace to use for the containers in the task. Not supported in Windows containers. Default: - PidMode used by the task is not specified\n')
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='An array of placement constraint objects to use for the task. You can specify a maximum of 10 constraints per task (this limit includes constraints in the task definition and those specified at run time). Default: - No placement constraints.\n\n:exampleMetadata: infused\n\nExample::\n\n    ec2_task_definition = ecs.Ec2TaskDefinition(self, "TaskDef",\n        network_mode=ecs.NetworkMode.BRIDGE\n    )\n\n    container = ec2_task_definition.add_container("WebContainer",\n        # Use an image from DockerHub\n        image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n        memory_limit_mi_b=1024\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes', 'inference_accelerators', 'ipc_mode', 'network_mode', 'pid_mode', 'placement_constraints']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Ec2TaskDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.EcsOptimizedImageOptions
class EcsOptimizedImageOptionsDef(BaseStruct):
    cached_in_context: typing.Optional[bool] = pydantic.Field(None, description='Whether the AMI ID is cached to be stable between deployments. By default, the newest image is used on each deployment. This will cause instances to be replaced whenever a new version is released, and may cause downtime if there aren\'t enough running instances in the AutoScalingGroup to reschedule the tasks on. If set to true, the AMI ID will be cached in ``cdk.context.json`` and the same value will be used on future runs. Your instances will not be replaced but your AMI version will grow old over time. To refresh the AMI lookup, you will have to evict the value from the cache using the ``cdk context`` command. See https://docs.aws.amazon.com/cdk/latest/guide/context.html for more information. Can not be set to ``true`` in environment-agnostic stacks. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    auto_scaling_group = autoscaling.AutoScalingGroup(self, "ASG",\n        machine_image=ecs.EcsOptimizedImage.amazon_linux(cached_in_context=True),\n        vpc=vpc,\n        instance_type=ec2.InstanceType("t2.micro")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cached_in_context']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EcsOptimizedImageOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.EcsTarget
class EcsTargetDef(BaseStruct):
    container_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the container.')
    listener: typing.Union[models.aws_ecs.ListenerConfigDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Listener and properties for adding target group to the listener.\n')
    new_target_group_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='ID for a target group to be created.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number of the container. Only applicable when using application/network load balancers. Default: - Container port of the first added port mapping.\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Only applicable when using application load balancers. Default: Protocol.TCP\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n    # vpc: ec2.Vpc\n\n    service = ecs.FargateService(self, "Service", cluster=cluster, task_definition=task_definition)\n\n    lb = elbv2.ApplicationLoadBalancer(self, "LB", vpc=vpc, internet_facing=True)\n    listener = lb.add_listener("Listener", port=80)\n    service.register_load_balancer_targets(\n        container_name="web",\n        container_port=80,\n        new_target_group_id="ECS",\n        listener=ecs.ListenerConfig.application_listener(listener,\n            protocol=elbv2.ApplicationProtocol.HTTPS\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'listener', 'new_target_group_id', 'container_port', 'protocol']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EcsTarget'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.EfsVolumeConfiguration
class EfsVolumeConfigurationDef(BaseStruct):
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon EFS file system ID to use.\n')
    authorization_config: typing.Union[models.aws_ecs.AuthorizationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The authorization configuration details for the Amazon EFS file system. Default: No configuration.\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. Specifying / will have the same effect as omitting this parameter. Default: The root of the Amazon EFS volume\n')
    transit_encryption: typing.Optional[str] = pydantic.Field(None, description='Whether or not to enable encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Transit encryption must be enabled if Amazon EFS IAM authorization is used. Valid values: ENABLED | DISABLED Default: DISABLED\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. EFS mount helper uses. Default: Port selection strategy that the Amazon EFS mount helper uses.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    efs_volume_configuration = ecs.EfsVolumeConfiguration(\n        file_system_id="fileSystemId",\n\n        # the properties below are optional\n        authorization_config=ecs.AuthorizationConfig(\n            access_point_id="accessPointId",\n            iam="iam"\n        ),\n        root_directory="rootDirectory",\n        transit_encryption="transitEncryption",\n        transit_encryption_port=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_system_id', 'authorization_config', 'root_directory', 'transit_encryption', 'transit_encryption_port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EfsVolumeConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.EnvironmentFileConfig
class EnvironmentFileConfigDef(BaseStruct):
    file_type: typing.Union[aws_cdk.aws_ecs.EnvironmentFileType, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of environment file.\n')
    s3_location: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.LocationDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The location of the environment file in S3.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    environment_file_config = ecs.EnvironmentFileConfig(\n        file_type=ecs.EnvironmentFileType.S3,\n        s3_location=Location(\n            bucket_name="bucketName",\n            object_key="objectKey",\n\n            # the properties below are optional\n            object_version="objectVersion"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_type', 's3_location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.EnvironmentFileConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ExecuteCommandConfiguration
class ExecuteCommandConfigurationDef(BaseStruct):
    kms_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS Key Management Service key ID to encrypt the data between the local client and the container. Default: - none\n')
    log_configuration: typing.Union[models.aws_ecs.ExecuteCommandLogConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The log configuration for the results of the execute command actions. The logs can be sent to CloudWatch Logs or an Amazon S3 bucket. Default: - none\n')
    logging: typing.Optional[aws_cdk.aws_ecs.ExecuteCommandLogging] = pydantic.Field(None, description='The log settings to use for logging the execute command session. Default: - none\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    kms_key = kms.Key(self, "KmsKey")\n\n    # Pass the KMS key in the `encryptionKey` field to associate the key to the log group\n    log_group = logs.LogGroup(self, "LogGroup",\n        encryption_key=kms_key\n    )\n\n    # Pass the KMS key in the `encryptionKey` field to associate the key to the S3 bucket\n    exec_bucket = s3.Bucket(self, "EcsExecBucket",\n        encryption_key=kms_key\n    )\n\n    cluster = ecs.Cluster(self, "Cluster",\n        vpc=vpc,\n        execute_command_configuration=ecs.ExecuteCommandConfiguration(\n            kms_key=kms_key,\n            log_configuration=ecs.ExecuteCommandLogConfiguration(\n                cloud_watch_log_group=log_group,\n                cloud_watch_encryption_enabled=True,\n                s3_bucket=exec_bucket,\n                s3_encryption_enabled=True,\n                s3_key_prefix="exec-command-output"\n            ),\n            logging=ecs.ExecuteCommandLogging.OVERRIDE\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['kms_key', 'log_configuration', 'logging']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExecuteCommandConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ExecuteCommandLogConfiguration
class ExecuteCommandLogConfigurationDef(BaseStruct):
    cloud_watch_encryption_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to enable encryption on the CloudWatch logs. Default: - encryption will be disabled.\n')
    cloud_watch_log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The name of the CloudWatch log group to send logs to. The CloudWatch log group must already be created. Default: - none\n')
    s3_bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='The name of the S3 bucket to send logs to. The S3 bucket must already be created. Default: - none\n')
    s3_encryption_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to enable encryption on the S3 bucket. Default: - encryption will be disabled.\n')
    s3_key_prefix: typing.Optional[str] = pydantic.Field(None, description='An optional folder in the S3 bucket to place logs in. Default: - none\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    kms_key = kms.Key(self, "KmsKey")\n\n    # Pass the KMS key in the `encryptionKey` field to associate the key to the log group\n    log_group = logs.LogGroup(self, "LogGroup",\n        encryption_key=kms_key\n    )\n\n    # Pass the KMS key in the `encryptionKey` field to associate the key to the S3 bucket\n    exec_bucket = s3.Bucket(self, "EcsExecBucket",\n        encryption_key=kms_key\n    )\n\n    cluster = ecs.Cluster(self, "Cluster",\n        vpc=vpc,\n        execute_command_configuration=ecs.ExecuteCommandConfiguration(\n            kms_key=kms_key,\n            log_configuration=ecs.ExecuteCommandLogConfiguration(\n                cloud_watch_log_group=log_group,\n                cloud_watch_encryption_enabled=True,\n                s3_bucket=exec_bucket,\n                s3_encryption_enabled=True,\n                s3_key_prefix="exec-command-output"\n            ),\n            logging=ecs.ExecuteCommandLogging.OVERRIDE\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cloud_watch_encryption_enabled', 'cloud_watch_log_group', 's3_bucket', 's3_encryption_enabled', 's3_key_prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExecuteCommandLogConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ExternalServiceAttributes
class ExternalServiceAttributesDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cluster that hosts the service.\n')
    service_arn: typing.Optional[str] = pydantic.Field(None, description='The service ARN. Default: - either this, or ``serviceName``, is required\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - either this, or ``serviceArn``, is required\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    # cluster: ecs.Cluster\n\n    external_service_attributes = ecs.ExternalServiceAttributes(\n        cluster=cluster,\n\n        # the properties below are optional\n        service_arn="serviceArn",\n        service_name="serviceName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'service_arn', 'service_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExternalServiceAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ExternalServiceProps
class ExternalServicePropsDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined\n')
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task definition to use for tasks in the service. [disable-awslint:ref-via-interface]\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to associate with the service. If you do not specify a security group, a new security group is created. Default: - A new security group is created.\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n\n\n    service = ecs.ExternalService(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        desired_count=5\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations', 'task_definition', 'security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExternalServiceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ExternalTaskDefinitionAttributes
class ExternalTaskDefinitionAttributesDef(BaseStruct):
    task_definition_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    external_task_definition_attributes = ecs.ExternalTaskDefinitionAttributes(\n        task_definition_arn="taskDefinitionArn",\n\n        # the properties below are optional\n        execution_role=role,\n        network_mode=ecs.NetworkMode.NONE,\n        task_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['task_definition_arn', 'execution_role', 'network_mode', 'task_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExternalTaskDefinitionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ExternalTaskDefinitionProps
class ExternalTaskDefinitionPropsDef(BaseStruct):
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. With ECS Anywhere, supported modes are bridge, host and none. Default: NetworkMode.BRIDGE\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # proxy_configuration: ecs.ProxyConfiguration\n    # role: iam.Role\n\n    external_task_definition_props = ecs.ExternalTaskDefinitionProps(\n        execution_role=role,\n        family="family",\n        network_mode=ecs.NetworkMode.NONE,\n        proxy_configuration=proxy_configuration,\n        task_role=role,\n        volumes=[ecs.Volume(\n            name="name",\n\n            # the properties below are optional\n            configured_at_launch=False,\n            docker_volume_configuration=ecs.DockerVolumeConfiguration(\n                driver="driver",\n                scope=ecs.Scope.TASK,\n\n                # the properties below are optional\n                autoprovision=False,\n                driver_opts={\n                    "driver_opts_key": "driverOpts"\n                },\n                labels={\n                    "labels_key": "labels"\n                }\n            ),\n            efs_volume_configuration=ecs.EfsVolumeConfiguration(\n                file_system_id="fileSystemId",\n\n                # the properties below are optional\n                authorization_config=ecs.AuthorizationConfig(\n                    access_point_id="accessPointId",\n                    iam="iam"\n                ),\n                root_directory="rootDirectory",\n                transit_encryption="transitEncryption",\n                transit_encryption_port=123\n            ),\n            host=ecs.Host(\n                source_path="sourcePath"\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes', 'network_mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ExternalTaskDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FargateServiceAttributes
class FargateServiceAttributesDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cluster that hosts the service.\n')
    service_arn: typing.Optional[str] = pydantic.Field(None, description='The service ARN. Default: - either this, or ``serviceName``, is required\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - either this, or ``serviceArn``, is required\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    # cluster: ecs.Cluster\n\n    fargate_service_attributes = ecs.FargateServiceAttributes(\n        cluster=cluster,\n\n        # the properties below are optional\n        service_arn="serviceArn",\n        service_name="serviceName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'service_arn', 'service_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FargateServiceAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FargateServiceProps
class FargateServicePropsDef(BaseStruct):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the cluster that hosts the service.\n')
    capacity_provider_strategies: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CapacityProviderStrategyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of Capacity Provider strategies used to place a service. Default: - undefined\n')
    circuit_breaker: typing.Union[models.aws_ecs.DeploymentCircuitBreakerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Whether to enable the deployment circuit breaker. If this property is defined, circuit breaker will be implicitly enabled. Default: - disabled\n')
    cloud_map_options: typing.Union[models.aws_ecs.CloudMapOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The options for configuring an Amazon ECS service to use service discovery. Default: - AWS Cloud Map service discovery is not enabled.\n')
    deployment_alarms: typing.Union[models.aws_ecs.DeploymentAlarmConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The alarm(s) to monitor during deployment, and behavior to apply if at least one enters a state of alarm during the deployment or bake time. Default: - No alarms will be monitored during deployment.\n')
    deployment_controller: typing.Union[models.aws_ecs.DeploymentControllerDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies which deployment controller to use for the service. For more information, see `Amazon ECS Deployment Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ Default: - Rolling update (ECS)\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The desired number of instantiations of the task definition to keep running on the service. Default: - When creating the service, default is 1; when updating the service, default uses the current task number.\n')
    enable_ecs_managed_tags: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to enable Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging Your Amazon ECS Resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ Default: false\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the ability to execute into a container. Default: - undefined\n')
    health_check_grace_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. Default: - defaults to 60 seconds if at least one load balancer is in-use and it is not already set\n')
    max_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that can run in a service during a deployment. Default: - 100 if daemon, otherwise 200\n")
    min_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of tasks, specified as a percentage of the Amazon ECS service's DesiredCount value, that must continue to run and remain healthy during a deployment. Default: - 0 if daemon, otherwise 50\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition or the service to the tasks in the service. Valid values are: PropagatedTagSource.SERVICE, PropagatedTagSource.TASK_DEFINITION or PropagatedTagSource.NONE Default: PropagatedTagSource.NONE\n')
    service_connect_configuration: typing.Union[models.aws_ecs.ServiceConnectPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for Service Connect. Default: No ports are advertised via Service Connect on this service, and the service cannot make requests to other services via Service Connect.\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of the service. Default: - CloudFormation-generated name.\n')
    task_definition_revision: typing.Optional[models.aws_ecs.TaskDefinitionRevisionDef] = pydantic.Field(None, description='Revision number for the task definition or ``latest`` to use the latest active task revision. Default: - Uses the revision of the passed task definition deployed by CloudFormation\n')
    volume_configurations: typing.Optional[typing.Sequence[models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None, description='Configuration details for a volume used by the service. This allows you to specify details about the EBS volume that can be attched to ECS tasks. Default: - undefined\n')
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task definition to use for tasks in the service. [disable-awslint:ref-via-interface]\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the task's elastic network interface receives a public IP address. If true, each task will receive a public IP address. Default: false\n")
    platform_version: typing.Optional[aws_cdk.aws_ecs.FargatePlatformVersion] = pydantic.Field(None, description='The platform version on which to run your service. If one is not specified, the LATEST platform version is used by default. For more information, see `AWS Fargate Platform Versions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html>`_ in the Amazon Elastic Container Service Developer Guide. Default: Latest\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to associate with the service. If you do not specify a security group, a new security group is created. Default: - A new security group is created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets to associate with the service. Default: - Public subnets if ``assignPublicIp`` is set, otherwise the first available one of Private, Isolated, Public, in that order.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_cloudwatch as cw\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n    # elb_alarm: cw.Alarm\n\n\n    service = ecs.FargateService(self, "Service",\n        cluster=cluster,\n        task_definition=task_definition,\n        deployment_alarms=ecs.DeploymentAlarmConfig(\n            alarm_names=[elb_alarm.alarm_name],\n            behavior=ecs.AlarmBehavior.ROLLBACK_ON_ALARM\n        )\n    )\n\n    # Defining a deployment alarm after the service has been created\n    cpu_alarm_name = "MyCpuMetricAlarm"\n    cw.Alarm(self, "CPUAlarm",\n        alarm_name=cpu_alarm_name,\n        metric=service.metric_cpu_utilization(),\n        evaluation_periods=2,\n        threshold=80\n    )\n    service.enable_deployment_alarms([cpu_alarm_name],\n        behavior=ecs.AlarmBehavior.FAIL_ON_ALARM\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'capacity_provider_strategies', 'circuit_breaker', 'cloud_map_options', 'deployment_alarms', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period', 'max_healthy_percent', 'min_healthy_percent', 'propagate_tags', 'service_connect_configuration', 'service_name', 'task_definition_revision', 'volume_configurations', 'task_definition', 'assign_public_ip', 'platform_version', 'security_groups', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FargateServiceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FargateTaskDefinitionAttributes
class FargateTaskDefinitionAttributesDef(BaseStruct):
    task_definition_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    fargate_task_definition_attributes = ecs.FargateTaskDefinitionAttributes(\n        task_definition_arn="taskDefinitionArn",\n\n        # the properties below are optional\n        execution_role=role,\n        network_mode=ecs.NetworkMode.NONE,\n        task_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['task_definition_arn', 'execution_role', 'network_mode', 'task_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FargateTaskDefinitionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FargateTaskDefinitionProps
class FargateTaskDefinitionPropsDef(BaseStruct):
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The number of cpu units used by the task. For tasks using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the memory parameter: 256 (.25 vCPU) - Available memory values: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) 512 (.5 vCPU) - Available memory values: 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) 1024 (1 vCPU) - Available memory values: 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) 2048 (2 vCPU) - Available memory values: Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) 4096 (4 vCPU) - Available memory values: Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) 8192 (8 vCPU) - Available memory values: Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) 16384 (16 vCPU) - Available memory values: Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) Default: 256\n')
    ephemeral_storage_gib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in GiB) of ephemeral storage to be allocated to the task. The maximum supported value is 200 GiB. NOTE: This parameter is only supported for tasks hosted on AWS Fargate using platform version 1.4.0 or later. Default: 20\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory used by the task. For tasks using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the cpu parameter: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - Available cpu values: 256 (.25 vCPU) 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - Available cpu values: 512 (.5 vCPU) 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - Available cpu values: 1024 (1 vCPU) Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - Available cpu values: 2048 (2 vCPU) Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - Available cpu values: 4096 (4 vCPU) Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) - Available cpu values: 8192 (8 vCPU) Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) - Available cpu values: 16384 (16 vCPU) Default: 512\n')
    pid_mode: typing.Optional[aws_cdk.aws_ecs.PidMode] = pydantic.Field(None, description='The process namespace to use for the containers in the task. Only supported for tasks that are hosted on AWS Fargate if the tasks are using platform version 1.4.0 or later (Linux). Only the TASK option is supported for Linux-based Fargate containers. Not supported in Windows containers. If pidMode is specified for a Fargate task, then runtimePlatform.operatingSystemFamily must also be specified. For more information, see `Task Definition Parameters <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#task_definition_pidmode>`_. Default: - PidMode used by the task is not specified\n')
    runtime_platform: typing.Union[models.aws_ecs.RuntimePlatformDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The operating system that your task definitions are running on. A runtimePlatform is supported only for tasks using the Fargate launch type. Default: - Undefined.\n\n:exampleMetadata: infused\n\nExample::\n\n    fargate_task_definition = ecs.FargateTaskDefinition(self, "TaskDef",\n        runtime_platform=ecs.RuntimePlatform(\n            operating_system_family=ecs.OperatingSystemFamily.LINUX,\n            cpu_architecture=ecs.CpuArchitecture.ARM64\n        ),\n        memory_limit_mi_b=512,\n        cpu=256,\n        pid_mode=ecs.PidMode.TASK\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes', 'cpu', 'ephemeral_storage_gib', 'memory_limit_mib', 'pid_mode', 'runtime_platform']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FargateTaskDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FirelensConfig
class FirelensConfigDef(BaseStruct):
    type: typing.Union[aws_cdk.aws_ecs.FirelensLogRouterType, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log router to use. Default: - fluentbit\n')
    options: typing.Union[models.aws_ecs.FirelensOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Firelens options. Default: - no additional options\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    firelens_config = ecs.FirelensConfig(\n        type=ecs.FirelensLogRouterType.FLUENTBIT,\n\n        # the properties below are optional\n        options=ecs.FirelensOptions(\n            config_file_type=ecs.FirelensConfigFileType.S3,\n            config_file_value="configFileValue",\n            enable_eCSLog_metadata=False\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FirelensConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FireLensLogDriverProps
class FireLensLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n')
    options: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The configuration options to send to the log driver. Default: - the log driver options\n')
    secret_options: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secrets to pass to the log configuration. Default: - No secret options provided.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Create a Task Definition for the container to start\n    task_definition = ecs.Ec2TaskDefinition(self, "TaskDef")\n    task_definition.add_container("TheContainer",\n        image=ecs.ContainerImage.from_registry("example-image"),\n        memory_limit_mi_b=256,\n        logging=ecs.LogDrivers.firelens(\n            options={\n                "Name": "firehose",\n                "region": "us-west-2",\n                "delivery_stream": "my-stream"\n            }\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FireLensLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FirelensLogRouterDefinitionOptions
class FirelensLogRouterDefinitionOptionsDef(BaseStruct):
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /\n')
    firelens_config: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.FirelensConfigDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Firelens configuration.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecs as ecs\n\n    # app_protocol: ecs.AppProtocol\n    # container_image: ecs.ContainerImage\n    # credential_spec: ecs.CredentialSpec\n    # environment_file: ecs.EnvironmentFile\n    # linux_parameters: ecs.LinuxParameters\n    # log_driver: ecs.LogDriver\n    # secret: ecs.Secret\n\n    firelens_log_router_definition_options = ecs.FirelensLogRouterDefinitionOptions(\n        firelens_config=ecs.FirelensConfig(\n            type=ecs.FirelensLogRouterType.FLUENTBIT,\n\n            # the properties below are optional\n            options=ecs.FirelensOptions(\n                config_file_type=ecs.FirelensConfigFileType.S3,\n                config_file_value="configFileValue",\n                enable_eCSLog_metadata=False\n            )\n        ),\n        image=container_image,\n\n        # the properties below are optional\n        command=["command"],\n        container_name="containerName",\n        cpu=123,\n        credential_specs=[credential_spec],\n        disable_networking=False,\n        dns_search_domains=["dnsSearchDomains"],\n        dns_servers=["dnsServers"],\n        docker_labels={\n            "docker_labels_key": "dockerLabels"\n        },\n        docker_security_options=["dockerSecurityOptions"],\n        entry_point=["entryPoint"],\n        environment={\n            "environment_key": "environment"\n        },\n        environment_files=[environment_file],\n        essential=False,\n        extra_hosts={\n            "extra_hosts_key": "extraHosts"\n        },\n        gpu_count=123,\n        health_check=ecs.HealthCheck(\n            command=["command"],\n\n            # the properties below are optional\n            interval=cdk.Duration.minutes(30),\n            retries=123,\n            start_period=cdk.Duration.minutes(30),\n            timeout=cdk.Duration.minutes(30)\n        ),\n        hostname="hostname",\n        inference_accelerator_resources=["inferenceAcceleratorResources"],\n        interactive=False,\n        linux_parameters=linux_parameters,\n        logging=log_driver,\n        memory_limit_mi_b=123,\n        memory_reservation_mi_b=123,\n        port_mappings=[ecs.PortMapping(\n            container_port=123,\n\n            # the properties below are optional\n            app_protocol=app_protocol,\n            container_port_range="containerPortRange",\n            host_port=123,\n            name="name",\n            protocol=ecs.Protocol.TCP\n        )],\n        privileged=False,\n        pseudo_terminal=False,\n        readonly_root_filesystem=False,\n        secrets={\n            "secrets_key": secret\n        },\n        start_timeout=cdk.Duration.minutes(30),\n        stop_timeout=cdk.Duration.minutes(30),\n        system_controls=[ecs.SystemControl(\n            namespace="namespace",\n            value="value"\n        )],\n        ulimits=[ecs.Ulimit(\n            hard_limit=123,\n            name=ecs.UlimitName.CORE,\n            soft_limit=123\n        )],\n        user="user",\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'container_name', 'cpu', 'credential_specs', 'disable_networking', 'dns_search_domains', 'dns_servers', 'docker_labels', 'docker_security_options', 'entry_point', 'environment', 'environment_files', 'essential', 'extra_hosts', 'gpu_count', 'health_check', 'hostname', 'inference_accelerator_resources', 'interactive', 'linux_parameters', 'logging', 'memory_limit_mib', 'memory_reservation_mib', 'port_mappings', 'privileged', 'pseudo_terminal', 'readonly_root_filesystem', 'secrets', 'start_timeout', 'stop_timeout', 'system_controls', 'ulimits', 'user', 'working_directory', 'firelens_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FirelensLogRouterDefinitionOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.FirelensLogRouterDefinitionOptionsDefConfig] = pydantic.Field(None)


class FirelensLogRouterDefinitionOptionsDefConfig(pydantic.BaseModel):
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.FirelensLogRouterProps
class FirelensLogRouterPropsDef(BaseStruct):
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with either repository-url/image:tag or repository-url/image@digest. TODO: Update these to specify using classes of IContainerImage\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command that is passed to the container. If you provide a shell command as a single string, you have to quote command-line arguments. Default: - CMD value built into container image.\n')
    container_name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. Default: - id of node associated with ContainerDefinition.\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of CPU units to reserve for the container. Default: - No minimum CPU units reserved.\n')
    credential_specs: typing.Optional[typing.Sequence[models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None, description='A list of ARNs in SSM or Amazon S3 to a credential spec (``CredSpec``) file that configures the container for Active Directory authentication. We recommend that you use this parameter instead of the ``dockerSecurityOptions``. Currently, only one credential spec is allowed per container definition. Default: - No credential specs.\n')
    disable_networking: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether networking is disabled within the container. When this parameter is true, networking is disabled within the container. Default: false\n')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS search domains that are presented to the container. Default: - No search domains.\n')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DNS servers that are presented to the container. Default: - Default DNS servers.\n')
    docker_labels: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A key/value map of labels to add to the container. Default: - No labels.\n')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of strings to provide custom labels for SELinux and AppArmor multi-level security systems. Default: - No security labels.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The ENTRYPOINT value to pass to the container. Default: - Entry point configured in container.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - No environment variables.\n')
    environment_files: typing.Optional[typing.Sequence[models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None, description='The environment files to pass to the container. Default: - No environment files.\n')
    essential: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked essential. If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped. If the essential parameter of a container is marked as false, then its failure does not affect the rest of the containers in a task. All tasks must have at least one essential container. If this parameter is omitted, a container is assumed to be essential. Default: true\n')
    extra_hosts: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='A list of hostnames and IP address mappings to append to the /etc/hosts file on the container. Default: - No extra hosts.\n')
    gpu_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of GPUs assigned to the container. Default: - No GPUs assigned.\n')
    health_check: typing.Union[models.aws_ecs.HealthCheckDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The health check command and associated configuration parameters for the container. Default: - Health check configuration from container.\n')
    hostname: typing.Optional[str] = pydantic.Field(None, description='The hostname to use for your container. Default: - Automatic hostname.\n')
    inference_accelerator_resources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The inference accelerators referenced by the container. Default: - No inference accelerators assigned.\n')
    interactive: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, you can deploy containerized applications that require stdin or a tty to be allocated. Default: - false\n')
    linux_parameters: typing.Optional[models.aws_ecs.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as Linux kernel capabilities. For more information see `KernelCapabilities <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_KernelCapabilities.html>`_. Default: - No Linux parameters.\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log configuration specification for the container. Default: - Containers use the same logging driver that the Docker daemon uses.\n')
    memory_limit_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, the container is terminated. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory limit.\n')
    memory_reservation_mib: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of memoryLimitMiB and memoryReservationMiB is required for non-Fargate services. Default: - No memory reserved.\n')
    port_mappings: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.PortMappingDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The port mappings to add to the container definition. Default: - No ports are mapped.\n')
    privileged: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the container is marked as privileged. When this parameter is true, the container is given elevated privileges on the host container instance (similar to the root user). Default: false\n')
    pseudo_terminal: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, a TTY is allocated. This parameter maps to Tty in the "Create a container section" of the Docker Remote API and the --tty option to ``docker run``. Default: - false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secret environment variables to pass to the container. Default: - No secret environment variables.\n')
    start_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Time duration (in seconds) to wait before giving up on resolving dependencies for a container. Default: - none\n')
    stop_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="Time duration (in seconds) to wait before the container is forcefully killed if it doesn't exit normally on its own. Default: - none\n")
    system_controls: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.SystemControlDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of namespaced kernel parameters to set in the container. Default: - No system controls are set.\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of ulimits to set in the container.\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use inside the container. This parameter maps to User in the Create a container section of the Docker Remote API and the --user option to docker run. Default: root\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='The working directory in which to run commands inside the container. Default: /\n')
    task_definition: typing.Union[models.aws_ecs.TaskDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the task definition that includes this container definition. [disable-awslint:ref-via-interface]\n')
    firelens_config: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.FirelensConfigDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Firelens configuration.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecs as ecs\n\n    # app_protocol: ecs.AppProtocol\n    # container_image: ecs.ContainerImage\n    # credential_spec: ecs.CredentialSpec\n    # environment_file: ecs.EnvironmentFile\n    # linux_parameters: ecs.LinuxParameters\n    # log_driver: ecs.LogDriver\n    # secret: ecs.Secret\n    # task_definition: ecs.TaskDefinition\n\n    firelens_log_router_props = ecs.FirelensLogRouterProps(\n        firelens_config=ecs.FirelensConfig(\n            type=ecs.FirelensLogRouterType.FLUENTBIT,\n\n            # the properties below are optional\n            options=ecs.FirelensOptions(\n                config_file_type=ecs.FirelensConfigFileType.S3,\n                config_file_value="configFileValue",\n                enable_eCSLog_metadata=False\n            )\n        ),\n        image=container_image,\n        task_definition=task_definition,\n\n        # the properties below are optional\n        command=["command"],\n        container_name="containerName",\n        cpu=123,\n        credential_specs=[credential_spec],\n        disable_networking=False,\n        dns_search_domains=["dnsSearchDomains"],\n        dns_servers=["dnsServers"],\n        docker_labels={\n            "docker_labels_key": "dockerLabels"\n        },\n        docker_security_options=["dockerSecurityOptions"],\n        entry_point=["entryPoint"],\n        environment={\n            "environment_key": "environment"\n        },\n        environment_files=[environment_file],\n        essential=False,\n        extra_hosts={\n            "extra_hosts_key": "extraHosts"\n        },\n        gpu_count=123,\n        health_check=ecs.HealthCheck(\n            command=["command"],\n\n            # the properties below are optional\n            interval=cdk.Duration.minutes(30),\n            retries=123,\n            start_period=cdk.Duration.minutes(30),\n            timeout=cdk.Duration.minutes(30)\n        ),\n        hostname="hostname",\n        inference_accelerator_resources=["inferenceAcceleratorResources"],\n        interactive=False,\n        linux_parameters=linux_parameters,\n        logging=log_driver,\n        memory_limit_mi_b=123,\n        memory_reservation_mi_b=123,\n        port_mappings=[ecs.PortMapping(\n            container_port=123,\n\n            # the properties below are optional\n            app_protocol=app_protocol,\n            container_port_range="containerPortRange",\n            host_port=123,\n            name="name",\n            protocol=ecs.Protocol.TCP\n        )],\n        privileged=False,\n        pseudo_terminal=False,\n        readonly_root_filesystem=False,\n        secrets={\n            "secrets_key": secret\n        },\n        start_timeout=cdk.Duration.minutes(30),\n        stop_timeout=cdk.Duration.minutes(30),\n        system_controls=[ecs.SystemControl(\n            namespace="namespace",\n            value="value"\n        )],\n        ulimits=[ecs.Ulimit(\n            hard_limit=123,\n            name=ecs.UlimitName.CORE,\n            soft_limit=123\n        )],\n        user="user",\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'container_name', 'cpu', 'credential_specs', 'disable_networking', 'dns_search_domains', 'dns_servers', 'docker_labels', 'docker_security_options', 'entry_point', 'environment', 'environment_files', 'essential', 'extra_hosts', 'gpu_count', 'health_check', 'hostname', 'inference_accelerator_resources', 'interactive', 'linux_parameters', 'logging', 'memory_limit_mib', 'memory_reservation_mib', 'port_mappings', 'privileged', 'pseudo_terminal', 'readonly_root_filesystem', 'secrets', 'start_timeout', 'stop_timeout', 'system_controls', 'ulimits', 'user', 'working_directory', 'task_definition', 'firelens_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FirelensLogRouterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.FirelensLogRouterPropsDefConfig] = pydantic.Field(None)


class FirelensLogRouterPropsDefConfig(pydantic.BaseModel):
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.FirelensOptions
class FirelensOptionsDef(BaseStruct):
    config_file_type: typing.Optional[aws_cdk.aws_ecs.FirelensConfigFileType] = pydantic.Field(None, description='Custom configuration file, s3 or file. Both configFileType and configFileValue must be used together to define a custom configuration source. Default: - determined by checking configFileValue with S3 ARN.\n')
    config_file_value: typing.Optional[str] = pydantic.Field(None, description='Custom configuration file, S3 ARN or a file path Both configFileType and configFileValue must be used together to define a custom configuration source. Default: - no config file value\n')
    enable_ecs_log_metadata: typing.Optional[bool] = pydantic.Field(None, description='By default, Amazon ECS adds additional fields in your log entries that help identify the source of the logs. You can disable this action by setting enable-ecs-log-metadata to false. Default: - true\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    firelens_options = ecs.FirelensOptions(\n        config_file_type=ecs.FirelensConfigFileType.S3,\n        config_file_value="configFileValue",\n        enable_eCSLog_metadata=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['config_file_type', 'config_file_value', 'enable_ecs_log_metadata']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FirelensOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.FluentdLogDriverProps
class FluentdLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n')
    address: typing.Optional[str] = pydantic.Field(None, description='By default, the logging driver connects to localhost:24224. Supply the address option to connect to a different address. tcp(default) and unix sockets are supported. Default: - address not set.\n')
    async_connect: typing.Optional[bool] = pydantic.Field(None, description='Docker connects to Fluentd in the background. Messages are buffered until the connection is established. Default: - false\n')
    buffer_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of data to buffer before flushing to disk. Default: - The amount of RAM available to the container.\n')
    max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of retries. Default: - 4294967295 (2**32 - 1).\n')
    retry_wait: typing.Optional[models.DurationDef] = pydantic.Field(None, description='How long to wait between retries. Default: - 1 second\n')
    sub_second_precision: typing.Optional[bool] = pydantic.Field(None, description='Generates event logs in nanosecond resolution. Default: - false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecs as ecs\n\n    fluentd_log_driver_props = ecs.FluentdLogDriverProps(\n        address="address",\n        async_connect=False,\n        buffer_limit=123,\n        env=["env"],\n        env_regex="envRegex",\n        labels=["labels"],\n        max_retries=123,\n        retry_wait=cdk.Duration.minutes(30),\n        sub_second_precision=False,\n        tag="tag"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag', 'address', 'async_connect', 'buffer_limit', 'max_retries', 'retry_wait', 'sub_second_precision']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.FluentdLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.GelfLogDriverProps
class GelfLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n')
    address: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The address of the GELF server. tcp and udp are the only supported URI specifier and you must specify the port.\n')
    compression_level: typing.Union[int, float, None] = pydantic.Field(None, description='UDP Only The level of compression when gzip or zlib is the gelf-compression-type. An integer in the range of -1 to 9 (BestCompression). Higher levels provide more compression at lower speed. Either -1 or 0 disables compression. Default: - 1\n')
    compression_type: typing.Optional[aws_cdk.aws_ecs.GelfCompressionType] = pydantic.Field(None, description='UDP Only The type of compression the GELF driver uses to compress each log message. Allowed values are gzip, zlib and none. Default: - gzip\n')
    tcp_max_reconnect: typing.Union[int, float, None] = pydantic.Field(None, description='TCP Only The maximum number of reconnection attempts when the connection drop. A positive integer. Default: - 3\n')
    tcp_reconnect_delay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='TCP Only The number of seconds to wait between reconnection attempts. A positive integer. Default: - 1\n\n:exampleMetadata: infused\n\nExample::\n\n    # Create a Task Definition for the container to start\n    task_definition = ecs.Ec2TaskDefinition(self, "TaskDef")\n    task_definition.add_container("TheContainer",\n        image=ecs.ContainerImage.from_registry("example-image"),\n        memory_limit_mi_b=256,\n        logging=ecs.LogDrivers.gelf(address="my-gelf-address")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag', 'address', 'compression_level', 'compression_type', 'tcp_max_reconnect', 'tcp_reconnect_delay']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.GelfLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.GenericLogDriverProps
class GenericLogDriverPropsDef(BaseStruct):
    log_driver: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log driver to use for the container. The valid values listed for this parameter are log drivers that the Amazon ECS container agent can communicate with by default. For tasks using the Fargate launch type, the supported log drivers are awslogs and splunk. For tasks using the EC2 launch type, the supported log drivers are awslogs, syslog, gelf, fluentd, splunk, journald, and json-file. For more information about using the awslogs log driver, see `Using the awslogs Log Driver <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html>`_ in the Amazon Elastic Container Service Developer Guide.\n')
    options: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The configuration options to send to the log driver. Default: - the log driver options.\n')
    secret_options: typing.Optional[typing.Mapping[str, models.aws_ecs.SecretDef]] = pydantic.Field(None, description='The secrets to pass to the log configuration. Default: - no secret options provided.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Create a Task Definition for the container to start\n    task_definition = ecs.Ec2TaskDefinition(self, "TaskDef")\n    task_definition.add_container("TheContainer",\n        image=ecs.ContainerImage.from_registry("example-image"),\n        memory_limit_mi_b=256,\n        logging=ecs.GenericLogDriver(\n            log_driver="fluentd",\n            options={\n                "tag": "example-tag"\n            }\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.GenericLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.HealthCheck
class HealthCheckDef(BaseStruct):
    command: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A string array representing the command that the container runs to determine if it is healthy. The string array must start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container\'s default shell. For example: [ "CMD-SHELL", "curl -f http://localhost/ || exit 1" ]\n')
    interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time period in seconds between each health check execution. You may specify between 5 and 300 seconds. Default: Duration.seconds(30)\n')
    retries: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a failed health check before the container is considered unhealthy. You may specify between 1 and 10 retries. Default: 3\n')
    start_period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The optional grace period within which to provide containers time to bootstrap before failed health checks count towards the maximum number of retries. You may specify between 0 and 300 seconds. Default: No start period\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time period in seconds to wait for a health check to succeed before it is considered a failure. You may specify between 2 and 60 seconds. Default: Duration.seconds(5)\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n    # security_group: ec2.SecurityGroup\n\n    queue_processing_fargate_service = ecs_patterns.QueueProcessingFargateService(self, "Service",\n        vpc=vpc,\n        memory_limit_mi_b=512,\n        image=ecs.ContainerImage.from_registry("test"),\n        health_check=ecs.HealthCheck(\n            command=["CMD-SHELL", "curl -f http://localhost/ || exit 1"],\n            # the properties below are optional\n            interval=Duration.minutes(30),\n            retries=123,\n            start_period=Duration.minutes(30),\n            timeout=Duration.minutes(30)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['command', 'interval', 'retries', 'start_period', 'timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.HealthCheck'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Host
class HostDef(BaseStruct):
    source_path: typing.Optional[str] = pydantic.Field(None, description='Specifies the path on the host container instance that is presented to the container. If the sourcePath value does not exist on the host container instance, the Docker daemon creates it. If the location does exist, the contents of the source path folder are exported. This property is not supported for tasks that use the Fargate launch type.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    host = ecs.Host(\n        source_path="sourcePath"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['source_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Host'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.InferenceAccelerator
class InferenceAcceleratorDef(BaseStruct):
    device_name: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator device name. Default: - empty\n')
    device_type: typing.Optional[str] = pydantic.Field(None, description='The Elastic Inference accelerator type to use. The allowed values are: eia2.medium, eia2.large and eia2.xlarge. Default: - empty\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    inference_accelerator = ecs.InferenceAccelerator(\n        device_name="deviceName",\n        device_type="deviceType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['device_name', 'device_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.InferenceAccelerator'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.JournaldLogDriverProps
class JournaldLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    journald_log_driver_props = ecs.JournaldLogDriverProps(\n        env=["env"],\n        env_regex="envRegex",\n        labels=["labels"],\n        tag="tag"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.JournaldLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.JsonFileLogDriverProps
class JsonFileLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n')
    compress: typing.Optional[bool] = pydantic.Field(None, description='Toggles compression for rotated logs. Default: - false\n')
    max_file: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed. Only effective when max-size is also set. A positive integer. Default: - 1\n')
    max_size: typing.Optional[str] = pydantic.Field(None, description='The maximum size of the log before it is rolled. A positive integer plus a modifier representing the unit of measure (k, m, or g). Default: - -1 (unlimited)\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    json_file_log_driver_props = ecs.JsonFileLogDriverProps(\n        compress=False,\n        env=["env"],\n        env_regex="envRegex",\n        labels=["labels"],\n        max_file=123,\n        max_size="maxSize",\n        tag="tag"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag', 'compress', 'max_file', 'max_size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.JsonFileLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.LinuxParametersProps
class LinuxParametersPropsDef(BaseStruct):
    init_process_enabled: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to run an init process inside the container that forwards signals and reaps processes. Default: false\n')
    max_swap: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The total amount of swap memory a container can use. This parameter will be translated to the --memory-swap option to docker run. This parameter is only supported when you are using the EC2 launch type. Accepted values are positive integers. Default: No swap.\n')
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The value for the size of the /dev/shm volume. Default: No shared memory.\n')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description='This allows you to tune a container\'s memory swappiness behavior. This parameter maps to the --memory-swappiness option to docker run. The swappiness relates to the kernel\'s tendency to swap memory. A value of 0 will cause swapping to not happen unless absolutely necessary. A value of 100 will cause pages to be swapped very aggressively. This parameter is only supported when you are using the EC2 launch type. Accepted values are whole numbers between 0 and 100. If a value is not specified for maxSwap then this parameter is ignored. Default: 60\n\n:exampleMetadata: infused\n\nExample::\n\n    # task_definition: ecs.TaskDefinition\n\n\n    task_definition.add_container("container",\n        image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n        memory_limit_mi_b=1024,\n        linux_parameters=ecs.LinuxParameters(self, "LinuxParameters",\n            init_process_enabled=True,\n            shared_memory_size=1024,\n            max_swap=Size.mebibytes(5000),\n            swappiness=90\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['init_process_enabled', 'max_swap', 'shared_memory_size', 'swappiness']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.LinuxParametersProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.LoadBalancerTargetOptions
class LoadBalancerTargetOptionsDef(BaseStruct):
    container_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the container.\n')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number of the container. Only applicable when using application/network load balancers. Default: - Container port of the first added port mapping.\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Only applicable when using application load balancers. Default: Protocol.TCP\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n    # vpc: ec2.Vpc\n\n    service = ecs.Ec2Service(self, "Service", cluster=cluster, task_definition=task_definition)\n\n    lb = elb.LoadBalancer(self, "LB", vpc=vpc)\n    lb.add_listener(external_port=80)\n    lb.add_target(service.load_balancer_target(\n        container_name="MyContainer",\n        container_port=80\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'container_port', 'protocol']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.LoadBalancerTargetOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.LogDriverConfig
class LogDriverConfigDef(BaseStruct):
    log_driver: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log driver to use for the container. The valid values listed for this parameter are log drivers that the Amazon ECS container agent can communicate with by default. For tasks using the Fargate launch type, the supported log drivers are awslogs, splunk, and awsfirelens. For tasks using the EC2 launch type, the supported log drivers are awslogs, fluentd, gelf, json-file, journald, logentries,syslog, splunk, and awsfirelens. For more information about using the awslogs log driver, see `Using the awslogs Log Driver <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html>`_ in the Amazon Elastic Container Service Developer Guide. For more information about using the awsfirelens log driver, see `Custom Log Routing <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_firelens.html>`_ in the Amazon Elastic Container Service Developer Guide.\n')
    options: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The configuration options to send to the log driver.\n')
    secret_options: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.CfnTaskDefinition_SecretPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The secrets to pass to the log configuration. Default: - No secret options provided.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    log_driver_config = ecs.LogDriverConfig(\n        log_driver="logDriver",\n\n        # the properties below are optional\n        options={\n            "options_key": "options"\n        },\n        secret_options=[ecs.CfnTaskDefinition.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.LogDriverConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.MemoryUtilizationScalingProps
class MemoryUtilizationScalingPropsDef(BaseStruct):
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    target_utilization_percent: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The target value for memory utilization across all tasks in the service.\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n\n    load_balanced_fargate_service = ecs_patterns.ApplicationLoadBalancedFargateService(self, "Service",\n        cluster=cluster,\n        memory_limit_mi_b=1024,\n        desired_count=1,\n        cpu=512,\n        task_image_options=ecsPatterns.ApplicationLoadBalancedTaskImageOptions(\n            image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample")\n        )\n    )\n\n    scalable_target = load_balanced_fargate_service.service.auto_scale_task_count(\n        min_capacity=1,\n        max_capacity=20\n    )\n\n    scalable_target.scale_on_cpu_utilization("CpuScaling",\n        target_utilization_percent=50\n    )\n\n    scalable_target.scale_on_memory_utilization("MemoryScaling",\n        target_utilization_percent=50\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['disable_scale_in', 'policy_name', 'scale_in_cooldown', 'scale_out_cooldown', 'target_utilization_percent']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.MemoryUtilizationScalingProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.MountPoint
class MountPointDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path on the container to mount the host volume at.\n')
    read_only: typing.Union[bool, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether to give the container read-only access to the volume. If this value is true, the container has read-only access to the volume. If this value is false, then the container can write to the volume.\n')
    source_volume: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume to mount. Must be a volume name referenced in the name parameter of task definition volume.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    mount_point = ecs.MountPoint(\n        container_path="containerPath",\n        read_only=False,\n        source_volume="sourceVolume"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'read_only', 'source_volume']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.MountPoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.PortMapping
class PortMappingDef(BaseStruct):
    container_port: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The port number on the container that is bound to the user-specified or automatically assigned host port. If you are using containers in a task with the awsvpc or host network mode, exposed ports should be specified using containerPort. If you are using containers in a task with the bridge network mode and you specify a container port and not a host port, your container automatically receives a host port in the ephemeral port range. For more information, see hostPort. Port mappings that are automatically assigned in this way do not count toward the 100 reserved ports limit of a container instance. If you want to expose a port range, you must specify ``CONTAINER_PORT_USE_RANGE`` as container port.\n')
    app_protocol: typing.Optional[models.aws_ecs.AppProtocolDef] = pydantic.Field(None, description='The protocol used by Service Connect. Valid values are AppProtocol.http, AppProtocol.http2, and AppProtocol.grpc. The protocol determines what telemetry will be shown in the ECS Console for Service Connect services using this port mapping. This field may only be set when the task definition uses Bridge or Awsvpc network modes. Default: - no app protocol\n')
    container_port_range: typing.Optional[str] = pydantic.Field(None, description="The port number range on the container that's bound to the dynamically mapped host port range. The following rules apply when you specify a ``containerPortRange``: - You must specify ``CONTAINER_PORT_USE_RANGE`` as ``containerPort`` - You must use either the ``bridge`` network mode or the ``awsvpc`` network mode. - The container instance must have at least version 1.67.0 of the container agent and at least version 1.67.0-1 of the ``ecs-init`` package - You can specify a maximum of 100 port ranges per container. - A port can only be included in one port mapping per container. - You cannot specify overlapping port ranges. - The first port in the range must be less than last port in the range. If you want to expose a single port, you must not set a range.\n")
    host_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number on the container instance to reserve for your container. If you are using containers in a task with the awsvpc or host network mode, the hostPort can either be left blank or set to the same value as the containerPort. If you are using containers in a task with the bridge network mode, you can specify a non-reserved host port for your container port mapping, or you can omit the hostPort (or set it to 0) while specifying a containerPort and your container automatically receives a port in the ephemeral port range for your container instance operating system and Docker version.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name to give the port mapping. Name is required in order to use the port mapping with ECS Service Connect. This field may only be set when the task definition uses Bridge or Awsvpc network modes. Default: - no port mapping name\n')
    protocol: typing.Optional[aws_cdk.aws_ecs.Protocol] = pydantic.Field(None, description='The protocol used for the port mapping. Valid values are Protocol.TCP and Protocol.UDP. Default: TCP\n\n:exampleMetadata: infused\n\nExample::\n\n    # container: ecs.ContainerDefinition\n\n\n    container.add_port_mappings(\n        container_port=ecs.ContainerDefinition.CONTAINER_PORT_USE_RANGE,\n        container_port_range="8080-8081"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_port', 'app_protocol', 'container_port_range', 'host_port', 'name', 'protocol']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.PortMapping'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.RepositoryImageProps
class RepositoryImagePropsDef(BaseStruct):
    credentials: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret to expose to the container that contains the credentials for the image repository. The supported value is the full ARN of an AWS Secrets Manager secret.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_secretsmanager as secretsmanager\n\n    # secret: secretsmanager.Secret\n\n    repository_image_props = ecs.RepositoryImageProps(\n        credentials=secret\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['credentials']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.RepositoryImageProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.RequestCountScalingProps
class RequestCountScalingPropsDef(BaseStruct):
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    requests_per_target: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of ALB requests per target.\n')
    target_group: typing.Union[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ALB target group name.\n\n:exampleMetadata: infused\n\nExample::\n\n    # target: elbv2.ApplicationTargetGroup\n    # service: ecs.BaseService\n\n    scaling = service.auto_scale_task_count(max_capacity=10)\n    scaling.scale_on_cpu_utilization("CpuScaling",\n        target_utilization_percent=50\n    )\n\n    scaling.scale_on_request_count("RequestScaling",\n        requests_per_target=10000,\n        target_group=target\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['disable_scale_in', 'policy_name', 'scale_in_cooldown', 'scale_out_cooldown', 'requests_per_target', 'target_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.RequestCountScalingProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.RequestCountScalingPropsDefConfig] = pydantic.Field(None)


class RequestCountScalingPropsDefConfig(pydantic.BaseModel):
    target_group_config: typing.Optional[models.aws_elasticloadbalancingv2.ApplicationTargetGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.RuntimePlatform
class RuntimePlatformDef(BaseStruct):
    cpu_architecture: typing.Optional[models.aws_ecs.CpuArchitectureDef] = pydantic.Field(None, description='The CpuArchitecture for Fargate Runtime Platform. Default: - Undefined.\n')
    operating_system_family: typing.Optional[models.aws_ecs.OperatingSystemFamilyDef] = pydantic.Field(None, description='The operating system for Fargate Runtime Platform. Default: - Undefined.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Create a Task Definition for the Windows container to start\n    task_definition = ecs.FargateTaskDefinition(self, "TaskDef",\n        runtime_platform=ecs.RuntimePlatform(\n            operating_system_family=ecs.OperatingSystemFamily.WINDOWS_SERVER_2019_CORE,\n            cpu_architecture=ecs.CpuArchitecture.X86_64\n        ),\n        cpu=1024,\n        memory_limit_mi_b=2048\n    )\n\n    task_definition.add_container("windowsservercore",\n        logging=ecs.LogDriver.aws_logs(stream_prefix="win-iis-on-fargate"),\n        port_mappings=[ecs.PortMapping(container_port=80)],\n        image=ecs.ContainerImage.from_registry("mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu_architecture', 'operating_system_family']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.RuntimePlatform'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ScalableTaskCountProps
class ScalableTaskCountPropsDef(BaseStruct):
    max_capacity: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1\n')
    dimension: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Scalable dimension of the attribute.\n')
    resource_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Resource ID of the attribute.\n')
    role: typing.Union[_REQUIRED_INIT_PARAM, models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Role to use for scaling.\n')
    service_namespace: typing.Union[aws_cdk.aws_applicationautoscaling.ServiceNamespace, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Service namespace of the scalable attribute.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_applicationautoscaling as appscaling\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    scalable_task_count_props = ecs.ScalableTaskCountProps(\n        dimension="dimension",\n        max_capacity=123,\n        resource_id="resourceId",\n        role=role,\n        service_namespace=appscaling.ServiceNamespace.ECS,\n\n        # the properties below are optional\n        min_capacity=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_capacity', 'min_capacity', 'dimension', 'resource_id', 'role', 'service_namespace']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ScalableTaskCountProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.ScalableTaskCountPropsDefConfig] = pydantic.Field(None)


class ScalableTaskCountPropsDefConfig(pydantic.BaseModel):
    role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.ScratchSpace
class ScratchSpaceDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path on the container to mount the scratch volume at.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the scratch volume to mount. Must be a volume name referenced in the name parameter of task definition volume.\n')
    read_only: typing.Union[bool, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether to give the container read-only access to the scratch volume. If this value is true, the container has read-only access to the scratch volume. If this value is false, then the container can write to the scratch volume.\n')
    source_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'name', 'read_only', 'source_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ScratchSpace'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.SecretVersionInfo
class SecretVersionInfoDef(BaseStruct):
    version_id: typing.Optional[str] = pydantic.Field(None, description='version id of the secret. Default: - use default version id\n')
    version_stage: typing.Optional[str] = pydantic.Field(None, description='version stage of the secret. Default: - use default version stage\n\n:exampleMetadata: infused\n\nExample::\n\n    # secret: secretsmanager.Secret\n    # db_secret: secretsmanager.Secret\n    # parameter: ssm.StringParameter\n    # task_definition: ecs.TaskDefinition\n    # s3_bucket: s3.Bucket\n\n\n    new_container = task_definition.add_container("container",\n        image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n        memory_limit_mi_b=1024,\n        environment={ # clear text, not for sensitive data\n            "STAGE": "prod"},\n        environment_files=[ # list of environment files hosted either on local disk or S3\n            ecs.EnvironmentFile.from_asset("./demo-env-file.env"),\n            ecs.EnvironmentFile.from_bucket(s3_bucket, "assets/demo-env-file.env")],\n        secrets={ # Retrieved from AWS Secrets Manager or AWS Systems Manager Parameter Store at container start-up.\n            "SECRET": ecs.Secret.from_secrets_manager(secret),\n            "DB_PASSWORD": ecs.Secret.from_secrets_manager(db_secret, "password"),  # Reference a specific JSON field, (requires platform version 1.4.0 or later for Fargate tasks)\n            "API_KEY": ecs.Secret.from_secrets_manager_version(secret, ecs.SecretVersionInfo(version_id="12345"), "apiKey"),  # Reference a specific version of the secret by its version id or version stage (requires platform version 1.4.0 or later for Fargate tasks)\n            "PARAMETER": ecs.Secret.from_ssm_parameter(parameter)}\n    )\n    new_container.add_environment("QUEUE_NAME", "MyQueue")\n    new_container.add_secret("API_KEY", ecs.Secret.from_secrets_manager(secret))\n    new_container.add_secret("DB_PASSWORD", ecs.Secret.from_secrets_manager(secret, "password"))\n')
    _init_params: typing.ClassVar[list[str]] = ['version_id', 'version_stage']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.SecretVersionInfo'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ServiceConnectProps
class ServiceConnectPropsDef(BaseStruct):
    log_driver: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The log driver configuration to use for the Service Connect agent logs. Default: - none\n')
    namespace: typing.Optional[str] = pydantic.Field(None, description='The cloudmap namespace to register this service into. Default: the cloudmap namespace specified on the cluster.\n')
    services: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.ServiceConnectServiceDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of Services, including a port mapping, terse client alias, and optional intermediate DNS name. This property may be left blank if the current ECS service does not need to advertise any ports via Service Connect. Default: none\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n\n\n    custom_service = ecs.FargateService(self, "CustomizedService",\n        cluster=cluster,\n        task_definition=task_definition,\n        service_connect_configuration=ecs.ServiceConnectProps(\n            log_driver=ecs.LogDrivers.aws_logs(\n                stream_prefix="sc-traffic"\n            ),\n            services=[ecs.ServiceConnectService(\n                port_mapping_name="api",\n                dns_name="customized-api",\n                port=80,\n                ingress_port_override=20040,\n                discovery_name="custom"\n            )\n            ]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'namespace', 'services']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ServiceConnectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ServiceConnectService
class ServiceConnectServiceDef(BaseStruct):
    port_mapping_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='portMappingName specifies which port and protocol combination should be used for this service connect service.\n')
    discovery_name: typing.Optional[str] = pydantic.Field(None, description='Optionally specifies an intermediate dns name to register in the CloudMap namespace. This is required if you wish to use the same port mapping name in more than one service. Default: - port mapping name\n')
    dns_name: typing.Optional[str] = pydantic.Field(None, description='The terse DNS alias to use for this port mapping in the service connect mesh. Service Connect-enabled clients will be able to reach this service at http://dnsName:port. Default: - No alias is created. The service is reachable at ``portMappingName.namespace:port``.\n')
    idle_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time in seconds a connection for Service Connect will stay active while idle. A value of 0 can be set to disable ``idleTimeout``. If ``idleTimeout`` is set to a time that is less than ``perRequestTimeout``, the connection will close when the ``idleTimeout`` is reached and not the ``perRequestTimeout``. Default: - Duration.minutes(5) for HTTP/HTTP2/GRPC, Duration.hours(1) for TCP.\n')
    ingress_port_override: typing.Union[int, float, None] = pydantic.Field(None, description='Optional. The port on the Service Connect agent container to use for traffic ingress to this service. Default: - none\n')
    per_request_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time waiting for the upstream to respond with a complete response per request for Service Connect. A value of 0 can be set to disable ``perRequestTimeout``. Can only be set when the ``appProtocol`` for the application container is HTTP/HTTP2/GRPC. If ``idleTimeout`` is set to a time that is less than ``perRequestTimeout``, the connection will close when the ``idleTimeout`` is reached and not the ``perRequestTimeout``. Default: - Duration.seconds(15)\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for clients to use to communicate with this service via Service Connect. Default: the container port specified by the port mapping in portMappingName.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecs as ecs\n\n    service_connect_service = ecs.ServiceConnectService(\n        port_mapping_name="portMappingName",\n\n        # the properties below are optional\n        discovery_name="discoveryName",\n        dns_name="dnsName",\n        idle_timeout=cdk.Duration.minutes(30),\n        ingress_port_override=123,\n        per_request_timeout=cdk.Duration.minutes(30),\n        port=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['port_mapping_name', 'discovery_name', 'dns_name', 'idle_timeout', 'ingress_port_override', 'per_request_timeout', 'port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ServiceConnectService'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ServiceManagedEBSVolumeConfiguration
class ServiceManagedEBSVolumeConfigurationDef(BaseStruct):
    encrypted: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the volume should be encrypted. Default: - Default Amazon EBS encryption.\n')
    file_system_type: typing.Optional[aws_cdk.aws_ecs.FileSystemType] = pydantic.Field(None, description='The Linux filesystem type for the volume. For volumes created from a snapshot, you must specify the same filesystem type that the volume was using when the snapshot was created. The available filesystem types are ext3, ext4, and xfs. Default: - FileSystemType.XFS\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS). For gp3, io1, and io2 volumes, this represents the number of IOPS that are provisioned for the volume. For gp2 volumes, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. The following are the supported values for each volume type. - gp3: 3,000 - 16,000 IOPS - io1: 100 - 64,000 IOPS - io2: 100 - 256,000 IOPS This parameter is required for io1 and io2 volume types. The default for gp3 volumes is 3,000 IOPS. This parameter is not supported for st1, sc1, or standard volume types. Default: - undefined\n')
    kms_key_id: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='AWS Key Management Service key to use for Amazon EBS encryption. Default: - When ``encryption`` is turned on and no ``kmsKey`` is specified, the default AWS managed key for Amazon EBS volumes is used.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='An IAM role that allows ECS to make calls to EBS APIs on your behalf. This role is required to create and manage the Amazon EBS volume. Default: - automatically generated role.\n')
    size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the volume in GiB. You must specify either ``size`` or ``snapshotId``. You can optionally specify a volume size greater than or equal to the snapshot size. The following are the supported volume size values for each volume type. - gp2 and gp3: 1-16,384 - io1 and io2: 4-16,384 - st1 and sc1: 125-16,384 - standard: 1-1,024 Default: - The snapshot size is used for the volume size if you specify ``snapshotId``, otherwise this parameter is required.\n')
    snap_shot_id: typing.Optional[str] = pydantic.Field(None, description='The snapshot that Amazon ECS uses to create the volume. You must specify either ``size`` or ``snapshotId``. Default: - No snapshot.\n')
    tag_specifications: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.EBSTagSpecificationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specifies the tags to apply to the volume and whether to propagate those tags to the volume. Default: - No tags are specified.\n')
    throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The throughput to provision for a volume, in MiB/s, with a maximum of 1,000 MiB/s. This parameter is only supported for the gp3 volume type. Default: - No throughput.\n')
    volume_type: typing.Optional[aws_cdk.aws_ec2.EbsDeviceVolumeType] = pydantic.Field(None, description='The volume type. Default: - ec2.EbsDeviceVolumeType.GP2\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n\n    task_definition = ecs.FargateTaskDefinition(self, "TaskDef")\n\n    container = task_definition.add_container("web",\n        image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n        port_mappings=[ecs.PortMapping(\n            container_port=80,\n            protocol=ecs.Protocol.TCP\n        )]\n    )\n\n    volume = ecs.ServiceManagedVolume(self, "EBSVolume",\n        name="ebs1",\n        managed_eBSVolume=ecs.ServiceManagedEBSVolumeConfiguration(\n            size=Size.gibibytes(15),\n            volume_type=ec2.EbsDeviceVolumeType.GP3,\n            file_system_type=ecs.FileSystemType.XFS,\n            tag_specifications=[ecs.EBSTagSpecification(\n                tags={\n                    "purpose": "production"\n                },\n                propagate_tags=ecs.EbsPropagatedTagSource.SERVICE\n            )]\n        )\n    )\n\n    volume.mount_in(container,\n        container_path="/var/lib",\n        read_only=False\n    )\n\n    task_definition.add_volume(volume)\n\n    service = ecs.FargateService(self, "FargateService",\n        cluster=cluster,\n        task_definition=task_definition\n    )\n\n    service.add_volume(volume)\n')
    _init_params: typing.ClassVar[list[str]] = ['encrypted', 'file_system_type', 'iops', 'kms_key_id', 'role', 'size', 'snap_shot_id', 'tag_specifications', 'throughput', 'volume_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ServiceManagedEBSVolumeConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.ServiceManagedVolumeProps
class ServiceManagedVolumePropsDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume. This corresponds to the name provided in the ECS TaskDefinition.\n')
    managed_ebs_volume: typing.Union[models.aws_ecs.ServiceManagedEBSVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration for an Amazon Elastic Block Store (EBS) volume managed by ECS. Default: - undefined\n\n:exampleMetadata: infused\n\nExample::\n\n    # cluster: ecs.Cluster\n\n    task_definition = ecs.FargateTaskDefinition(self, "TaskDef")\n\n    container = task_definition.add_container("web",\n        image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n        port_mappings=[ecs.PortMapping(\n            container_port=80,\n            protocol=ecs.Protocol.TCP\n        )]\n    )\n\n    volume = ecs.ServiceManagedVolume(self, "EBSVolume",\n        name="ebs1",\n        managed_eBSVolume=ecs.ServiceManagedEBSVolumeConfiguration(\n            size=Size.gibibytes(15),\n            volume_type=ec2.EbsDeviceVolumeType.GP3,\n            file_system_type=ecs.FileSystemType.XFS,\n            tag_specifications=[ecs.EBSTagSpecification(\n                tags={\n                    "purpose": "production"\n                },\n                propagate_tags=ecs.EbsPropagatedTagSource.SERVICE\n            )]\n        )\n    )\n\n    volume.mount_in(container,\n        container_path="/var/lib",\n        read_only=False\n    )\n\n    task_definition.add_volume(volume)\n\n    service = ecs.FargateService(self, "FargateService",\n        cluster=cluster,\n        task_definition=task_definition\n    )\n\n    service.add_volume(volume)\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'managed_ebs_volume']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.ServiceManagedVolumeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.SplunkLogDriverProps
class SplunkLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n')
    secret_token: typing.Union[models.aws_ecs.SecretDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Splunk HTTP Event Collector token (Secret). The splunk-token is added to the SecretOptions property of the Log Driver Configuration. So the secret value will not be resolved or viewable as plain text.\n')
    url: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Path to your Splunk Enterprise, self-service Splunk Cloud instance, or Splunk Cloud managed cluster (including port and scheme used by HTTP Event Collector) in one of the following formats: https://your_splunk_instance:8088 or https://input-prd-p-XXXXXXX.cloud.splunk.com:8088 or https://http-inputs-XXXXXXXX.splunkcloud.com.\n')
    ca_name: typing.Optional[str] = pydantic.Field(None, description='Name to use for validating server certificate. Default: - The hostname of the splunk-url\n')
    ca_path: typing.Optional[str] = pydantic.Field(None, description='Path to root certificate. Default: - caPath not set.\n')
    format: typing.Optional[aws_cdk.aws_ecs.SplunkLogFormat] = pydantic.Field(None, description='Message format. Can be inline, json or raw. Default: - inline\n')
    gzip: typing.Optional[bool] = pydantic.Field(None, description='Enable/disable gzip compression to send events to Splunk Enterprise or Splunk Cloud instance. Default: - false\n')
    gzip_level: typing.Union[int, float, None] = pydantic.Field(None, description='Set compression level for gzip. Valid values are -1 (default), 0 (no compression), 1 (best speed) ... 9 (best compression). Default: - -1 (Default Compression)\n')
    index: typing.Optional[str] = pydantic.Field(None, description='Event index. Default: - index not set.\n')
    insecure_skip_verify: typing.Optional[str] = pydantic.Field(None, description='Ignore server certificate validation. Default: - insecureSkipVerify not set.\n')
    source: typing.Optional[str] = pydantic.Field(None, description='Event source. Default: - source not set.\n')
    source_type: typing.Optional[str] = pydantic.Field(None, description='Event source type. Default: - sourceType not set.\n')
    verify_connection: typing.Optional[bool] = pydantic.Field(None, description='Verify on start, that docker can connect to Splunk server. Default: - true\n\n:exampleMetadata: infused\n\nExample::\n\n    # secret: ecs.Secret\n\n\n    # Create a Task Definition for the container to start\n    task_definition = ecs.Ec2TaskDefinition(self, "TaskDef")\n    task_definition.add_container("TheContainer",\n        image=ecs.ContainerImage.from_registry("example-image"),\n        memory_limit_mi_b=256,\n        logging=ecs.LogDrivers.splunk(\n            secret_token=secret,\n            url="my-splunk-url"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag', 'secret_token', 'url', 'ca_name', 'ca_path', 'format', 'gzip', 'gzip_level', 'index', 'insecure_skip_verify', 'source', 'source_type', 'verify_connection']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.SplunkLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.SplunkLogDriverPropsDefConfig] = pydantic.Field(None)


class SplunkLogDriverPropsDefConfig(pydantic.BaseModel):
    secret_token_config: typing.Optional[models.aws_ecs.SecretDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_ecs.SyslogLogDriverProps
class SyslogLogDriverPropsDef(BaseStruct):
    env: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The env option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No env\n')
    env_regex: typing.Optional[str] = pydantic.Field(None, description='The env-regex option is similar to and compatible with env. Its value is a regular expression to match logging-related environment variables. It is used for advanced log tag options. Default: - No envRegex\n')
    labels: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The labels option takes an array of keys. If there is collision between label and env keys, the value of the env takes precedence. Adds additional fields to the extra attributes of a logging message. Default: - No labels\n')
    tag: typing.Optional[str] = pydantic.Field(None, description='By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format. Default: - The first 12 characters of the container ID\n')
    address: typing.Optional[str] = pydantic.Field(None, description='The address of an external syslog server. The URI specifier may be [tcp|udp|tcp+tls]://host:port, unix://path, or unixgram://path. Default: - If the transport is tcp, udp, or tcp+tls, the default port is 514.\n')
    facility: typing.Optional[str] = pydantic.Field(None, description='The syslog facility to use. Can be the number or name for any valid syslog facility. See the syslog documentation: https://tools.ietf.org/html/rfc5424#section-6.2.1. Default: - facility not set\n')
    format: typing.Optional[str] = pydantic.Field(None, description='The syslog message format to use. If not specified the local UNIX syslog format is used, without a specified hostname. Specify rfc3164 for the RFC-3164 compatible format, rfc5424 for RFC-5424 compatible format, or rfc5424micro for RFC-5424 compatible format with microsecond timestamp resolution. Default: - format not set\n')
    tls_ca_cert: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the trust certificates signed by the CA. Ignored if the address protocol is not tcp+tls. Default: - tlsCaCert not set\n')
    tls_cert: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the TLS certificate file. Ignored if the address protocol is not tcp+tls. Default: - tlsCert not set\n')
    tls_key: typing.Optional[str] = pydantic.Field(None, description='The absolute path to the TLS key file. Ignored if the address protocol is not tcp+tls. Default: - tlsKey not set\n')
    tls_skip_verify: typing.Optional[bool] = pydantic.Field(None, description='If set to true, TLS verification is skipped when connecting to the syslog daemon. Ignored if the address protocol is not tcp+tls. Default: - false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    syslog_log_driver_props = ecs.SyslogLogDriverProps(\n        address="address",\n        env=["env"],\n        env_regex="envRegex",\n        facility="facility",\n        format="format",\n        labels=["labels"],\n        tag="tag",\n        tls_ca_cert="tlsCaCert",\n        tls_cert="tlsCert",\n        tls_key="tlsKey",\n        tls_skip_verify=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['env', 'env_regex', 'labels', 'tag', 'address', 'facility', 'format', 'tls_ca_cert', 'tls_cert', 'tls_key', 'tls_skip_verify']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.SyslogLogDriverProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.SystemControl
class SystemControlDef(BaseStruct):
    namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The namespaced kernel parameter for which to set a value.\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The value for the namespaced kernel parameter specified in namespace.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    system_control = ecs.SystemControl(\n        namespace="namespace",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['namespace', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.SystemControl'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.TaskDefinitionAttributes
class TaskDefinitionAttributesDef(BaseStruct):
    task_definition_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The arn of the task definition.\n')
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role that grants containers and Fargate agents permission to make AWS API calls on your behalf. Some tasks do not have an execution role. Default: - undefined\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. Default: Network mode cannot be provided to the imported task.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: Permissions cannot be granted to the imported task.\n')
    compatibility: typing.Optional[aws_cdk.aws_ecs.Compatibility] = pydantic.Field(None, description='What launch types this task definition should be compatible with. Default: Compatibility.EC2_AND_FARGATE\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    task_definition_attributes = ecs.TaskDefinitionAttributes(\n        task_definition_arn="taskDefinitionArn",\n\n        # the properties below are optional\n        compatibility=ecs.Compatibility.EC2,\n        execution_role=role,\n        network_mode=ecs.NetworkMode.NONE,\n        task_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['task_definition_arn', 'execution_role', 'network_mode', 'task_role', 'compatibility']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.TaskDefinitionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.TaskDefinitionProps
class TaskDefinitionPropsDef(BaseStruct):
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM task execution role that grants the ECS agent permission to call AWS APIs on your behalf. The role will be used to retrieve container images from ECR and create CloudWatch log groups. Default: - An execution role will be automatically created if you use ECR images in your task definition.\n')
    family: typing.Optional[str] = pydantic.Field(None, description='The name of a family that this task definition is registered to. A family groups multiple versions of a task definition. Default: - Automatically generated name.\n')
    proxy_configuration: typing.Optional[models.aws_ecs.ProxyConfigurationDef] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Default: - No proxy configuration.\n')
    task_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The name of the IAM role that grants containers in the task permission to call AWS APIs on your behalf. Default: - A task role is automatically created for you.\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.VolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The list of volume definitions for the task. For more information, see `Task Definition Parameter Volumes <https://docs.aws.amazon.com/AmazonECS/latest/developerguide//task_definition_parameters.html#volumes>`_. Default: - No volumes are passed to the Docker daemon on a container instance.\n')
    compatibility: typing.Union[aws_cdk.aws_ecs.Compatibility, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The task launch type compatiblity requirement.\n')
    cpu: typing.Optional[str] = pydantic.Field(None, description='The number of cpu units used by the task. If you are using the EC2 launch type, this field is optional and any value can be used. If you are using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the memory parameter: 256 (.25 vCPU) - Available memory values: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) 512 (.5 vCPU) - Available memory values: 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) 1024 (1 vCPU) - Available memory values: 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) 2048 (2 vCPU) - Available memory values: Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) 4096 (4 vCPU) - Available memory values: Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) 8192 (8 vCPU) - Available memory values: Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) 16384 (16 vCPU) - Available memory values: Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) Default: - CPU units are not specified.\n')
    ephemeral_storage_gib: typing.Union[int, float, None] = pydantic.Field(None, description='The amount (in GiB) of ephemeral storage to be allocated to the task. Only supported in Fargate platform version 1.4.0 or later. Default: - Undefined, in which case, the task will receive 20GiB ephemeral storage.\n')
    inference_accelerators: typing.Optional[typing.Sequence[typing.Union[models.aws_ecs.InferenceAcceleratorDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The inference accelerators to use for the containers in the task. Not supported in Fargate. Default: - No inference accelerators.\n')
    ipc_mode: typing.Optional[aws_cdk.aws_ecs.IpcMode] = pydantic.Field(None, description='The IPC resource namespace to use for the containers in the task. Not supported in Fargate and Windows containers. Default: - IpcMode used by the task is not specified\n')
    memory_mib: typing.Optional[str] = pydantic.Field(None, description='The amount (in MiB) of memory used by the task. If using the EC2 launch type, this field is optional and any value can be used. If using the Fargate launch type, this field is required and you must use one of the following values, which determines your range of valid values for the cpu parameter: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - Available cpu values: 256 (.25 vCPU) 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - Available cpu values: 512 (.5 vCPU) 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - Available cpu values: 1024 (1 vCPU) Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - Available cpu values: 2048 (2 vCPU) Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - Available cpu values: 4096 (4 vCPU) Between 16384 (16 GB) and 61440 (60 GB) in increments of 4096 (4 GB) - Available cpu values: 8192 (8 vCPU) Between 32768 (32 GB) and 122880 (120 GB) in increments of 8192 (8 GB) - Available cpu values: 16384 (16 vCPU) Default: - Memory used by task is not specified.\n')
    network_mode: typing.Optional[aws_cdk.aws_ecs.NetworkMode] = pydantic.Field(None, description='The networking mode to use for the containers in the task. On Fargate, the only supported networking mode is AwsVpc. Default: - NetworkMode.Bridge for EC2 & External tasks, AwsVpc for Fargate tasks.\n')
    pid_mode: typing.Optional[aws_cdk.aws_ecs.PidMode] = pydantic.Field(None, description='The process namespace to use for the containers in the task. Only supported for tasks that are hosted on AWS Fargate if the tasks are using platform version 1.4.0 or later (Linux). Only the TASK option is supported for Linux-based Fargate containers. Not supported in Windows containers. If pidMode is specified for a Fargate task, then runtimePlatform.operatingSystemFamily must also be specified. For more information, see `Task Definition Parameters <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#task_definition_pidmode>`_. Default: - PidMode used by the task is not specified\n')
    placement_constraints: typing.Optional[typing.Sequence[models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None, description='The placement constraints to use for tasks in the service. You can specify a maximum of 10 constraints per task (this limit includes constraints in the task definition and those specified at run time). Not supported in Fargate. Default: - No placement constraints.\n')
    runtime_platform: typing.Union[models.aws_ecs.RuntimePlatformDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The operating system that your task definitions are running on. A runtimePlatform is supported only for tasks using the Fargate launch type. Default: - Undefined.\n\n:exampleMetadata: infused\n\nExample::\n\n    vpc = ec2.Vpc.from_lookup(self, "Vpc",\n        is_default=True\n    )\n\n    cluster = ecs.Cluster(self, "FargateCluster", vpc=vpc)\n\n    task_definition = ecs.TaskDefinition(self, "TD",\n        memory_mi_b="512",\n        cpu="256",\n        compatibility=ecs.Compatibility.FARGATE\n    )\n\n    container_definition = task_definition.add_container("TheContainer",\n        image=ecs.ContainerImage.from_registry("foo/bar"),\n        memory_limit_mi_b=256\n    )\n\n    run_task = tasks.EcsRunTask(self, "RunFargate",\n        integration_pattern=sfn.IntegrationPattern.RUN_JOB,\n        cluster=cluster,\n        task_definition=task_definition,\n        assign_public_ip=True,\n        container_overrides=[tasks.ContainerOverride(\n            container_definition=container_definition,\n            environment=[tasks.TaskEnvironmentVariable(name="SOME_KEY", value=sfn.JsonPath.string_at("$.SomeKey"))]\n        )],\n        launch_target=tasks.EcsFargateLaunchTarget(),\n        propagated_tag_source=ecs.PropagatedTagSource.TASK_DEFINITION\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execution_role', 'family', 'proxy_configuration', 'task_role', 'volumes', 'compatibility', 'cpu', 'ephemeral_storage_gib', 'inference_accelerators', 'ipc_mode', 'memory_mib', 'network_mode', 'pid_mode', 'placement_constraints', 'runtime_platform']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.TaskDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Tmpfs
class TmpfsDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The absolute file path where the tmpfs volume is to be mounted.\n')
    size: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The size (in MiB) of the tmpfs volume.\n')
    mount_options: typing.Optional[typing.Sequence[aws_cdk.aws_ecs.TmpfsMountOption]] = pydantic.Field(None, description='The list of tmpfs volume mount options. For more information, see `TmpfsMountOptions <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_Tmpfs.html>`_.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    tmpfs = ecs.Tmpfs(\n        container_path="containerPath",\n        size=123,\n\n        # the properties below are optional\n        mount_options=[ecs.TmpfsMountOption.DEFAULTS]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'size', 'mount_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Tmpfs'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.TrackCustomMetricProps
class TrackCustomMetricPropsDef(BaseStruct):
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    metric: typing.Union[_REQUIRED_INIT_PARAM, models.aws_cloudwatch.MathExpressionDef, models.aws_cloudwatch.MetricDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The custom CloudWatch metric to track. The metric must represent utilization; that is, you will always get the following behavior: - metric > targetValue => scale out - metric < targetValue => scale in\n')
    target_value: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The target value for the custom CloudWatch metric.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_cloudwatch as cloudwatch\n    from aws_cdk import aws_ecs as ecs\n\n    # metric: cloudwatch.Metric\n\n    track_custom_metric_props = ecs.TrackCustomMetricProps(\n        metric=metric,\n        target_value=123,\n\n        # the properties below are optional\n        disable_scale_in=False,\n        policy_name="policyName",\n        scale_in_cooldown=cdk.Duration.minutes(30),\n        scale_out_cooldown=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['disable_scale_in', 'policy_name', 'scale_in_cooldown', 'scale_out_cooldown', 'metric', 'target_value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.TrackCustomMetricProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Ulimit
class UlimitDef(BaseStruct):
    hard_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The hard limit for the ulimit type.\n')
    name: typing.Union[aws_cdk.aws_ecs.UlimitName, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of the ulimit. For more information, see `UlimitName <https://docs.aws.amazon.com/cdk/api/latest/typescript/api/aws-ecs/ulimitname.html#aws_ecs_UlimitName>`_.\n')
    soft_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The soft limit for the ulimit type.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    ulimit = ecs.Ulimit(\n        hard_limit=123,\n        name=ecs.UlimitName.CORE,\n        soft_limit=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['hard_limit', 'name', 'soft_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Ulimit'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.Volume
class VolumeDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume. Up to 255 letters (uppercase and lowercase), numbers, and hyphens are allowed. This name is referenced in the sourceVolume parameter of container definition mountPoints.\n')
    configured_at_launch: typing.Optional[bool] = pydantic.Field(None, description='Indicates if the volume should be configured at launch. Default: false\n')
    docker_volume_configuration: typing.Union[models.aws_ecs.DockerVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using Docker volumes. Docker volumes are only supported when you are using the EC2 launch type. Windows containers only support the use of the local driver. To use bind mounts, specify a host instead.\n')
    efs_volume_configuration: typing.Union[models.aws_ecs.EfsVolumeConfigurationDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This property is specified when you are using Amazon EFS. When specifying Amazon EFS volumes in tasks using the Fargate launch type, Fargate creates a supervisor container that is responsible for managing the Amazon EFS volume. The supervisor container uses a small amount of the task's memory. The supervisor container is visible when querying the task metadata version 4 endpoint, but is not visible in CloudWatch Container Insights. Default: No Elastic FileSystem is setup\n")
    host: typing.Union[models.aws_ecs.HostDef, dict[str, typing.Any], None] = pydantic.Field(None, description='This property is specified when you are using bind mount host volumes. Bind mount host volumes are supported when you are using either the EC2 or Fargate launch types. The contents of the host parameter determine whether your bind mount host volume persists on the host container instance and where it is stored. If the host parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data is not guaranteed to persist after the containers associated with it stop running.\n\n:exampleMetadata: infused\n\nExample::\n\n    # container: ecs.ContainerDefinition\n    # cluster: ecs.Cluster\n    # task_definition: ecs.TaskDefinition\n\n\n    volume_from_snapshot = ecs.ServiceManagedVolume(self, "EBSVolume",\n        name="nginx-vol",\n        managed_eBSVolume=ecs.ServiceManagedEBSVolumeConfiguration(\n            snap_shot_id="snap-066877671789bd71b",\n            volume_type=ec2.EbsDeviceVolumeType.GP3,\n            file_system_type=ecs.FileSystemType.XFS\n        )\n    )\n\n    volume_from_snapshot.mount_in(container,\n        container_path="/var/lib",\n        read_only=False\n    )\n    task_definition.add_volume(volume_from_snapshot)\n    service = ecs.FargateService(self, "FargateService",\n        cluster=cluster,\n        task_definition=task_definition\n    )\n\n    service.add_volume(volume_from_snapshot)\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'configured_at_launch', 'docker_volume_configuration', 'efs_volume_configuration', 'host']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.Volume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.VolumeFrom
class VolumeFromDef(BaseStruct):
    read_only: typing.Union[bool, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies whether the container has read-only access to the volume. If this value is true, the container has read-only access to the volume. If this value is false, then the container can write to the volume.\n')
    source_container: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of another container within the same task definition from which to mount volumes.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    volume_from = ecs.VolumeFrom(\n        read_only=False,\n        source_container="sourceContainer"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['read_only', 'source_container']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.VolumeFrom'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.AlarmBehavior
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.AmiHardwareType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.AwsLogDriverMode
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.BinPackResource
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.BottlerocketEcsVariant
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.Capability
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.Compatibility
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.ContainerDependencyCondition
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.DeploymentControllerType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.DevicePermission
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.EbsPropagatedTagSource
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.EnvironmentFileType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.ExecuteCommandLogging
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.FargatePlatformVersion
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.FileSystemType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.FirelensConfigFileType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.FirelensLogRouterType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.GelfCompressionType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.IpcMode
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.LaunchType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.MachineImageType
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.NetworkMode
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.PidMode
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.PropagatedTagSource
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.Protocol
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.Scope
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.SplunkLogFormat
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.TmpfsMountOption
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.UlimitName
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.WindowsOptimizedVersion
# skipping emum

#  autogenerated from aws_cdk.aws_ecs.IBaseService
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.ICluster
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IEc2Service
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IEc2TaskDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IEcsLoadBalancerTarget
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IExternalService
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IExternalTaskDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IFargateService
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IFargateTaskDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.IService
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.ITaskDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.ITaskDefinitionExtension
#  skipping Interface

#  autogenerated from aws_cdk.aws_ecs.CfnCapacityProvider
class CfnCapacityProviderDef(BaseCfnResource):
    auto_scaling_group_provider: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_ecs.CfnCapacityProvider_AutoScalingGroupProviderPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Auto Scaling group settings for the capacity provider.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the capacity provider. If a name is specified, it cannot start with ``aws`` , ``ecs`` , or ``fargate`` . If no name is specified, a default name in the ``CFNStackName-CFNResourceName-RandomString`` format is used.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the capacity provider to help you categorize and organize it. Each tag consists of a key and an optional value. You define both. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /')
    _init_params: typing.ClassVar[list[str]] = ['auto_scaling_group_provider', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['AutoScalingGroupProviderProperty', 'ManagedScalingProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCapacityProvider'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CfnCapacityProviderDefConfig] = pydantic.Field(None)


class CfnCapacityProviderDefConfig(pydantic.BaseModel):
    AutoScalingGroupProviderProperty: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAutoscalinggroupproviderpropertyParams]] = pydantic.Field(None, description='')
    ManagedScalingProperty: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefManagedscalingpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_ecs.CfnCapacityProviderDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnCapacityProviderDefAutoscalinggroupproviderpropertyParams(pydantic.BaseModel):
    auto_scaling_group_arn: str = pydantic.Field(..., description='')
    managed_draining: typing.Optional[str] = pydantic.Field(None, description='')
    managed_scaling: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCapacityProvider_ManagedScalingPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    managed_termination_protection: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnCapacityProviderDefManagedscalingpropertyParams(pydantic.BaseModel):
    instance_warmup_period: typing.Union[int, float, None] = pydantic.Field(None, description='')
    maximum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    minimum_scaling_step_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    status: typing.Optional[str] = pydantic.Field(None, description='')
    target_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnCapacityProviderDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnCapacityProviderDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCapacityProviderDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnCapacityProviderDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCapacityProviderDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnCapacityProviderDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnCapacityProviderDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnCapacityProviderDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnCapacityProviderDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnCapacityProviderDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCapacityProviderDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnCapacityProviderDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnCapacityProviderDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCapacityProviderDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CfnCluster
class CfnClusterDef(BaseCfnResource):
    capacity_providers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The short name of one or more capacity providers to associate with the cluster. A capacity provider must be associated with a cluster before it can be included as part of the default capacity provider strategy of the cluster or used in a capacity provider strategy when calling the `CreateService <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html>`_ or `RunTask <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html>`_ actions. If specifying a capacity provider that uses an Auto Scaling group, the capacity provider must be created but not associated with another cluster. New Auto Scaling group capacity providers can be created with the `CreateCapacityProvider <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateCapacityProvider.html>`_ API operation. To use a AWS Fargate capacity provider, specify either the ``FARGATE`` or ``FARGATE_SPOT`` capacity providers. The AWS Fargate capacity providers are available to all accounts and only need to be associated with a cluster to be used. The `PutCapacityProvider <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PutCapacityProvider.html>`_ API operation is used to update the list of available capacity providers for a cluster after the cluster is created.\n')
    cluster_name: typing.Optional[str] = pydantic.Field(None, description="A user-generated string that you use to identify your cluster. If you don't specify a name, AWS CloudFormation generates a unique physical ID for the name.\n")
    cluster_settings: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ClusterSettingsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The settings to use when creating a cluster. This parameter is used to turn on CloudWatch Container Insights for a cluster.\n')
    configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ClusterConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The execute command and managed storage configuration for the cluster.\n')
    default_capacity_provider_strategy: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_CapacityProviderStrategyItemPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The default capacity provider strategy for the cluster. When services or tasks are run in the cluster with no launch type or capacity provider strategy specified, the default capacity provider strategy is used.\n')
    service_connect_defaults: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ServiceConnectDefaultsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Use this parameter to set a default Service Connect namespace. After you set a default Service Connect namespace, any new services with Service Connect turned on that are created in the cluster are added as client services in the namespace. This setting only applies to new services that set the ``enabled`` parameter to ``true`` in the ``ServiceConnectConfiguration`` . You can set the namespace of each service individually in the ``ServiceConnectConfiguration`` to override this default parameter. Tasks that run in a namespace can use short names to connect to services in the namespace. Tasks can connect to services across all of the clusters in the namespace. Tasks connect through a managed proxy container that collects logs and metrics for increased visibility. Only the tasks that Amazon ECS services create are supported with Service Connect. For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the cluster to help you categorize and organize them. Each tag consists of a key and an optional value. You define both. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /')
    _init_params: typing.ClassVar[list[str]] = ['capacity_providers', 'cluster_name', 'cluster_settings', 'configuration', 'default_capacity_provider_strategy', 'service_connect_defaults', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['CapacityProviderStrategyItemProperty', 'ClusterConfigurationProperty', 'ClusterSettingsProperty', 'ExecuteCommandConfigurationProperty', 'ExecuteCommandLogConfigurationProperty', 'ManagedStorageConfigurationProperty', 'ServiceConnectDefaultsProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CfnClusterDefConfig] = pydantic.Field(None)


class CfnClusterDefConfig(pydantic.BaseModel):
    CapacityProviderStrategyItemProperty: typing.Optional[list[models.aws_ecs.CfnClusterDefCapacityproviderstrategyitempropertyParams]] = pydantic.Field(None, description='')
    ClusterConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnClusterDefClusterconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ClusterSettingsProperty: typing.Optional[list[models.aws_ecs.CfnClusterDefClustersettingspropertyParams]] = pydantic.Field(None, description='')
    ExecuteCommandConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnClusterDefExecutecommandconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ExecuteCommandLogConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnClusterDefExecutecommandlogconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ManagedStorageConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnClusterDefManagedstorageconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ServiceConnectDefaultsProperty: typing.Optional[list[models.aws_ecs.CfnClusterDefServiceconnectdefaultspropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_ecs.CfnClusterDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_ecs.CfnClusterDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_ecs.CfnClusterDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_ecs.CfnClusterDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_ecs.CfnClusterDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_ecs.CfnClusterDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_ecs.CfnClusterDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_ecs.CfnClusterDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_ecs.CfnClusterDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_ecs.CfnClusterDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_ecs.CfnClusterDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_ecs.CfnClusterDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_ecs.CfnClusterDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnClusterDefCapacityproviderstrategyitempropertyParams(pydantic.BaseModel):
    base: typing.Union[int, float, None] = pydantic.Field(None, description='')
    capacity_provider: typing.Optional[str] = pydantic.Field(None, description='')
    weight: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnClusterDefClusterconfigurationpropertyParams(pydantic.BaseModel):
    execute_command_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ExecuteCommandConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    managed_storage_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ManagedStorageConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnClusterDefClustersettingspropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnClusterDefExecutecommandconfigurationpropertyParams(pydantic.BaseModel):
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ExecuteCommandLogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    logging: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnClusterDefExecutecommandlogconfigurationpropertyParams(pydantic.BaseModel):
    cloud_watch_encryption_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    cloud_watch_log_group_name: typing.Optional[str] = pydantic.Field(None, description='')
    s3_bucket_name: typing.Optional[str] = pydantic.Field(None, description='')
    s3_encryption_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    s3_key_prefix: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnClusterDefManagedstorageconfigurationpropertyParams(pydantic.BaseModel):
    fargate_ephemeral_storage_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnClusterDefServiceconnectdefaultspropertyParams(pydantic.BaseModel):
    namespace: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnClusterDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnClusterDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnClusterDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnClusterDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnClusterDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnClusterDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnClusterDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnClusterDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnClusterDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnClusterDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnClusterDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnClusterDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnClusterDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CfnClusterCapacityProviderAssociations
class CfnClusterCapacityProviderAssociationsDef(BaseCfnResource):
    capacity_providers: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The capacity providers to associate with the cluster.\n')
    cluster: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cluster the capacity provider association is the target of.\n')
    default_capacity_provider_strategy: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnClusterCapacityProviderAssociations_CapacityProviderStrategyPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The default capacity provider strategy to associate with the cluster.')
    _init_params: typing.ClassVar[list[str]] = ['capacity_providers', 'cluster', 'default_capacity_provider_strategy']
    _method_names: typing.ClassVar[list[str]] = ['CapacityProviderStrategyProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnClusterCapacityProviderAssociations'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefConfig] = pydantic.Field(None)


class CfnClusterCapacityProviderAssociationsDefConfig(pydantic.BaseModel):
    CapacityProviderStrategyProperty: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefCapacityproviderstrategypropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_ecs.CfnClusterCapacityProviderAssociationsDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnClusterCapacityProviderAssociationsDefCapacityproviderstrategypropertyParams(pydantic.BaseModel):
    capacity_provider: str = pydantic.Field(..., description='')
    base: typing.Union[int, float, None] = pydantic.Field(None, description='')
    weight: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnClusterCapacityProviderAssociationsDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnClusterCapacityProviderAssociationsDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnClusterCapacityProviderAssociationsDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnClusterCapacityProviderAssociationsDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnClusterCapacityProviderAssociationsDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnClusterCapacityProviderAssociationsDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnClusterCapacityProviderAssociationsDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnClusterCapacityProviderAssociationsDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnClusterCapacityProviderAssociationsDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnClusterCapacityProviderAssociationsDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnClusterCapacityProviderAssociationsDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnClusterCapacityProviderAssociationsDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnClusterCapacityProviderAssociationsDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnClusterCapacityProviderAssociationsDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CfnPrimaryTaskSet
class CfnPrimaryTaskSetDef(BaseCfnResource):
    cluster: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the cluster that hosts the service that the task set exists in.\n')
    service: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the service that the task set exists in.\n')
    task_set_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the task set to set as the primary task set in the deployment.')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'service', 'task_set_id']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnPrimaryTaskSet'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CfnPrimaryTaskSetDefConfig] = pydantic.Field(None)


class CfnPrimaryTaskSetDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_ecs.CfnPrimaryTaskSetDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnPrimaryTaskSetDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnPrimaryTaskSetDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnPrimaryTaskSetDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnPrimaryTaskSetDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnPrimaryTaskSetDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnPrimaryTaskSetDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnPrimaryTaskSetDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnPrimaryTaskSetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnPrimaryTaskSetDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnPrimaryTaskSetDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnPrimaryTaskSetDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnPrimaryTaskSetDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnPrimaryTaskSetDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnPrimaryTaskSetDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CfnService
class CfnServiceDef(BaseCfnResource):
    capacity_provider_strategy: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_CapacityProviderStrategyItemPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The capacity provider strategy to use for the service. If a ``capacityProviderStrategy`` is specified, the ``launchType`` parameter must be omitted. If no ``capacityProviderStrategy`` or ``launchType`` is specified, the ``defaultCapacityProviderStrategy`` for the cluster is used. A capacity provider strategy may contain a maximum of 6 capacity providers.\n')
    cluster: typing.Optional[str] = pydantic.Field(None, description='The short name or full Amazon Resource Name (ARN) of the cluster that you run your service on. If you do not specify a cluster, the default cluster is assumed.\n')
    deployment_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Optional deployment parameters that control how many tasks run during the deployment and the ordering of stopping and starting tasks.\n')
    deployment_controller: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentControllerPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The deployment controller to use for the service. If no deployment controller is specified, the default value of ``ECS`` is used.\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of instantiations of the specified task definition to place and keep running in your service. For new services, if a desired count is not specified, a default value of ``1`` is used. When using the ``DAEMON`` scheduling strategy, the desired count is not required. For existing services, if a desired count is not specified, it is omitted from the operation.\n')
    enable_ecs_managed_tags: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether to turn on Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging your Amazon ECS resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ in the *Amazon Elastic Container Service Developer Guide* . When you use Amazon ECS managed tags, you need to set the ``propagateTags`` request parameter.\n')
    enable_execute_command: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Determines whether the execute command functionality is turned on for the service. If ``true`` , the execute command functionality is turned on for all containers in tasks as part of the service.\n')
    health_check_grace_period_seconds: typing.Union[int, float, None] = pydantic.Field(None, description="The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. This is only used when your service is configured to use a load balancer. If your service has a load balancer defined and you don't specify a health check grace period value, the default value of ``0`` is used. If you do not use an Elastic Load Balancing, we recommend that you use the ``startPeriod`` in the task definition health check parameters. For more information, see `Health check <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_HealthCheck.html>`_ . If your service's tasks take a while to start and respond to Elastic Load Balancing health checks, you can specify a health check grace period of up to 2,147,483,647 seconds (about 69 years). During that time, the Amazon ECS service scheduler ignores health check status. This grace period can prevent the service scheduler from marking tasks as unhealthy and stopping them before they have time to come up.\n")
    launch_type: typing.Optional[str] = pydantic.Field(None, description='The launch type on which to run your service. For more information, see `Amazon ECS Launch Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    load_balancers: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_LoadBalancerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of load balancer objects to associate with the service. If you specify the ``Role`` property, ``LoadBalancers`` must be specified as well. For information about the number of load balancers that you can specify per service, see `Service Load Balancing <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    network_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The network configuration for the service. This parameter is required for task definitions that use the ``awsvpc`` network mode to receive their own elastic network interface, and it is not supported for other network modes. For more information, see `Task Networking <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    placement_constraints: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_PlacementConstraintPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of placement constraint objects to use for tasks in your service. You can specify a maximum of 10 constraints for each task. This limit includes constraints in the task definition and those specified at runtime.\n')
    placement_strategies: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_PlacementStrategyPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The placement strategy objects to use for tasks in your service. You can specify a maximum of 5 strategy rules for each service.\n')
    platform_version: typing.Optional[str] = pydantic.Field(None, description='The platform version that your tasks in the service are running on. A platform version is specified only for tasks using the Fargate launch type. If one isn\'t specified, the ``LATEST`` platform version is used. For more information, see `AWS Fargate platform versions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html>`_ in the *Amazon Elastic Container Service Developer Guide* . Default: - "LATEST"\n')
    propagate_tags: typing.Optional[str] = pydantic.Field(None, description="Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags aren't propagated. Tags can only be propagated to the task during task creation. To add tags to a task after task creation, use the `TagResource <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_TagResource.html>`_ API action. You must set this to a value other than ``NONE`` when you use Cost Explorer. For more information, see `Amazon ECS usage reports <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/usage-reports.html>`_ in the *Amazon Elastic Container Service Developer Guide* . The default is ``NONE`` .\n")
    role: typing.Optional[str] = pydantic.Field(None, description="The name or full Amazon Resource Name (ARN) of the IAM role that allows Amazon ECS to make calls to your load balancer on your behalf. This parameter is only permitted if you are using a load balancer with your service and your task definition doesn't use the ``awsvpc`` network mode. If you specify the ``role`` parameter, you must also specify a load balancer object with the ``loadBalancers`` parameter. .. epigraph:: If your account has already created the Amazon ECS service-linked role, that role is used for your service unless you specify a role here. The service-linked role is required if your task definition uses the ``awsvpc`` network mode or if the service is configured to use service discovery, an external deployment controller, multiple target groups, or Elastic Inference accelerators in which case you don't specify a role here. For more information, see `Using service-linked roles for Amazon ECS <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using-service-linked-roles.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If your specified role has a path other than ``/`` , then you must either specify the full role ARN (this is recommended) or prefix the role name with the path. For example, if a role with the name ``bar`` has a path of ``/foo/`` then you would specify ``/foo/bar`` as the role name. For more information, see `Friendly names and paths <https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-friendly-names>`_ in the *IAM User Guide* .\n")
    scheduling_strategy: typing.Optional[str] = pydantic.Field(None, description="The scheduling strategy to use for the service. For more information, see `Services <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html>`_ . There are two service scheduler strategies available: - ``REPLICA`` -The replica scheduling strategy places and maintains the desired number of tasks across your cluster. By default, the service scheduler spreads tasks across Availability Zones. You can use task placement strategies and constraints to customize task placement decisions. This scheduler strategy is required if the service uses the ``CODE_DEPLOY`` or ``EXTERNAL`` deployment controller types. - ``DAEMON`` -The daemon scheduling strategy deploys exactly one task on each active container instance that meets all of the task placement constraints that you specify in your cluster. The service scheduler also evaluates the task placement constraints for running tasks and will stop tasks that don't meet the placement constraints. When you're using this strategy, you don't need to specify a desired number of tasks, a task placement strategy, or use Service Auto Scaling policies. .. epigraph:: Tasks using the Fargate launch type or the ``CODE_DEPLOY`` or ``EXTERNAL`` deployment controller types don't support the ``DAEMON`` scheduling strategy.\n")
    service_connect_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration for this service to discover and connect to services, and be discovered by, and connected from, other services within a namespace. Tasks that run in a namespace can use short names to connect to services in the namespace. Tasks can connect to services across all of the clusters in the namespace. Tasks connect through a managed proxy container that collects logs and metrics for increased visibility. Only the tasks that Amazon ECS services create are supported with Service Connect. For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of your service. Up to 255 letters (uppercase and lowercase), numbers, underscores, and hyphens are allowed. Service names must be unique within a cluster, but you can have similarly named services in multiple clusters within a Region or across multiple Regions. .. epigraph:: The stack update fails if you change any properties that require replacement and the ``ServiceName`` is configured. This is because AWS CloudFormation creates the replacement service first, but each ``ServiceName`` must be unique in the cluster.\n')
    service_registries: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceRegistryPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The details of the service discovery registry to associate with this service. For more information, see `Service discovery <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html>`_ . .. epigraph:: Each service may be associated with one service registry. Multiple service registries for each service isn't supported.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the service to help you categorize and organize them. Each tag consists of a key and an optional value, both of which you define. When a service is deleted, the tags are deleted as well. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /\n')
    task_definition: typing.Optional[str] = pydantic.Field(None, description="The ``family`` and ``revision`` ( ``family:revision`` ) or full ARN of the task definition to run in your service. If a ``revision`` isn't specified, the latest ``ACTIVE`` revision is used. A task definition must be specified if the service uses either the ``ECS`` or ``CODE_DEPLOY`` deployment controllers. For more information about deployment types, see `Amazon ECS deployment types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ .\n")
    volume_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceVolumeConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The configuration for a volume specified in the task definition as a volume that is configured at launch time. Currently, the only supported volume type is an Amazon EBS volume.')
    _init_params: typing.ClassVar[list[str]] = ['capacity_provider_strategy', 'cluster', 'deployment_configuration', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period_seconds', 'launch_type', 'load_balancers', 'network_configuration', 'placement_constraints', 'placement_strategies', 'platform_version', 'propagate_tags', 'role', 'scheduling_strategy', 'service_connect_configuration', 'service_name', 'service_registries', 'tags', 'task_definition', 'volume_configurations']
    _method_names: typing.ClassVar[list[str]] = ['AwsVpcConfigurationProperty', 'CapacityProviderStrategyItemProperty', 'DeploymentAlarmsProperty', 'DeploymentCircuitBreakerProperty', 'DeploymentConfigurationProperty', 'DeploymentControllerProperty', 'EBSTagSpecificationProperty', 'LoadBalancerProperty', 'LogConfigurationProperty', 'NetworkConfigurationProperty', 'PlacementConstraintProperty', 'PlacementStrategyProperty', 'SecretProperty', 'ServiceConnectClientAliasProperty', 'ServiceConnectConfigurationProperty', 'ServiceConnectServiceProperty', 'ServiceConnectTlsCertificateAuthorityProperty', 'ServiceConnectTlsConfigurationProperty', 'ServiceManagedEBSVolumeConfigurationProperty', 'ServiceRegistryProperty', 'ServiceVolumeConfigurationProperty', 'TimeoutConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnService'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CfnServiceDefConfig] = pydantic.Field(None)


class CfnServiceDefConfig(pydantic.BaseModel):
    AwsVpcConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefAwsvpcconfigurationpropertyParams]] = pydantic.Field(None, description='')
    CapacityProviderStrategyItemProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefCapacityproviderstrategyitempropertyParams]] = pydantic.Field(None, description='')
    DeploymentAlarmsProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefDeploymentalarmspropertyParams]] = pydantic.Field(None, description='')
    DeploymentCircuitBreakerProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefDeploymentcircuitbreakerpropertyParams]] = pydantic.Field(None, description='')
    DeploymentConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefDeploymentconfigurationpropertyParams]] = pydantic.Field(None, description='')
    DeploymentControllerProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefDeploymentcontrollerpropertyParams]] = pydantic.Field(None, description='')
    EBSTagSpecificationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefEbstagspecificationpropertyParams]] = pydantic.Field(None, description='')
    LoadBalancerProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefLoadbalancerpropertyParams]] = pydantic.Field(None, description='')
    LogConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefLogconfigurationpropertyParams]] = pydantic.Field(None, description='')
    NetworkConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefNetworkconfigurationpropertyParams]] = pydantic.Field(None, description='')
    PlacementConstraintProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefPlacementconstraintpropertyParams]] = pydantic.Field(None, description='')
    PlacementStrategyProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefPlacementstrategypropertyParams]] = pydantic.Field(None, description='')
    SecretProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefSecretpropertyParams]] = pydantic.Field(None, description='')
    ServiceConnectClientAliasProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServiceconnectclientaliaspropertyParams]] = pydantic.Field(None, description='')
    ServiceConnectConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServiceconnectconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ServiceConnectServiceProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServiceconnectservicepropertyParams]] = pydantic.Field(None, description='')
    ServiceConnectTlsCertificateAuthorityProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServiceconnecttlscertificateauthoritypropertyParams]] = pydantic.Field(None, description='')
    ServiceConnectTlsConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServiceconnecttlsconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ServiceManagedEBSVolumeConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServicemanagedebsvolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ServiceRegistryProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServiceregistrypropertyParams]] = pydantic.Field(None, description='')
    ServiceVolumeConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefServicevolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    TimeoutConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnServiceDefTimeoutconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_ecs.CfnServiceDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_ecs.CfnServiceDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_ecs.CfnServiceDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_ecs.CfnServiceDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_ecs.CfnServiceDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_ecs.CfnServiceDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_ecs.CfnServiceDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_ecs.CfnServiceDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_ecs.CfnServiceDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_ecs.CfnServiceDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_ecs.CfnServiceDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_ecs.CfnServiceDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_ecs.CfnServiceDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnServiceDefAwsvpcconfigurationpropertyParams(pydantic.BaseModel):
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description='')
    security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    subnets: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnServiceDefCapacityproviderstrategyitempropertyParams(pydantic.BaseModel):
    base: typing.Union[int, float, None] = pydantic.Field(None, description='')
    capacity_provider: typing.Optional[str] = pydantic.Field(None, description='')
    weight: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefDeploymentalarmspropertyParams(pydantic.BaseModel):
    alarm_names: typing.Sequence[str] = pydantic.Field(..., description='')
    enable: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    rollback: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    ...

class CfnServiceDefDeploymentcircuitbreakerpropertyParams(pydantic.BaseModel):
    enable: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    rollback: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    ...

class CfnServiceDefDeploymentconfigurationpropertyParams(pydantic.BaseModel):
    alarms: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentAlarmsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    deployment_circuit_breaker: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentCircuitBreakerPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    maximum_percent: typing.Union[int, float, None] = pydantic.Field(None, description='')
    minimum_healthy_percent: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefDeploymentcontrollerpropertyParams(pydantic.BaseModel):
    type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefEbstagspecificationpropertyParams(pydantic.BaseModel):
    resource_type: str = pydantic.Field(..., description='')
    propagate_tags: typing.Optional[str] = pydantic.Field(None, description='')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='')
    ...

class CfnServiceDefLoadbalancerpropertyParams(pydantic.BaseModel):
    container_name: typing.Optional[str] = pydantic.Field(None, description='')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    load_balancer_name: typing.Optional[str] = pydantic.Field(None, description='')
    target_group_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefLogconfigurationpropertyParams(pydantic.BaseModel):
    log_driver: typing.Optional[str] = pydantic.Field(None, description='')
    options: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    secret_options: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefNetworkconfigurationpropertyParams(pydantic.BaseModel):
    awsvpc_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_AwsVpcConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefPlacementconstraintpropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    expression: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefPlacementstrategypropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    field: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefSecretpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value_from: str = pydantic.Field(..., description='')
    ...

class CfnServiceDefServiceconnectclientaliaspropertyParams(pydantic.BaseModel):
    port: typing.Union[int, float] = pydantic.Field(..., description='')
    dns_name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefServiceconnectconfigurationpropertyParams(pydantic.BaseModel):
    enabled: typing.Union[bool, models.UnsupportedResource] = pydantic.Field(..., description='')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    namespace: typing.Optional[str] = pydantic.Field(None, description='')
    services: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectServicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefServiceconnectservicepropertyParams(pydantic.BaseModel):
    port_name: str = pydantic.Field(..., description='')
    client_aliases: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectClientAliasPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    discovery_name: typing.Optional[str] = pydantic.Field(None, description='')
    ingress_port_override: typing.Union[int, float, None] = pydantic.Field(None, description='')
    timeout: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_TimeoutConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    tls: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectTlsConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefServiceconnecttlscertificateauthoritypropertyParams(pydantic.BaseModel):
    aws_pca_authority_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefServiceconnecttlsconfigurationpropertyParams(pydantic.BaseModel):
    issuer_certificate_authority: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectTlsCertificateAuthorityPropertyDef, dict[str, typing.Any]] = pydantic.Field(..., description='')
    kms_key: typing.Optional[str] = pydantic.Field(None, description='')
    role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefServicemanagedebsvolumeconfigurationpropertyParams(pydantic.BaseModel):
    role_arn: str = pydantic.Field(..., description='')
    encrypted: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    filesystem_type: typing.Optional[str] = pydantic.Field(None, description='')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    size_in_gib: typing.Union[int, float, None] = pydantic.Field(None, description='')
    snapshot_id: typing.Optional[str] = pydantic.Field(None, description='')
    tag_specifications: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_EBSTagSpecificationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    throughput: typing.Union[int, float, None] = pydantic.Field(None, description='')
    volume_type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefServiceregistrypropertyParams(pydantic.BaseModel):
    container_name: typing.Optional[str] = pydantic.Field(None, description='')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    registry_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnServiceDefServicevolumeconfigurationpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    managed_ebs_volume: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceManagedEBSVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefTimeoutconfigurationpropertyParams(pydantic.BaseModel):
    idle_timeout_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='')
    per_request_timeout_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnServiceDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnServiceDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnServiceDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnServiceDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnServiceDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnServiceDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnServiceDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnServiceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnServiceDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnServiceDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnServiceDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnServiceDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnServiceDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnServiceDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinition
class CfnTaskDefinitionDef(BaseCfnResource):
    container_definitions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ContainerDefinitionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of container definitions in JSON format that describe the different containers that make up your task. For more information about container definition parameters and defaults, see `Amazon ECS Task Definitions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_defintions.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    cpu: typing.Optional[str] = pydantic.Field(None, description='The number of ``cpu`` units used by the task. If you use the EC2 launch type, this field is optional. Any value can be used. If you use the Fargate launch type, this field is required. You must use one of the following values. The value that you choose determines your range of valid values for the ``memory`` parameter. The CPU units cannot be less than 1 vCPU when you use Windows containers on Fargate. - 256 (.25 vCPU) - Available ``memory`` values: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - 512 (.5 vCPU) - Available ``memory`` values: 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - 1024 (1 vCPU) - Available ``memory`` values: 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - 2048 (2 vCPU) - Available ``memory`` values: 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - 4096 (4 vCPU) - Available ``memory`` values: 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - 8192 (8 vCPU) - Available ``memory`` values: 16 GB and 60 GB in 4 GB increments This option requires Linux platform ``1.4.0`` or later. - 16384 (16vCPU) - Available ``memory`` values: 32GB and 120 GB in 8 GB increments This option requires Linux platform ``1.4.0`` or later.\n')
    ephemeral_storage: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ephemeral storage settings to use for tasks run with the task definition.\n')
    execution_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the task execution role that grants the Amazon ECS container agent permission to make AWS API calls on your behalf. For informationabout the required IAM roles for Amazon ECS, see `IAM roles for Amazon ECS <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/security-ecs-iam-role-overview.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    family: typing.Optional[str] = pydantic.Field(None, description="The name of a family that this task definition is registered to. Up to 255 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed. A family groups multiple versions of a task definition. Amazon ECS gives the first task definition that you registered to a family a revision number of 1. Amazon ECS gives sequential revision numbers to each task definition that you add. .. epigraph:: To use revision numbers when you update a task definition, specify this property. If you don't specify a value, AWS CloudFormation generates a new task definition each time that you update it.\n")
    inference_accelerators: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_InferenceAcceleratorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The Elastic Inference accelerators to use for the containers in the task.\n')
    ipc_mode: typing.Optional[str] = pydantic.Field(None, description='The IPC resource namespace to use for the containers in the task. The valid values are ``host`` , ``task`` , or ``none`` . If ``host`` is specified, then all containers within the tasks that specified the ``host`` IPC mode on the same container instance share the same IPC resources with the host Amazon EC2 instance. If ``task`` is specified, all containers within the specified task share the same IPC resources. If ``none`` is specified, then IPC resources within the containers of a task are private and not shared with other containers in a task or on the container instance. If no value is specified, then the IPC resource namespace sharing depends on the Docker daemon setting on the container instance. For more information, see `IPC settings <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#ipc-settings---ipc>`_ in the *Docker run reference* . If the ``host`` IPC mode is used, be aware that there is a heightened risk of undesired IPC namespace expose. For more information, see `Docker security <https://docs.aws.amazon.com/https://docs.docker.com/engine/security/security/>`_ . If you are setting namespaced kernel parameters using ``systemControls`` for the containers in the task, the following will apply to your IPC resource namespace. For more information, see `System Controls <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html>`_ in the *Amazon Elastic Container Service Developer Guide* . - For tasks that use the ``host`` IPC mode, IPC namespace related ``systemControls`` are not supported. - For tasks that use the ``task`` IPC mode, IPC namespace related ``systemControls`` will apply to all containers within a task. .. epigraph:: This parameter is not supported for Windows containers or tasks run on AWS Fargate .\n')
    memory: typing.Optional[str] = pydantic.Field(None, description='The amount (in MiB) of memory used by the task. If your tasks runs on Amazon EC2 instances, you must specify either a task-level memory value or a container-level memory value. This field is optional and any value can be used. If a task-level memory value is specified, the container-level memory value is optional. For more information regarding container-level memory and memory reservation, see `ContainerDefinition <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ContainerDefinition.html>`_ . If your tasks runs on AWS Fargate , this field is required. You must use one of the following values. The value you choose determines your range of valid values for the ``cpu`` parameter. - 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - Available ``cpu`` values: 256 (.25 vCPU) - 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - Available ``cpu`` values: 512 (.5 vCPU) - 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - Available ``cpu`` values: 1024 (1 vCPU) - Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - Available ``cpu`` values: 2048 (2 vCPU) - Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - Available ``cpu`` values: 4096 (4 vCPU) - Between 16 GB and 60 GB in 4 GB increments - Available ``cpu`` values: 8192 (8 vCPU) This option requires Linux platform ``1.4.0`` or later. - Between 32GB and 120 GB in 8 GB increments - Available ``cpu`` values: 16384 (16 vCPU) This option requires Linux platform ``1.4.0`` or later.\n')
    network_mode: typing.Optional[str] = pydantic.Field(None, description='The Docker networking mode to use for the containers in the task. The valid values are ``none`` , ``bridge`` , ``awsvpc`` , and ``host`` . If no network mode is specified, the default is ``bridge`` . For Amazon ECS tasks on Fargate, the ``awsvpc`` network mode is required. For Amazon ECS tasks on Amazon EC2 Linux instances, any network mode can be used. For Amazon ECS tasks on Amazon EC2 Windows instances, ``<default>`` or ``awsvpc`` can be used. If the network mode is set to ``none`` , you cannot specify port mappings in your container definitions, and the tasks containers do not have external connectivity. The ``host`` and ``awsvpc`` network modes offer the highest networking performance for containers because they use the EC2 network stack instead of the virtualized network stack provided by the ``bridge`` mode. With the ``host`` and ``awsvpc`` network modes, exposed container ports are mapped directly to the corresponding host port (for the ``host`` network mode) or the attached elastic network interface port (for the ``awsvpc`` network mode), so you cannot take advantage of dynamic host port mappings. .. epigraph:: When using the ``host`` network mode, you should not run containers using the root user (UID 0). It is considered best practice to use a non-root user. If the network mode is ``awsvpc`` , the task is allocated an elastic network interface, and you must specify a ``NetworkConfiguration`` value when you create a service or run a task with the task definition. For more information, see `Task Networking <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If the network mode is ``host`` , you cannot run multiple instantiations of the same task on a single container instance when port mappings are used. For more information, see `Network settings <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#network-settings>`_ in the *Docker run reference* .\n')
    pid_mode: typing.Optional[str] = pydantic.Field(None, description="The process namespace to use for the containers in the task. The valid values are ``host`` or ``task`` . On Fargate for Linux containers, the only valid value is ``task`` . For example, monitoring sidecars might need ``pidMode`` to access information about other containers running in the same task. If ``host`` is specified, all containers within the tasks that specified the ``host`` PID mode on the same container instance share the same process namespace with the host Amazon EC2 instance. If ``task`` is specified, all containers within the specified task share the same process namespace. If no value is specified, the default is a private namespace for each container. For more information, see `PID settings <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#pid-settings---pid>`_ in the *Docker run reference* . If the ``host`` PID mode is used, there's a heightened risk of undesired process namespace exposure. For more information, see `Docker security <https://docs.aws.amazon.com/https://docs.docker.com/engine/security/security/>`_ . .. epigraph:: This parameter is not supported for Windows containers. > This parameter is only supported for tasks that are hosted on AWS Fargate if the tasks are using platform version ``1.4.0`` or later (Linux). This isn't supported for Windows containers on Fargate.\n")
    placement_constraints: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_TaskDefinitionPlacementConstraintPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="An array of placement constraint objects to use for tasks. .. epigraph:: This parameter isn't supported for tasks run on AWS Fargate .\n")
    proxy_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ProxyConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Your Amazon ECS container instances require at least version 1.26.0 of the container agent and at least version 1.26.0-1 of the ``ecs-init`` package to use a proxy configuration. If your container instances are launched from the Amazon ECS optimized AMI version ``20190301`` or later, they contain the required versions of the container agent and ``ecs-init`` . For more information, see `Amazon ECS-optimized Linux AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    requires_compatibilities: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The task launch types the task definition was validated against. The valid values are ``EC2`` , ``FARGATE`` , and ``EXTERNAL`` . For more information, see `Amazon ECS launch types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    runtime_platform: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_RuntimePlatformPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The operating system that your tasks definitions run on. A platform family is specified only for tasks using the Fargate launch type.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the task definition to help you categorize and organize them. Each tag consists of a key and an optional value. You define both of them. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /\n')
    task_role_arn: typing.Optional[str] = pydantic.Field(None, description='The short name or full Amazon Resource Name (ARN) of the AWS Identity and Access Management role that grants containers in the task permission to call AWS APIs on your behalf. For informationabout the required IAM roles for Amazon ECS, see `IAM roles for Amazon ECS <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/security-ecs-iam-role-overview.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    volumes: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_VolumePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The list of data volume definitions for the task. For more information, see `Using data volumes in tasks <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html>`_ in the *Amazon Elastic Container Service Developer Guide* . .. epigraph:: The ``host`` and ``sourcePath`` parameters aren't supported for tasks run on AWS Fargate .")
    _init_params: typing.ClassVar[list[str]] = ['container_definitions', 'cpu', 'ephemeral_storage', 'execution_role_arn', 'family', 'inference_accelerators', 'ipc_mode', 'memory', 'network_mode', 'pid_mode', 'placement_constraints', 'proxy_configuration', 'requires_compatibilities', 'runtime_platform', 'tags', 'task_role_arn', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['AuthorizationConfigProperty', 'ContainerDefinitionProperty', 'ContainerDependencyProperty', 'DeviceProperty', 'DockerVolumeConfigurationProperty', 'EFSVolumeConfigurationProperty', 'EnvironmentFileProperty', 'EphemeralStorageProperty', 'FSxAuthorizationConfigProperty', 'FSxWindowsFileServerVolumeConfigurationProperty', 'FirelensConfigurationProperty', 'HealthCheckProperty', 'HostEntryProperty', 'HostVolumePropertiesProperty', 'InferenceAcceleratorProperty', 'KernelCapabilitiesProperty', 'KeyValuePairProperty', 'LinuxParametersProperty', 'LogConfigurationProperty', 'MountPointProperty', 'PortMappingProperty', 'ProxyConfigurationProperty', 'RepositoryCredentialsProperty', 'ResourceRequirementProperty', 'RuntimePlatformProperty', 'SecretProperty', 'SystemControlProperty', 'TaskDefinitionPlacementConstraintProperty', 'TmpfsProperty', 'UlimitProperty', 'VolumeFromProperty', 'VolumeProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CfnTaskDefinitionDefConfig] = pydantic.Field(None)


class CfnTaskDefinitionDefConfig(pydantic.BaseModel):
    AuthorizationConfigProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAuthorizationconfigpropertyParams]] = pydantic.Field(None, description='')
    ContainerDefinitionProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefContainerdefinitionpropertyParams]] = pydantic.Field(None, description='')
    ContainerDependencyProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefContainerdependencypropertyParams]] = pydantic.Field(None, description='')
    DeviceProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefDevicepropertyParams]] = pydantic.Field(None, description='')
    DockerVolumeConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefDockervolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    EFSVolumeConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefEfsvolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    EnvironmentFileProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefEnvironmentfilepropertyParams]] = pydantic.Field(None, description='')
    EphemeralStorageProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefEphemeralstoragepropertyParams]] = pydantic.Field(None, description='')
    FSxAuthorizationConfigProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefFsxauthorizationconfigpropertyParams]] = pydantic.Field(None, description='')
    FSxWindowsFileServerVolumeConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefFsxwindowsfileservervolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    FirelensConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefFirelensconfigurationpropertyParams]] = pydantic.Field(None, description='')
    HealthCheckProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefHealthcheckpropertyParams]] = pydantic.Field(None, description='')
    HostEntryProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefHostentrypropertyParams]] = pydantic.Field(None, description='')
    HostVolumePropertiesProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefHostvolumepropertiespropertyParams]] = pydantic.Field(None, description='')
    InferenceAcceleratorProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefInferenceacceleratorpropertyParams]] = pydantic.Field(None, description='')
    KernelCapabilitiesProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefKernelcapabilitiespropertyParams]] = pydantic.Field(None, description='')
    KeyValuePairProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefKeyvaluepairpropertyParams]] = pydantic.Field(None, description='')
    LinuxParametersProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefLinuxparameterspropertyParams]] = pydantic.Field(None, description='')
    LogConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefLogconfigurationpropertyParams]] = pydantic.Field(None, description='')
    MountPointProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefMountpointpropertyParams]] = pydantic.Field(None, description='')
    PortMappingProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefPortmappingpropertyParams]] = pydantic.Field(None, description='')
    ProxyConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefProxyconfigurationpropertyParams]] = pydantic.Field(None, description='')
    RepositoryCredentialsProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefRepositorycredentialspropertyParams]] = pydantic.Field(None, description='')
    ResourceRequirementProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefResourcerequirementpropertyParams]] = pydantic.Field(None, description='')
    RuntimePlatformProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefRuntimeplatformpropertyParams]] = pydantic.Field(None, description='')
    SecretProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefSecretpropertyParams]] = pydantic.Field(None, description='')
    SystemControlProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefSystemcontrolpropertyParams]] = pydantic.Field(None, description='')
    TaskDefinitionPlacementConstraintProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefTaskdefinitionplacementconstraintpropertyParams]] = pydantic.Field(None, description='')
    TmpfsProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefTmpfspropertyParams]] = pydantic.Field(None, description='')
    UlimitProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefUlimitpropertyParams]] = pydantic.Field(None, description='')
    VolumeFromProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefVolumefrompropertyParams]] = pydantic.Field(None, description='')
    VolumeProperty: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefVolumepropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_ecs.CfnTaskDefinitionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnTaskDefinitionDefAuthorizationconfigpropertyParams(pydantic.BaseModel):
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='')
    iam: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefContainerdefinitionpropertyParams(pydantic.BaseModel):
    image: str = pydantic.Field(..., description='')
    name: str = pydantic.Field(..., description='')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='')
    credential_specs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    depends_on: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ContainerDependencyPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    disable_networking: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    dns_search_domains: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    dns_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    docker_labels: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    docker_security_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    environment: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_KeyValuePairPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    environment_files: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_EnvironmentFilePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    essential: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    extra_hosts: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_HostEntryPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    firelens_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_FirelensConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    health_check: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_HealthCheckPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    hostname: typing.Optional[str] = pydantic.Field(None, description='')
    interactive: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    links: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    linux_parameters: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_LinuxParametersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    memory: typing.Union[int, float, None] = pydantic.Field(None, description='')
    memory_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='')
    mount_points: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_MountPointPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    port_mappings: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_PortMappingPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    privileged: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    pseudo_terminal: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    readonly_root_filesystem: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    repository_credentials: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_RepositoryCredentialsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    resource_requirements: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ResourceRequirementPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    secrets: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    start_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    stop_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    system_controls: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_SystemControlPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ulimits: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_UlimitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    user: typing.Optional[str] = pydantic.Field(None, description='')
    volumes_from: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_VolumeFromPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefContainerdependencypropertyParams(pydantic.BaseModel):
    condition: typing.Optional[str] = pydantic.Field(None, description='')
    container_name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefDevicepropertyParams(pydantic.BaseModel):
    container_path: typing.Optional[str] = pydantic.Field(None, description='')
    host_path: typing.Optional[str] = pydantic.Field(None, description='')
    permissions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefDockervolumeconfigurationpropertyParams(pydantic.BaseModel):
    autoprovision: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    driver: typing.Optional[str] = pydantic.Field(None, description='')
    driver_opts: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    labels: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    scope: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefEfsvolumeconfigurationpropertyParams(pydantic.BaseModel):
    filesystem_id: str = pydantic.Field(..., description='')
    authorization_config: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_AuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='')
    transit_encryption: typing.Optional[str] = pydantic.Field(None, description='')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefEnvironmentfilepropertyParams(pydantic.BaseModel):
    type: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefEphemeralstoragepropertyParams(pydantic.BaseModel):
    size_in_gib: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefFsxauthorizationconfigpropertyParams(pydantic.BaseModel):
    credentials_parameter: str = pydantic.Field(..., description='')
    domain: str = pydantic.Field(..., description='')
    ...

class CfnTaskDefinitionDefFsxwindowsfileservervolumeconfigurationpropertyParams(pydantic.BaseModel):
    file_system_id: str = pydantic.Field(..., description='')
    root_directory: str = pydantic.Field(..., description='')
    authorization_config: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_FSxAuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefFirelensconfigurationpropertyParams(pydantic.BaseModel):
    options: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefHealthcheckpropertyParams(pydantic.BaseModel):
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    interval: typing.Union[int, float, None] = pydantic.Field(None, description='')
    retries: typing.Union[int, float, None] = pydantic.Field(None, description='')
    start_period: typing.Union[int, float, None] = pydantic.Field(None, description='')
    timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefHostentrypropertyParams(pydantic.BaseModel):
    hostname: typing.Optional[str] = pydantic.Field(None, description='')
    ip_address: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefHostvolumepropertiespropertyParams(pydantic.BaseModel):
    source_path: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefInferenceacceleratorpropertyParams(pydantic.BaseModel):
    device_name: typing.Optional[str] = pydantic.Field(None, description='')
    device_type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefKernelcapabilitiespropertyParams(pydantic.BaseModel):
    add: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    drop: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefKeyvaluepairpropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefLinuxparameterspropertyParams(pydantic.BaseModel):
    capabilities: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_KernelCapabilitiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    devices: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_DevicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    init_process_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    max_swap: typing.Union[int, float, None] = pydantic.Field(None, description='')
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description='')
    tmpfs: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_TmpfsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefLogconfigurationpropertyParams(pydantic.BaseModel):
    log_driver: str = pydantic.Field(..., description='')
    options: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    secret_options: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefMountpointpropertyParams(pydantic.BaseModel):
    container_path: typing.Optional[str] = pydantic.Field(None, description='')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    source_volume: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefPortmappingpropertyParams(pydantic.BaseModel):
    app_protocol: typing.Optional[str] = pydantic.Field(None, description='')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    container_port_range: typing.Optional[str] = pydantic.Field(None, description='')
    host_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    protocol: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefProxyconfigurationpropertyParams(pydantic.BaseModel):
    container_name: str = pydantic.Field(..., description='')
    proxy_configuration_properties: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_KeyValuePairPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefRepositorycredentialspropertyParams(pydantic.BaseModel):
    credentials_parameter: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefResourcerequirementpropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    value: str = pydantic.Field(..., description='')
    ...

class CfnTaskDefinitionDefRuntimeplatformpropertyParams(pydantic.BaseModel):
    cpu_architecture: typing.Optional[str] = pydantic.Field(None, description='')
    operating_system_family: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefSecretpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value_from: str = pydantic.Field(..., description='')
    ...

class CfnTaskDefinitionDefSystemcontrolpropertyParams(pydantic.BaseModel):
    namespace: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefTaskdefinitionplacementconstraintpropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    expression: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefTmpfspropertyParams(pydantic.BaseModel):
    size: typing.Union[int, float] = pydantic.Field(..., description='')
    container_path: typing.Optional[str] = pydantic.Field(None, description='')
    mount_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefUlimitpropertyParams(pydantic.BaseModel):
    hard_limit: typing.Union[int, float] = pydantic.Field(..., description='')
    name: str = pydantic.Field(..., description='')
    soft_limit: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnTaskDefinitionDefVolumefrompropertyParams(pydantic.BaseModel):
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    source_container: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefVolumepropertyParams(pydantic.BaseModel):
    configured_at_launch: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    docker_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_DockerVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    efs_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_EFSVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    f_sx_windows_file_server_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_FSxWindowsFileServerVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    host: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_HostVolumePropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskDefinitionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnTaskDefinitionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnTaskDefinitionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnTaskDefinitionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnTaskDefinitionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnTaskDefinitionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnTaskDefinitionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnTaskDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnTaskDefinitionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnTaskDefinitionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnTaskDefinitionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnTaskDefinitionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnTaskDefinitionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnTaskDefinitionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CfnTaskSet
class CfnTaskSetDef(BaseCfnResource):
    cluster: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the cluster that hosts the service to create the task set in.\n')
    service: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the service to create the task set in.\n')
    task_definition: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The task definition for the tasks in the task set to use. If a revision isn't specified, the latest ``ACTIVE`` revision is used.\n")
    external_id: typing.Optional[str] = pydantic.Field(None, description='An optional non-unique tag that identifies this task set in external systems. If the task set is associated with a service discovery registry, the tasks in this task set will have the ``ECS_TASK_SET_EXTERNAL_ID`` AWS Cloud Map attribute set to the provided value.\n')
    launch_type: typing.Optional[str] = pydantic.Field(None, description='The launch type that new tasks in the task set uses. For more information, see `Amazon ECS launch types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If a ``launchType`` is specified, the ``capacityProviderStrategy`` parameter must be omitted.\n')
    load_balancers: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_LoadBalancerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A load balancer object representing the load balancer to use with the task set. The supported load balancer types are either an Application Load Balancer or a Network Load Balancer.\n')
    network_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The network configuration for the task set.\n')
    platform_version: typing.Optional[str] = pydantic.Field(None, description="The platform version that the tasks in the task set uses. A platform version is specified only for tasks using the Fargate launch type. If one isn't specified, the ``LATEST`` platform version is used.\n")
    scale: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_ScalePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A floating-point percentage of your desired number of tasks to place and keep running in the task set.\n')
    service_registries: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_ServiceRegistryPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The details of the service discovery registries to assign to this task set. For more information, see `Service discovery <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the task set to help you categorize and organize them. Each tag consists of a key and an optional value. You define both. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'service', 'task_definition', 'external_id', 'launch_type', 'load_balancers', 'network_configuration', 'platform_version', 'scale', 'service_registries', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['AwsVpcConfigurationProperty', 'LoadBalancerProperty', 'NetworkConfigurationProperty', 'ScaleProperty', 'ServiceRegistryProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskSet'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_ecs.CfnTaskSetDefConfig] = pydantic.Field(None)


class CfnTaskSetDefConfig(pydantic.BaseModel):
    AwsVpcConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAwsvpcconfigurationpropertyParams]] = pydantic.Field(None, description='')
    LoadBalancerProperty: typing.Optional[list[models.aws_ecs.CfnTaskSetDefLoadbalancerpropertyParams]] = pydantic.Field(None, description='')
    NetworkConfigurationProperty: typing.Optional[list[models.aws_ecs.CfnTaskSetDefNetworkconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ScaleProperty: typing.Optional[list[models.aws_ecs.CfnTaskSetDefScalepropertyParams]] = pydantic.Field(None, description='')
    ServiceRegistryProperty: typing.Optional[list[models.aws_ecs.CfnTaskSetDefServiceregistrypropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_ecs.CfnTaskSetDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_ecs.CfnTaskSetDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_ecs.CfnTaskSetDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_ecs.CfnTaskSetDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_ecs.CfnTaskSetDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_ecs.CfnTaskSetDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_ecs.CfnTaskSetDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnTaskSetDefAwsvpcconfigurationpropertyParams(pydantic.BaseModel):
    subnets: typing.Sequence[str] = pydantic.Field(..., description='')
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description='')
    security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnTaskSetDefLoadbalancerpropertyParams(pydantic.BaseModel):
    container_name: typing.Optional[str] = pydantic.Field(None, description='')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    target_group_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskSetDefNetworkconfigurationpropertyParams(pydantic.BaseModel):
    aws_vpc_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_AwsVpcConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnTaskSetDefScalepropertyParams(pydantic.BaseModel):
    unit: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnTaskSetDefServiceregistrypropertyParams(pydantic.BaseModel):
    container_name: typing.Optional[str] = pydantic.Field(None, description='')
    container_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    registry_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTaskSetDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnTaskSetDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnTaskSetDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnTaskSetDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnTaskSetDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnTaskSetDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnTaskSetDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnTaskSetDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnTaskSetDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnTaskSetDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnTaskSetDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnTaskSetDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnTaskSetDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnTaskSetDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_ecs.CfnCapacityProviderProps
class CfnCapacityProviderPropsDef(BaseCfnProperty):
    auto_scaling_group_provider: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_ecs.CfnCapacityProvider_AutoScalingGroupProviderPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Auto Scaling group settings for the capacity provider.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the capacity provider. If a name is specified, it cannot start with ``aws`` , ``ecs`` , or ``fargate`` . If no name is specified, a default name in the ``CFNStackName-CFNResourceName-RandomString`` format is used.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the capacity provider to help you categorize and organize it. Each tag consists of a key and an optional value. You define both. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-capacityprovider.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cfn_capacity_provider_props = ecs.CfnCapacityProviderProps(\n        auto_scaling_group_provider=ecs.CfnCapacityProvider.AutoScalingGroupProviderProperty(\n            auto_scaling_group_arn="autoScalingGroupArn",\n\n            # the properties below are optional\n            managed_draining="managedDraining",\n            managed_scaling=ecs.CfnCapacityProvider.ManagedScalingProperty(\n                instance_warmup_period=123,\n                maximum_scaling_step_size=123,\n                minimum_scaling_step_size=123,\n                status="status",\n                target_capacity=123\n            ),\n            managed_termination_protection="managedTerminationProtection"\n        ),\n\n        # the properties below are optional\n        name="name",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_scaling_group_provider', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnCapacityProviderProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnClusterCapacityProviderAssociationsProps
class CfnClusterCapacityProviderAssociationsPropsDef(BaseCfnProperty):
    capacity_providers: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The capacity providers to associate with the cluster.\n')
    cluster: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cluster the capacity provider association is the target of.\n')
    default_capacity_provider_strategy: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnClusterCapacityProviderAssociations_CapacityProviderStrategyPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The default capacity provider strategy to associate with the cluster.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-clustercapacityproviderassociations.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cfn_cluster_capacity_provider_associations_props = ecs.CfnClusterCapacityProviderAssociationsProps(\n        capacity_providers=["capacityProviders"],\n        cluster="cluster",\n        default_capacity_provider_strategy=[ecs.CfnClusterCapacityProviderAssociations.CapacityProviderStrategyProperty(\n            capacity_provider="capacityProvider",\n\n            # the properties below are optional\n            base=123,\n            weight=123\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capacity_providers', 'cluster', 'default_capacity_provider_strategy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnClusterCapacityProviderAssociationsProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnClusterProps
class CfnClusterPropsDef(BaseCfnProperty):
    capacity_providers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The short name of one or more capacity providers to associate with the cluster. A capacity provider must be associated with a cluster before it can be included as part of the default capacity provider strategy of the cluster or used in a capacity provider strategy when calling the `CreateService <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html>`_ or `RunTask <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html>`_ actions. If specifying a capacity provider that uses an Auto Scaling group, the capacity provider must be created but not associated with another cluster. New Auto Scaling group capacity providers can be created with the `CreateCapacityProvider <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateCapacityProvider.html>`_ API operation. To use a AWS Fargate capacity provider, specify either the ``FARGATE`` or ``FARGATE_SPOT`` capacity providers. The AWS Fargate capacity providers are available to all accounts and only need to be associated with a cluster to be used. The `PutCapacityProvider <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PutCapacityProvider.html>`_ API operation is used to update the list of available capacity providers for a cluster after the cluster is created.\n')
    cluster_name: typing.Optional[str] = pydantic.Field(None, description="A user-generated string that you use to identify your cluster. If you don't specify a name, AWS CloudFormation generates a unique physical ID for the name.\n")
    cluster_settings: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ClusterSettingsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The settings to use when creating a cluster. This parameter is used to turn on CloudWatch Container Insights for a cluster.\n')
    configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ClusterConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The execute command and managed storage configuration for the cluster.\n')
    default_capacity_provider_strategy: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_CapacityProviderStrategyItemPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The default capacity provider strategy for the cluster. When services or tasks are run in the cluster with no launch type or capacity provider strategy specified, the default capacity provider strategy is used.\n')
    service_connect_defaults: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnCluster_ServiceConnectDefaultsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Use this parameter to set a default Service Connect namespace. After you set a default Service Connect namespace, any new services with Service Connect turned on that are created in the cluster are added as client services in the namespace. This setting only applies to new services that set the ``enabled`` parameter to ``true`` in the ``ServiceConnectConfiguration`` . You can set the namespace of each service individually in the ``ServiceConnectConfiguration`` to override this default parameter. Tasks that run in a namespace can use short names to connect to services in the namespace. Tasks can connect to services across all of the clusters in the namespace. Tasks connect through a managed proxy container that collects logs and metrics for increased visibility. Only the tasks that Amazon ECS services create are supported with Service Connect. For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the cluster to help you categorize and organize them. Each tag consists of a key and an optional value. You define both. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-cluster.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cfn_cluster_props = ecs.CfnClusterProps(\n        capacity_providers=["capacityProviders"],\n        cluster_name="clusterName",\n        cluster_settings=[ecs.CfnCluster.ClusterSettingsProperty(\n            name="name",\n            value="value"\n        )],\n        configuration=ecs.CfnCluster.ClusterConfigurationProperty(\n            execute_command_configuration=ecs.CfnCluster.ExecuteCommandConfigurationProperty(\n                kms_key_id="kmsKeyId",\n                log_configuration=ecs.CfnCluster.ExecuteCommandLogConfigurationProperty(\n                    cloud_watch_encryption_enabled=False,\n                    cloud_watch_log_group_name="cloudWatchLogGroupName",\n                    s3_bucket_name="s3BucketName",\n                    s3_encryption_enabled=False,\n                    s3_key_prefix="s3KeyPrefix"\n                ),\n                logging="logging"\n            ),\n            managed_storage_configuration=ecs.CfnCluster.ManagedStorageConfigurationProperty(\n                fargate_ephemeral_storage_kms_key_id="fargateEphemeralStorageKmsKeyId",\n                kms_key_id="kmsKeyId"\n            )\n        ),\n        default_capacity_provider_strategy=[ecs.CfnCluster.CapacityProviderStrategyItemProperty(\n            base=123,\n            capacity_provider="capacityProvider",\n            weight=123\n        )],\n        service_connect_defaults=ecs.CfnCluster.ServiceConnectDefaultsProperty(\n            namespace="namespace"\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capacity_providers', 'cluster_name', 'cluster_settings', 'configuration', 'default_capacity_provider_strategy', 'service_connect_defaults', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnPrimaryTaskSetProps
class CfnPrimaryTaskSetPropsDef(BaseCfnProperty):
    cluster: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the cluster that hosts the service that the task set exists in.\n')
    service: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the service that the task set exists in.\n')
    task_set_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the task set to set as the primary task set in the deployment.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-primarytaskset.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cfn_primary_task_set_props = ecs.CfnPrimaryTaskSetProps(\n        cluster="cluster",\n        service="service",\n        task_set_id="taskSetId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'service', 'task_set_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnPrimaryTaskSetProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnServiceProps
class CfnServicePropsDef(BaseCfnProperty):
    capacity_provider_strategy: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_CapacityProviderStrategyItemPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The capacity provider strategy to use for the service. If a ``capacityProviderStrategy`` is specified, the ``launchType`` parameter must be omitted. If no ``capacityProviderStrategy`` or ``launchType`` is specified, the ``defaultCapacityProviderStrategy`` for the cluster is used. A capacity provider strategy may contain a maximum of 6 capacity providers.\n')
    cluster: typing.Optional[str] = pydantic.Field(None, description='The short name or full Amazon Resource Name (ARN) of the cluster that you run your service on. If you do not specify a cluster, the default cluster is assumed.\n')
    deployment_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Optional deployment parameters that control how many tasks run during the deployment and the ordering of stopping and starting tasks.\n')
    deployment_controller: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_DeploymentControllerPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The deployment controller to use for the service. If no deployment controller is specified, the default value of ``ECS`` is used.\n')
    desired_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of instantiations of the specified task definition to place and keep running in your service. For new services, if a desired count is not specified, a default value of ``1`` is used. When using the ``DAEMON`` scheduling strategy, the desired count is not required. For existing services, if a desired count is not specified, it is omitted from the operation.\n')
    enable_ecs_managed_tags: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether to turn on Amazon ECS managed tags for the tasks within the service. For more information, see `Tagging your Amazon ECS resources <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html>`_ in the *Amazon Elastic Container Service Developer Guide* . When you use Amazon ECS managed tags, you need to set the ``propagateTags`` request parameter.\n')
    enable_execute_command: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Determines whether the execute command functionality is turned on for the service. If ``true`` , the execute command functionality is turned on for all containers in tasks as part of the service.\n')
    health_check_grace_period_seconds: typing.Union[int, float, None] = pydantic.Field(None, description="The period of time, in seconds, that the Amazon ECS service scheduler ignores unhealthy Elastic Load Balancing target health checks after a task has first started. This is only used when your service is configured to use a load balancer. If your service has a load balancer defined and you don't specify a health check grace period value, the default value of ``0`` is used. If you do not use an Elastic Load Balancing, we recommend that you use the ``startPeriod`` in the task definition health check parameters. For more information, see `Health check <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_HealthCheck.html>`_ . If your service's tasks take a while to start and respond to Elastic Load Balancing health checks, you can specify a health check grace period of up to 2,147,483,647 seconds (about 69 years). During that time, the Amazon ECS service scheduler ignores health check status. This grace period can prevent the service scheduler from marking tasks as unhealthy and stopping them before they have time to come up.\n")
    launch_type: typing.Optional[str] = pydantic.Field(None, description='The launch type on which to run your service. For more information, see `Amazon ECS Launch Types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    load_balancers: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_LoadBalancerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of load balancer objects to associate with the service. If you specify the ``Role`` property, ``LoadBalancers`` must be specified as well. For information about the number of load balancers that you can specify per service, see `Service Load Balancing <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    network_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The network configuration for the service. This parameter is required for task definitions that use the ``awsvpc`` network mode to receive their own elastic network interface, and it is not supported for other network modes. For more information, see `Task Networking <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    placement_constraints: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_PlacementConstraintPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of placement constraint objects to use for tasks in your service. You can specify a maximum of 10 constraints for each task. This limit includes constraints in the task definition and those specified at runtime.\n')
    placement_strategies: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_PlacementStrategyPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The placement strategy objects to use for tasks in your service. You can specify a maximum of 5 strategy rules for each service.\n')
    platform_version: typing.Optional[str] = pydantic.Field(None, description='The platform version that your tasks in the service are running on. A platform version is specified only for tasks using the Fargate launch type. If one isn\'t specified, the ``LATEST`` platform version is used. For more information, see `AWS Fargate platform versions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html>`_ in the *Amazon Elastic Container Service Developer Guide* . Default: - "LATEST"\n')
    propagate_tags: typing.Optional[str] = pydantic.Field(None, description="Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags aren't propagated. Tags can only be propagated to the task during task creation. To add tags to a task after task creation, use the `TagResource <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_TagResource.html>`_ API action. You must set this to a value other than ``NONE`` when you use Cost Explorer. For more information, see `Amazon ECS usage reports <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/usage-reports.html>`_ in the *Amazon Elastic Container Service Developer Guide* . The default is ``NONE`` .\n")
    role: typing.Optional[str] = pydantic.Field(None, description="The name or full Amazon Resource Name (ARN) of the IAM role that allows Amazon ECS to make calls to your load balancer on your behalf. This parameter is only permitted if you are using a load balancer with your service and your task definition doesn't use the ``awsvpc`` network mode. If you specify the ``role`` parameter, you must also specify a load balancer object with the ``loadBalancers`` parameter. .. epigraph:: If your account has already created the Amazon ECS service-linked role, that role is used for your service unless you specify a role here. The service-linked role is required if your task definition uses the ``awsvpc`` network mode or if the service is configured to use service discovery, an external deployment controller, multiple target groups, or Elastic Inference accelerators in which case you don't specify a role here. For more information, see `Using service-linked roles for Amazon ECS <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using-service-linked-roles.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If your specified role has a path other than ``/`` , then you must either specify the full role ARN (this is recommended) or prefix the role name with the path. For example, if a role with the name ``bar`` has a path of ``/foo/`` then you would specify ``/foo/bar`` as the role name. For more information, see `Friendly names and paths <https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-friendly-names>`_ in the *IAM User Guide* .\n")
    scheduling_strategy: typing.Optional[str] = pydantic.Field(None, description="The scheduling strategy to use for the service. For more information, see `Services <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html>`_ . There are two service scheduler strategies available: - ``REPLICA`` -The replica scheduling strategy places and maintains the desired number of tasks across your cluster. By default, the service scheduler spreads tasks across Availability Zones. You can use task placement strategies and constraints to customize task placement decisions. This scheduler strategy is required if the service uses the ``CODE_DEPLOY`` or ``EXTERNAL`` deployment controller types. - ``DAEMON`` -The daemon scheduling strategy deploys exactly one task on each active container instance that meets all of the task placement constraints that you specify in your cluster. The service scheduler also evaluates the task placement constraints for running tasks and will stop tasks that don't meet the placement constraints. When you're using this strategy, you don't need to specify a desired number of tasks, a task placement strategy, or use Service Auto Scaling policies. .. epigraph:: Tasks using the Fargate launch type or the ``CODE_DEPLOY`` or ``EXTERNAL`` deployment controller types don't support the ``DAEMON`` scheduling strategy.\n")
    service_connect_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceConnectConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration for this service to discover and connect to services, and be discovered by, and connected from, other services within a namespace. Tasks that run in a namespace can use short names to connect to services in the namespace. Tasks can connect to services across all of the clusters in the namespace. Tasks connect through a managed proxy container that collects logs and metrics for increased visibility. Only the tasks that Amazon ECS services create are supported with Service Connect. For more information, see `Service Connect <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    service_name: typing.Optional[str] = pydantic.Field(None, description='The name of your service. Up to 255 letters (uppercase and lowercase), numbers, underscores, and hyphens are allowed. Service names must be unique within a cluster, but you can have similarly named services in multiple clusters within a Region or across multiple Regions. .. epigraph:: The stack update fails if you change any properties that require replacement and the ``ServiceName`` is configured. This is because AWS CloudFormation creates the replacement service first, but each ``ServiceName`` must be unique in the cluster.\n')
    service_registries: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceRegistryPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The details of the service discovery registry to associate with this service. For more information, see `Service discovery <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html>`_ . .. epigraph:: Each service may be associated with one service registry. Multiple service registries for each service isn't supported.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the service to help you categorize and organize them. Each tag consists of a key and an optional value, both of which you define. When a service is deleted, the tags are deleted as well. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /\n')
    task_definition: typing.Optional[str] = pydantic.Field(None, description="The ``family`` and ``revision`` ( ``family:revision`` ) or full ARN of the task definition to run in your service. If a ``revision`` isn't specified, the latest ``ACTIVE`` revision is used. A task definition must be specified if the service uses either the ``ECS`` or ``CODE_DEPLOY`` deployment controllers. For more information about deployment types, see `Amazon ECS deployment types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-types.html>`_ .\n")
    volume_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnService_ServiceVolumeConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The configuration for a volume specified in the task definition as a volume that is configured at launch time. Currently, the only supported volume type is an Amazon EBS volume.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-service.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cfn_service_props = ecs.CfnServiceProps(\n        capacity_provider_strategy=[ecs.CfnService.CapacityProviderStrategyItemProperty(\n            base=123,\n            capacity_provider="capacityProvider",\n            weight=123\n        )],\n        cluster="cluster",\n        deployment_configuration=ecs.CfnService.DeploymentConfigurationProperty(\n            alarms=ecs.CfnService.DeploymentAlarmsProperty(\n                alarm_names=["alarmNames"],\n                enable=False,\n                rollback=False\n            ),\n            deployment_circuit_breaker=ecs.CfnService.DeploymentCircuitBreakerProperty(\n                enable=False,\n                rollback=False\n            ),\n            maximum_percent=123,\n            minimum_healthy_percent=123\n        ),\n        deployment_controller=ecs.CfnService.DeploymentControllerProperty(\n            type="type"\n        ),\n        desired_count=123,\n        enable_ecs_managed_tags=False,\n        enable_execute_command=False,\n        health_check_grace_period_seconds=123,\n        launch_type="launchType",\n        load_balancers=[ecs.CfnService.LoadBalancerProperty(\n            container_name="containerName",\n            container_port=123,\n            load_balancer_name="loadBalancerName",\n            target_group_arn="targetGroupArn"\n        )],\n        network_configuration=ecs.CfnService.NetworkConfigurationProperty(\n            awsvpc_configuration=ecs.CfnService.AwsVpcConfigurationProperty(\n                assign_public_ip="assignPublicIp",\n                security_groups=["securityGroups"],\n                subnets=["subnets"]\n            )\n        ),\n        placement_constraints=[ecs.CfnService.PlacementConstraintProperty(\n            type="type",\n\n            # the properties below are optional\n            expression="expression"\n        )],\n        placement_strategies=[ecs.CfnService.PlacementStrategyProperty(\n            type="type",\n\n            # the properties below are optional\n            field="field"\n        )],\n        platform_version="platformVersion",\n        propagate_tags="propagateTags",\n        role="role",\n        scheduling_strategy="schedulingStrategy",\n        service_connect_configuration=ecs.CfnService.ServiceConnectConfigurationProperty(\n            enabled=False,\n\n            # the properties below are optional\n            log_configuration=ecs.CfnService.LogConfigurationProperty(\n                log_driver="logDriver",\n                options={\n                    "options_key": "options"\n                },\n                secret_options=[ecs.CfnService.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )]\n            ),\n            namespace="namespace",\n            services=[ecs.CfnService.ServiceConnectServiceProperty(\n                port_name="portName",\n\n                # the properties below are optional\n                client_aliases=[ecs.CfnService.ServiceConnectClientAliasProperty(\n                    port=123,\n\n                    # the properties below are optional\n                    dns_name="dnsName"\n                )],\n                discovery_name="discoveryName",\n                ingress_port_override=123,\n                timeout=ecs.CfnService.TimeoutConfigurationProperty(\n                    idle_timeout_seconds=123,\n                    per_request_timeout_seconds=123\n                ),\n                tls=ecs.CfnService.ServiceConnectTlsConfigurationProperty(\n                    issuer_certificate_authority=ecs.CfnService.ServiceConnectTlsCertificateAuthorityProperty(\n                        aws_pca_authority_arn="awsPcaAuthorityArn"\n                    ),\n\n                    # the properties below are optional\n                    kms_key="kmsKey",\n                    role_arn="roleArn"\n                )\n            )]\n        ),\n        service_name="serviceName",\n        service_registries=[ecs.CfnService.ServiceRegistryProperty(\n            container_name="containerName",\n            container_port=123,\n            port=123,\n            registry_arn="registryArn"\n        )],\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        task_definition="taskDefinition",\n        volume_configurations=[ecs.CfnService.ServiceVolumeConfigurationProperty(\n            name="name",\n\n            # the properties below are optional\n            managed_ebs_volume=ecs.CfnService.ServiceManagedEBSVolumeConfigurationProperty(\n                role_arn="roleArn",\n\n                # the properties below are optional\n                encrypted=False,\n                filesystem_type="filesystemType",\n                iops=123,\n                kms_key_id="kmsKeyId",\n                size_in_gi_b=123,\n                snapshot_id="snapshotId",\n                tag_specifications=[ecs.CfnService.EBSTagSpecificationProperty(\n                    resource_type="resourceType",\n\n                    # the properties below are optional\n                    propagate_tags="propagateTags",\n                    tags=[CfnTag(\n                        key="key",\n                        value="value"\n                    )]\n                )],\n                throughput=123,\n                volume_type="volumeType"\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['capacity_provider_strategy', 'cluster', 'deployment_configuration', 'deployment_controller', 'desired_count', 'enable_ecs_managed_tags', 'enable_execute_command', 'health_check_grace_period_seconds', 'launch_type', 'load_balancers', 'network_configuration', 'placement_constraints', 'placement_strategies', 'platform_version', 'propagate_tags', 'role', 'scheduling_strategy', 'service_connect_configuration', 'service_name', 'service_registries', 'tags', 'task_definition', 'volume_configurations']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnServiceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskDefinitionProps
class CfnTaskDefinitionPropsDef(BaseCfnProperty):
    container_definitions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ContainerDefinitionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of container definitions in JSON format that describe the different containers that make up your task. For more information about container definition parameters and defaults, see `Amazon ECS Task Definitions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_defintions.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    cpu: typing.Optional[str] = pydantic.Field(None, description='The number of ``cpu`` units used by the task. If you use the EC2 launch type, this field is optional. Any value can be used. If you use the Fargate launch type, this field is required. You must use one of the following values. The value that you choose determines your range of valid values for the ``memory`` parameter. The CPU units cannot be less than 1 vCPU when you use Windows containers on Fargate. - 256 (.25 vCPU) - Available ``memory`` values: 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - 512 (.5 vCPU) - Available ``memory`` values: 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - 1024 (1 vCPU) - Available ``memory`` values: 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - 2048 (2 vCPU) - Available ``memory`` values: 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - 4096 (4 vCPU) - Available ``memory`` values: 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - 8192 (8 vCPU) - Available ``memory`` values: 16 GB and 60 GB in 4 GB increments This option requires Linux platform ``1.4.0`` or later. - 16384 (16vCPU) - Available ``memory`` values: 32GB and 120 GB in 8 GB increments This option requires Linux platform ``1.4.0`` or later.\n')
    ephemeral_storage: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ephemeral storage settings to use for tasks run with the task definition.\n')
    execution_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the task execution role that grants the Amazon ECS container agent permission to make AWS API calls on your behalf. For informationabout the required IAM roles for Amazon ECS, see `IAM roles for Amazon ECS <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/security-ecs-iam-role-overview.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    family: typing.Optional[str] = pydantic.Field(None, description="The name of a family that this task definition is registered to. Up to 255 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed. A family groups multiple versions of a task definition. Amazon ECS gives the first task definition that you registered to a family a revision number of 1. Amazon ECS gives sequential revision numbers to each task definition that you add. .. epigraph:: To use revision numbers when you update a task definition, specify this property. If you don't specify a value, AWS CloudFormation generates a new task definition each time that you update it.\n")
    inference_accelerators: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_InferenceAcceleratorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The Elastic Inference accelerators to use for the containers in the task.\n')
    ipc_mode: typing.Optional[str] = pydantic.Field(None, description='The IPC resource namespace to use for the containers in the task. The valid values are ``host`` , ``task`` , or ``none`` . If ``host`` is specified, then all containers within the tasks that specified the ``host`` IPC mode on the same container instance share the same IPC resources with the host Amazon EC2 instance. If ``task`` is specified, all containers within the specified task share the same IPC resources. If ``none`` is specified, then IPC resources within the containers of a task are private and not shared with other containers in a task or on the container instance. If no value is specified, then the IPC resource namespace sharing depends on the Docker daemon setting on the container instance. For more information, see `IPC settings <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#ipc-settings---ipc>`_ in the *Docker run reference* . If the ``host`` IPC mode is used, be aware that there is a heightened risk of undesired IPC namespace expose. For more information, see `Docker security <https://docs.aws.amazon.com/https://docs.docker.com/engine/security/security/>`_ . If you are setting namespaced kernel parameters using ``systemControls`` for the containers in the task, the following will apply to your IPC resource namespace. For more information, see `System Controls <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html>`_ in the *Amazon Elastic Container Service Developer Guide* . - For tasks that use the ``host`` IPC mode, IPC namespace related ``systemControls`` are not supported. - For tasks that use the ``task`` IPC mode, IPC namespace related ``systemControls`` will apply to all containers within a task. .. epigraph:: This parameter is not supported for Windows containers or tasks run on AWS Fargate .\n')
    memory: typing.Optional[str] = pydantic.Field(None, description='The amount (in MiB) of memory used by the task. If your tasks runs on Amazon EC2 instances, you must specify either a task-level memory value or a container-level memory value. This field is optional and any value can be used. If a task-level memory value is specified, the container-level memory value is optional. For more information regarding container-level memory and memory reservation, see `ContainerDefinition <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ContainerDefinition.html>`_ . If your tasks runs on AWS Fargate , this field is required. You must use one of the following values. The value you choose determines your range of valid values for the ``cpu`` parameter. - 512 (0.5 GB), 1024 (1 GB), 2048 (2 GB) - Available ``cpu`` values: 256 (.25 vCPU) - 1024 (1 GB), 2048 (2 GB), 3072 (3 GB), 4096 (4 GB) - Available ``cpu`` values: 512 (.5 vCPU) - 2048 (2 GB), 3072 (3 GB), 4096 (4 GB), 5120 (5 GB), 6144 (6 GB), 7168 (7 GB), 8192 (8 GB) - Available ``cpu`` values: 1024 (1 vCPU) - Between 4096 (4 GB) and 16384 (16 GB) in increments of 1024 (1 GB) - Available ``cpu`` values: 2048 (2 vCPU) - Between 8192 (8 GB) and 30720 (30 GB) in increments of 1024 (1 GB) - Available ``cpu`` values: 4096 (4 vCPU) - Between 16 GB and 60 GB in 4 GB increments - Available ``cpu`` values: 8192 (8 vCPU) This option requires Linux platform ``1.4.0`` or later. - Between 32GB and 120 GB in 8 GB increments - Available ``cpu`` values: 16384 (16 vCPU) This option requires Linux platform ``1.4.0`` or later.\n')
    network_mode: typing.Optional[str] = pydantic.Field(None, description='The Docker networking mode to use for the containers in the task. The valid values are ``none`` , ``bridge`` , ``awsvpc`` , and ``host`` . If no network mode is specified, the default is ``bridge`` . For Amazon ECS tasks on Fargate, the ``awsvpc`` network mode is required. For Amazon ECS tasks on Amazon EC2 Linux instances, any network mode can be used. For Amazon ECS tasks on Amazon EC2 Windows instances, ``<default>`` or ``awsvpc`` can be used. If the network mode is set to ``none`` , you cannot specify port mappings in your container definitions, and the tasks containers do not have external connectivity. The ``host`` and ``awsvpc`` network modes offer the highest networking performance for containers because they use the EC2 network stack instead of the virtualized network stack provided by the ``bridge`` mode. With the ``host`` and ``awsvpc`` network modes, exposed container ports are mapped directly to the corresponding host port (for the ``host`` network mode) or the attached elastic network interface port (for the ``awsvpc`` network mode), so you cannot take advantage of dynamic host port mappings. .. epigraph:: When using the ``host`` network mode, you should not run containers using the root user (UID 0). It is considered best practice to use a non-root user. If the network mode is ``awsvpc`` , the task is allocated an elastic network interface, and you must specify a ``NetworkConfiguration`` value when you create a service or run a task with the task definition. For more information, see `Task Networking <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If the network mode is ``host`` , you cannot run multiple instantiations of the same task on a single container instance when port mappings are used. For more information, see `Network settings <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#network-settings>`_ in the *Docker run reference* .\n')
    pid_mode: typing.Optional[str] = pydantic.Field(None, description="The process namespace to use for the containers in the task. The valid values are ``host`` or ``task`` . On Fargate for Linux containers, the only valid value is ``task`` . For example, monitoring sidecars might need ``pidMode`` to access information about other containers running in the same task. If ``host`` is specified, all containers within the tasks that specified the ``host`` PID mode on the same container instance share the same process namespace with the host Amazon EC2 instance. If ``task`` is specified, all containers within the specified task share the same process namespace. If no value is specified, the default is a private namespace for each container. For more information, see `PID settings <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/#pid-settings---pid>`_ in the *Docker run reference* . If the ``host`` PID mode is used, there's a heightened risk of undesired process namespace exposure. For more information, see `Docker security <https://docs.aws.amazon.com/https://docs.docker.com/engine/security/security/>`_ . .. epigraph:: This parameter is not supported for Windows containers. > This parameter is only supported for tasks that are hosted on AWS Fargate if the tasks are using platform version ``1.4.0`` or later (Linux). This isn't supported for Windows containers on Fargate.\n")
    placement_constraints: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_TaskDefinitionPlacementConstraintPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="An array of placement constraint objects to use for tasks. .. epigraph:: This parameter isn't supported for tasks run on AWS Fargate .\n")
    proxy_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_ProxyConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The configuration details for the App Mesh proxy. Your Amazon ECS container instances require at least version 1.26.0 of the container agent and at least version 1.26.0-1 of the ``ecs-init`` package to use a proxy configuration. If your container instances are launched from the Amazon ECS optimized AMI version ``20190301`` or later, they contain the required versions of the container agent and ``ecs-init`` . For more information, see `Amazon ECS-optimized Linux AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    requires_compatibilities: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The task launch types the task definition was validated against. The valid values are ``EC2`` , ``FARGATE`` , and ``EXTERNAL`` . For more information, see `Amazon ECS launch types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    runtime_platform: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_RuntimePlatformPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The operating system that your tasks definitions run on. A platform family is specified only for tasks using the Fargate launch type.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the task definition to help you categorize and organize them. Each tag consists of a key and an optional value. You define both of them. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /\n')
    task_role_arn: typing.Optional[str] = pydantic.Field(None, description='The short name or full Amazon Resource Name (ARN) of the AWS Identity and Access Management role that grants containers in the task permission to call AWS APIs on your behalf. For informationabout the required IAM roles for Amazon ECS, see `IAM roles for Amazon ECS <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/security-ecs-iam-role-overview.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    volumes: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskDefinition_VolumePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The list of data volume definitions for the task. For more information, see `Using data volumes in tasks <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html>`_ in the *Amazon Elastic Container Service Developer Guide* . .. epigraph:: The ``host`` and ``sourcePath`` parameters aren\'t supported for tasks run on AWS Fargate .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-taskdefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cfn_task_definition_props = ecs.CfnTaskDefinitionProps(\n        container_definitions=[ecs.CfnTaskDefinition.ContainerDefinitionProperty(\n            image="image",\n            name="name",\n\n            # the properties below are optional\n            command=["command"],\n            cpu=123,\n            credential_specs=["credentialSpecs"],\n            depends_on=[ecs.CfnTaskDefinition.ContainerDependencyProperty(\n                condition="condition",\n                container_name="containerName"\n            )],\n            disable_networking=False,\n            dns_search_domains=["dnsSearchDomains"],\n            dns_servers=["dnsServers"],\n            docker_labels={\n                "docker_labels_key": "dockerLabels"\n            },\n            docker_security_options=["dockerSecurityOptions"],\n            entry_point=["entryPoint"],\n            environment=[ecs.CfnTaskDefinition.KeyValuePairProperty(\n                name="name",\n                value="value"\n            )],\n            environment_files=[ecs.CfnTaskDefinition.EnvironmentFileProperty(\n                type="type",\n                value="value"\n            )],\n            essential=False,\n            extra_hosts=[ecs.CfnTaskDefinition.HostEntryProperty(\n                hostname="hostname",\n                ip_address="ipAddress"\n            )],\n            firelens_configuration=ecs.CfnTaskDefinition.FirelensConfigurationProperty(\n                options={\n                    "options_key": "options"\n                },\n                type="type"\n            ),\n            health_check=ecs.CfnTaskDefinition.HealthCheckProperty(\n                command=["command"],\n                interval=123,\n                retries=123,\n                start_period=123,\n                timeout=123\n            ),\n            hostname="hostname",\n            interactive=False,\n            links=["links"],\n            linux_parameters=ecs.CfnTaskDefinition.LinuxParametersProperty(\n                capabilities=ecs.CfnTaskDefinition.KernelCapabilitiesProperty(\n                    add=["add"],\n                    drop=["drop"]\n                ),\n                devices=[ecs.CfnTaskDefinition.DeviceProperty(\n                    container_path="containerPath",\n                    host_path="hostPath",\n                    permissions=["permissions"]\n                )],\n                init_process_enabled=False,\n                max_swap=123,\n                shared_memory_size=123,\n                swappiness=123,\n                tmpfs=[ecs.CfnTaskDefinition.TmpfsProperty(\n                    size=123,\n\n                    # the properties below are optional\n                    container_path="containerPath",\n                    mount_options=["mountOptions"]\n                )]\n            ),\n            log_configuration=ecs.CfnTaskDefinition.LogConfigurationProperty(\n                log_driver="logDriver",\n\n                # the properties below are optional\n                options={\n                    "options_key": "options"\n                },\n                secret_options=[ecs.CfnTaskDefinition.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )]\n            ),\n            memory=123,\n            memory_reservation=123,\n            mount_points=[ecs.CfnTaskDefinition.MountPointProperty(\n                container_path="containerPath",\n                read_only=False,\n                source_volume="sourceVolume"\n            )],\n            port_mappings=[ecs.CfnTaskDefinition.PortMappingProperty(\n                app_protocol="appProtocol",\n                container_port=123,\n                container_port_range="containerPortRange",\n                host_port=123,\n                name="name",\n                protocol="protocol"\n            )],\n            privileged=False,\n            pseudo_terminal=False,\n            readonly_root_filesystem=False,\n            repository_credentials=ecs.CfnTaskDefinition.RepositoryCredentialsProperty(\n                credentials_parameter="credentialsParameter"\n            ),\n            resource_requirements=[ecs.CfnTaskDefinition.ResourceRequirementProperty(\n                type="type",\n                value="value"\n            )],\n            secrets=[ecs.CfnTaskDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )],\n            start_timeout=123,\n            stop_timeout=123,\n            system_controls=[ecs.CfnTaskDefinition.SystemControlProperty(\n                namespace="namespace",\n                value="value"\n            )],\n            ulimits=[ecs.CfnTaskDefinition.UlimitProperty(\n                hard_limit=123,\n                name="name",\n                soft_limit=123\n            )],\n            user="user",\n            volumes_from=[ecs.CfnTaskDefinition.VolumeFromProperty(\n                read_only=False,\n                source_container="sourceContainer"\n            )],\n            working_directory="workingDirectory"\n        )],\n        cpu="cpu",\n        ephemeral_storage=ecs.CfnTaskDefinition.EphemeralStorageProperty(\n            size_in_gi_b=123\n        ),\n        execution_role_arn="executionRoleArn",\n        family="family",\n        inference_accelerators=[ecs.CfnTaskDefinition.InferenceAcceleratorProperty(\n            device_name="deviceName",\n            device_type="deviceType"\n        )],\n        ipc_mode="ipcMode",\n        memory="memory",\n        network_mode="networkMode",\n        pid_mode="pidMode",\n        placement_constraints=[ecs.CfnTaskDefinition.TaskDefinitionPlacementConstraintProperty(\n            type="type",\n\n            # the properties below are optional\n            expression="expression"\n        )],\n        proxy_configuration=ecs.CfnTaskDefinition.ProxyConfigurationProperty(\n            container_name="containerName",\n\n            # the properties below are optional\n            proxy_configuration_properties=[ecs.CfnTaskDefinition.KeyValuePairProperty(\n                name="name",\n                value="value"\n            )],\n            type="type"\n        ),\n        requires_compatibilities=["requiresCompatibilities"],\n        runtime_platform=ecs.CfnTaskDefinition.RuntimePlatformProperty(\n            cpu_architecture="cpuArchitecture",\n            operating_system_family="operatingSystemFamily"\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        task_role_arn="taskRoleArn",\n        volumes=[ecs.CfnTaskDefinition.VolumeProperty(\n            configured_at_launch=False,\n            docker_volume_configuration=ecs.CfnTaskDefinition.DockerVolumeConfigurationProperty(\n                autoprovision=False,\n                driver="driver",\n                driver_opts={\n                    "driver_opts_key": "driverOpts"\n                },\n                labels={\n                    "labels_key": "labels"\n                },\n                scope="scope"\n            ),\n            efs_volume_configuration=ecs.CfnTaskDefinition.EFSVolumeConfigurationProperty(\n                filesystem_id="filesystemId",\n\n                # the properties below are optional\n                authorization_config=ecs.CfnTaskDefinition.AuthorizationConfigProperty(\n                    access_point_id="accessPointId",\n                    iam="iam"\n                ),\n                root_directory="rootDirectory",\n                transit_encryption="transitEncryption",\n                transit_encryption_port=123\n            ),\n            f_sx_windows_file_server_volume_configuration=ecs.CfnTaskDefinition.FSxWindowsFileServerVolumeConfigurationProperty(\n                file_system_id="fileSystemId",\n                root_directory="rootDirectory",\n\n                # the properties below are optional\n                authorization_config=ecs.CfnTaskDefinition.FSxAuthorizationConfigProperty(\n                    credentials_parameter="credentialsParameter",\n                    domain="domain"\n                )\n            ),\n            host=ecs.CfnTaskDefinition.HostVolumePropertiesProperty(\n                source_path="sourcePath"\n            ),\n            name="name"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_definitions', 'cpu', 'ephemeral_storage', 'execution_role_arn', 'family', 'inference_accelerators', 'ipc_mode', 'memory', 'network_mode', 'pid_mode', 'placement_constraints', 'proxy_configuration', 'requires_compatibilities', 'runtime_platform', 'tags', 'task_role_arn', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_ecs.CfnTaskSetProps
class CfnTaskSetPropsDef(BaseCfnProperty):
    cluster: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the cluster that hosts the service to create the task set in.\n')
    service: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The short name or full Amazon Resource Name (ARN) of the service to create the task set in.\n')
    task_definition: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The task definition for the tasks in the task set to use. If a revision isn't specified, the latest ``ACTIVE`` revision is used.\n")
    external_id: typing.Optional[str] = pydantic.Field(None, description='An optional non-unique tag that identifies this task set in external systems. If the task set is associated with a service discovery registry, the tasks in this task set will have the ``ECS_TASK_SET_EXTERNAL_ID`` AWS Cloud Map attribute set to the provided value.\n')
    launch_type: typing.Optional[str] = pydantic.Field(None, description='The launch type that new tasks in the task set uses. For more information, see `Amazon ECS launch types <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html>`_ in the *Amazon Elastic Container Service Developer Guide* . If a ``launchType`` is specified, the ``capacityProviderStrategy`` parameter must be omitted.\n')
    load_balancers: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_LoadBalancerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A load balancer object representing the load balancer to use with the task set. The supported load balancer types are either an Application Load Balancer or a Network Load Balancer.\n')
    network_configuration: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The network configuration for the task set.\n')
    platform_version: typing.Optional[str] = pydantic.Field(None, description="The platform version that the tasks in the task set uses. A platform version is specified only for tasks using the Fargate launch type. If one isn't specified, the ``LATEST`` platform version is used.\n")
    scale: typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_ScalePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A floating-point percentage of your desired number of tasks to place and keep running in the task set.\n')
    service_registries: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_ecs.CfnTaskSet_ServiceRegistryPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The details of the service discovery registries to assign to this task set. For more information, see `Service discovery <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the task set to help you categorize and organize them. Each tag consists of a key and an optional value. You define both. The following basic restrictions apply to tags: - Maximum number of tags per resource - 50 - For each resource, each tag key must be unique, and each tag key can have only one value. - Maximum key length - 128 Unicode characters in UTF-8 - Maximum value length - 256 Unicode characters in UTF-8 - If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-taskset.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ecs as ecs\n\n    cfn_task_set_props = ecs.CfnTaskSetProps(\n        cluster="cluster",\n        service="service",\n        task_definition="taskDefinition",\n\n        # the properties below are optional\n        external_id="externalId",\n        launch_type="launchType",\n        load_balancers=[ecs.CfnTaskSet.LoadBalancerProperty(\n            container_name="containerName",\n            container_port=123,\n            target_group_arn="targetGroupArn"\n        )],\n        network_configuration=ecs.CfnTaskSet.NetworkConfigurationProperty(\n            aws_vpc_configuration=ecs.CfnTaskSet.AwsVpcConfigurationProperty(\n                subnets=["subnets"],\n\n                # the properties below are optional\n                assign_public_ip="assignPublicIp",\n                security_groups=["securityGroups"]\n            )\n        ),\n        platform_version="platformVersion",\n        scale=ecs.CfnTaskSet.ScaleProperty(\n            unit="unit",\n            value=123\n        ),\n        service_registries=[ecs.CfnTaskSet.ServiceRegistryProperty(\n            container_name="containerName",\n            container_port=123,\n            port=123,\n            registry_arn="registryArn"\n        )],\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'service', 'task_definition', 'external_id', 'launch_type', 'load_balancers', 'network_configuration', 'platform_version', 'scale', 'service_registries', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_ecs.CfnTaskSetProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    AppMeshProxyConfiguration: typing.Optional[dict[str, models.aws_ecs.AppMeshProxyConfigurationDef]] = pydantic.Field(None)
    AppProtocol: typing.Optional[dict[str, models.aws_ecs.AppProtocolDef]] = pydantic.Field(None)
    AssetEnvironmentFile: typing.Optional[dict[str, models.aws_ecs.AssetEnvironmentFileDef]] = pydantic.Field(None)
    AssetImage: typing.Optional[dict[str, models.aws_ecs.AssetImageDef]] = pydantic.Field(None)
    AwsLogDriver: typing.Optional[dict[str, models.aws_ecs.AwsLogDriverDef]] = pydantic.Field(None)
    BaseService: typing.Optional[dict[str, models.aws_ecs.BaseServiceDef]] = pydantic.Field(None)
    BottleRocketImage: typing.Optional[dict[str, models.aws_ecs.BottleRocketImageDef]] = pydantic.Field(None)
    BuiltInAttributes: typing.Optional[dict[str, models.aws_ecs.BuiltInAttributesDef]] = pydantic.Field(None)
    ContainerImage: typing.Optional[dict[str, models.aws_ecs.ContainerImageDef]] = pydantic.Field(None)
    CpuArchitecture: typing.Optional[dict[str, models.aws_ecs.CpuArchitectureDef]] = pydantic.Field(None)
    CredentialSpec: typing.Optional[dict[str, models.aws_ecs.CredentialSpecDef]] = pydantic.Field(None)
    DomainJoinedCredentialSpec: typing.Optional[dict[str, models.aws_ecs.DomainJoinedCredentialSpecDef]] = pydantic.Field(None)
    DomainlessCredentialSpec: typing.Optional[dict[str, models.aws_ecs.DomainlessCredentialSpecDef]] = pydantic.Field(None)
    EcrImage: typing.Optional[dict[str, models.aws_ecs.EcrImageDef]] = pydantic.Field(None)
    EcsOptimizedImage: typing.Optional[dict[str, models.aws_ecs.EcsOptimizedImageDef]] = pydantic.Field(None)
    EnvironmentFile: typing.Optional[dict[str, models.aws_ecs.EnvironmentFileDef]] = pydantic.Field(None)
    FireLensLogDriver: typing.Optional[dict[str, models.aws_ecs.FireLensLogDriverDef]] = pydantic.Field(None)
    FluentdLogDriver: typing.Optional[dict[str, models.aws_ecs.FluentdLogDriverDef]] = pydantic.Field(None)
    GelfLogDriver: typing.Optional[dict[str, models.aws_ecs.GelfLogDriverDef]] = pydantic.Field(None)
    GenericLogDriver: typing.Optional[dict[str, models.aws_ecs.GenericLogDriverDef]] = pydantic.Field(None)
    JournaldLogDriver: typing.Optional[dict[str, models.aws_ecs.JournaldLogDriverDef]] = pydantic.Field(None)
    JsonFileLogDriver: typing.Optional[dict[str, models.aws_ecs.JsonFileLogDriverDef]] = pydantic.Field(None)
    ListenerConfig: typing.Optional[dict[str, models.aws_ecs.ListenerConfigDef]] = pydantic.Field(None)
    LogDriver: typing.Optional[dict[str, models.aws_ecs.LogDriverDef]] = pydantic.Field(None)
    LogDrivers: typing.Optional[dict[str, models.aws_ecs.LogDriversDef]] = pydantic.Field(None)
    OperatingSystemFamily: typing.Optional[dict[str, models.aws_ecs.OperatingSystemFamilyDef]] = pydantic.Field(None)
    PlacementConstraint: typing.Optional[dict[str, models.aws_ecs.PlacementConstraintDef]] = pydantic.Field(None)
    PlacementStrategy: typing.Optional[dict[str, models.aws_ecs.PlacementStrategyDef]] = pydantic.Field(None)
    PortMap: typing.Optional[dict[str, models.aws_ecs.PortMapDef]] = pydantic.Field(None)
    ProxyConfiguration: typing.Optional[dict[str, models.aws_ecs.ProxyConfigurationDef]] = pydantic.Field(None)
    ProxyConfigurations: typing.Optional[dict[str, models.aws_ecs.ProxyConfigurationsDef]] = pydantic.Field(None)
    RepositoryImage: typing.Optional[dict[str, models.aws_ecs.RepositoryImageDef]] = pydantic.Field(None)
    S3EnvironmentFile: typing.Optional[dict[str, models.aws_ecs.S3EnvironmentFileDef]] = pydantic.Field(None)
    Secret: typing.Optional[dict[str, models.aws_ecs.SecretDef]] = pydantic.Field(None)
    ServiceConnect: typing.Optional[dict[str, models.aws_ecs.ServiceConnectDef]] = pydantic.Field(None)
    SplunkLogDriver: typing.Optional[dict[str, models.aws_ecs.SplunkLogDriverDef]] = pydantic.Field(None)
    SyslogLogDriver: typing.Optional[dict[str, models.aws_ecs.SyslogLogDriverDef]] = pydantic.Field(None)
    TagParameterContainerImage: typing.Optional[dict[str, models.aws_ecs.TagParameterContainerImageDef]] = pydantic.Field(None)
    TaskDefinitionRevision: typing.Optional[dict[str, models.aws_ecs.TaskDefinitionRevisionDef]] = pydantic.Field(None)
    AsgCapacityProvider: typing.Optional[dict[str, models.aws_ecs.AsgCapacityProviderDef]] = pydantic.Field(None)
    Cluster: typing.Optional[dict[str, models.aws_ecs.ClusterDef]] = pydantic.Field(None)
    ContainerDefinition: typing.Optional[dict[str, models.aws_ecs.ContainerDefinitionDef]] = pydantic.Field(None)
    Ec2Service: typing.Optional[dict[str, models.aws_ecs.Ec2ServiceDef]] = pydantic.Field(None)
    Ec2TaskDefinition: typing.Optional[dict[str, models.aws_ecs.Ec2TaskDefinitionDef]] = pydantic.Field(None)
    ExternalService: typing.Optional[dict[str, models.aws_ecs.ExternalServiceDef]] = pydantic.Field(None)
    ExternalTaskDefinition: typing.Optional[dict[str, models.aws_ecs.ExternalTaskDefinitionDef]] = pydantic.Field(None)
    FargateService: typing.Optional[dict[str, models.aws_ecs.FargateServiceDef]] = pydantic.Field(None)
    FargateTaskDefinition: typing.Optional[dict[str, models.aws_ecs.FargateTaskDefinitionDef]] = pydantic.Field(None)
    FirelensLogRouter: typing.Optional[dict[str, models.aws_ecs.FirelensLogRouterDef]] = pydantic.Field(None)
    LinuxParameters: typing.Optional[dict[str, models.aws_ecs.LinuxParametersDef]] = pydantic.Field(None)
    ScalableTaskCount: typing.Optional[dict[str, models.aws_ecs.ScalableTaskCountDef]] = pydantic.Field(None)
    ServiceManagedVolume: typing.Optional[dict[str, models.aws_ecs.ServiceManagedVolumeDef]] = pydantic.Field(None)
    TaskDefinition: typing.Optional[dict[str, models.aws_ecs.TaskDefinitionDef]] = pydantic.Field(None)
    AddAutoScalingGroupCapacityOptions: typing.Optional[dict[str, models.aws_ecs.AddAutoScalingGroupCapacityOptionsDef]] = pydantic.Field(None)
    AddCapacityOptions: typing.Optional[dict[str, models.aws_ecs.AddCapacityOptionsDef]] = pydantic.Field(None)
    AppMeshProxyConfigurationConfigProps: typing.Optional[dict[str, models.aws_ecs.AppMeshProxyConfigurationConfigPropsDef]] = pydantic.Field(None)
    AppMeshProxyConfigurationProps: typing.Optional[dict[str, models.aws_ecs.AppMeshProxyConfigurationPropsDef]] = pydantic.Field(None)
    AsgCapacityProviderProps: typing.Optional[dict[str, models.aws_ecs.AsgCapacityProviderPropsDef]] = pydantic.Field(None)
    AssetImageProps: typing.Optional[dict[str, models.aws_ecs.AssetImagePropsDef]] = pydantic.Field(None)
    AssociateCloudMapServiceOptions: typing.Optional[dict[str, models.aws_ecs.AssociateCloudMapServiceOptionsDef]] = pydantic.Field(None)
    AuthorizationConfig: typing.Optional[dict[str, models.aws_ecs.AuthorizationConfigDef]] = pydantic.Field(None)
    AwsLogDriverProps: typing.Optional[dict[str, models.aws_ecs.AwsLogDriverPropsDef]] = pydantic.Field(None)
    BaseLogDriverProps: typing.Optional[dict[str, models.aws_ecs.BaseLogDriverPropsDef]] = pydantic.Field(None)
    BaseMountPoint: typing.Optional[dict[str, models.aws_ecs.BaseMountPointDef]] = pydantic.Field(None)
    BaseServiceOptions: typing.Optional[dict[str, models.aws_ecs.BaseServiceOptionsDef]] = pydantic.Field(None)
    BaseServiceProps: typing.Optional[dict[str, models.aws_ecs.BaseServicePropsDef]] = pydantic.Field(None)
    BottleRocketImageProps: typing.Optional[dict[str, models.aws_ecs.BottleRocketImagePropsDef]] = pydantic.Field(None)
    CapacityProviderStrategy: typing.Optional[dict[str, models.aws_ecs.CapacityProviderStrategyDef]] = pydantic.Field(None)
    CfnCapacityProvider_AutoScalingGroupProviderProperty: typing.Optional[dict[str, models.aws_ecs.CfnCapacityProvider_AutoScalingGroupProviderPropertyDef]] = pydantic.Field(None)
    CfnCapacityProvider_ManagedScalingProperty: typing.Optional[dict[str, models.aws_ecs.CfnCapacityProvider_ManagedScalingPropertyDef]] = pydantic.Field(None)
    CfnCluster_CapacityProviderStrategyItemProperty: typing.Optional[dict[str, models.aws_ecs.CfnCluster_CapacityProviderStrategyItemPropertyDef]] = pydantic.Field(None)
    CfnCluster_ClusterConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnCluster_ClusterConfigurationPropertyDef]] = pydantic.Field(None)
    CfnCluster_ClusterSettingsProperty: typing.Optional[dict[str, models.aws_ecs.CfnCluster_ClusterSettingsPropertyDef]] = pydantic.Field(None)
    CfnCluster_ExecuteCommandConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnCluster_ExecuteCommandConfigurationPropertyDef]] = pydantic.Field(None)
    CfnCluster_ExecuteCommandLogConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnCluster_ExecuteCommandLogConfigurationPropertyDef]] = pydantic.Field(None)
    CfnCluster_ManagedStorageConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnCluster_ManagedStorageConfigurationPropertyDef]] = pydantic.Field(None)
    CfnCluster_ServiceConnectDefaultsProperty: typing.Optional[dict[str, models.aws_ecs.CfnCluster_ServiceConnectDefaultsPropertyDef]] = pydantic.Field(None)
    CfnClusterCapacityProviderAssociations_CapacityProviderStrategyProperty: typing.Optional[dict[str, models.aws_ecs.CfnClusterCapacityProviderAssociations_CapacityProviderStrategyPropertyDef]] = pydantic.Field(None)
    CfnService_AwsVpcConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_AwsVpcConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_CapacityProviderStrategyItemProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_CapacityProviderStrategyItemPropertyDef]] = pydantic.Field(None)
    CfnService_DeploymentAlarmsProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_DeploymentAlarmsPropertyDef]] = pydantic.Field(None)
    CfnService_DeploymentCircuitBreakerProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_DeploymentCircuitBreakerPropertyDef]] = pydantic.Field(None)
    CfnService_DeploymentConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_DeploymentConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_DeploymentControllerProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_DeploymentControllerPropertyDef]] = pydantic.Field(None)
    CfnService_EBSTagSpecificationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_EBSTagSpecificationPropertyDef]] = pydantic.Field(None)
    CfnService_LoadBalancerProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_LoadBalancerPropertyDef]] = pydantic.Field(None)
    CfnService_LogConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_LogConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_NetworkConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_NetworkConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_PlacementConstraintProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_PlacementConstraintPropertyDef]] = pydantic.Field(None)
    CfnService_PlacementStrategyProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_PlacementStrategyPropertyDef]] = pydantic.Field(None)
    CfnService_SecretProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_SecretPropertyDef]] = pydantic.Field(None)
    CfnService_ServiceConnectClientAliasProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceConnectClientAliasPropertyDef]] = pydantic.Field(None)
    CfnService_ServiceConnectConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceConnectConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_ServiceConnectServiceProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceConnectServicePropertyDef]] = pydantic.Field(None)
    CfnService_ServiceConnectTlsCertificateAuthorityProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceConnectTlsCertificateAuthorityPropertyDef]] = pydantic.Field(None)
    CfnService_ServiceConnectTlsConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceConnectTlsConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_ServiceManagedEBSVolumeConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceManagedEBSVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_ServiceRegistryProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceRegistryPropertyDef]] = pydantic.Field(None)
    CfnService_ServiceVolumeConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_ServiceVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnService_TimeoutConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnService_TimeoutConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_AuthorizationConfigProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_AuthorizationConfigPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_ContainerDefinitionProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_ContainerDefinitionPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_ContainerDependencyProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_ContainerDependencyPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_DeviceProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_DevicePropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_DockerVolumeConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_DockerVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_EFSVolumeConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_EFSVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_EnvironmentFileProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_EnvironmentFilePropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_EphemeralStorageProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_EphemeralStoragePropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_FirelensConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_FirelensConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_FSxAuthorizationConfigProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_FSxAuthorizationConfigPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_FSxWindowsFileServerVolumeConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_FSxWindowsFileServerVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_HealthCheckProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_HealthCheckPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_HostEntryProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_HostEntryPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_HostVolumePropertiesProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_HostVolumePropertiesPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_InferenceAcceleratorProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_InferenceAcceleratorPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_KernelCapabilitiesProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_KernelCapabilitiesPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_KeyValuePairProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_KeyValuePairPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_LinuxParametersProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_LinuxParametersPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_LogConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_LogConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_MountPointProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_MountPointPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_PortMappingProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_PortMappingPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_ProxyConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_ProxyConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_RepositoryCredentialsProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_RepositoryCredentialsPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_ResourceRequirementProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_ResourceRequirementPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_RuntimePlatformProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_RuntimePlatformPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_SecretProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_SecretPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_SystemControlProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_SystemControlPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_TaskDefinitionPlacementConstraintProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_TaskDefinitionPlacementConstraintPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_TmpfsProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_TmpfsPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_UlimitProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_UlimitPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_VolumeFromProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_VolumeFromPropertyDef]] = pydantic.Field(None)
    CfnTaskDefinition_VolumeProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinition_VolumePropertyDef]] = pydantic.Field(None)
    CfnTaskSet_AwsVpcConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskSet_AwsVpcConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskSet_LoadBalancerProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskSet_LoadBalancerPropertyDef]] = pydantic.Field(None)
    CfnTaskSet_NetworkConfigurationProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskSet_NetworkConfigurationPropertyDef]] = pydantic.Field(None)
    CfnTaskSet_ScaleProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskSet_ScalePropertyDef]] = pydantic.Field(None)
    CfnTaskSet_ServiceRegistryProperty: typing.Optional[dict[str, models.aws_ecs.CfnTaskSet_ServiceRegistryPropertyDef]] = pydantic.Field(None)
    CloudMapNamespaceOptions: typing.Optional[dict[str, models.aws_ecs.CloudMapNamespaceOptionsDef]] = pydantic.Field(None)
    CloudMapOptions: typing.Optional[dict[str, models.aws_ecs.CloudMapOptionsDef]] = pydantic.Field(None)
    ClusterAttributes: typing.Optional[dict[str, models.aws_ecs.ClusterAttributesDef]] = pydantic.Field(None)
    ClusterProps: typing.Optional[dict[str, models.aws_ecs.ClusterPropsDef]] = pydantic.Field(None)
    CommonTaskDefinitionAttributes: typing.Optional[dict[str, models.aws_ecs.CommonTaskDefinitionAttributesDef]] = pydantic.Field(None)
    CommonTaskDefinitionProps: typing.Optional[dict[str, models.aws_ecs.CommonTaskDefinitionPropsDef]] = pydantic.Field(None)
    ContainerDefinitionOptions: typing.Optional[dict[str, models.aws_ecs.ContainerDefinitionOptionsDef]] = pydantic.Field(None)
    ContainerDefinitionProps: typing.Optional[dict[str, models.aws_ecs.ContainerDefinitionPropsDef]] = pydantic.Field(None)
    ContainerDependency: typing.Optional[dict[str, models.aws_ecs.ContainerDependencyDef]] = pydantic.Field(None)
    ContainerImageConfig: typing.Optional[dict[str, models.aws_ecs.ContainerImageConfigDef]] = pydantic.Field(None)
    ContainerMountPoint: typing.Optional[dict[str, models.aws_ecs.ContainerMountPointDef]] = pydantic.Field(None)
    CpuUtilizationScalingProps: typing.Optional[dict[str, models.aws_ecs.CpuUtilizationScalingPropsDef]] = pydantic.Field(None)
    CredentialSpecConfig: typing.Optional[dict[str, models.aws_ecs.CredentialSpecConfigDef]] = pydantic.Field(None)
    DeploymentAlarmConfig: typing.Optional[dict[str, models.aws_ecs.DeploymentAlarmConfigDef]] = pydantic.Field(None)
    DeploymentAlarmOptions: typing.Optional[dict[str, models.aws_ecs.DeploymentAlarmOptionsDef]] = pydantic.Field(None)
    DeploymentCircuitBreaker: typing.Optional[dict[str, models.aws_ecs.DeploymentCircuitBreakerDef]] = pydantic.Field(None)
    DeploymentController: typing.Optional[dict[str, models.aws_ecs.DeploymentControllerDef]] = pydantic.Field(None)
    Device: typing.Optional[dict[str, models.aws_ecs.DeviceDef]] = pydantic.Field(None)
    DockerVolumeConfiguration: typing.Optional[dict[str, models.aws_ecs.DockerVolumeConfigurationDef]] = pydantic.Field(None)
    EBSTagSpecification: typing.Optional[dict[str, models.aws_ecs.EBSTagSpecificationDef]] = pydantic.Field(None)
    Ec2ServiceAttributes: typing.Optional[dict[str, models.aws_ecs.Ec2ServiceAttributesDef]] = pydantic.Field(None)
    Ec2ServiceProps: typing.Optional[dict[str, models.aws_ecs.Ec2ServicePropsDef]] = pydantic.Field(None)
    Ec2TaskDefinitionAttributes: typing.Optional[dict[str, models.aws_ecs.Ec2TaskDefinitionAttributesDef]] = pydantic.Field(None)
    Ec2TaskDefinitionProps: typing.Optional[dict[str, models.aws_ecs.Ec2TaskDefinitionPropsDef]] = pydantic.Field(None)
    EcsOptimizedImageOptions: typing.Optional[dict[str, models.aws_ecs.EcsOptimizedImageOptionsDef]] = pydantic.Field(None)
    EcsTarget: typing.Optional[dict[str, models.aws_ecs.EcsTargetDef]] = pydantic.Field(None)
    EfsVolumeConfiguration: typing.Optional[dict[str, models.aws_ecs.EfsVolumeConfigurationDef]] = pydantic.Field(None)
    EnvironmentFileConfig: typing.Optional[dict[str, models.aws_ecs.EnvironmentFileConfigDef]] = pydantic.Field(None)
    ExecuteCommandConfiguration: typing.Optional[dict[str, models.aws_ecs.ExecuteCommandConfigurationDef]] = pydantic.Field(None)
    ExecuteCommandLogConfiguration: typing.Optional[dict[str, models.aws_ecs.ExecuteCommandLogConfigurationDef]] = pydantic.Field(None)
    ExternalServiceAttributes: typing.Optional[dict[str, models.aws_ecs.ExternalServiceAttributesDef]] = pydantic.Field(None)
    ExternalServiceProps: typing.Optional[dict[str, models.aws_ecs.ExternalServicePropsDef]] = pydantic.Field(None)
    ExternalTaskDefinitionAttributes: typing.Optional[dict[str, models.aws_ecs.ExternalTaskDefinitionAttributesDef]] = pydantic.Field(None)
    ExternalTaskDefinitionProps: typing.Optional[dict[str, models.aws_ecs.ExternalTaskDefinitionPropsDef]] = pydantic.Field(None)
    FargateServiceAttributes: typing.Optional[dict[str, models.aws_ecs.FargateServiceAttributesDef]] = pydantic.Field(None)
    FargateServiceProps: typing.Optional[dict[str, models.aws_ecs.FargateServicePropsDef]] = pydantic.Field(None)
    FargateTaskDefinitionAttributes: typing.Optional[dict[str, models.aws_ecs.FargateTaskDefinitionAttributesDef]] = pydantic.Field(None)
    FargateTaskDefinitionProps: typing.Optional[dict[str, models.aws_ecs.FargateTaskDefinitionPropsDef]] = pydantic.Field(None)
    FirelensConfig: typing.Optional[dict[str, models.aws_ecs.FirelensConfigDef]] = pydantic.Field(None)
    FireLensLogDriverProps: typing.Optional[dict[str, models.aws_ecs.FireLensLogDriverPropsDef]] = pydantic.Field(None)
    FirelensLogRouterDefinitionOptions: typing.Optional[dict[str, models.aws_ecs.FirelensLogRouterDefinitionOptionsDef]] = pydantic.Field(None)
    FirelensLogRouterProps: typing.Optional[dict[str, models.aws_ecs.FirelensLogRouterPropsDef]] = pydantic.Field(None)
    FirelensOptions: typing.Optional[dict[str, models.aws_ecs.FirelensOptionsDef]] = pydantic.Field(None)
    FluentdLogDriverProps: typing.Optional[dict[str, models.aws_ecs.FluentdLogDriverPropsDef]] = pydantic.Field(None)
    GelfLogDriverProps: typing.Optional[dict[str, models.aws_ecs.GelfLogDriverPropsDef]] = pydantic.Field(None)
    GenericLogDriverProps: typing.Optional[dict[str, models.aws_ecs.GenericLogDriverPropsDef]] = pydantic.Field(None)
    HealthCheck: typing.Optional[dict[str, models.aws_ecs.HealthCheckDef]] = pydantic.Field(None)
    Host: typing.Optional[dict[str, models.aws_ecs.HostDef]] = pydantic.Field(None)
    InferenceAccelerator: typing.Optional[dict[str, models.aws_ecs.InferenceAcceleratorDef]] = pydantic.Field(None)
    JournaldLogDriverProps: typing.Optional[dict[str, models.aws_ecs.JournaldLogDriverPropsDef]] = pydantic.Field(None)
    JsonFileLogDriverProps: typing.Optional[dict[str, models.aws_ecs.JsonFileLogDriverPropsDef]] = pydantic.Field(None)
    LinuxParametersProps: typing.Optional[dict[str, models.aws_ecs.LinuxParametersPropsDef]] = pydantic.Field(None)
    LoadBalancerTargetOptions: typing.Optional[dict[str, models.aws_ecs.LoadBalancerTargetOptionsDef]] = pydantic.Field(None)
    LogDriverConfig: typing.Optional[dict[str, models.aws_ecs.LogDriverConfigDef]] = pydantic.Field(None)
    MemoryUtilizationScalingProps: typing.Optional[dict[str, models.aws_ecs.MemoryUtilizationScalingPropsDef]] = pydantic.Field(None)
    MountPoint: typing.Optional[dict[str, models.aws_ecs.MountPointDef]] = pydantic.Field(None)
    PortMapping: typing.Optional[dict[str, models.aws_ecs.PortMappingDef]] = pydantic.Field(None)
    RepositoryImageProps: typing.Optional[dict[str, models.aws_ecs.RepositoryImagePropsDef]] = pydantic.Field(None)
    RequestCountScalingProps: typing.Optional[dict[str, models.aws_ecs.RequestCountScalingPropsDef]] = pydantic.Field(None)
    RuntimePlatform: typing.Optional[dict[str, models.aws_ecs.RuntimePlatformDef]] = pydantic.Field(None)
    ScalableTaskCountProps: typing.Optional[dict[str, models.aws_ecs.ScalableTaskCountPropsDef]] = pydantic.Field(None)
    ScratchSpace: typing.Optional[dict[str, models.aws_ecs.ScratchSpaceDef]] = pydantic.Field(None)
    SecretVersionInfo: typing.Optional[dict[str, models.aws_ecs.SecretVersionInfoDef]] = pydantic.Field(None)
    ServiceConnectProps: typing.Optional[dict[str, models.aws_ecs.ServiceConnectPropsDef]] = pydantic.Field(None)
    ServiceConnectService: typing.Optional[dict[str, models.aws_ecs.ServiceConnectServiceDef]] = pydantic.Field(None)
    ServiceManagedEBSVolumeConfiguration: typing.Optional[dict[str, models.aws_ecs.ServiceManagedEBSVolumeConfigurationDef]] = pydantic.Field(None)
    ServiceManagedVolumeProps: typing.Optional[dict[str, models.aws_ecs.ServiceManagedVolumePropsDef]] = pydantic.Field(None)
    SplunkLogDriverProps: typing.Optional[dict[str, models.aws_ecs.SplunkLogDriverPropsDef]] = pydantic.Field(None)
    SyslogLogDriverProps: typing.Optional[dict[str, models.aws_ecs.SyslogLogDriverPropsDef]] = pydantic.Field(None)
    SystemControl: typing.Optional[dict[str, models.aws_ecs.SystemControlDef]] = pydantic.Field(None)
    TaskDefinitionAttributes: typing.Optional[dict[str, models.aws_ecs.TaskDefinitionAttributesDef]] = pydantic.Field(None)
    TaskDefinitionProps: typing.Optional[dict[str, models.aws_ecs.TaskDefinitionPropsDef]] = pydantic.Field(None)
    Tmpfs: typing.Optional[dict[str, models.aws_ecs.TmpfsDef]] = pydantic.Field(None)
    TrackCustomMetricProps: typing.Optional[dict[str, models.aws_ecs.TrackCustomMetricPropsDef]] = pydantic.Field(None)
    Ulimit: typing.Optional[dict[str, models.aws_ecs.UlimitDef]] = pydantic.Field(None)
    Volume: typing.Optional[dict[str, models.aws_ecs.VolumeDef]] = pydantic.Field(None)
    VolumeFrom: typing.Optional[dict[str, models.aws_ecs.VolumeFromDef]] = pydantic.Field(None)
    CfnCapacityProvider: typing.Optional[dict[str, models.aws_ecs.CfnCapacityProviderDef]] = pydantic.Field(None)
    CfnCluster: typing.Optional[dict[str, models.aws_ecs.CfnClusterDef]] = pydantic.Field(None)
    CfnClusterCapacityProviderAssociations: typing.Optional[dict[str, models.aws_ecs.CfnClusterCapacityProviderAssociationsDef]] = pydantic.Field(None)
    CfnPrimaryTaskSet: typing.Optional[dict[str, models.aws_ecs.CfnPrimaryTaskSetDef]] = pydantic.Field(None)
    CfnService: typing.Optional[dict[str, models.aws_ecs.CfnServiceDef]] = pydantic.Field(None)
    CfnTaskDefinition: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinitionDef]] = pydantic.Field(None)
    CfnTaskSet: typing.Optional[dict[str, models.aws_ecs.CfnTaskSetDef]] = pydantic.Field(None)
    CfnCapacityProviderProps: typing.Optional[dict[str, models.aws_ecs.CfnCapacityProviderPropsDef]] = pydantic.Field(None)
    CfnClusterCapacityProviderAssociationsProps: typing.Optional[dict[str, models.aws_ecs.CfnClusterCapacityProviderAssociationsPropsDef]] = pydantic.Field(None)
    CfnClusterProps: typing.Optional[dict[str, models.aws_ecs.CfnClusterPropsDef]] = pydantic.Field(None)
    CfnPrimaryTaskSetProps: typing.Optional[dict[str, models.aws_ecs.CfnPrimaryTaskSetPropsDef]] = pydantic.Field(None)
    CfnServiceProps: typing.Optional[dict[str, models.aws_ecs.CfnServicePropsDef]] = pydantic.Field(None)
    CfnTaskDefinitionProps: typing.Optional[dict[str, models.aws_ecs.CfnTaskDefinitionPropsDef]] = pydantic.Field(None)
    CfnTaskSetProps: typing.Optional[dict[str, models.aws_ecs.CfnTaskSetPropsDef]] = pydantic.Field(None)
    ...

import models
