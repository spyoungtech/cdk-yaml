from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_events_targets.ApiDestination
class ApiDestinationDef(BaseClass):
    api_destination: typing.Union[_REQUIRED_INIT_PARAM, models.aws_events.ApiDestinationDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send. Default: - the entire EventBridge event\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target. Default: - a new role will be created\n')
    header_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Additional headers sent to the API Destination. These are merged with headers specified on the Connection, with the headers on the Connection taking precedence. You can only specify secret values on the Connection. Default: - none\n')
    path_parameter_values: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Path parameters to insert in place of path wildcards (``*``). If the API destination has a wilcard in the path, these path parts will be inserted in that place. Default: - none\n')
    query_string_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Additional query string parameters sent to the API Destination. These are merged with headers specified on the Connection, with the headers on the Connection taking precedence. You can only specify secret values on the Connection. Default: - none\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['api_destination', 'event', 'event_role', 'header_parameters', 'path_parameter_values', 'query_string_parameters', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.ApiDestination'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.ApiDestinationDefConfig] = pydantic.Field(None)


class ApiDestinationDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.ApiDestinationDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger API destinations from an EventBridge event.')

class ApiDestinationDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.ApiGateway
class ApiGatewayDef(BaseClass):
    rest_api: typing.Union[models.aws_apigateway.RestApiDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target (i.e., the pipeline) when the given rule is triggered. Default: - a new role will be created\n')
    header_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The headers to be set when requesting API. Default: no header parameters\n')
    method: typing.Optional[str] = pydantic.Field(None, description="The method for api resource invoked by the rule. Default: '*' that treated as ANY\n")
    path: typing.Optional[str] = pydantic.Field(None, description="The api resource invoked by the rule. We can use wildcards('*') to specify the path. In that case, an equal number of real values must be specified for pathParameterValues. Default: '/'\n")
    path_parameter_values: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The path parameter values to be used to populate to wildcards("*") of requesting api path. Default: no path parameters\n')
    post_body: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='This will be the post request body send to the API. Default: the entire EventBridge event\n')
    query_string_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The query parameters to be set when requesting API. Default: no querystring parameters\n')
    stage: typing.Optional[str] = pydantic.Field(None, description='The deploy stage of api gateway invoked by the rule. Default: the value of deploymentStage.stageName of target api gateway.\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['rest_api', 'event_role', 'header_parameters', 'method', 'path', 'path_parameter_values', 'post_body', 'query_string_parameters', 'stage', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.ApiGateway'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.ApiGatewayDefConfig] = pydantic.Field(None)


class ApiGatewayDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.ApiGatewayDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger this API Gateway REST APIs as a result from an EventBridge event.')
    rest_api_config: typing.Optional[models.aws_apigateway.RestApiDefConfig] = pydantic.Field(None)

class ApiGatewayDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='-\n')
    ...


#  autogenerated from aws_cdk.aws_events_targets.AwsApi
class AwsApiDef(BaseClass):
    policy_statement: typing.Optional[models.aws_iam.PolicyStatementDef] = pydantic.Field(None, description='The IAM policy statement to allow the API call. Use only if resource restriction is needed. Default: - extract the permission from the API call')
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The service action to call.\n')
    service: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The service to call.\n')
    api_version: typing.Optional[str] = pydantic.Field(None, description='(deprecated) API version to use for the service.\n')
    catch_error_pattern: typing.Optional[str] = pydantic.Field(None, description='The regex pattern to use to catch API errors. The ``code`` property of the ``Error`` object will be tested against this pattern. If there is a match an error will not be thrown. Default: - do not catch errors\n')
    parameters: typing.Any = pydantic.Field(None, description='The parameters for the service action. Default: - no parameters')
    _init_params: typing.ClassVar[list[str]] = ['policy_statement', 'action', 'service', 'api_version', 'catch_error_pattern', 'parameters']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.AwsApi'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.AwsApiDefConfig] = pydantic.Field(None)


class AwsApiDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.AwsApiDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger this AwsApi as a result from an EventBridge event.')

class AwsApiDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='-\n')
    id: typing.Optional[str] = pydantic.Field(None, description='-')
    ...


#  autogenerated from aws_cdk.aws_events_targets.BatchJob
class BatchJobDef(BaseClass):
    job_queue_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The JobQueue arn.')
    job_queue_scope: typing.Union[models.AnyResource, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The JobQueue Resource.\n')
    job_definition_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The jobDefinition arn.\n')
    job_definition_scope: typing.Union[models.AnyResource, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The JobQueue Resource.\n')
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to attempt to retry, if the job fails. Valid values are 1–10. Default: no retryStrategy is set\n')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send to the Lambda. This will be the payload sent to the Lambda Function. Default: the entire EventBridge event\n')
    job_name: typing.Optional[str] = pydantic.Field(None, description='The name of the submitted job. Default: - Automatically generated\n')
    size: typing.Union[int, float, None] = pydantic.Field(None, description='The size of the array, if this is an array batch job. Valid values are integers between 2 and 10,000. Default: no arrayProperties are set\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['job_queue_arn', 'job_queue_scope', 'job_definition_arn', 'job_definition_scope', 'attempts', 'event', 'job_name', 'size', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.BatchJob'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.BatchJobDefConfig] = pydantic.Field(None)


class BatchJobDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.BatchJobDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger queue this batch job as a result from an EventBridge event.')

class BatchJobDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='-\n')
    ...


#  autogenerated from aws_cdk.aws_events_targets.CloudWatchLogGroup
class CloudWatchLogGroupDef(BaseClass):
    log_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.LogGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='(deprecated) The event to send to the CloudWatch LogGroup. This will be the event logged into the CloudWatch LogGroup Default: - the entire EventBridge event\n')
    log_event: typing.Optional[models.aws_events_targets.LogGroupTargetInputDef] = pydantic.Field(None, description='The event to send to the CloudWatch LogGroup. This will be the event logged into the CloudWatch LogGroup Default: - the entire EventBridge event\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['log_group', 'event', 'log_event', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.CloudWatchLogGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.CloudWatchLogGroupDefConfig] = pydantic.Field(None)


class CloudWatchLogGroupDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.CloudWatchLogGroupDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to log an event into a CloudWatch LogGroup.')

class CloudWatchLogGroupDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.CodeBuildProject
class CodeBuildProjectDef(BaseClass):
    project: typing.Union[_REQUIRED_INIT_PARAM, models.aws_codebuild.PipelineProjectDef, models.aws_codebuild.ProjectDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send to CodeBuild. This will be the payload for the StartBuild API. Default: - the entire EventBridge event\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target (i.e., the codebuild) when the given rule is triggered. Default: - a new role will be created\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['project', 'event', 'event_role', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.CodeBuildProject'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.CodeBuildProjectDefConfig] = pydantic.Field(None)


class CodeBuildProjectDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.CodeBuildProjectDefBindParams]] = pydantic.Field(None, description='Allows using build projects as event rule targets.')

class CodeBuildProjectDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.CodePipeline
class CodePipelineDef(BaseClass):
    pipeline: typing.Union[_REQUIRED_INIT_PARAM, models.aws_codepipeline.PipelineDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target (i.e., the pipeline) when the given rule is triggered. Default: - a new role will be created\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['pipeline', 'event_role', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.CodePipeline'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.CodePipelineDefConfig] = pydantic.Field(None)


class CodePipelineDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.CodePipelineDefBindParams]] = pydantic.Field(None, description='Returns the rule target specification.\nNOTE: Do not use the various ``inputXxx`` options. They can be set in a call to ``addTarget``.')

class CodePipelineDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.EcsTask
class EcsTaskDef(BaseClass):
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Cluster where service will be deployed.')
    task_definition: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.Ec2TaskDefinitionDef, models.aws_ecs.ExternalTaskDefinitionDef, models.aws_ecs.FargateTaskDefinitionDef, models.aws_ecs.TaskDefinitionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Task Definition of the task that should be started.\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the task's elastic network interface receives a public IP address. You can specify true only when LaunchType is set to FARGATE. Default: - true if the subnet type is PUBLIC, otherwise false\n")
    container_overrides: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ContainerOverrideDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Container setting overrides. Key is the name of the container to override, value is the values you want to override.\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to enable the execute command functionality for the containers in this task. If true, this enables execute command functionality on all containers in the task. Default: - false\n')
    platform_version: typing.Optional[aws_cdk.aws_ecs.FargatePlatformVersion] = pydantic.Field(None, description="The platform version on which to run your task. Unless you have specific compatibility requirements, you don't need to specify this. Default: - ECS will set the Fargate platform version to 'LATEST'\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags are not propagated. Default: - Tags will not be propagated\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Existing IAM role to run the ECS task. Default: A new IAM role is created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="Existing security groups to use for the task's ENIs. (Only applicable in case the TaskDefinition is configured for AwsVpc networking) Default: A new security group is created\n")
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="In what subnets to place the task's ENIs. (Only applicable in case the TaskDefinition is configured for AwsVpc networking) Default: Private subnets\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.TagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the task to help you categorize and organize them. Each tag consists of a key and an optional value, both of which you define. Default: - No additional tags are applied to the task\n')
    task_count: typing.Union[int, float, None] = pydantic.Field(None, description='How many tasks should be started when this event is triggered. Default: 1\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['cluster', 'task_definition', 'assign_public_ip', 'container_overrides', 'enable_execute_command', 'platform_version', 'propagate_tags', 'role', 'security_groups', 'subnet_selection', 'tags', 'task_count', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.EcsTask'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.EcsTaskDefConfig] = pydantic.Field(None)


class EcsTaskDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.EcsTaskDefBindParams]] = pydantic.Field(None, description='Allows using tasks as target of EventBridge events.')

class EcsTaskDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.EventBus
class EventBusDef(BaseClass):
    event_bus: typing.Union[_REQUIRED_INIT_PARAM, models.aws_events.EventBusDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role to be used to publish the event. Default: a new role is created.')
    _init_params: typing.ClassVar[list[str]] = ['event_bus', 'dead_letter_queue', 'role']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.EventBus'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.EventBusDefConfig] = pydantic.Field(None)


class EventBusDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.EventBusDefBindParams]] = pydantic.Field(None, description='Returns the rule target specification.\nNOTE: Do not use the various ``inputXxx`` options. They can be set in a call to ``addTarget``.')

class EventBusDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='-\n')
    ...


#  autogenerated from aws_cdk.aws_events_targets.KinesisFirehoseStream
class KinesisFirehoseStreamDef(BaseClass):
    stream: typing.Union[models.aws_kinesisfirehose.CfnDeliveryStreamDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the stream. Must be a valid JSON text passed to the target stream. Default: - the entire Event Bridge event')
    _init_params: typing.ClassVar[list[str]] = ['stream', 'message']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.KinesisFirehoseStream'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.KinesisFirehoseStreamDefConfig] = pydantic.Field(None)


class KinesisFirehoseStreamDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.KinesisFirehoseStreamDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger this Firehose Stream as a result from a Event Bridge event.')

class KinesisFirehoseStreamDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.KinesisStream
class KinesisStreamDef(BaseClass):
    stream: typing.Union[_REQUIRED_INIT_PARAM, models.aws_kinesis.StreamDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the stream. Must be a valid JSON text passed to the target stream. Default: - the entire CloudWatch event\n')
    partition_key_path: typing.Optional[str] = pydantic.Field(None, description='Partition Key Path for records sent to this stream. Default: - eventId as the partition key')
    _init_params: typing.ClassVar[list[str]] = ['stream', 'message', 'partition_key_path']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.KinesisStream'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.KinesisStreamDefConfig] = pydantic.Field(None)


class KinesisStreamDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.KinesisStreamDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger this Kinesis Stream as a result from a CloudWatch event.')

class KinesisStreamDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.LambdaFunction
class LambdaFunctionDef(BaseClass):
    handler: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send to the Lambda. This will be the payload sent to the Lambda Function. Default: the entire EventBridge event\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['handler', 'event', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.LambdaFunction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.LambdaFunctionDefConfig] = pydantic.Field(None)


class LambdaFunctionDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.LambdaFunctionDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger this Lambda as a result from an EventBridge event.')

class LambdaFunctionDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='-\n')
    ...


#  autogenerated from aws_cdk.aws_events_targets.LogGroupTargetInput
class LogGroupTargetInputDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_object']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.LogGroupTargetInput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_object']
    ...


    from_object: typing.Optional[models.aws_events_targets.LogGroupTargetInputDefFromObjectParams] = pydantic.Field(None, description='Pass a JSON object to the the log group event target.\nMay contain strings returned by ``EventField.from()`` to substitute in parts of the\nmatched event.')
    resource_config: typing.Optional[models.aws_events_targets.LogGroupTargetInputDefConfig] = pydantic.Field(None)


class LogGroupTargetInputDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.LogGroupTargetInputDefBindParams]] = pydantic.Field(None, description='Return the input properties for this input object.')

class LogGroupTargetInputDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='-')
    ...

class LogGroupTargetInputDefFromObjectParams(pydantic.BaseModel):
    message: typing.Any = pydantic.Field(None, description='The value provided here will be used in the Log "message" field. This field must be a string. If an object is passed (e.g. JSON data) it will not throw an error, but the message that makes it to CloudWatch logs will be incorrect. This is a likely scenario if doing something like: EventField.fromPath(\'$.detail\') since in most cases the ``detail`` field contains JSON data. Default: EventField.detailType\n')
    timestamp: typing.Any = pydantic.Field(None, description='The timestamp that will appear in the CloudWatch Logs record. Default: EventField.time')
    ...


#  autogenerated from aws_cdk.aws_events_targets.SfnStateMachine
class SfnStateMachineDef(BaseClass):
    machine: typing.Union[_REQUIRED_INIT_PARAM, models.aws_stepfunctions.StateMachineDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    input: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The input to the state machine execution. Default: the entire EventBridge event\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be assumed to execute the State Machine. Default: - a new role will be created\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['machine', 'input', 'role', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.SfnStateMachine'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.SfnStateMachineDefConfig] = pydantic.Field(None)


class SfnStateMachineDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.SfnStateMachineDefBindParams]] = pydantic.Field(None, description='Returns a properties that are used in an Rule to trigger this State Machine.')
    machine_config: typing.Optional[models._interface_methods.AwsStepfunctionsIStateMachineDefConfig] = pydantic.Field(None)

class SfnStateMachineDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.SnsTopic
class SnsTopicDef(BaseClass):
    topic: typing.Union[_REQUIRED_INIT_PARAM, models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the topic. Default: the entire EventBridge event\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['topic', 'message', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.SnsTopic'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.SnsTopicDefConfig] = pydantic.Field(None)


class SnsTopicDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.SnsTopicDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger this SNS topic as a result from an EventBridge event.')
    topic_config: typing.Optional[models._interface_methods.AwsSnsITopicDefConfig] = pydantic.Field(None)

class SnsTopicDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_events_targets.SqsQueue
class SqsQueueDef(BaseClass):
    queue: typing.Union[_REQUIRED_INIT_PARAM, models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the queue. Must be a valid JSON text passed to the target queue. Default: the entire EventBridge event\n')
    message_group_id: typing.Optional[str] = pydantic.Field(None, description='Message Group ID for messages sent to this queue. Required for FIFO queues, leave empty for regular queues. Default: - no message group ID (regular queue)\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185')
    _init_params: typing.ClassVar[list[str]] = ['queue', 'message', 'message_group_id', 'dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.SqsQueue'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.SqsQueueDefConfig] = pydantic.Field(None)


class SqsQueueDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_events_targets.SqsQueueDefBindParams]] = pydantic.Field(None, description='Returns a RuleTarget that can be used to trigger this SQS queue as a result from an EventBridge event.')
    queue_config: typing.Optional[models._interface_methods.AwsSqsIQueueDefConfig] = pydantic.Field(None)

class SqsQueueDefBindParams(pydantic.BaseModel):
    rule: typing.Union[models.aws_events.RuleDef] = pydantic.Field(..., description='-\n')
    ...


#  autogenerated from aws_cdk.aws_events_targets.ApiDestinationProps
class ApiDestinationPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send. Default: - the entire EventBridge event\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target. Default: - a new role will be created\n')
    header_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Additional headers sent to the API Destination. These are merged with headers specified on the Connection, with the headers on the Connection taking precedence. You can only specify secret values on the Connection. Default: - none\n')
    path_parameter_values: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Path parameters to insert in place of path wildcards (``*``). If the API destination has a wilcard in the path, these path parts will be inserted in that place. Default: - none\n')
    query_string_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Additional query string parameters sent to the API Destination. These are merged with headers specified on the Connection, with the headers on the Connection taking precedence. You can only specify secret values on the Connection. Default: - none\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_events as events\n    from aws_cdk import aws_events_targets as events_targets\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_sqs as sqs\n\n    # queue: sqs.Queue\n    # role: iam.Role\n    # rule_target_input: events.RuleTargetInput\n\n    api_destination_props = events_targets.ApiDestinationProps(\n        dead_letter_queue=queue,\n        event=rule_target_input,\n        event_role=role,\n        header_parameters={\n            "header_parameters_key": "headerParameters"\n        },\n        max_event_age=cdk.Duration.minutes(30),\n        path_parameter_values=["pathParameterValues"],\n        query_string_parameters={\n            "query_string_parameters_key": "queryStringParameters"\n        },\n        retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'event', 'event_role', 'header_parameters', 'path_parameter_values', 'query_string_parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.ApiDestinationProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.ApiGatewayProps
class ApiGatewayPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target (i.e., the pipeline) when the given rule is triggered. Default: - a new role will be created\n')
    header_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The headers to be set when requesting API. Default: no header parameters\n')
    method: typing.Optional[str] = pydantic.Field(None, description="The method for api resource invoked by the rule. Default: '*' that treated as ANY\n")
    path: typing.Optional[str] = pydantic.Field(None, description="The api resource invoked by the rule. We can use wildcards('*') to specify the path. In that case, an equal number of real values must be specified for pathParameterValues. Default: '/'\n")
    path_parameter_values: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The path parameter values to be used to populate to wildcards("*") of requesting api path. Default: no path parameters\n')
    post_body: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='This will be the post request body send to the API. Default: the entire EventBridge event\n')
    query_string_parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The query parameters to be set when requesting API. Default: no querystring parameters\n')
    stage: typing.Optional[str] = pydantic.Field(None, description='The deploy stage of api gateway invoked by the rule. Default: the value of deploymentStage.stageName of target api gateway.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_apigateway as api\n    import aws_cdk.aws_lambda as lambda_\n\n\n    rule = events.Rule(self, "Rule",\n        schedule=events.Schedule.rate(Duration.minutes(1))\n    )\n\n    fn = lambda_.Function(self, "MyFunc",\n        handler="index.handler",\n        runtime=lambda_.Runtime.NODEJS_LATEST,\n        code=lambda_.Code.from_inline("exports.handler = e => {}")\n    )\n\n    rest_api = api.LambdaRestApi(self, "MyRestAPI", handler=fn)\n\n    dlq = sqs.Queue(self, "DeadLetterQueue")\n\n    rule.add_target(\n        targets.ApiGateway(rest_api,\n            path="/*/test",\n            method="GET",\n            stage="prod",\n            path_parameter_values=["path-value"],\n            header_parameters={\n                "Header1": "header1"\n            },\n            query_string_parameters={\n                "QueryParam1": "query-param-1"\n            },\n            dead_letter_queue=dlq\n        ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'event_role', 'header_parameters', 'method', 'path', 'path_parameter_values', 'post_body', 'query_string_parameters', 'stage']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.ApiGatewayProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.AwsApiInput
class AwsApiInputDef(BaseStruct):
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The service action to call.\n')
    service: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The service to call.\n')
    api_version: typing.Optional[str] = pydantic.Field(None, description='(deprecated) API version to use for the service.\n')
    catch_error_pattern: typing.Optional[str] = pydantic.Field(None, description='The regex pattern to use to catch API errors. The ``code`` property of the ``Error`` object will be tested against this pattern. If there is a match an error will not be thrown. Default: - do not catch errors\n')
    parameters: typing.Any = pydantic.Field(None, description='The parameters for the service action. Default: - no parameters\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events_targets as events_targets\n\n    # parameters: Any\n\n    aws_api_input = events_targets.AwsApiInput(\n        action="action",\n        service="service",\n\n        # the properties below are optional\n        api_version="apiVersion",\n        catch_error_pattern="catchErrorPattern",\n        parameters=parameters\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'service', 'api_version', 'catch_error_pattern', 'parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.AwsApiInput'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.AwsApiProps
class AwsApiPropsDef(BaseStruct):
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The service action to call.\n')
    service: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The service to call.\n')
    api_version: typing.Optional[str] = pydantic.Field(None, description='(deprecated) API version to use for the service.\n')
    catch_error_pattern: typing.Optional[str] = pydantic.Field(None, description='The regex pattern to use to catch API errors. The ``code`` property of the ``Error`` object will be tested against this pattern. If there is a match an error will not be thrown. Default: - do not catch errors\n')
    parameters: typing.Any = pydantic.Field(None, description='The parameters for the service action. Default: - no parameters\n')
    policy_statement: typing.Optional[models.aws_iam.PolicyStatementDef] = pydantic.Field(None, description='The IAM policy statement to allow the API call. Use only if resource restriction is needed. Default: - extract the permission from the API call\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events_targets as events_targets\n    from aws_cdk import aws_iam as iam\n\n    # parameters: Any\n    # policy_statement: iam.PolicyStatement\n\n    aws_api_props = events_targets.AwsApiProps(\n        action="action",\n        service="service",\n\n        # the properties below are optional\n        api_version="apiVersion",\n        catch_error_pattern="catchErrorPattern",\n        parameters=parameters,\n        policy_statement=policy_statement\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'service', 'api_version', 'catch_error_pattern', 'parameters', 'policy_statement']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.AwsApiProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.BatchJobProps
class BatchJobPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to attempt to retry, if the job fails. Valid values are 1–10. Default: no retryStrategy is set\n')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send to the Lambda. This will be the payload sent to the Lambda Function. Default: the entire EventBridge event\n')
    job_name: typing.Optional[str] = pydantic.Field(None, description='The name of the submitted job. Default: - Automatically generated\n')
    size: typing.Union[int, float, None] = pydantic.Field(None, description='The size of the array, if this is an array batch job. Valid values are integers between 2 and 10,000. Default: no arrayProperties are set\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_ec2 as ec2\n    import aws_cdk.aws_ecs as ecs\n    import aws_cdk.aws_batch as batch\n    from aws_cdk.aws_ecs import ContainerImage\n\n    # vpc: ec2.Vpc\n\n\n    compute_environment = batch.FargateComputeEnvironment(self, "ComputeEnv",\n        vpc=vpc\n    )\n\n    job_queue = batch.JobQueue(self, "JobQueue",\n        priority=1,\n        compute_environments=[batch.OrderedComputeEnvironment(\n            compute_environment=compute_environment,\n            order=1\n        )\n        ]\n    )\n\n    job_definition = batch.EcsJobDefinition(self, "MyJob",\n        container=batch.EcsEc2ContainerDefinition(self, "Container",\n            image=ecs.ContainerImage.from_registry("test-repo"),\n            memory=cdk.Size.mebibytes(2048),\n            cpu=256\n        )\n    )\n\n    queue = sqs.Queue(self, "Queue")\n\n    rule = events.Rule(self, "Rule",\n        schedule=events.Schedule.rate(Duration.hours(1))\n    )\n\n    rule.add_target(targets.BatchJob(job_queue.job_queue_arn, job_queue, job_definition.job_definition_arn, job_definition,\n        dead_letter_queue=queue,\n        event=events.RuleTargetInput.from_object({"SomeParam": "SomeValue"}),\n        retry_attempts=2,\n        max_event_age=Duration.hours(2)\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'attempts', 'event', 'job_name', 'size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.BatchJobProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.CodeBuildProjectProps
class CodeBuildProjectPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send to CodeBuild. This will be the payload for the StartBuild API. Default: - the entire EventBridge event\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target (i.e., the codebuild) when the given rule is triggered. Default: - a new role will be created\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_codebuild as codebuild\n    import aws_cdk.aws_codecommit as codecommit\n\n\n    repo = codecommit.Repository(self, "MyRepo",\n        repository_name="aws-cdk-codebuild-events"\n    )\n\n    project = codebuild.Project(self, "MyProject",\n        source=codebuild.Source.code_commit(repository=repo)\n    )\n\n    dead_letter_queue = sqs.Queue(self, "DeadLetterQueue")\n\n    # trigger a build when a commit is pushed to the repo\n    on_commit_rule = repo.on_commit("OnCommit",\n        target=targets.CodeBuildProject(project,\n            dead_letter_queue=dead_letter_queue\n        ),\n        branches=["master"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'event', 'event_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.CodeBuildProjectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.CodePipelineTargetOptions
class CodePipelineTargetOptionsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    event_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume before invoking the target (i.e., the pipeline) when the given rule is triggered. Default: - a new role will be created\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_events_targets as events_targets\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_sqs as sqs\n\n    # queue: sqs.Queue\n    # role: iam.Role\n\n    code_pipeline_target_options = events_targets.CodePipelineTargetOptions(\n        dead_letter_queue=queue,\n        event_role=role,\n        max_event_age=cdk.Duration.minutes(30),\n        retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'event_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.CodePipelineTargetOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.ContainerOverride
class ContainerOverrideDef(BaseStruct):
    container_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name of the container inside the task definition.')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Command to run inside the container. Default: Default command\n')
    cpu: typing.Union[int, float, None] = pydantic.Field(None, description='The number of cpu units reserved for the container. Default: The default value from the task definition.\n')
    environment: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.TaskEnvironmentVariableDef, dict[str, typing.Any]]]] = pydantic.Field(None, description="Variables to set in the container's environment.\n")
    memory_limit: typing.Union[int, float, None] = pydantic.Field(None, description='Hard memory limit on the container. Default: The default value from the task definition.\n')
    memory_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='Soft memory limit on the container. Default: The default value from the task definition.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events_targets as events_targets\n\n    container_override = events_targets.ContainerOverride(\n        container_name="containerName",\n\n        # the properties below are optional\n        command=["command"],\n        cpu=123,\n        environment=[events_targets.TaskEnvironmentVariable(\n            name="name",\n            value="value"\n        )],\n        memory_limit=123,\n        memory_reservation=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_name', 'command', 'cpu', 'environment', 'memory_limit', 'memory_reservation']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.ContainerOverride'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.EcsTaskProps
class EcsTaskPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.ClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Cluster where service will be deployed.\n')
    task_definition: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecs.Ec2TaskDefinitionDef, models.aws_ecs.ExternalTaskDefinitionDef, models.aws_ecs.FargateTaskDefinitionDef, models.aws_ecs.TaskDefinitionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Task Definition of the task that should be started.\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the task's elastic network interface receives a public IP address. You can specify true only when LaunchType is set to FARGATE. Default: - true if the subnet type is PUBLIC, otherwise false\n")
    container_overrides: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.ContainerOverrideDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Container setting overrides. Key is the name of the container to override, value is the values you want to override.\n')
    enable_execute_command: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to enable the execute command functionality for the containers in this task. If true, this enables execute command functionality on all containers in the task. Default: - false\n')
    platform_version: typing.Optional[aws_cdk.aws_ecs.FargatePlatformVersion] = pydantic.Field(None, description="The platform version on which to run your task. Unless you have specific compatibility requirements, you don't need to specify this. Default: - ECS will set the Fargate platform version to 'LATEST'\n")
    propagate_tags: typing.Optional[aws_cdk.aws_ecs.PropagatedTagSource] = pydantic.Field(None, description='Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags are not propagated. Default: - Tags will not be propagated\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Existing IAM role to run the ECS task. Default: A new IAM role is created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="Existing security groups to use for the task's ENIs. (Only applicable in case the TaskDefinition is configured for AwsVpc networking) Default: A new security group is created\n")
    subnet_selection: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description="In what subnets to place the task's ENIs. (Only applicable in case the TaskDefinition is configured for AwsVpc networking) Default: Private subnets\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.aws_events_targets.TagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The metadata that you apply to the task to help you categorize and organize them. Each tag consists of a key and an optional value, both of which you define. Default: - No additional tags are applied to the task\n')
    task_count: typing.Union[int, float, None] = pydantic.Field(None, description='How many tasks should be started when this event is triggered. Default: 1\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_ecs as ecs\n\n    # cluster: ecs.ICluster\n    # task_definition: ecs.TaskDefinition\n\n\n    rule = events.Rule(self, "Rule",\n        schedule=events.Schedule.rate(cdk.Duration.hours(1))\n    )\n\n    rule.add_target(targets.EcsTask(\n        cluster=cluster,\n        task_definition=task_definition,\n        task_count=1,\n        container_overrides=[targets.ContainerOverride(\n            container_name="TheContainer",\n            command=["echo", events.EventField.from_path("$.detail.event")]\n        )],\n        enable_execute_command=True\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'cluster', 'task_definition', 'assign_public_ip', 'container_overrides', 'enable_execute_command', 'platform_version', 'propagate_tags', 'role', 'security_groups', 'subnet_selection', 'tags', 'task_count']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.EcsTaskProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_events_targets.EcsTaskPropsDefConfig] = pydantic.Field(None)


class EcsTaskPropsDefConfig(pydantic.BaseModel):
    cluster_config: typing.Optional[models._interface_methods.AwsEcsIClusterDefConfig] = pydantic.Field(None)
    task_definition_config: typing.Optional[models._interface_methods.AwsEcsITaskDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_events_targets.EventBusProps
class EventBusPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role to be used to publish the event. Default: a new role is created.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events_targets as events_targets\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_sqs as sqs\n\n    # queue: sqs.Queue\n    # role: iam.Role\n\n    event_bus_props = events_targets.EventBusProps(\n        dead_letter_queue=queue,\n        role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.EventBusProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.KinesisFirehoseStreamProps
class KinesisFirehoseStreamPropsDef(BaseStruct):
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the stream. Must be a valid JSON text passed to the target stream. Default: - the entire Event Bridge event\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events as events\n    from aws_cdk import aws_events_targets as events_targets\n\n    # rule_target_input: events.RuleTargetInput\n\n    kinesis_firehose_stream_props = events_targets.KinesisFirehoseStreamProps(\n        message=rule_target_input\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['message']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.KinesisFirehoseStreamProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.KinesisStreamProps
class KinesisStreamPropsDef(BaseStruct):
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the stream. Must be a valid JSON text passed to the target stream. Default: - the entire CloudWatch event\n')
    partition_key_path: typing.Optional[str] = pydantic.Field(None, description='Partition Key Path for records sent to this stream. Default: - eventId as the partition key\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events as events\n    from aws_cdk import aws_events_targets as events_targets\n\n    # rule_target_input: events.RuleTargetInput\n\n    kinesis_stream_props = events_targets.KinesisStreamProps(\n        message=rule_target_input,\n        partition_key_path="partitionKeyPath"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['message', 'partition_key_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.KinesisStreamProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.LambdaFunctionProps
class LambdaFunctionPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The event to send to the Lambda. This will be the payload sent to the Lambda Function. Default: the entire EventBridge event\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_lambda as lambda_\n\n\n    fn = lambda_.Function(self, "MyFunc",\n        runtime=lambda_.Runtime.NODEJS_LATEST,\n        handler="index.handler",\n        code=lambda_.Code.from_inline("exports.handler = handler.toString()")\n    )\n\n    rule = events.Rule(self, "rule",\n        event_pattern=events.EventPattern(\n            source=["aws.ec2"]\n        )\n    )\n\n    queue = sqs.Queue(self, "Queue")\n\n    rule.add_target(targets.LambdaFunction(fn,\n        dead_letter_queue=queue,  # Optional: add a dead letter queue\n        max_event_age=Duration.hours(2),  # Optional: set the maxEventAge retry policy\n        retry_attempts=2\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'event']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.LambdaFunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.LogGroupProps
class LogGroupPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    event: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='(deprecated) The event to send to the CloudWatch LogGroup. This will be the event logged into the CloudWatch LogGroup Default: - the entire EventBridge event\n')
    log_event: typing.Optional[models.aws_events_targets.LogGroupTargetInputDef] = pydantic.Field(None, description='The event to send to the CloudWatch LogGroup. This will be the event logged into the CloudWatch LogGroup Default: - the entire EventBridge event\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_logs as logs\n    # log_group: logs.LogGroup\n    # rule: events.Rule\n\n\n    rule.add_target(targets.CloudWatchLogGroup(log_group,\n        log_event=targets.LogGroupTargetInput.from_object(\n            timestamp=events.EventField.from_path("$.time"),\n            message=events.EventField.from_path("$.detail-type")\n        )\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'event', 'log_event']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.LogGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.LogGroupTargetInputOptions
class LogGroupTargetInputOptionsDef(BaseStruct):
    message: typing.Any = pydantic.Field(None, description='The value provided here will be used in the Log "message" field. This field must be a string. If an object is passed (e.g. JSON data) it will not throw an error, but the message that makes it to CloudWatch logs will be incorrect. This is a likely scenario if doing something like: EventField.fromPath(\'$.detail\') since in most cases the ``detail`` field contains JSON data. Default: EventField.detailType\n')
    timestamp: typing.Any = pydantic.Field(None, description='The timestamp that will appear in the CloudWatch Logs record. Default: EventField.time\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_logs as logs\n    # log_group: logs.LogGroup\n    # rule: events.Rule\n\n\n    rule.add_target(targets.CloudWatchLogGroup(log_group,\n        log_event=targets.LogGroupTargetInput.from_object(\n            timestamp=events.EventField.from_path("$.time"),\n            message=events.EventField.from_path("$.detail-type")\n        )\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['message', 'timestamp']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.LogGroupTargetInputOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.SfnStateMachineProps
class SfnStateMachinePropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    input: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The input to the state machine execution. Default: the entire EventBridge event\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be assumed to execute the State Machine. Default: - a new role will be created\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_iam as iam\n    import aws_cdk.aws_stepfunctions as sfn\n\n\n    rule = events.Rule(self, "Rule",\n        schedule=events.Schedule.rate(Duration.minutes(1))\n    )\n\n    dlq = sqs.Queue(self, "DeadLetterQueue")\n\n    role = iam.Role(self, "Role",\n        assumed_by=iam.ServicePrincipal("events.amazonaws.com")\n    )\n    state_machine = sfn.StateMachine(self, "SM",\n        definition=sfn.Wait(self, "Hello", time=sfn.WaitTime.duration(Duration.seconds(10)))\n    )\n\n    rule.add_target(targets.SfnStateMachine(state_machine,\n        input=events.RuleTargetInput.from_object({"SomeParam": "SomeValue"}),\n        dead_letter_queue=dlq,\n        role=role\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'input', 'role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.SfnStateMachineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.SnsTopicProps
class SnsTopicPropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the topic. Default: the entire EventBridge event\n\n:exampleMetadata: infused\n\nExample::\n\n    # on_commit_rule: events.Rule\n    # topic: sns.Topic\n\n\n    on_commit_rule.add_target(targets.SnsTopic(topic,\n        message=events.RuleTargetInput.from_text(f"A commit was pushed to the repository {codecommit.ReferenceEvent.repositoryName} on branch {codecommit.ReferenceEvent.referenceName}")\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'message']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.SnsTopicProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.SqsQueueProps
class SqsQueuePropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n')
    message: typing.Optional[models.aws_events.RuleTargetInputDef] = pydantic.Field(None, description='The message to send to the queue. Must be a valid JSON text passed to the target queue. Default: the entire EventBridge event\n')
    message_group_id: typing.Optional[str] = pydantic.Field(None, description='Message Group ID for messages sent to this queue. Required for FIFO queues, leave empty for regular queues. Default: - no message group ID (regular queue)\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_events as events\n    from aws_cdk import aws_events_targets as events_targets\n    from aws_cdk import aws_sqs as sqs\n\n    # queue: sqs.Queue\n    # rule_target_input: events.RuleTargetInput\n\n    sqs_queue_props = events_targets.SqsQueueProps(\n        dead_letter_queue=queue,\n        max_event_age=cdk.Duration.minutes(30),\n        message=rule_target_input,\n        message_group_id="messageGroupId",\n        retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts', 'message', 'message_group_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.SqsQueueProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.Tag
class TagDef(BaseStruct):
    key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Key is the name of the tag.\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Value is the metadata contents of the tag.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events_targets as events_targets\n\n    tag = events_targets.Tag(\n        key="key",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.Tag'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.TargetBaseProps
class TargetBasePropsDef(BaseStruct):
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to be used as deadLetterQueue. Check out the `considerations for using a dead-letter queue <https://docs.aws.amazon.com/eventbridge/latest/userguide/rule-dlq.html#dlq-considerations>`_. The events not successfully delivered are automatically retried for a specified period of time, depending on the retry policy of the target. If an event is not delivered before all retry attempts are exhausted, it will be sent to the dead letter queue. Default: - no dead-letter queue\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum value of 60. Maximum value of 86400. Default: Duration.hours(24)\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum value of 0. Maximum value of 185. Default: 185\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_events_targets as events_targets\n    from aws_cdk import aws_sqs as sqs\n\n    # queue: sqs.Queue\n\n    target_base_props = events_targets.TargetBaseProps(\n        dead_letter_queue=queue,\n        max_event_age=cdk.Duration.minutes(30),\n        retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['dead_letter_queue', 'max_event_age', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.TargetBaseProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_events_targets.TaskEnvironmentVariable
class TaskEnvironmentVariableDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name for the environment variable. Exactly one of ``name`` and ``namePath`` must be specified.\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Value of the environment variable. Exactly one of ``value`` and ``valuePath`` must be specified.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_events_targets as events_targets\n\n    task_environment_variable = events_targets.TaskEnvironmentVariable(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_events_targets.TaskEnvironmentVariable'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    ApiDestination: typing.Optional[dict[str, models.aws_events_targets.ApiDestinationDef]] = pydantic.Field(None)
    ApiGateway: typing.Optional[dict[str, models.aws_events_targets.ApiGatewayDef]] = pydantic.Field(None)
    AwsApi: typing.Optional[dict[str, models.aws_events_targets.AwsApiDef]] = pydantic.Field(None)
    BatchJob: typing.Optional[dict[str, models.aws_events_targets.BatchJobDef]] = pydantic.Field(None)
    CloudWatchLogGroup: typing.Optional[dict[str, models.aws_events_targets.CloudWatchLogGroupDef]] = pydantic.Field(None)
    CodeBuildProject: typing.Optional[dict[str, models.aws_events_targets.CodeBuildProjectDef]] = pydantic.Field(None)
    CodePipeline: typing.Optional[dict[str, models.aws_events_targets.CodePipelineDef]] = pydantic.Field(None)
    EcsTask: typing.Optional[dict[str, models.aws_events_targets.EcsTaskDef]] = pydantic.Field(None)
    EventBus: typing.Optional[dict[str, models.aws_events_targets.EventBusDef]] = pydantic.Field(None)
    KinesisFirehoseStream: typing.Optional[dict[str, models.aws_events_targets.KinesisFirehoseStreamDef]] = pydantic.Field(None)
    KinesisStream: typing.Optional[dict[str, models.aws_events_targets.KinesisStreamDef]] = pydantic.Field(None)
    LambdaFunction: typing.Optional[dict[str, models.aws_events_targets.LambdaFunctionDef]] = pydantic.Field(None)
    LogGroupTargetInput: typing.Optional[dict[str, models.aws_events_targets.LogGroupTargetInputDef]] = pydantic.Field(None)
    SfnStateMachine: typing.Optional[dict[str, models.aws_events_targets.SfnStateMachineDef]] = pydantic.Field(None)
    SnsTopic: typing.Optional[dict[str, models.aws_events_targets.SnsTopicDef]] = pydantic.Field(None)
    SqsQueue: typing.Optional[dict[str, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None)
    ApiDestinationProps: typing.Optional[dict[str, models.aws_events_targets.ApiDestinationPropsDef]] = pydantic.Field(None)
    ApiGatewayProps: typing.Optional[dict[str, models.aws_events_targets.ApiGatewayPropsDef]] = pydantic.Field(None)
    AwsApiInput: typing.Optional[dict[str, models.aws_events_targets.AwsApiInputDef]] = pydantic.Field(None)
    AwsApiProps: typing.Optional[dict[str, models.aws_events_targets.AwsApiPropsDef]] = pydantic.Field(None)
    BatchJobProps: typing.Optional[dict[str, models.aws_events_targets.BatchJobPropsDef]] = pydantic.Field(None)
    CodeBuildProjectProps: typing.Optional[dict[str, models.aws_events_targets.CodeBuildProjectPropsDef]] = pydantic.Field(None)
    CodePipelineTargetOptions: typing.Optional[dict[str, models.aws_events_targets.CodePipelineTargetOptionsDef]] = pydantic.Field(None)
    ContainerOverride: typing.Optional[dict[str, models.aws_events_targets.ContainerOverrideDef]] = pydantic.Field(None)
    EcsTaskProps: typing.Optional[dict[str, models.aws_events_targets.EcsTaskPropsDef]] = pydantic.Field(None)
    EventBusProps: typing.Optional[dict[str, models.aws_events_targets.EventBusPropsDef]] = pydantic.Field(None)
    KinesisFirehoseStreamProps: typing.Optional[dict[str, models.aws_events_targets.KinesisFirehoseStreamPropsDef]] = pydantic.Field(None)
    KinesisStreamProps: typing.Optional[dict[str, models.aws_events_targets.KinesisStreamPropsDef]] = pydantic.Field(None)
    LambdaFunctionProps: typing.Optional[dict[str, models.aws_events_targets.LambdaFunctionPropsDef]] = pydantic.Field(None)
    LogGroupProps: typing.Optional[dict[str, models.aws_events_targets.LogGroupPropsDef]] = pydantic.Field(None)
    LogGroupTargetInputOptions: typing.Optional[dict[str, models.aws_events_targets.LogGroupTargetInputOptionsDef]] = pydantic.Field(None)
    SfnStateMachineProps: typing.Optional[dict[str, models.aws_events_targets.SfnStateMachinePropsDef]] = pydantic.Field(None)
    SnsTopicProps: typing.Optional[dict[str, models.aws_events_targets.SnsTopicPropsDef]] = pydantic.Field(None)
    SqsQueueProps: typing.Optional[dict[str, models.aws_events_targets.SqsQueuePropsDef]] = pydantic.Field(None)
    Tag: typing.Optional[dict[str, models.aws_events_targets.TagDef]] = pydantic.Field(None)
    TargetBaseProps: typing.Optional[dict[str, models.aws_events_targets.TargetBasePropsDef]] = pydantic.Field(None)
    TaskEnvironmentVariable: typing.Optional[dict[str, models.aws_events_targets.TaskEnvironmentVariableDef]] = pydantic.Field(None)
    ...

import models
