from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_batch.EcsVolume
class EcsVolumeDef(BaseClass):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the path on the container where this volume is mounted.')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'name', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['efs', 'host']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EcsVolumeDefConfig] = pydantic.Field(None)


class EcsVolumeDefConfig(pydantic.BaseModel):
    efs: typing.Optional[list[models.aws_batch.EcsVolumeDefEfsParams]] = pydantic.Field(None, description='Creates a Volume that uses an AWS Elastic File System (EFS);\nthis volume can grow and shrink as needed')
    host: typing.Optional[list[models.aws_batch.EcsVolumeDefHostParams]] = pydantic.Field(None, description='Creates a Host volume.\nThis volume will persist on the host at the specified ``hostPath``.\nIf the ``hostPath`` is not specified, Docker will choose the host path. In this case,\nthe data may not persist after the containers that use it stop running.')

class EcsVolumeDefEfsParams(pydantic.BaseModel):
    file_system: typing.Union[models.aws_efs.FileSystemDef] = pydantic.Field(..., description='The EFS File System that supports this volume.\n')
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, ``rootDirectory`` must either be omitted or set to ``/`` which enforces the path set on the EFS access point. If an access point is used, ``enableTransitEncryption`` must be ``true``. Default: - no accessPointId\n')
    enable_transit_encryption: typing.Optional[bool] = pydantic.Field(None, description='Enables encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Default: false\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume is used instead. Specifying ``/`` has the same effect as omitting this parameter. The maximum length is 4,096 characters. Default: - root of the EFS File System\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. The value must be between 0 and 65,535. Default: - chosen by the EFS Mount Helper\n')
    use_job_role: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use the AWS Batch job IAM role defined in a job definition when mounting the Amazon EFS file system. If specified, ``enableTransitEncryption`` must be ``true``. Default: false\n')
    container_path: str = pydantic.Field(..., description='the path on the container where this volume is mounted.\n')
    name: str = pydantic.Field(..., description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false\n\n:see: https://docs.aws.amazon.com/batch/latest/userguide/efs-volumes.html\n')
    return_config: typing.Optional[list[models.aws_batch.EfsVolumeDefConfig]] = pydantic.Field(None)
    ...

class EcsVolumeDefHostParams(pydantic.BaseModel):
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path on the host machine this container will have access to. Default: - Docker will choose the host path. The data may not persist after the containers that use it stop running.\n')
    container_path: str = pydantic.Field(..., description='the path on the container where this volume is mounted.\n')
    name: str = pydantic.Field(..., description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false')
    return_config: typing.Optional[list[models.aws_batch.HostVolumeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.EfsVolume
class EfsVolumeDef(BaseClass):
    file_system: typing.Union[_REQUIRED_INIT_PARAM, models.aws_efs.FileSystemDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The EFS File System that supports this volume.')
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, ``rootDirectory`` must either be omitted or set to ``/`` which enforces the path set on the EFS access point. If an access point is used, ``enableTransitEncryption`` must be ``true``. Default: - no accessPointId\n')
    enable_transit_encryption: typing.Optional[bool] = pydantic.Field(None, description='Enables encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Default: false\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume is used instead. Specifying ``/`` has the same effect as omitting this parameter. The maximum length is 4,096 characters. Default: - root of the EFS File System\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. The value must be between 0 and 65,535. Default: - chosen by the EFS Mount Helper\n')
    use_job_role: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use the AWS Batch job IAM role defined in a job definition when mounting the Amazon EFS file system. If specified, ``enableTransitEncryption`` must be ``true``. Default: false\n')
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the path on the container where this volume is mounted.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['file_system', 'access_point_id', 'enable_transit_encryption', 'root_directory', 'transit_encryption_port', 'use_job_role', 'container_path', 'name', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['efs', 'host']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EfsVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EfsVolumeDefConfig] = pydantic.Field(None)


class EfsVolumeDefConfig(pydantic.BaseModel):
    efs: typing.Optional[list[models.aws_batch.EfsVolumeDefEfsParams]] = pydantic.Field(None, description='Creates a Volume that uses an AWS Elastic File System (EFS);\nthis volume can grow and shrink as needed')
    host: typing.Optional[list[models.aws_batch.EfsVolumeDefHostParams]] = pydantic.Field(None, description='Creates a Host volume.\nThis volume will persist on the host at the specified ``hostPath``.\nIf the ``hostPath`` is not specified, Docker will choose the host path. In this case,\nthe data may not persist after the containers that use it stop running.')
    file_system_config: typing.Optional[models._interface_methods.AwsEfsIFileSystemDefConfig] = pydantic.Field(None)

class EfsVolumeDefEfsParams(pydantic.BaseModel):
    file_system: typing.Union[models.aws_efs.FileSystemDef] = pydantic.Field(..., description='The EFS File System that supports this volume.\n')
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, ``rootDirectory`` must either be omitted or set to ``/`` which enforces the path set on the EFS access point. If an access point is used, ``enableTransitEncryption`` must be ``true``. Default: - no accessPointId\n')
    enable_transit_encryption: typing.Optional[bool] = pydantic.Field(None, description='Enables encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Default: false\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume is used instead. Specifying ``/`` has the same effect as omitting this parameter. The maximum length is 4,096 characters. Default: - root of the EFS File System\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. The value must be between 0 and 65,535. Default: - chosen by the EFS Mount Helper\n')
    use_job_role: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use the AWS Batch job IAM role defined in a job definition when mounting the Amazon EFS file system. If specified, ``enableTransitEncryption`` must be ``true``. Default: false\n')
    container_path: str = pydantic.Field(..., description='the path on the container where this volume is mounted.\n')
    name: str = pydantic.Field(..., description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false\n\n:see: https://docs.aws.amazon.com/batch/latest/userguide/efs-volumes.html\n')
    return_config: typing.Optional[list[models.aws_batch.EfsVolumeDefConfig]] = pydantic.Field(None)
    ...

class EfsVolumeDefHostParams(pydantic.BaseModel):
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path on the host machine this container will have access to. Default: - Docker will choose the host path. The data may not persist after the containers that use it stop running.\n')
    container_path: str = pydantic.Field(..., description='the path on the container where this volume is mounted.\n')
    name: str = pydantic.Field(..., description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false')
    return_config: typing.Optional[list[models.aws_batch.HostVolumeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.EksVolume
class EksVolumeDef(BaseClass):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['name', 'mount_path', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['empty_dir', 'host_path', 'secret']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EksVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EksVolumeDefConfig] = pydantic.Field(None)


class EksVolumeDefConfig(pydantic.BaseModel):
    empty_dir: typing.Optional[list[models.aws_batch.EksVolumeDefEmptyDirParams]] = pydantic.Field(None, description='Creates a Kubernetes EmptyDir volume.')
    host_path: typing.Optional[list[models.aws_batch.EksVolumeDefHostPathParams]] = pydantic.Field(None, description='Creates a Kubernetes HostPath volume.')
    secret: typing.Optional[list[models.aws_batch.EksVolumeDefSecretParams]] = pydantic.Field(None, description='Creates a Kubernetes Secret volume.')

class EksVolumeDefEmptyDirParams(pydantic.BaseModel):
    medium: typing.Optional[aws_cdk.aws_batch.EmptyDirMediumType] = pydantic.Field(None, description='The storage type to use for this Volume. Default: ``EmptyDirMediumType.DISK``\n')
    size_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The maximum size for this Volume. Default: - no size limit\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\n')
    return_config: typing.Optional[list[models.aws_batch.EmptyDirVolumeDefConfig]] = pydantic.Field(None)
    ...

class EksVolumeDefHostPathParams(pydantic.BaseModel):
    host_path: str = pydantic.Field(..., description='The path of the file or directory on the host to mount into containers on the pod. *Note*: HothPath Volumes present many security risks, and should be avoided when possible.\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\n')
    return_config: typing.Optional[list[models.aws_batch.HostPathVolumeDefConfig]] = pydantic.Field(None)
    ...

class EksVolumeDefSecretParams(pydantic.BaseModel):
    secret_name: str = pydantic.Field(..., description='The name of the secret. Must be a valid DNS subdomain name.\n')
    optional: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the secret or the secret's keys must be defined. Default: true\n")
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#secret\n')
    return_config: typing.Optional[list[models.aws_batch.SecretPathVolumeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.EmptyDirVolume
class EmptyDirVolumeDef(BaseClass):
    medium: typing.Optional[aws_cdk.aws_batch.EmptyDirMediumType] = pydantic.Field(None, description='The storage type to use for this Volume. Default: ``EmptyDirMediumType.DISK``')
    size_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The maximum size for this Volume. Default: - no size limit\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['medium', 'size_limit', 'name', 'mount_path', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['empty_dir', 'host_path', 'secret']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EmptyDirVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EmptyDirVolumeDefConfig] = pydantic.Field(None)


class EmptyDirVolumeDefConfig(pydantic.BaseModel):
    empty_dir: typing.Optional[list[models.aws_batch.EmptyDirVolumeDefEmptyDirParams]] = pydantic.Field(None, description='Creates a Kubernetes EmptyDir volume.')
    host_path: typing.Optional[list[models.aws_batch.EmptyDirVolumeDefHostPathParams]] = pydantic.Field(None, description='Creates a Kubernetes HostPath volume.')
    secret: typing.Optional[list[models.aws_batch.EmptyDirVolumeDefSecretParams]] = pydantic.Field(None, description='Creates a Kubernetes Secret volume.')

class EmptyDirVolumeDefEmptyDirParams(pydantic.BaseModel):
    medium: typing.Optional[aws_cdk.aws_batch.EmptyDirMediumType] = pydantic.Field(None, description='The storage type to use for this Volume. Default: ``EmptyDirMediumType.DISK``\n')
    size_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The maximum size for this Volume. Default: - no size limit\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\n')
    return_config: typing.Optional[list[models.aws_batch.EmptyDirVolumeDefConfig]] = pydantic.Field(None)
    ...

class EmptyDirVolumeDefHostPathParams(pydantic.BaseModel):
    host_path: str = pydantic.Field(..., description='The path of the file or directory on the host to mount into containers on the pod. *Note*: HothPath Volumes present many security risks, and should be avoided when possible.\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\n')
    return_config: typing.Optional[list[models.aws_batch.HostPathVolumeDefConfig]] = pydantic.Field(None)
    ...

class EmptyDirVolumeDefSecretParams(pydantic.BaseModel):
    secret_name: str = pydantic.Field(..., description='The name of the secret. Must be a valid DNS subdomain name.\n')
    optional: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the secret or the secret's keys must be defined. Default: true\n")
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#secret\n')
    return_config: typing.Optional[list[models.aws_batch.SecretPathVolumeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.HostPathVolume
class HostPathVolumeDef(BaseClass):
    host_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path of the file or directory on the host to mount into containers on the pod. *Note*: HothPath Volumes present many security risks, and should be avoided when possible.')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['host_path', 'name', 'mount_path', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['empty_dir', 'host_path', 'secret']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.HostPathVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.HostPathVolumeDefConfig] = pydantic.Field(None)


class HostPathVolumeDefConfig(pydantic.BaseModel):
    empty_dir: typing.Optional[list[models.aws_batch.HostPathVolumeDefEmptyDirParams]] = pydantic.Field(None, description='Creates a Kubernetes EmptyDir volume.')
    host_path: typing.Optional[list[models.aws_batch.HostPathVolumeDefHostPathParams]] = pydantic.Field(None, description='Creates a Kubernetes HostPath volume.')
    secret: typing.Optional[list[models.aws_batch.HostPathVolumeDefSecretParams]] = pydantic.Field(None, description='Creates a Kubernetes Secret volume.')

class HostPathVolumeDefEmptyDirParams(pydantic.BaseModel):
    medium: typing.Optional[aws_cdk.aws_batch.EmptyDirMediumType] = pydantic.Field(None, description='The storage type to use for this Volume. Default: ``EmptyDirMediumType.DISK``\n')
    size_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The maximum size for this Volume. Default: - no size limit\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\n')
    return_config: typing.Optional[list[models.aws_batch.EmptyDirVolumeDefConfig]] = pydantic.Field(None)
    ...

class HostPathVolumeDefHostPathParams(pydantic.BaseModel):
    host_path: str = pydantic.Field(..., description='The path of the file or directory on the host to mount into containers on the pod. *Note*: HothPath Volumes present many security risks, and should be avoided when possible.\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\n')
    return_config: typing.Optional[list[models.aws_batch.HostPathVolumeDefConfig]] = pydantic.Field(None)
    ...

class HostPathVolumeDefSecretParams(pydantic.BaseModel):
    secret_name: str = pydantic.Field(..., description='The name of the secret. Must be a valid DNS subdomain name.\n')
    optional: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the secret or the secret's keys must be defined. Default: true\n")
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#secret\n')
    return_config: typing.Optional[list[models.aws_batch.SecretPathVolumeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.HostVolume
class HostVolumeDef(BaseClass):
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path on the host machine this container will have access to. Default: - Docker will choose the host path. The data may not persist after the containers that use it stop running.')
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the path on the container where this volume is mounted.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['host_path', 'container_path', 'name', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['efs', 'host']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.HostVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.HostVolumeDefConfig] = pydantic.Field(None)


class HostVolumeDefConfig(pydantic.BaseModel):
    efs: typing.Optional[list[models.aws_batch.HostVolumeDefEfsParams]] = pydantic.Field(None, description='Creates a Volume that uses an AWS Elastic File System (EFS);\nthis volume can grow and shrink as needed')
    host: typing.Optional[list[models.aws_batch.HostVolumeDefHostParams]] = pydantic.Field(None, description='Creates a Host volume.\nThis volume will persist on the host at the specified ``hostPath``.\nIf the ``hostPath`` is not specified, Docker will choose the host path. In this case,\nthe data may not persist after the containers that use it stop running.')

class HostVolumeDefEfsParams(pydantic.BaseModel):
    file_system: typing.Union[models.aws_efs.FileSystemDef] = pydantic.Field(..., description='The EFS File System that supports this volume.\n')
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, ``rootDirectory`` must either be omitted or set to ``/`` which enforces the path set on the EFS access point. If an access point is used, ``enableTransitEncryption`` must be ``true``. Default: - no accessPointId\n')
    enable_transit_encryption: typing.Optional[bool] = pydantic.Field(None, description='Enables encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Default: false\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume is used instead. Specifying ``/`` has the same effect as omitting this parameter. The maximum length is 4,096 characters. Default: - root of the EFS File System\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. The value must be between 0 and 65,535. Default: - chosen by the EFS Mount Helper\n')
    use_job_role: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use the AWS Batch job IAM role defined in a job definition when mounting the Amazon EFS file system. If specified, ``enableTransitEncryption`` must be ``true``. Default: false\n')
    container_path: str = pydantic.Field(..., description='the path on the container where this volume is mounted.\n')
    name: str = pydantic.Field(..., description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false\n\n:see: https://docs.aws.amazon.com/batch/latest/userguide/efs-volumes.html\n')
    return_config: typing.Optional[list[models.aws_batch.EfsVolumeDefConfig]] = pydantic.Field(None)
    ...

class HostVolumeDefHostParams(pydantic.BaseModel):
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path on the host machine this container will have access to. Default: - Docker will choose the host path. The data may not persist after the containers that use it stop running.\n')
    container_path: str = pydantic.Field(..., description='the path on the container where this volume is mounted.\n')
    name: str = pydantic.Field(..., description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false')
    return_config: typing.Optional[list[models.aws_batch.HostVolumeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.OptimalInstanceType
class OptimalInstanceTypeDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['same_instance_class_as']
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.OptimalInstanceType'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.OptimalInstanceTypeDefConfig] = pydantic.Field(None)


class OptimalInstanceTypeDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[models.aws_batch.OptimalInstanceTypeDefOfParams]] = pydantic.Field(None, description='Instance type for EC2 instances.\nThis class takes a combination of a class and size.\n\nBe aware that not all combinations of class and size are available, and not all\nclasses are available in all regions.')
    same_instance_class_as: typing.Optional[list[models.aws_batch.OptimalInstanceTypeDefSameInstanceClassAsParams]] = pydantic.Field(None, description='')

class OptimalInstanceTypeDefOfParams(pydantic.BaseModel):
    instance_class: aws_cdk.aws_ec2.InstanceClass = pydantic.Field(..., description='-\n')
    instance_size: aws_cdk.aws_ec2.InstanceSize = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_ec2.InstanceTypeDefConfig]] = pydantic.Field(None)
    ...

class OptimalInstanceTypeDefSameInstanceClassAsParams(pydantic.BaseModel):
    other: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.Reason
class ReasonDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['custom']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.Reason'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.ReasonDefConfig] = pydantic.Field(None)


class ReasonDefConfig(pydantic.BaseModel):
    custom: typing.Optional[list[models.aws_batch.ReasonDefCustomParams]] = pydantic.Field(None, description='A custom Reason that can match on multiple conditions.\nNote that all specified conditions must be met for this reason to match.')

class ReasonDefCustomParams(pydantic.BaseModel):
    on_exit_code: typing.Optional[str] = pydantic.Field(None, description="A glob string that will match on the job exit code. For example, ``'40*'`` will match 400, 404, 40123456789012 Default: - will not match on the exit code\n")
    on_reason: typing.Optional[str] = pydantic.Field(None, description="A glob string that will match on the reason returned by the exiting job For example, ``'CannotPullContainerError*'`` indicates that container needed to start the job could not be pulled. Default: - will not match on the reason\n")
    on_status_reason: typing.Optional[str] = pydantic.Field(None, description="A glob string that will match on the statusReason returned by the exiting job. For example, ``'Host EC2*'`` indicates that the spot instance has been reclaimed. Default: - will not match on the status reason")
    return_config: typing.Optional[list[models.aws_batch.ReasonDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.RetryStrategy
class RetryStrategyDef(BaseClass):
    action: typing.Union[aws_cdk.aws_batch.Action, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    on: typing.Union[models.aws_batch.ReasonDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['action', 'on']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.RetryStrategy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.RetryStrategyDefConfig] = pydantic.Field(None)


class RetryStrategyDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[models.aws_batch.RetryStrategyDefOfParams]] = pydantic.Field(None, description='Create a new RetryStrategy.')
    on_config: typing.Optional[models.aws_batch.ReasonDefConfig] = pydantic.Field(None)

class RetryStrategyDefOfParams(pydantic.BaseModel):
    action: aws_cdk.aws_batch.Action = pydantic.Field(..., description='-\n')
    on: models.aws_batch.ReasonDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_batch.RetryStrategyDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.Secret
class SecretDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['grant_read']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_secrets_manager', 'from_secrets_manager_version', 'from_ssm_parameter']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.Secret'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_secrets_manager', 'from_secrets_manager_version', 'from_ssm_parameter']
    ...


    from_secrets_manager: typing.Optional[models.aws_batch.SecretDefFromSecretsManagerParams] = pydantic.Field(None, description='Creates a environment variable value from a secret stored in AWS Secrets Manager.')
    from_secrets_manager_version: typing.Optional[models.aws_batch.SecretDefFromSecretsManagerVersionParams] = pydantic.Field(None, description='Creates a environment variable value from a secret stored in AWS Secrets Manager.')
    from_ssm_parameter: typing.Optional[models.aws_batch.SecretDefFromSsmParameterParams] = pydantic.Field(None, description='Creates an environment variable value from a parameter stored in AWS Systems Manager Parameter Store.')
    resource_config: typing.Optional[models.aws_batch.SecretDefConfig] = pydantic.Field(None)


class SecretDefConfig(pydantic.BaseModel):
    grant_read: typing.Optional[list[models.aws_batch.SecretDefGrantReadParams]] = pydantic.Field(None, description='Grants reading the secret to a principal.')

class SecretDefFromSecretsManagerParams(pydantic.BaseModel):
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='the secret stored in AWS Secrets Manager.\n')
    field: typing.Optional[str] = pydantic.Field(None, description='the name of the field with the value that you want to set as the environment variable value. Only values in JSON format are supported. If you do not specify a JSON field, then the full content of the secret is used.')
    ...

class SecretDefFromSecretsManagerVersionParams(pydantic.BaseModel):
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='the secret stored in AWS Secrets Manager.\n')
    version_info: typing.Union[models.aws_batch.SecretVersionInfoDef, dict[str, typing.Any]] = pydantic.Field(..., description='the version information to reference the secret.\n')
    field: typing.Optional[str] = pydantic.Field(None, description='the name of the field with the value that you want to set as the environment variable value. Only values in JSON format are supported. If you do not specify a JSON field, then the full content of the secret is used.')
    ...

class SecretDefFromSsmParameterParams(pydantic.BaseModel):
    parameter: typing.Union[models.aws_ssm.StringListParameterDef, models.aws_ssm.StringParameterDef] = pydantic.Field(..., description='-')
    ...

class SecretDefGrantReadParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.SecretPathVolume
class SecretPathVolumeDef(BaseClass):
    secret_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the secret. Must be a valid DNS subdomain name.')
    optional: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the secret or the secret's keys must be defined. Default: true\n")
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['secret_name', 'optional', 'name', 'mount_path', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['empty_dir', 'host_path', 'secret']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.SecretPathVolume'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.SecretPathVolumeDefConfig] = pydantic.Field(None)


class SecretPathVolumeDefConfig(pydantic.BaseModel):
    empty_dir: typing.Optional[list[models.aws_batch.SecretPathVolumeDefEmptyDirParams]] = pydantic.Field(None, description='Creates a Kubernetes EmptyDir volume.')
    host_path: typing.Optional[list[models.aws_batch.SecretPathVolumeDefHostPathParams]] = pydantic.Field(None, description='Creates a Kubernetes HostPath volume.')
    secret: typing.Optional[list[models.aws_batch.SecretPathVolumeDefSecretParams]] = pydantic.Field(None, description='Creates a Kubernetes Secret volume.')

class SecretPathVolumeDefEmptyDirParams(pydantic.BaseModel):
    medium: typing.Optional[aws_cdk.aws_batch.EmptyDirMediumType] = pydantic.Field(None, description='The storage type to use for this Volume. Default: ``EmptyDirMediumType.DISK``\n')
    size_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The maximum size for this Volume. Default: - no size limit\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\n')
    return_config: typing.Optional[list[models.aws_batch.EmptyDirVolumeDefConfig]] = pydantic.Field(None)
    ...

class SecretPathVolumeDefHostPathParams(pydantic.BaseModel):
    host_path: str = pydantic.Field(..., description='The path of the file or directory on the host to mount into containers on the pod. *Note*: HothPath Volumes present many security risks, and should be avoided when possible.\n')
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\n')
    return_config: typing.Optional[list[models.aws_batch.HostPathVolumeDefConfig]] = pydantic.Field(None)
    ...

class SecretPathVolumeDefSecretParams(pydantic.BaseModel):
    secret_name: str = pydantic.Field(..., description='The name of the secret. Must be a valid DNS subdomain name.\n')
    optional: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether the secret or the secret's keys must be defined. Default: true\n")
    name: str = pydantic.Field(..., description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#secret\n')
    return_config: typing.Optional[list[models.aws_batch.SecretPathVolumeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_batch.EcsEc2ContainerDefinition
class EcsEc2ContainerDefinitionDef(BaseConstruct):
    gpu: typing.Union[int, float, None] = pydantic.Field(None, description="The number of physical GPUs to reserve for the container. Make sure that the number of GPUs reserved for all containers in a job doesn't exceed the number of available GPUs on the compute resource that the job is launched on. Default: - no gpus\n")
    privileged: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given elevated permissions on the host container instance (similar to the root user). Default: false\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Limits to set for the user this docker container will run as. Default: - no ulimits\n')
    cpu: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of vCPUs reserved for the container. Each vCPU is equivalent to 1,024 CPU shares. For containers running on EC2 resources, you must specify at least one vCPU.\n')
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image that this container will run.\n')
    memory: typing.Union[models.SizeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The memory hard limit present to the container. If your container attempts to exceed the memory specified, the container is terminated. You must specify at least 4 MiB of memory for a job.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. Default: - no command\n")
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description="The environment variables to pass to a container. Cannot start with ``AWS_BATCH``. We don't recommend using plaintext environment variables for sensitive information, such as credential data. Default: - no environment variables\n")
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used by Amazon ECS container and AWS Fargate agents to make AWS API calls on your behalf. Default: - a Role will be created\n')
    job_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that the container can assume. Default: - no job role\n')
    linux_parameters: typing.Optional[models.aws_batch.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as details for device mappings. Default: none\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The loging configuration for this Job. Default: - the log configuration of the Docker daemon\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='Gives the container readonly access to its root filesystem. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_batch.SecretDef]] = pydantic.Field(None, description='A map from environment variable names to the secrets for the container. Allows your job definitions to reference the secret by the environment variable name defined in this property. Default: - no secrets\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user name to use inside the container. Default: - no user\n')
    volumes: typing.Optional[typing.Sequence[models.aws_batch.EcsVolumeDef]] = pydantic.Field(None, description='The volumes to mount to this container. Automatically added to the job definition. Default: - no volumes')
    _init_params: typing.ClassVar[list[str]] = ['gpu', 'privileged', 'ulimits', 'cpu', 'image', 'memory', 'command', 'environment', 'execution_role', 'job_role', 'linux_parameters', 'logging', 'readonly_root_filesystem', 'secrets', 'user', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['add_ulimit', 'add_volume']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsEc2ContainerDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EcsEc2ContainerDefinitionDefConfig] = pydantic.Field(None)


class EcsEc2ContainerDefinitionDefConfig(pydantic.BaseModel):
    add_ulimit: typing.Optional[list[models.aws_batch.EcsEc2ContainerDefinitionDefAddUlimitParams]] = pydantic.Field(None, description='Add a ulimit to this container.')
    add_volume: typing.Optional[list[models.aws_batch.EcsEc2ContainerDefinitionDefAddVolumeParams]] = pydantic.Field(None, description='Add a Volume to this container.')
    execution_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)
    memory_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)

class EcsEc2ContainerDefinitionDefAddUlimitParams(pydantic.BaseModel):
    hard_limit: typing.Union[int, float] = pydantic.Field(..., description='The hard limit for this resource. The container will be terminated if it exceeds this limit.\n')
    name: aws_cdk.aws_batch.UlimitName = pydantic.Field(..., description='The resource to limit.\n')
    soft_limit: typing.Union[int, float] = pydantic.Field(..., description='The reservation for this resource. The container will not be terminated if it exceeds this limit.')
    ...

class EcsEc2ContainerDefinitionDefAddVolumeParams(pydantic.BaseModel):
    volume: models.aws_batch.EcsVolumeDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.EcsFargateContainerDefinition
class EcsFargateContainerDefinitionDef(BaseConstruct):
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether the job has a public IP address. For a job that's running on Fargate resources in a private subnet to send outbound traffic to the internet (for example, to pull container images), the private subnet requires a NAT gateway be attached to route requests to the internet. Default: false\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size for ephemeral storage. Default: - 20 GiB\n')
    fargate_platform_version: typing.Optional[aws_cdk.aws_ecs.FargatePlatformVersion] = pydantic.Field(None, description='Which version of Fargate to use when running this container. Default: LATEST\n')
    cpu: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of vCPUs reserved for the container. Each vCPU is equivalent to 1,024 CPU shares. For containers running on EC2 resources, you must specify at least one vCPU.\n')
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image that this container will run.\n')
    memory: typing.Union[models.SizeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The memory hard limit present to the container. If your container attempts to exceed the memory specified, the container is terminated. You must specify at least 4 MiB of memory for a job.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. Default: - no command\n")
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description="The environment variables to pass to a container. Cannot start with ``AWS_BATCH``. We don't recommend using plaintext environment variables for sensitive information, such as credential data. Default: - no environment variables\n")
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used by Amazon ECS container and AWS Fargate agents to make AWS API calls on your behalf. Default: - a Role will be created\n')
    job_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that the container can assume. Default: - no job role\n')
    linux_parameters: typing.Optional[models.aws_batch.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as details for device mappings. Default: none\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The loging configuration for this Job. Default: - the log configuration of the Docker daemon\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='Gives the container readonly access to its root filesystem. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_batch.SecretDef]] = pydantic.Field(None, description='A map from environment variable names to the secrets for the container. Allows your job definitions to reference the secret by the environment variable name defined in this property. Default: - no secrets\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user name to use inside the container. Default: - no user\n')
    volumes: typing.Optional[typing.Sequence[models.aws_batch.EcsVolumeDef]] = pydantic.Field(None, description='The volumes to mount to this container. Automatically added to the job definition. Default: - no volumes')
    _init_params: typing.ClassVar[list[str]] = ['assign_public_ip', 'ephemeral_storage_size', 'fargate_platform_version', 'cpu', 'image', 'memory', 'command', 'environment', 'execution_role', 'job_role', 'linux_parameters', 'logging', 'readonly_root_filesystem', 'secrets', 'user', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['add_volume']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsFargateContainerDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EcsFargateContainerDefinitionDefConfig] = pydantic.Field(None)


class EcsFargateContainerDefinitionDefConfig(pydantic.BaseModel):
    add_volume: typing.Optional[list[models.aws_batch.EcsFargateContainerDefinitionDefAddVolumeParams]] = pydantic.Field(None, description='Add a Volume to this container.')
    execution_role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)
    memory_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)

class EcsFargateContainerDefinitionDefAddVolumeParams(pydantic.BaseModel):
    volume: models.aws_batch.EcsVolumeDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.EcsJobDefinition
class EcsJobDefinitionDef(BaseConstruct):
    container: typing.Union[_REQUIRED_INIT_PARAM, models.aws_batch.EcsEc2ContainerDefinitionDef, models.aws_batch.EcsFargateContainerDefinitionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The container that this job will run.\n')
    propagate_tags: typing.Optional[bool] = pydantic.Field(None, description='Whether to propogate tags from the JobDefinition to the ECS task that Batch spawns. Default: false\n')
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of this job definition. Default: - generated by CloudFormation\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The default parameters passed to the container These parameters can be referenced in the ``command`` that you give to the container. Default: none\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a job. The job is retried on failure the same number of attempts as the value. Default: 1\n')
    retry_strategies: typing.Optional[typing.Sequence[models.aws_batch.RetryStrategyDef]] = pydantic.Field(None, description='Defines the retry behavior for this job. Default: - no ``RetryStrategy``\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of this Job. Only used in Fairshare Scheduling to decide which job to run first when there are multiple jobs with the same share identifier. Default: none\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, Batch terminates your jobs if they aren't finished. Default: - no timeout")
    _init_params: typing.ClassVar[list[str]] = ['container', 'propagate_tags', 'job_definition_name', 'parameters', 'retry_attempts', 'retry_strategies', 'scheduling_priority', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_retry_strategy', 'apply_removal_policy', 'grant_submit_job']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_job_definition_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsJobDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_job_definition_arn']
    ...


    from_job_definition_arn: typing.Optional[models.aws_batch.EcsJobDefinitionDefFromJobDefinitionArnParams] = pydantic.Field(None, description='Import a JobDefinition by its arn.')
    resource_config: typing.Optional[models.aws_batch.EcsJobDefinitionDefConfig] = pydantic.Field(None)


class EcsJobDefinitionDefConfig(pydantic.BaseModel):
    add_retry_strategy: typing.Optional[list[models.aws_batch.EcsJobDefinitionDefAddRetryStrategyParams]] = pydantic.Field(None, description='Add a RetryStrategy to this JobDefinition.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_submit_job: typing.Optional[list[models.aws_batch.EcsJobDefinitionDefGrantSubmitJobParams]] = pydantic.Field(None, description='Grants the ``batch:submitJob`` permission to the identity on both this job definition and the ``queue``.')
    container_config: typing.Optional[models._interface_methods.AwsBatchIEcsContainerDefinitionDefConfig] = pydantic.Field(None)

class EcsJobDefinitionDefAddRetryStrategyParams(pydantic.BaseModel):
    strategy: models.aws_batch.RetryStrategyDef = pydantic.Field(..., description='-')
    ...

class EcsJobDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class EcsJobDefinitionDefFromJobDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    job_definition_arn: str = pydantic.Field(..., description='-')
    ...

class EcsJobDefinitionDefGrantSubmitJobParams(pydantic.BaseModel):
    identity: models.AnyResource = pydantic.Field(..., description='-\n')
    queue: typing.Union[models.aws_batch.JobQueueDef] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.EksContainerDefinition
class EksContainerDefinitionDef(BaseConstruct):
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image that this container will run.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The entrypoint for the container. This isn\'t run within a shell. If this isn\'t specified, the ``ENTRYPOINT`` of the container image is used. Environment variable references are expanded using the container\'s environment. If the referenced environment variable doesn\'t exist, the reference in the command isn\'t changed. For example, if the reference is to ``"$(NAME1)"`` and the ``NAME1`` environment variable doesn\'t exist, the command string will remain ``"$(NAME1)."`` ``$$`` is replaced with ``$`` and the resulting string isn\'t expanded. For example, ``$$(VAR_NAME)`` will be passed as ``$(VAR_NAME)`` whether or not the ``VAR_NAME`` environment variable exists. The entrypoint can\'t be updated. Default: - no command\n')
    cpu_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The hard limit of CPUs to present to this container. Must be an even multiple of 0.25. If your container attempts to exceed this limit, it will be terminated. At least one of ``cpuReservation`` and ``cpuLimit`` is required. If both are specified, then ``cpuLimit`` must be at least as large as ``cpuReservation``. Default: - No CPU limit\n')
    cpu_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit of CPUs to reserve for the container Must be an even multiple of 0.25. The container will given at least this many CPUs, but may consume more. At least one of ``cpuReservation`` and ``cpuLimit`` is required. If both are specified, then ``cpuLimit`` must be at least as large as ``cpuReservation``. Default: - No CPUs reserved\n')
    env: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to this container. *Note*: Environment variables cannot start with "AWS_BATCH". This naming convention is reserved for variables that AWS Batch sets. Default: - no environment variables\n')
    gpu_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The hard limit of GPUs to present to this container. If your container attempts to exceed this limit, it will be terminated. If both ``gpuReservation`` and ``gpuLimit`` are specified, then ``gpuLimit`` must be equal to ``gpuReservation``. Default: - No GPU limit\n')
    gpu_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit of CPUs to reserve for the container Must be an even multiple of 0.25. The container will given at least this many CPUs, but may consume more. If both ``gpuReservation`` and ``gpuLimit`` are specified, then ``gpuLimit`` must be equal to ``gpuReservation``. Default: - No GPUs reserved\n')
    image_pull_policy: typing.Optional[aws_cdk.aws_batch.ImagePullPolicy] = pydantic.Field(None, description='The image pull policy for this container. Default: - ``ALWAYS`` if the ``:latest`` tag is specified, ``IF_NOT_PRESENT`` otherwise\n')
    memory_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, it will be terminated. Must be larger that 4 MiB At least one of ``memoryLimit`` and ``memoryReservation`` is required *Note*: To maximize your resource utilization, provide your jobs with as much memory as possible for the specific instance type that you are using. Default: - No memory limit\n')
    memory_reservation: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. Your container will be given at least this much memory, but may consume more. Must be larger that 4 MiB When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of ``memoryLimit`` and ``memoryReservation`` is required. If both are specified, then ``memoryLimit`` must be equal to ``memoryReservation`` *Note*: To maximize your resource utilization, provide your jobs with as much memory as possible for the specific instance type that you are using. Default: - No memory reserved\n')
    name: typing.Optional[str] = pydantic.Field(None, description="The name of this container. Default: : ``'Default'``\n")
    privileged: typing.Optional[bool] = pydantic.Field(None, description='If specified, gives this container elevated permissions on the host container instance. The level of permissions are similar to the root user permissions. This parameter maps to ``privileged`` policy in the Privileged pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='If specified, gives this container readonly access to its root file system. This parameter maps to ``ReadOnlyRootFilesystem`` policy in the Volumes and file systems pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: false\n')
    run_as_group: typing.Union[int, float, None] = pydantic.Field(None, description="If specified, the container is run as the specified group ID (``gid``). If this parameter isn't specified, the default is the group that's specified in the image metadata. This parameter maps to ``RunAsGroup`` and ``MustRunAs`` policy in the Users and groups pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: none\n")
    run_as_root: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container is run as a user with a ``uid`` other than 0. Otherwise, no such rule is enforced. This parameter maps to ``RunAsUser`` and ``MustRunAsNonRoot`` policy in the Users and groups pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: - the container is *not* required to run as a non-root user\n')
    run_as_user: typing.Union[int, float, None] = pydantic.Field(None, description='If specified, this container is run as the specified user ID (``uid``). This parameter maps to ``RunAsUser`` and ``MustRunAs`` policy in the Users and groups pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: - the user that is specified in the image metadata.\n')
    volumes: typing.Optional[typing.Sequence[models.aws_batch.EksVolumeDef]] = pydantic.Field(None, description='The Volumes to mount to this container. Automatically added to the Pod. Default: - no volumes')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'cpu_limit', 'cpu_reservation', 'env', 'gpu_limit', 'gpu_reservation', 'image_pull_policy', 'memory_limit', 'memory_reservation', 'name', 'privileged', 'readonly_root_filesystem', 'run_as_group', 'run_as_root', 'run_as_user', 'volumes']
    _method_names: typing.ClassVar[list[str]] = ['add_volume']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EksContainerDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EksContainerDefinitionDefConfig] = pydantic.Field(None)


class EksContainerDefinitionDefConfig(pydantic.BaseModel):
    add_volume: typing.Optional[list[models.aws_batch.EksContainerDefinitionDefAddVolumeParams]] = pydantic.Field(None, description='Mount a Volume to this container.\nAutomatically added to the Pod.')
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)

class EksContainerDefinitionDefAddVolumeParams(pydantic.BaseModel):
    volume: models.aws_batch.EksVolumeDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.EksJobDefinition
class EksJobDefinitionDef(BaseConstruct):
    container: typing.Union[models.aws_batch.EksContainerDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The container this Job Definition will run.\n')
    dns_policy: typing.Optional[aws_cdk.aws_batch.DnsPolicy] = pydantic.Field(None, description='The DNS Policy of the pod used by this Job Definition. Default: ``DnsPolicy.CLUSTER_FIRST``\n')
    service_account: typing.Optional[str] = pydantic.Field(None, description="The name of the service account that's used to run the container. service accounts are Kubernetes method of identification and authentication, roughly analogous to IAM users. Default: - the default service account of the container\n")
    use_host_network: typing.Optional[bool] = pydantic.Field(None, description="If specified, the Pod used by this Job Definition will use the host's network IP address. Otherwise, the Kubernetes pod networking model is enabled. Most AWS Batch workloads are egress-only and don't require the overhead of IP allocation for each pod for incoming connections. Default: true\n")
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of this job definition. Default: - generated by CloudFormation\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The default parameters passed to the container These parameters can be referenced in the ``command`` that you give to the container. Default: none\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a job. The job is retried on failure the same number of attempts as the value. Default: 1\n')
    retry_strategies: typing.Optional[typing.Sequence[models.aws_batch.RetryStrategyDef]] = pydantic.Field(None, description='Defines the retry behavior for this job. Default: - no ``RetryStrategy``\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of this Job. Only used in Fairshare Scheduling to decide which job to run first when there are multiple jobs with the same share identifier. Default: none\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, Batch terminates your jobs if they aren't finished. Default: - no timeout")
    _init_params: typing.ClassVar[list[str]] = ['container', 'dns_policy', 'service_account', 'use_host_network', 'job_definition_name', 'parameters', 'retry_attempts', 'retry_strategies', 'scheduling_priority', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_retry_strategy', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_eks_job_definition_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EksJobDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_eks_job_definition_arn']
    ...


    from_eks_job_definition_arn: typing.Optional[models.aws_batch.EksJobDefinitionDefFromEksJobDefinitionArnParams] = pydantic.Field(None, description='Import an EksJobDefinition by its arn.')
    resource_config: typing.Optional[models.aws_batch.EksJobDefinitionDefConfig] = pydantic.Field(None)


class EksJobDefinitionDefConfig(pydantic.BaseModel):
    add_retry_strategy: typing.Optional[list[models.aws_batch.EksJobDefinitionDefAddRetryStrategyParams]] = pydantic.Field(None, description='Add a RetryStrategy to this JobDefinition.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    container_config: typing.Optional[models.aws_batch.EksContainerDefinitionDefConfig] = pydantic.Field(None)

class EksJobDefinitionDefAddRetryStrategyParams(pydantic.BaseModel):
    strategy: models.aws_batch.RetryStrategyDef = pydantic.Field(..., description='-')
    ...

class EksJobDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class EksJobDefinitionDefFromEksJobDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    eks_job_definition_arn: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.FairshareSchedulingPolicy
class FairshareSchedulingPolicyDef(BaseConstruct):
    compute_reservation: typing.Union[int, float, None] = pydantic.Field(None, description="Used to calculate the percentage of the maximum available vCPU to reserve for share identifiers not present in the Queue. The percentage reserved is defined by the Scheduler as: ``(computeReservation/100)^ActiveFairShares`` where ``ActiveFairShares`` is the number of active fair share identifiers. For example, a computeReservation value of 50 indicates that AWS Batch reserves 50% of the maximum available vCPU if there's only one fair share identifier. It reserves 25% if there are two fair share identifiers. It reserves 12.5% if there are three fair share identifiers. A computeReservation value of 25 indicates that AWS Batch should reserve 25% of the maximum available vCPU if there's only one fair share identifier, 6.25% if there are two fair share identifiers, and 1.56% if there are three fair share identifiers. Default: - no vCPU is reserved\n")
    scheduling_policy_name: typing.Optional[str] = pydantic.Field(None, description='The name of this SchedulingPolicy. Default: - generated by CloudFormation\n')
    share_decay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time to use to measure the usage of each job. The usage is used to calculate a fair share percentage for each fair share identifier currently in the Queue. A value of zero (0) indicates that only current usage is measured. The decay is linear and gives preference to newer jobs. The maximum supported value is 604800 seconds (1 week). Default: - 0: only the current job usage is considered\n')
    shares: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.ShareDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The shares that this Scheduling Policy applies to. *Note*: It is possible to submit Jobs to the queue with Share Identifiers that are not recognized by the Scheduling Policy. Default: - no shares')
    _init_params: typing.ClassVar[list[str]] = ['compute_reservation', 'scheduling_policy_name', 'share_decay', 'shares']
    _method_names: typing.ClassVar[list[str]] = ['add_share', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_fairshare_scheduling_policy_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.FairshareSchedulingPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_fairshare_scheduling_policy_arn']
    ...


    from_fairshare_scheduling_policy_arn: typing.Optional[models.aws_batch.FairshareSchedulingPolicyDefFromFairshareSchedulingPolicyArnParams] = pydantic.Field(None, description='Reference an exisiting Scheduling Policy by its ARN.')
    resource_config: typing.Optional[models.aws_batch.FairshareSchedulingPolicyDefConfig] = pydantic.Field(None)


class FairshareSchedulingPolicyDefConfig(pydantic.BaseModel):
    add_share: typing.Optional[list[models.aws_batch.FairshareSchedulingPolicyDefAddShareParams]] = pydantic.Field(None, description='Add a share this to this Fairshare SchedulingPolicy.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class FairshareSchedulingPolicyDefAddShareParams(pydantic.BaseModel):
    share_identifier: str = pydantic.Field(..., description='The identifier of this Share. All jobs that specify this share identifier when submitted to the queue will be considered as part of this Share.\n')
    weight_factor: typing.Union[int, float] = pydantic.Field(..., description='The weight factor given to this Share. The Scheduler decides which jobs to put in the Compute Environment such that the following ratio is equal for each job: ``sharevCpu / weightFactor``, where ``sharevCpu`` is the total amount of vCPU given to that particular share; that is, the sum of the vCPU of each job currently in the Compute Environment for that share. See the readme of this module for a detailed example that shows how these are used, how it relates to ``computeReservation``, and how ``shareDecay`` affects these calculations.')
    ...

class FairshareSchedulingPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class FairshareSchedulingPolicyDefFromFairshareSchedulingPolicyArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    fairshare_scheduling_policy_arn: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.FargateComputeEnvironment
class FargateComputeEnvironmentDef(BaseConstruct):
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='VPC in which this Compute Environment will launch Instances.\n')
    maxv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum vCpus this ``ManagedComputeEnvironment`` can scale up to. Each vCPU is equivalent to 1024 CPU shares. *Note*: if this Compute Environment uses EC2 resources (not Fargate) with either ``AllocationStrategy.BEST_FIT_PROGRESSIVE`` or ``AllocationStrategy.SPOT_CAPACITY_OPTIMIZED``, or ``AllocationStrategy.BEST_FIT`` with Spot instances, The scheduler may exceed this number by at most one of the instances specified in ``instanceTypes`` or ``instanceClasses``. Default: 256\n')
    replace_compute_environment: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether this Compute Environment is replaced if an update is made that requires replacing its instances. To enable more properties to be updated, set this property to ``false``. When changing the value of this property to false, do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the UPDATE_ROLLBACK_FAILED state. You can't update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. The properties which require a replacement of the Compute Environment are: Default: false\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups this Compute Environment will launch instances in. Default: new security groups will be created\n')
    spot: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use spot instances. Spot instances are less expensive EC2 instances that can be reclaimed by EC2 at any time; your job will be given two minutes of notice before reclamation. Default: false\n')
    terminate_on_update: typing.Optional[bool] = pydantic.Field(None, description="Whether or not any running jobs will be immediately terminated when an infrastructure update occurs. If this is enabled, any terminated jobs may be retried, depending on the job's retry policy. Default: false\n")
    update_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Only meaningful if ``terminateOnUpdate`` is ``false``. If so, when an infrastructure update is triggered, any running jobs will be allowed to run until ``updateTimeout`` has expired. Default: 30 minutes\n')
    update_to_latest_image_version: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the AMI is updated to the latest one supported by Batch when an infrastructure update occurs. If you specify a specific AMI, this property will be ignored. Default: true\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC Subnets this Compute Environment will launch instances in. Default: new subnets will be created\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'maxv_cpus', 'replace_compute_environment', 'security_groups', 'spot', 'terminate_on_update', 'update_timeout', 'update_to_latest_image_version', 'vpc_subnets', 'compute_environment_name', 'enabled', 'service_role']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_fargate_compute_environment_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.FargateComputeEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_fargate_compute_environment_arn']
    ...


    from_fargate_compute_environment_arn: typing.Optional[models.aws_batch.FargateComputeEnvironmentDefFromFargateComputeEnvironmentArnParams] = pydantic.Field(None, description='Reference an existing FargateComputeEnvironment by its arn.')
    resource_config: typing.Optional[models.aws_batch.FargateComputeEnvironmentDefConfig] = pydantic.Field(None)


class FargateComputeEnvironmentDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class FargateComputeEnvironmentDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class FargateComputeEnvironmentDefFromFargateComputeEnvironmentArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    fargate_compute_environment_arn: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.JobQueue
class JobQueueDef(BaseConstruct):
    compute_environments: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.OrderedComputeEnvironmentDef, dict[str, typing.Any]]]] = pydantic.Field(None, description="The set of compute environments mapped to a job queue and their order relative to each other. The job scheduler uses this parameter to determine which compute environment runs a specific job. Compute environments must be in the VALID state before you can associate them with a job queue. You can associate up to three compute environments with a job queue. All of the compute environments must be either EC2 (EC2 or SPOT) or Fargate (FARGATE or FARGATE_SPOT); EC2 and Fargate compute environments can't be mixed. *Note*: All compute environments that are associated with a job queue must share the same architecture. AWS Batch doesn't support mixing compute environment architecture types in a single job queue. Default: none\n")
    enabled: typing.Optional[bool] = pydantic.Field(None, description="If the job queue is enabled, it is able to accept jobs. Otherwise, new jobs can't be added to the queue, but jobs already in the queue can finish. Default: true\n")
    job_queue_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job queue. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_) Default: - no name\n')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of the job queue. Job queues with a higher priority are evaluated first when associated with the same compute environment. Priority is determined in descending order. For example, a job queue with a priority of 10 is given scheduling preference over a job queue with a priority of 1. Default: 1\n')
    scheduling_policy: typing.Optional[typing.Union[models.aws_batch.FairshareSchedulingPolicyDef]] = pydantic.Field(None, description='The SchedulingPolicy for this JobQueue. Instructs the Scheduler how to schedule different jobs. Default: - no scheduling policy')
    _init_params: typing.ClassVar[list[str]] = ['compute_environments', 'enabled', 'job_queue_name', 'priority', 'scheduling_policy']
    _method_names: typing.ClassVar[list[str]] = ['add_compute_environment', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_job_queue_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.JobQueue'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_job_queue_arn']
    ...


    from_job_queue_arn: typing.Optional[models.aws_batch.JobQueueDefFromJobQueueArnParams] = pydantic.Field(None, description='refer to an existing JobQueue by its arn.')
    resource_config: typing.Optional[models.aws_batch.JobQueueDefConfig] = pydantic.Field(None)


class JobQueueDefConfig(pydantic.BaseModel):
    add_compute_environment: typing.Optional[list[models.aws_batch.JobQueueDefAddComputeEnvironmentParams]] = pydantic.Field(None, description='Add a ``ComputeEnvironment`` to this Queue.\nThe Queue will prefer lower-order ``ComputeEnvironment``s.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class JobQueueDefAddComputeEnvironmentParams(pydantic.BaseModel):
    compute_environment: typing.Union[models.aws_batch.FargateComputeEnvironmentDef, models.aws_batch.ManagedEc2EcsComputeEnvironmentDef, models.aws_batch.ManagedEc2EksComputeEnvironmentDef, models.aws_batch.UnmanagedComputeEnvironmentDef] = pydantic.Field(..., description='-\n')
    order: typing.Union[int, float] = pydantic.Field(..., description='-')
    ...

class JobQueueDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class JobQueueDefFromJobQueueArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    job_queue_arn: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.LinuxParameters
class LinuxParametersDef(BaseConstruct):
    init_process_enabled: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to run an init process inside the container that forwards signals and reaps processes. Default: false\n')
    max_swap: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The total amount of swap memory a container can use. This parameter will be translated to the --memory-swap option to docker run. This parameter is only supported when you are using the EC2 launch type. Accepted values are positive integers. Default: No swap.\n')
    shared_memory_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The value for the size of the /dev/shm volume. Default: No shared memory.\n')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description="This allows you to tune a container's memory swappiness behavior. This parameter maps to the --memory-swappiness option to docker run. The swappiness relates to the kernel's tendency to swap memory. A value of 0 will cause swapping to not happen unless absolutely necessary. A value of 100 will cause pages to be swapped very aggressively. This parameter is only supported when you are using the EC2 launch type. Accepted values are whole numbers between 0 and 100. If a value is not specified for maxSwap then this parameter is ignored. Default: 60")
    _init_params: typing.ClassVar[list[str]] = ['init_process_enabled', 'max_swap', 'shared_memory_size', 'swappiness']
    _method_names: typing.ClassVar[list[str]] = ['add_devices', 'add_tmpfs', 'render_linux_parameters']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.LinuxParameters'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.LinuxParametersDefConfig] = pydantic.Field(None)


class LinuxParametersDefConfig(pydantic.BaseModel):
    add_devices: typing.Optional[list[models.aws_batch.LinuxParametersDefAddDevicesParams]] = pydantic.Field(None, description='Adds one or more host devices to a container.')
    add_tmpfs: typing.Optional[list[models.aws_batch.LinuxParametersDefAddTmpfsParams]] = pydantic.Field(None, description='Specifies the container path, mount options, and size (in MiB) of the tmpfs mount for a container.\nOnly works with EC2 launch type.')
    render_linux_parameters: typing.Optional[list[models.aws_batch.LinuxParametersDefRenderLinuxParametersParams]] = pydantic.Field(None, description="Renders the Linux parameters to the Batch version of this resource, which does not have 'capabilities' and requires tmpfs.containerPath to be defined.")

class LinuxParametersDefAddDevicesParams(pydantic.BaseModel):
    device: list[models.aws_batch.DeviceDef] = pydantic.Field(...)
    ...

class LinuxParametersDefAddTmpfsParams(pydantic.BaseModel):
    tmpfs: list[models.aws_batch.TmpfsDef] = pydantic.Field(...)
    ...

class LinuxParametersDefRenderLinuxParametersParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_batch.ManagedEc2EcsComputeEnvironment
class ManagedEc2EcsComputeEnvironmentDef(BaseConstruct):
    allocation_strategy: typing.Optional[aws_cdk.aws_batch.AllocationStrategy] = pydantic.Field(None, description='The allocation strategy to use if not enough instances of the best fitting instance type can be allocated. Default: - ``BEST_FIT_PROGRESSIVE`` if not using Spot instances, ``SPOT_CAPACITY_OPTIMIZED`` if using Spot instances.\n')
    images: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.EcsMachineImageDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configure which AMIs this Compute Environment can launch. If you specify this property with only ``image`` specified, then the ``imageType`` will default to ``ECS_AL2``. *If your image needs GPU resources, specify ``ECS_AL2_NVIDIA``; otherwise, the instances will not be able to properly join the ComputeEnvironment*. Default: - ECS_AL2 for non-GPU instances, ECS_AL2_NVIDIA for GPU instances\n')
    instance_classes: typing.Optional[typing.Sequence[aws_cdk.aws_ec2.InstanceClass]] = pydantic.Field(None, description='The instance classes that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Batch will automatically choose the instance size. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    instance_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The execution Role that instances launched by this Compute Environment will use. Default: - a role will be created\n')
    instance_types: typing.Optional[typing.Sequence[models.aws_ec2.InstanceTypeDef]] = pydantic.Field(None, description='The instance types that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    launch_template: typing.Optional[typing.Union[models.aws_ec2.LaunchTemplateDef]] = pydantic.Field(None, description='The Launch Template that this Compute Environment will use to provision EC2 Instances. *Note*: if ``securityGroups`` is specified on both your launch template and this Compute Environment, **the ``securityGroup``s on the Compute Environment override the ones on the launch template. Default: no launch template\n')
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum vCPUs that an environment should maintain, even if the compute environment is DISABLED. Default: 0\n')
    placement_group: typing.Optional[typing.Union[models.aws_ec2.PlacementGroupDef]] = pydantic.Field(None, description='The EC2 placement group to associate with your compute resources. If you intend to submit multi-node parallel jobs to this Compute Environment, you should consider creating a cluster placement group and associate it with your compute resources. This keeps your multi-node parallel job on a logical grouping of instances within a single Availability Zone with high network flow potential. Default: - no placement group\n')
    spot_bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage that a Spot Instance price can be when compared with the On-Demand price for that instance type before instances are launched. For example, if your maximum percentage is 20%, the Spot price must be less than 20% of the current On-Demand price for that Instance. You always pay the lowest market price and never more than your maximum percentage. For most use cases, Batch recommends leaving this field empty. Implies ``spot == true`` if set Default: 100%\n')
    spot_fleet_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The service-linked role that Spot Fleet needs to launch instances on your behalf. Default: - a new role will be created\n')
    use_optimal_instance_classes: typing.Optional[bool] = pydantic.Field(None, description="Whether or not to use batch's optimal instance type. The optimal instance type is equivalent to adding the C4, M4, and R4 instance classes. You can specify other instance classes (of the same architecture) in addition to the optimal instance classes. Default: true\n")
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='VPC in which this Compute Environment will launch Instances.\n')
    maxv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum vCpus this ``ManagedComputeEnvironment`` can scale up to. Each vCPU is equivalent to 1024 CPU shares. *Note*: if this Compute Environment uses EC2 resources (not Fargate) with either ``AllocationStrategy.BEST_FIT_PROGRESSIVE`` or ``AllocationStrategy.SPOT_CAPACITY_OPTIMIZED``, or ``AllocationStrategy.BEST_FIT`` with Spot instances, The scheduler may exceed this number by at most one of the instances specified in ``instanceTypes`` or ``instanceClasses``. Default: 256\n')
    replace_compute_environment: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether this Compute Environment is replaced if an update is made that requires replacing its instances. To enable more properties to be updated, set this property to ``false``. When changing the value of this property to false, do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the UPDATE_ROLLBACK_FAILED state. You can't update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. The properties which require a replacement of the Compute Environment are: Default: false\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups this Compute Environment will launch instances in. Default: new security groups will be created\n')
    spot: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use spot instances. Spot instances are less expensive EC2 instances that can be reclaimed by EC2 at any time; your job will be given two minutes of notice before reclamation. Default: false\n')
    terminate_on_update: typing.Optional[bool] = pydantic.Field(None, description="Whether or not any running jobs will be immediately terminated when an infrastructure update occurs. If this is enabled, any terminated jobs may be retried, depending on the job's retry policy. Default: false\n")
    update_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Only meaningful if ``terminateOnUpdate`` is ``false``. If so, when an infrastructure update is triggered, any running jobs will be allowed to run until ``updateTimeout`` has expired. Default: 30 minutes\n')
    update_to_latest_image_version: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the AMI is updated to the latest one supported by Batch when an infrastructure update occurs. If you specify a specific AMI, this property will be ignored. Default: true\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC Subnets this Compute Environment will launch instances in. Default: new subnets will be created\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs')
    _init_params: typing.ClassVar[list[str]] = ['allocation_strategy', 'images', 'instance_classes', 'instance_role', 'instance_types', 'launch_template', 'minv_cpus', 'placement_group', 'spot_bid_percentage', 'spot_fleet_role', 'use_optimal_instance_classes', 'vpc', 'maxv_cpus', 'replace_compute_environment', 'security_groups', 'spot', 'terminate_on_update', 'update_timeout', 'update_to_latest_image_version', 'vpc_subnets', 'compute_environment_name', 'enabled', 'service_role']
    _method_names: typing.ClassVar[list[str]] = ['add_instance_class', 'add_instance_type', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_managed_ec2_ecs_compute_environment_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.ManagedEc2EcsComputeEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_managed_ec2_ecs_compute_environment_arn']
    ...


    from_managed_ec2_ecs_compute_environment_arn: typing.Optional[models.aws_batch.ManagedEc2EcsComputeEnvironmentDefFromManagedEc2EcsComputeEnvironmentArnParams] = pydantic.Field(None, description='refer to an existing ComputeEnvironment by its arn.')
    resource_config: typing.Optional[models.aws_batch.ManagedEc2EcsComputeEnvironmentDefConfig] = pydantic.Field(None)


class ManagedEc2EcsComputeEnvironmentDefConfig(pydantic.BaseModel):
    add_instance_class: typing.Optional[list[models.aws_batch.ManagedEc2EcsComputeEnvironmentDefAddInstanceClassParams]] = pydantic.Field(None, description='Add an instance class to this compute environment.')
    add_instance_type: typing.Optional[list[models.aws_batch.ManagedEc2EcsComputeEnvironmentDefAddInstanceTypeParams]] = pydantic.Field(None, description='Add an instance type to this compute environment.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class ManagedEc2EcsComputeEnvironmentDefAddInstanceClassParams(pydantic.BaseModel):
    instance_class: aws_cdk.aws_ec2.InstanceClass = pydantic.Field(..., description='-')
    ...

class ManagedEc2EcsComputeEnvironmentDefAddInstanceTypeParams(pydantic.BaseModel):
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='-')
    ...

class ManagedEc2EcsComputeEnvironmentDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ManagedEc2EcsComputeEnvironmentDefFromManagedEc2EcsComputeEnvironmentArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    managed_ec2_ecs_compute_environment_arn: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.ManagedEc2EksComputeEnvironment
class ManagedEc2EksComputeEnvironmentDef(BaseConstruct):
    eks_cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_eks.ClusterDef, models.aws_eks.FargateClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cluster that backs this Compute Environment. Required for Compute Environments running Kubernetes jobs. Please ensure that you have followed the steps at https://docs.aws.amazon.com/batch/latest/userguide/getting-started-eks.html before attempting to deploy a ``ManagedEc2EksComputeEnvironment`` that uses this cluster. If you do not follow the steps in the link, the deployment fail with a message that the compute environment did not stabilize.\n')
    kubernetes_namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The namespace of the Cluster.\n')
    allocation_strategy: typing.Optional[aws_cdk.aws_batch.AllocationStrategy] = pydantic.Field(None, description='The allocation strategy to use if not enough instances of the best fitting instance type can be allocated. Default: - ``BEST_FIT_PROGRESSIVE`` if not using Spot instances, ``SPOT_CAPACITY_OPTIMIZED`` if using Spot instances.\n')
    images: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.EksMachineImageDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configure which AMIs this Compute Environment can launch. Default: If ``imageKubernetesVersion`` is specified, - EKS_AL2 for non-GPU instances, EKS_AL2_NVIDIA for GPU instances, Otherwise, - ECS_AL2 for non-GPU instances, ECS_AL2_NVIDIA for GPU instances,\n')
    instance_classes: typing.Optional[typing.Sequence[aws_cdk.aws_ec2.InstanceClass]] = pydantic.Field(None, description='The instance types that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Batch will automatically choose the instance size. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    instance_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The execution Role that instances launched by this Compute Environment will use. Default: - a role will be created\n')
    instance_types: typing.Optional[typing.Sequence[models.aws_ec2.InstanceTypeDef]] = pydantic.Field(None, description='The instance types that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    launch_template: typing.Optional[typing.Union[models.aws_ec2.LaunchTemplateDef]] = pydantic.Field(None, description='The Launch Template that this Compute Environment will use to provision EC2 Instances. *Note*: if ``securityGroups`` is specified on both your launch template and this Compute Environment, **the ``securityGroup``s on the Compute Environment override the ones on the launch template.** Default: - no launch template\n')
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum vCPUs that an environment should maintain, even if the compute environment is DISABLED. Default: 0\n')
    placement_group: typing.Optional[typing.Union[models.aws_ec2.PlacementGroupDef]] = pydantic.Field(None, description='The EC2 placement group to associate with your compute resources. If you intend to submit multi-node parallel jobs to this Compute Environment, you should consider creating a cluster placement group and associate it with your compute resources. This keeps your multi-node parallel job on a logical grouping of instances within a single Availability Zone with high network flow potential. Default: - no placement group\n')
    spot_bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage that a Spot Instance price can be when compared with the On-Demand price for that instance type before instances are launched. For example, if your maximum percentage is 20%, the Spot price must be less than 20% of the current On-Demand price for that Instance. You always pay the lowest market price and never more than your maximum percentage. For most use cases, Batch recommends leaving this field empty. Implies ``spot == true`` if set Default: - 100%\n')
    use_optimal_instance_classes: typing.Optional[bool] = pydantic.Field(None, description="Whether or not to use batch's optimal instance type. The optimal instance type is equivalent to adding the C4, M4, and R4 instance classes. You can specify other instance classes (of the same architecture) in addition to the optimal instance classes. Default: true\n")
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='VPC in which this Compute Environment will launch Instances.\n')
    maxv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum vCpus this ``ManagedComputeEnvironment`` can scale up to. Each vCPU is equivalent to 1024 CPU shares. *Note*: if this Compute Environment uses EC2 resources (not Fargate) with either ``AllocationStrategy.BEST_FIT_PROGRESSIVE`` or ``AllocationStrategy.SPOT_CAPACITY_OPTIMIZED``, or ``AllocationStrategy.BEST_FIT`` with Spot instances, The scheduler may exceed this number by at most one of the instances specified in ``instanceTypes`` or ``instanceClasses``. Default: 256\n')
    replace_compute_environment: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether this Compute Environment is replaced if an update is made that requires replacing its instances. To enable more properties to be updated, set this property to ``false``. When changing the value of this property to false, do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the UPDATE_ROLLBACK_FAILED state. You can't update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. The properties which require a replacement of the Compute Environment are: Default: false\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups this Compute Environment will launch instances in. Default: new security groups will be created\n')
    spot: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use spot instances. Spot instances are less expensive EC2 instances that can be reclaimed by EC2 at any time; your job will be given two minutes of notice before reclamation. Default: false\n')
    terminate_on_update: typing.Optional[bool] = pydantic.Field(None, description="Whether or not any running jobs will be immediately terminated when an infrastructure update occurs. If this is enabled, any terminated jobs may be retried, depending on the job's retry policy. Default: false\n")
    update_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Only meaningful if ``terminateOnUpdate`` is ``false``. If so, when an infrastructure update is triggered, any running jobs will be allowed to run until ``updateTimeout`` has expired. Default: 30 minutes\n')
    update_to_latest_image_version: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the AMI is updated to the latest one supported by Batch when an infrastructure update occurs. If you specify a specific AMI, this property will be ignored. Default: true\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC Subnets this Compute Environment will launch instances in. Default: new subnets will be created\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs')
    _init_params: typing.ClassVar[list[str]] = ['eks_cluster', 'kubernetes_namespace', 'allocation_strategy', 'images', 'instance_classes', 'instance_role', 'instance_types', 'launch_template', 'minv_cpus', 'placement_group', 'spot_bid_percentage', 'use_optimal_instance_classes', 'vpc', 'maxv_cpus', 'replace_compute_environment', 'security_groups', 'spot', 'terminate_on_update', 'update_timeout', 'update_to_latest_image_version', 'vpc_subnets', 'compute_environment_name', 'enabled', 'service_role']
    _method_names: typing.ClassVar[list[str]] = ['add_instance_class', 'add_instance_type', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.ManagedEc2EksComputeEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.ManagedEc2EksComputeEnvironmentDefConfig] = pydantic.Field(None)


class ManagedEc2EksComputeEnvironmentDefConfig(pydantic.BaseModel):
    add_instance_class: typing.Optional[list[models.aws_batch.ManagedEc2EksComputeEnvironmentDefAddInstanceClassParams]] = pydantic.Field(None, description='Add an instance class to this compute environment.')
    add_instance_type: typing.Optional[list[models.aws_batch.ManagedEc2EksComputeEnvironmentDefAddInstanceTypeParams]] = pydantic.Field(None, description='Add an instance type to this compute environment.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    eks_cluster_config: typing.Optional[models._interface_methods.AwsEksIClusterDefConfig] = pydantic.Field(None)
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class ManagedEc2EksComputeEnvironmentDefAddInstanceClassParams(pydantic.BaseModel):
    instance_class: aws_cdk.aws_ec2.InstanceClass = pydantic.Field(..., description='-')
    ...

class ManagedEc2EksComputeEnvironmentDefAddInstanceTypeParams(pydantic.BaseModel):
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='-')
    ...

class ManagedEc2EksComputeEnvironmentDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.MultiNodeJobDefinition
class MultiNodeJobDefinitionDef(BaseConstruct):
    containers: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.MultiNodeContainerDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The containers that this multinode job will run. Default: none\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The instance type that this job definition will run. Default: - optimal instance, selected by Batch\n')
    main_node: typing.Union[int, float, None] = pydantic.Field(None, description='The index of the main node in this job. The main node is responsible for orchestration. Default: 0\n')
    propagate_tags: typing.Optional[bool] = pydantic.Field(None, description='Whether to propogate tags from the JobDefinition to the ECS task that Batch spawns. Default: false\n')
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of this job definition. Default: - generated by CloudFormation\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The default parameters passed to the container These parameters can be referenced in the ``command`` that you give to the container. Default: none\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a job. The job is retried on failure the same number of attempts as the value. Default: 1\n')
    retry_strategies: typing.Optional[typing.Sequence[models.aws_batch.RetryStrategyDef]] = pydantic.Field(None, description='Defines the retry behavior for this job. Default: - no ``RetryStrategy``\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of this Job. Only used in Fairshare Scheduling to decide which job to run first when there are multiple jobs with the same share identifier. Default: none\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, Batch terminates your jobs if they aren't finished. Default: - no timeout")
    _init_params: typing.ClassVar[list[str]] = ['containers', 'instance_type', 'main_node', 'propagate_tags', 'job_definition_name', 'parameters', 'retry_attempts', 'retry_strategies', 'scheduling_priority', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['add_container', 'add_retry_strategy', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_job_definition_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.MultiNodeJobDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_job_definition_arn']
    ...


    from_job_definition_arn: typing.Optional[models.aws_batch.MultiNodeJobDefinitionDefFromJobDefinitionArnParams] = pydantic.Field(None, description='refer to an existing JobDefinition by its arn.')
    resource_config: typing.Optional[models.aws_batch.MultiNodeJobDefinitionDefConfig] = pydantic.Field(None)


class MultiNodeJobDefinitionDefConfig(pydantic.BaseModel):
    add_container: typing.Optional[list[models.aws_batch.MultiNodeJobDefinitionDefAddContainerParams]] = pydantic.Field(None, description='Add a container to this multinode job.')
    add_retry_strategy: typing.Optional[list[models.aws_batch.MultiNodeJobDefinitionDefAddRetryStrategyParams]] = pydantic.Field(None, description='Add a RetryStrategy to this JobDefinition.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    instance_type_config: typing.Optional[models.aws_ec2.InstanceTypeDefConfig] = pydantic.Field(None)

class MultiNodeJobDefinitionDefAddContainerParams(pydantic.BaseModel):
    container: typing.Union[models.aws_batch.EcsEc2ContainerDefinitionDef, models.aws_batch.EcsFargateContainerDefinitionDef] = pydantic.Field(..., description='The container that this node range will run.\n')
    end_node: typing.Union[int, float] = pydantic.Field(..., description='The index of the last node to run this container. The container is run on all nodes in the range [startNode, endNode] (inclusive)\n')
    start_node: typing.Union[int, float] = pydantic.Field(..., description='The index of the first node to run this container. The container is run on all nodes in the range [startNode, endNode] (inclusive)')
    ...

class MultiNodeJobDefinitionDefAddRetryStrategyParams(pydantic.BaseModel):
    strategy: models.aws_batch.RetryStrategyDef = pydantic.Field(..., description='-')
    ...

class MultiNodeJobDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class MultiNodeJobDefinitionDefFromJobDefinitionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    job_definition_arn: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.UnmanagedComputeEnvironment
class UnmanagedComputeEnvironmentDef(BaseConstruct):
    unmanagedv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The vCPUs this Compute Environment provides. Used only by the scheduler to schedule jobs in ``Queue``s that use ``FairshareSchedulingPolicy``s. **If this parameter is not provided on a fairshare queue, no capacity is reserved**; that is, the ``FairshareSchedulingPolicy`` is ignored. Default: 0\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs')
    _init_params: typing.ClassVar[list[str]] = ['unmanagedv_cpus', 'compute_environment_name', 'enabled', 'service_role']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_unmanaged_compute_environment_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.UnmanagedComputeEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_unmanaged_compute_environment_arn']
    ...


    from_unmanaged_compute_environment_arn: typing.Optional[models.aws_batch.UnmanagedComputeEnvironmentDefFromUnmanagedComputeEnvironmentArnParams] = pydantic.Field(None, description='Import an UnmanagedComputeEnvironment by its arn.')
    resource_config: typing.Optional[models.aws_batch.UnmanagedComputeEnvironmentDefConfig] = pydantic.Field(None)


class UnmanagedComputeEnvironmentDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class UnmanagedComputeEnvironmentDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class UnmanagedComputeEnvironmentDefFromUnmanagedComputeEnvironmentArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    unmanaged_compute_environment_arn: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.ComputeResourcesProperty
class CfnComputeEnvironment_ComputeResourcesPropertyDef(BaseStruct):
    maxv_cpus: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The maximum number of Amazon EC2 vCPUs that an environment can reach. .. epigraph:: With ``BEST_FIT_PROGRESSIVE`` , ``SPOT_CAPACITY_OPTIMIZED`` and ``SPOT_PRICE_CAPACITY_OPTIMIZED`` (recommended) strategies using On-Demand or Spot Instances, and the ``BEST_FIT`` strategy using Spot Instances, AWS Batch might need to exceed ``maxvCpus`` to meet your capacity requirements. In this event, AWS Batch never exceeds ``maxvCpus`` by more than a single instance.\n')
    subnets: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The VPC subnets where the compute resources are launched. Fargate compute resources can contain up to 16 subnets. For Fargate compute resources, providing an empty list will be handled as if this parameter wasn't specified and no change is made. For EC2 compute resources, providing an empty list removes the VPC subnets from the compute resource. For more information, see `VPCs and subnets <https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html>`_ in the *Amazon VPC User Guide* . When updating a compute environment, changing the VPC subnets requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: AWS Batch on Amazon EC2 and AWS Batch on Amazon EKS support Local Zones. For more information, see `Local Zones <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-local-zones>`_ in the *Amazon EC2 User Guide for Linux Instances* , `Amazon EKS and AWS Local Zones <https://docs.aws.amazon.com/eks/latest/userguide/local-zones.html>`_ in the *Amazon EKS User Guide* and `Amazon ECS clusters in Local Zones, Wavelength Zones, and AWS Outposts <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-regions-zones.html#clusters-local-zones>`_ in the *Amazon ECS Developer Guide* . AWS Batch on Fargate doesn't currently support Local Zones.\n")
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of compute environment: ``EC2`` , ``SPOT`` , ``FARGATE`` , or ``FARGATE_SPOT`` . For more information, see `Compute environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the *AWS Batch User Guide* . If you choose ``SPOT`` , you must also specify an Amazon EC2 Spot Fleet role with the ``spotIamFleetRole`` parameter. For more information, see `Amazon EC2 spot fleet role <https://docs.aws.amazon.com/batch/latest/userguide/spot_fleet_IAM_role.html>`_ in the *AWS Batch User Guide* . When updating compute environment, changing the type of a compute environment requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . When updating the type of a compute environment, changing between ``EC2`` and ``SPOT`` or between ``FARGATE`` and ``FARGATE_SPOT`` will initiate an infrastructure update, but if you switch between ``EC2`` and ``FARGATE`` , AWS CloudFormation will create a new compute environment.\n')
    allocation_strategy: typing.Optional[str] = pydantic.Field(None, description="The allocation strategy to use for the compute resource if not enough instances of the best fitting instance type can be allocated. This might be because of availability of the instance type in the Region or `Amazon EC2 service limits <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html>`_ . For more information, see `Allocation strategies <https://docs.aws.amazon.com/batch/latest/userguide/allocation-strategies.html>`_ in the *AWS Batch User Guide* . When updating a compute environment, changing the allocation strategy requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . ``BEST_FIT`` is not supported when updating a compute environment. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources, and shouldn't be specified. - **BEST_FIT (default)** - AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren't available, AWS Batch waits for the additional instances to be available. If there aren't enough instances available, or if the user is reaching `Amazon EC2 service limits <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html>`_ then additional jobs aren't run until the currently running jobs have completed. This allocation strategy keeps costs lower but can limit scaling. If you are using Spot Fleets with ``BEST_FIT`` then the Spot Fleet IAM role must be specified. - **BEST_FIT_PROGRESSIVE** - AWS Batch will select additional instance types that are large enough to meet the requirements of the jobs in the queue, with a preference for instance types with a lower cost per unit vCPU. If additional instances of the previously selected instance types aren't available, AWS Batch will select new instance types. - **SPOT_CAPACITY_OPTIMIZED** - AWS Batch will select one or more instance types that are large enough to meet the requirements of the jobs in the queue, with a preference for instance types that are less likely to be interrupted. This allocation strategy is only available for Spot Instance compute resources. - **SPOT_PRICE_CAPACITY_OPTIMIZED** - The price and capacity optimized allocation strategy looks at both price and capacity to select the Spot Instance pools that are the least likely to be interrupted and have the lowest possible price. This allocation strategy is only available for Spot Instance compute resources. .. epigraph:: We recommend that you use ``SPOT_PRICE_CAPACITY_OPTIMIZED`` rather than ``SPOT_CAPACITY_OPTIMIZED`` in most instances. With ``BEST_FIT_PROGRESSIVE`` , ``SPOT_CAPACITY_OPTIMIZED`` , and ``SPOT_PRICE_CAPACITY_OPTIMIZED`` allocation strategies using On-Demand or Spot Instances, and the ``BEST_FIT`` strategy using Spot Instances, AWS Batch might need to go above ``maxvCpus`` to meet your capacity requirements. In this event, AWS Batch never exceeds ``maxvCpus`` by more than a single instance.\n")
    bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum percentage that a Spot Instance price can be when compared with the On-Demand price for that instance type before instances are launched. For example, if your maximum percentage is 20%, the Spot price must be less than 20% of the current On-Demand price for that Amazon EC2 instance. You always pay the lowest (market) price and never more than your maximum percentage. For most use cases, we recommend leaving this field empty. When updating a compute environment, changing the bid percentage requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    desiredv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The desired number of vCPUS in the compute environment. AWS Batch modifies this value between the minimum and maximum values based on job queue demand. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > AWS Batch doesn't support changing the desired number of vCPUs of an existing compute environment. Don't specify this parameter for compute environments using Amazon EKS clusters. > When you update the ``desiredvCpus`` setting, the value must be between the ``minvCpus`` and ``maxvCpus`` values. Additionally, the updated ``desiredvCpus`` value must be greater than or equal to the current ``desiredvCpus`` value. For more information, see `Troubleshooting AWS Batch <https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#error-desired-vcpus-update>`_ in the *AWS Batch User Guide* .\n")
    ec2_configuration: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Provides information used to select Amazon Machine Images (AMIs) for EC2 instances in the compute environment. If ``Ec2Configuration`` isn't specified, the default is ``ECS_AL2`` . When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . To remove the EC2 configuration and any custom AMI ID specified in ``imageIdOverride`` , set this value to an empty string. One or two values can be provided. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    ec2_key_pair: typing.Optional[str] = pydantic.Field(None, description="The Amazon EC2 key pair that's used for instances launched in the compute environment. You can use this key pair to log in to your instances with SSH. To remove the Amazon EC2 key pair, set this value to an empty string. When updating a compute environment, changing the EC2 key pair requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    image_id: typing.Optional[str] = pydantic.Field(None, description="The Amazon Machine Image (AMI) ID used for instances launched in the compute environment. This parameter is overridden by the ``imageIdOverride`` member of the ``Ec2Configuration`` structure. To remove the custom AMI ID and use the default AMI ID, set this value to an empty string. When updating a compute environment, changing the AMI ID requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > The AMI that you choose for a compute environment must match the architecture of the instance types that you intend to use for that compute environment. For example, if your compute environment uses A1 instance types, the compute resource AMI that you choose must support ARM instances. Amazon ECS vends both x86 and ARM versions of the Amazon ECS-optimized Amazon Linux 2 AMI. For more information, see `Amazon ECS-optimized Amazon Linux 2 AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#ecs-optimized-ami-linux-variants.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n")
    instance_role: typing.Optional[str] = pydantic.Field(None, description="The Amazon ECS instance profile applied to Amazon EC2 instances in a compute environment. Required for Amazon EC2 instances. You can specify the short name or full Amazon Resource Name (ARN) of an instance profile. For example, ``*ecsInstanceRole*`` or ``arn:aws:iam:: *<aws_account_id>* :instance-profile/ *ecsInstanceRole*`` . For more information, see `Amazon ECS instance role <https://docs.aws.amazon.com/batch/latest/userguide/instance_IAM_role.html>`_ in the *AWS Batch User Guide* . When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    instance_types: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The instances types that can be launched. You can specify instance families to launch any instance type within those families (for example, ``c5`` or ``p3`` ), or you can specify specific sizes within a family (such as ``c5.8xlarge`` ). You can also choose ``optimal`` to select instance types (from the C4, M4, and R4 instance families) that match the demand of your job queues. When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > When you create a compute environment, the instance types that you select for the compute environment must share the same architecture. For example, you can't mix x86 and ARM instances in the same compute environment. > Currently, ``optimal`` uses instance types from the C4, M4, and R4 instance families. In Regions that don't have instance types from those instance families, instance types from the C5, M5, and R5 instance families are used.\n")
    launch_template: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The launch template to use for your compute resources. Any other compute resource parameters that you specify in a `CreateComputeEnvironment <https://docs.aws.amazon.com/batch/latest/APIReference/API_CreateComputeEnvironment.html>`_ API operation override the same parameters in the launch template. You must specify either the launch template ID or launch template name in the request, but not both. For more information, see `Launch Template Support <https://docs.aws.amazon.com/batch/latest/userguide/launch-templates.html>`_ in the ** . Removing the launch template from a compute environment will not remove the AMI specified in the launch template. In order to update the AMI specified in a launch template, the ``updateToLatestImageVersion`` parameter must be set to ``true`` . When updating a compute environment, changing the launch template requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the ** . .. epigraph:: This parameter isn't applicable to jobs running on Fargate resources, and shouldn't be specified.\n")
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of vCPUs that an environment should maintain (even if the compute environment is ``DISABLED`` ). .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    placement_group: typing.Optional[str] = pydantic.Field(None, description="The Amazon EC2 placement group to associate with your compute resources. If you intend to submit multi-node parallel jobs to your compute environment, you should consider creating a cluster placement group and associate it with your compute resources. This keeps your multi-node parallel job on a logical grouping of instances within a single Availability Zone with high network flow potential. For more information, see `Placement groups <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html>`_ in the *Amazon EC2 User Guide for Linux Instances* . When updating a compute environment, changing the placement group requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The Amazon EC2 security groups that are associated with instances launched in the compute environment. This parameter is required for Fargate compute resources, where it can contain up to 5 security groups. For Fargate compute resources, providing an empty list is handled as if this parameter wasn't specified and no change is made. For EC2 compute resources, providing an empty list removes the security groups from the compute resource. When updating a compute environment, changing the EC2 security groups requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* .\n")
    spot_iam_fleet_role: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the Amazon EC2 Spot Fleet IAM role applied to a ``SPOT`` compute environment. This role is required if the allocation strategy set to ``BEST_FIT`` or if the allocation strategy isn't specified. For more information, see `Amazon EC2 spot fleet role <https://docs.aws.amazon.com/batch/latest/userguide/spot_fleet_IAM_role.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > To tag your Spot Instances on creation, the Spot Fleet IAM role specified here must use the newer *AmazonEC2SpotFleetTaggingRole* managed policy. The previously recommended *AmazonEC2SpotFleetRole* managed policy doesn't have the required permissions to tag Spot Instances. For more information, see `Spot instances not tagged on creation <https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#spot-instance-no-tag>`_ in the *AWS Batch User Guide* .\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pair tags to be applied to EC2 resources that are launched in the compute environment. For AWS Batch , these take the form of ``"String1": "String2"`` , where ``String1`` is the tag key and ``String2`` is the tag value-for example, ``{ "Name": "Batch Instance - C4OnDemand" }`` . This is helpful for recognizing your Batch instances in the Amazon EC2 console. These tags aren\'t seen when using the AWS Batch ``ListTagsForResource`` API operation. When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn\'t applicable to jobs that are running on Fargate resources. Don\'t specify it.\n')
    update_to_latest_image_version: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the AMI ID is updated to the latest one that\'s supported by AWS Batch when the compute environment has an infrastructure update. The default value is ``false`` . .. epigraph:: An AMI ID can either be specified in the ``imageId`` or ``imageIdOverride`` parameters or be determined by the launch template that\'s specified in the ``launchTemplate`` parameter. If an AMI ID is specified any of these ways, this parameter is ignored. For more information about to update AMI IDs during an infrastructure update, see `Updating the AMI ID <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html#updating-compute-environments-ami>`_ in the *AWS Batch User Guide* . When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . Default: - false\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    compute_resources_property = batch.CfnComputeEnvironment.ComputeResourcesProperty(\n        maxv_cpus=123,\n        subnets=["subnets"],\n        type="type",\n\n        # the properties below are optional\n        allocation_strategy="allocationStrategy",\n        bid_percentage=123,\n        desiredv_cpus=123,\n        ec2_configuration=[batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty(\n            image_type="imageType",\n\n            # the properties below are optional\n            image_id_override="imageIdOverride",\n            image_kubernetes_version="imageKubernetesVersion"\n        )],\n        ec2_key_pair="ec2KeyPair",\n        image_id="imageId",\n        instance_role="instanceRole",\n        instance_types=["instanceTypes"],\n        launch_template=batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty(\n            launch_template_id="launchTemplateId",\n            launch_template_name="launchTemplateName",\n            version="version"\n        ),\n        minv_cpus=123,\n        placement_group="placementGroup",\n        security_group_ids=["securityGroupIds"],\n        spot_iam_fleet_role="spotIamFleetRole",\n        tags={\n            "tags_key": "tags"\n        },\n        update_to_latest_image_version=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['maxv_cpus', 'subnets', 'type', 'allocation_strategy', 'bid_percentage', 'desiredv_cpus', 'ec2_configuration', 'ec2_key_pair', 'image_id', 'instance_role', 'instance_types', 'launch_template', 'minv_cpus', 'placement_group', 'security_group_ids', 'spot_iam_fleet_role', 'tags', 'update_to_latest_image_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.ComputeResourcesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty
class CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef(BaseStruct):
    image_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The image type to match with the instance type to select an AMI. The supported values are different for ``ECS`` and ``EKS`` resources. - **ECS** - If the ``imageIdOverride`` parameter isn't specified, then a recent `Amazon ECS-optimized Amazon Linux 2 AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#al2ami>`_ ( ``ECS_AL2`` ) is used. If a new image type is specified in an update, but neither an ``imageId`` nor a ``imageIdOverride`` parameter is specified, then the latest Amazon ECS optimized AMI for that image type that's supported by AWS Batch is used. - **ECS_AL2** - `Amazon Linux 2 <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#al2ami>`_ : Default for all non-GPU instance families. - **ECS_AL2_NVIDIA** - `Amazon Linux 2 (GPU) <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#gpuami>`_ : Default for all GPU instance families (for example ``P4`` and ``G4`` ) and can be used for all non AWS Graviton-based instance types. - **ECS_AL2023** - `Amazon Linux 2023 <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html>`_ : AWS Batch supports Amazon Linux 2023. .. epigraph:: Amazon Linux 2023 does not support ``A1`` instances. - **ECS_AL1** - `Amazon Linux <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#alami>`_ . Amazon Linux has reached the end-of-life of standard support. For more information, see `Amazon Linux AMI <https://docs.aws.amazon.com/amazon-linux-ami/>`_ . - **EKS** - If the ``imageIdOverride`` parameter isn't specified, then a recent `Amazon EKS-optimized Amazon Linux AMI <https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html>`_ ( ``EKS_AL2`` ) is used. If a new image type is specified in an update, but neither an ``imageId`` nor a ``imageIdOverride`` parameter is specified, then the latest Amazon EKS optimized AMI for that image type that AWS Batch supports is used. - **EKS_AL2** - `Amazon Linux 2 <https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html>`_ : Default for all non-GPU instance families. - **EKS_AL2_NVIDIA** - `Amazon Linux 2 (accelerated) <https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html>`_ : Default for all GPU instance families (for example, ``P4`` and ``G4`` ) and can be used for all non AWS Graviton-based instance types.\n")
    image_id_override: typing.Optional[str] = pydantic.Field(None, description='The AMI ID used for instances launched in the compute environment that match the image type. This setting overrides the ``imageId`` set in the ``computeResource`` object. .. epigraph:: The AMI that you choose for a compute environment must match the architecture of the instance types that you intend to use for that compute environment. For example, if your compute environment uses A1 instance types, the compute resource AMI that you choose must support ARM instances. Amazon ECS vends both x86 and ARM versions of the Amazon ECS-optimized Amazon Linux 2 AMI. For more information, see `Amazon ECS-optimized Amazon Linux 2 AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#ecs-optimized-ami-linux-variants.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    image_kubernetes_version: typing.Optional[str] = pydantic.Field(None, description='The Kubernetes version for the compute environment. If you don\'t specify a value, the latest version that AWS Batch supports is used.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-ec2configurationobject.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    ec2_configuration_object_property = batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty(\n        image_type="imageType",\n\n        # the properties below are optional\n        image_id_override="imageIdOverride",\n        image_kubernetes_version="imageKubernetesVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image_type', 'image_id_override', 'image_kubernetes_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.EksConfigurationProperty
class CfnComputeEnvironment_EksConfigurationPropertyDef(BaseStruct):
    eks_cluster_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the Amazon EKS cluster. An example is ``arn: *aws* :eks: *us-east-1* : *123456789012* :cluster/ *ClusterForBatch*`` .\n')
    kubernetes_namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The namespace of the Amazon EKS cluster. AWS Batch manages pods in this namespace. The value can\'t left empty or null. It must be fewer than 64 characters long, can\'t be set to ``default`` , can\'t start with " ``kube-`` ," and must match this regular expression: ``^[a-z0-9]([-a-z0-9]*[a-z0-9])?$`` . For more information, see `Namespaces <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/>`_ in the Kubernetes documentation.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-eksconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_configuration_property = batch.CfnComputeEnvironment.EksConfigurationProperty(\n        eks_cluster_arn="eksClusterArn",\n        kubernetes_namespace="kubernetesNamespace"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['eks_cluster_arn', 'kubernetes_namespace']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.EksConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty
class CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef(BaseStruct):
    launch_template_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the launch template.\n')
    launch_template_name: typing.Optional[str] = pydantic.Field(None, description='The name of the launch template.\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version number of the launch template, ``$Latest`` , or ``$Default`` . If the value is ``$Latest`` , the latest version of the launch template is used. If the value is ``$Default`` , the default version of the launch template is used. .. epigraph:: If the AMI ID that\'s used in a compute environment is from the launch template, the AMI isn\'t changed when the compute environment is updated. It\'s only changed if the ``updateToLatestImageVersion`` parameter for the compute environment is set to ``true`` . During an infrastructure update, if either ``$Latest`` or ``$Default`` is specified, AWS Batch re-evaluates the launch template version, and it might use a different version of the launch template. This is the case even if the launch template isn\'t specified in the update. When updating a compute environment, changing the launch template requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . Default: ``$Default`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-launchtemplatespecification.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    launch_template_specification_property = batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty(\n        launch_template_id="launchTemplateId",\n        launch_template_name="launchTemplateName",\n        version="version"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['launch_template_id', 'launch_template_name', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.UpdatePolicyProperty
class CfnComputeEnvironment_UpdatePolicyPropertyDef(BaseStruct):
    job_execution_timeout_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the job timeout (in minutes) when the compute environment infrastructure is updated. The default value is 30. Default: - 30\n')
    terminate_jobs_on_update: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether jobs are automatically terminated when the computer environment infrastructure is updated. The default value is ``false`` . Default: - false\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-updatepolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    update_policy_property = batch.CfnComputeEnvironment.UpdatePolicyProperty(\n        job_execution_timeout_minutes=123,\n        terminate_jobs_on_update=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['job_execution_timeout_minutes', 'terminate_jobs_on_update']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.UpdatePolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.AuthorizationConfigProperty
class CfnJobDefinition_AuthorizationConfigPropertyDef(BaseStruct):
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, the root directory value specified in the ``EFSVolumeConfiguration`` must either be omitted or set to ``/`` which enforces the path set on the EFS access point. If an access point is used, transit encryption must be enabled in the ``EFSVolumeConfiguration`` . For more information, see `Working with Amazon EFS access points <https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html>`_ in the *Amazon Elastic File System User Guide* .\n')
    iam: typing.Optional[str] = pydantic.Field(None, description='Whether or not to use the AWS Batch job IAM role defined in a job definition when mounting the Amazon EFS file system. If enabled, transit encryption must be enabled in the ``EFSVolumeConfiguration`` . If this parameter is omitted, the default value of ``DISABLED`` is used. For more information, see `Using Amazon EFS access points <https://docs.aws.amazon.com/batch/latest/userguide/efs-volumes.html#efs-volume-accesspoints>`_ in the *AWS Batch User Guide* . EFS IAM authorization requires that ``TransitEncryption`` be ``ENABLED`` and that a ``JobRoleArn`` is specified.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-authorizationconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    authorization_config_property = batch.CfnJobDefinition.AuthorizationConfigProperty(\n        access_point_id="accessPointId",\n        iam="iam"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_point_id', 'iam']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.AuthorizationConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.ContainerPropertiesProperty
class CfnJobDefinition_ContainerPropertiesPropertyDef(BaseStruct):
    image: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Required. The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with ``*repository-url* / *image* : *tag*`` . It can be 255 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), underscores (_), colons (:), periods (.), forward slashes (/), and number signs (#). This parameter maps to ``Image`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``IMAGE`` parameter of `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: Docker image architecture must match the processor architecture of the compute resources that they're scheduled on. For example, ARM-based Docker images can only run on ARM-based compute resources. - Images in Amazon ECR Public repositories use the full ``registry/repository[:tag]`` or ``registry/repository[@digest]`` naming conventions. For example, ``public.ecr.aws/ *registry_alias* / *my-web-app* : *latest*`` . - Images in Amazon ECR repositories use the full registry and repository URI (for example, ``123456789012.dkr.ecr.<region-name>.amazonaws.com/<repository-name>`` ). - Images in official repositories on Docker Hub use a single name (for example, ``ubuntu`` or ``mongo`` ). - Images in other repositories on Docker Hub are qualified with an organization name (for example, ``amazon/amazon-ecs-agent`` ). - Images in other online repositories are qualified further by a domain name (for example, ``quay.io/assemblyline/ubuntu`` ).\n")
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. This parameter maps to ``Cmd`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``COMMAND`` parameter to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . For more information, see `https://docs.docker.com/engine/reference/builder/#cmd <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/builder/#cmd>`_ .\n")
    environment: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EnvironmentPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The environment variables to pass to a container. This parameter maps to ``Env`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--env`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: We don\'t recommend using plaintext environment variables for sensitive information, such as credential data. > Environment variables cannot start with " ``AWS_BATCH`` ". This naming convention is reserved for variables that AWS Batch sets.\n')
    ephemeral_storage: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The amount of ephemeral storage to allocate for the task. This parameter is used to expand the total amount of ephemeral storage available, beyond the default amount, for tasks hosted on AWS Fargate .\n')
    execution_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the execution role that AWS Batch can assume. For jobs that run on Fargate resources, you must provide an execution role. For more information, see `AWS Batch execution IAM role <https://docs.aws.amazon.com/batch/latest/userguide/execution-IAM-role.html>`_ in the *AWS Batch User Guide* .\n')
    fargate_platform_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_FargatePlatformConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The platform configuration for jobs that are running on Fargate resources. Jobs that are running on EC2 resources must not specify this parameter.\n')
    instance_type: typing.Optional[str] = pydantic.Field(None, description="The instance type to use for a multi-node parallel job. All node groups in a multi-node parallel job must use the same instance type. .. epigraph:: This parameter isn't applicable to single-node container jobs or jobs that run on Fargate resources, and shouldn't be provided.\n")
    job_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the IAM role that the container can assume for AWS permissions. For more information, see `IAM roles for tasks <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    linux_parameters: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_LinuxParametersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as details for device mappings.\n')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The log configuration specification for the container. This parameter maps to ``LogConfig`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--log-driver`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . By default, containers use the same logging driver that the Docker daemon uses. However the container might use a different logging driver than the Docker daemon by specifying a log driver with this parameter in the container definition. To use a different logging driver for a container, the log system must be configured properly on the container instance (or on a different log server for remote logging options). For more information on the options for different supported log drivers, see `Configure logging drivers <https://docs.aws.amazon.com/https://docs.docker.com/engine/admin/logging/overview/>`_ in the Docker documentation. .. epigraph:: AWS Batch currently supports a subset of the logging drivers available to the Docker daemon (shown in the ``LogConfiguration`` data type). This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"`` .. epigraph:: The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with the ``ECS_AVAILABLE_LOGGING_DRIVERS`` environment variable before containers placed on that instance can use these log configuration options. For more information, see `Amazon ECS container agent configuration <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    memory: typing.Union[int, float, None] = pydantic.Field(None, description="This parameter is deprecated, use ``resourceRequirements`` to specify the memory requirements for the job definition. It's not supported for jobs running on Fargate resources. For jobs that run on EC2 resources, it specifies the memory hard limit (in MiB) for a container. If your container attempts to exceed the specified number, it's terminated. You must specify at least 4 MiB of memory for a job using this parameter. The memory hard limit can be specified in several places. It must be specified for each node at least once.\n")
    mount_points: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_MountPointsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The mount points for data volumes in your container. This parameter maps to ``Volumes`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--volume`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ .\n')
    network_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The network configuration for jobs that are running on Fargate resources. Jobs that are running on EC2 resources must not specify this parameter.\n')
    privileged: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="When this parameter is true, the container is given elevated permissions on the host container instance (similar to the ``root`` user). This parameter maps to ``Privileged`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--privileged`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . The default value is false. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources and shouldn't be provided, or specified as false.\n")
    readonly_root_filesystem: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. This parameter maps to ``ReadonlyRootfs`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--read-only`` option to ``docker run`` .\n')
    resource_requirements: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ResourceRequirementPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The type and amount of resources to assign to a container. The supported resources include ``GPU`` , ``MEMORY`` , and ``VCPU`` .\n')
    runtime_platform: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_RuntimePlatformPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object that represents the compute environment architecture for AWS Batch jobs on Fargate.\n')
    secrets: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The secrets for the container. For more information, see `Specifying sensitive data <https://docs.aws.amazon.com/batch/latest/userguide/specifying-sensitive-data.html>`_ in the *AWS Batch User Guide* .\n')
    ulimits: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_UlimitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="A list of ``ulimits`` to set in the container. This parameter maps to ``Ulimits`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--ulimit`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources and shouldn't be provided.\n")
    user: typing.Optional[str] = pydantic.Field(None, description='The user name to use inside the container. This parameter maps to ``User`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--user`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ .\n')
    vcpus: typing.Union[int, float, None] = pydantic.Field(None, description="This parameter is deprecated, use ``resourceRequirements`` to specify the vCPU requirements for the job definition. It's not supported for jobs running on Fargate resources. For jobs running on EC2 resources, it specifies the number of vCPUs reserved for the job. Each vCPU is equivalent to 1,024 CPU shares. This parameter maps to ``CpuShares`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--cpu-shares`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . The number of vCPUs must be specified but can be specified in several places. You must specify it at least once for each node.\n")
    volumes: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_VolumesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of data volumes used in a job.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-containerproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    container_properties_property = batch.CfnJobDefinition.ContainerPropertiesProperty(\n        image="image",\n\n        # the properties below are optional\n        command=["command"],\n        environment=[batch.CfnJobDefinition.EnvironmentProperty(\n            name="name",\n            value="value"\n        )],\n        ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n            size_in_gi_b=123\n        ),\n        execution_role_arn="executionRoleArn",\n        fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n            platform_version="platformVersion"\n        ),\n        instance_type="instanceType",\n        job_role_arn="jobRoleArn",\n        linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n            devices=[batch.CfnJobDefinition.DeviceProperty(\n                container_path="containerPath",\n                host_path="hostPath",\n                permissions=["permissions"]\n            )],\n            init_process_enabled=False,\n            max_swap=123,\n            shared_memory_size=123,\n            swappiness=123,\n            tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                container_path="containerPath",\n                size=123,\n\n                # the properties below are optional\n                mount_options=["mountOptions"]\n            )]\n        ),\n        log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n            log_driver="logDriver",\n\n            # the properties below are optional\n            options=options,\n            secret_options=[batch.CfnJobDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )]\n        ),\n        memory=123,\n        mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n            container_path="containerPath",\n            read_only=False,\n            source_volume="sourceVolume"\n        )],\n        network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n            assign_public_ip="assignPublicIp"\n        ),\n        privileged=False,\n        readonly_root_filesystem=False,\n        resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n            type="type",\n            value="value"\n        )],\n        runtime_platform=batch.CfnJobDefinition.RuntimePlatformProperty(\n            cpu_architecture="cpuArchitecture",\n            operating_system_family="operatingSystemFamily"\n        ),\n        secrets=[batch.CfnJobDefinition.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )],\n        ulimits=[batch.CfnJobDefinition.UlimitProperty(\n            hard_limit=123,\n            name="name",\n            soft_limit=123\n        )],\n        user="user",\n        vcpus=123,\n        volumes=[batch.CfnJobDefinition.VolumesProperty(\n            efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                file_system_id="fileSystemId",\n\n                # the properties below are optional\n                authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                    access_point_id="accessPointId",\n                    iam="iam"\n                ),\n                root_directory="rootDirectory",\n                transit_encryption="transitEncryption",\n                transit_encryption_port=123\n            ),\n            host=batch.CfnJobDefinition.VolumesHostProperty(\n                source_path="sourcePath"\n            ),\n            name="name"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'environment', 'ephemeral_storage', 'execution_role_arn', 'fargate_platform_configuration', 'instance_type', 'job_role_arn', 'linux_parameters', 'log_configuration', 'memory', 'mount_points', 'network_configuration', 'privileged', 'readonly_root_filesystem', 'resource_requirements', 'runtime_platform', 'secrets', 'ulimits', 'user', 'vcpus', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.ContainerPropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.DeviceProperty
class CfnJobDefinition_DevicePropertyDef(BaseStruct):
    container_path: typing.Optional[str] = pydantic.Field(None, description="The path inside the container that's used to expose the host device. By default, the ``hostPath`` value is used.\n")
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path for the device on the host container instance.\n')
    permissions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The explicit permissions to provide to the container for the device. By default, the container has permissions for ``read`` , ``write`` , and ``mknod`` for the device.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-device.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    device_property = batch.CfnJobDefinition.DeviceProperty(\n        container_path="containerPath",\n        host_path="hostPath",\n        permissions=["permissions"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'host_path', 'permissions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.DeviceProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EfsVolumeConfigurationProperty
class CfnJobDefinition_EfsVolumeConfigurationPropertyDef(BaseStruct):
    file_system_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon EFS file system ID to use.\n')
    authorization_config: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_AuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The authorization configuration details for the Amazon EFS file system.\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume is used instead. Specifying ``/`` has the same effect as omitting this parameter. The maximum length is 4,096 characters. .. epigraph:: If an EFS access point is specified in the ``authorizationConfig`` , the root directory parameter must either be omitted or set to ``/`` , which enforces the path set on the Amazon EFS access point.\n')
    transit_encryption: typing.Optional[str] = pydantic.Field(None, description='Determines whether to enable encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Transit encryption must be enabled if Amazon EFS IAM authorization is used. If this parameter is omitted, the default value of ``DISABLED`` is used. For more information, see `Encrypting data in transit <https://docs.aws.amazon.com/efs/latest/ug/encryption-in-transit.html>`_ in the *Amazon Elastic File System User Guide* .\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. If you don\'t specify a transit encryption port, it uses the port selection strategy that the Amazon EFS mount helper uses. The value must be between 0 and 65,535. For more information, see `EFS mount helper <https://docs.aws.amazon.com/efs/latest/ug/efs-mount-helper.html>`_ in the *Amazon Elastic File System User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-efsvolumeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    efs_volume_configuration_property = batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n        file_system_id="fileSystemId",\n\n        # the properties below are optional\n        authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n            access_point_id="accessPointId",\n            iam="iam"\n        ),\n        root_directory="rootDirectory",\n        transit_encryption="transitEncryption",\n        transit_encryption_port=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_system_id', 'authorization_config', 'root_directory', 'transit_encryption', 'transit_encryption_port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EfsVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty
class CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the environment variable.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value of the environment variable.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainerenvironmentvariable.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_container_environment_variable_property = batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n        name="name",\n\n        # the properties below are optional\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksContainerProperty
class CfnJobDefinition_EksContainerPropertyDef(BaseStruct):
    image: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Docker image used to start the container.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The entrypoint for the container. This isn\'t run within a shell. If this isn\'t specified, the ``ENTRYPOINT`` of the container image is used. Environment variable references are expanded using the container\'s environment. If the referenced environment variable doesn\'t exist, the reference in the command isn\'t changed. For example, if the reference is to " ``$(NAME1)`` " and the ``NAME1`` environment variable doesn\'t exist, the command string will remain " ``$(NAME1)`` ." ``$$`` is replaced with ``$`` and the resulting string isn\'t expanded. For example, ``$$(VAR_NAME)`` will be passed as ``$(VAR_NAME)`` whether or not the ``VAR_NAME`` environment variable exists. The entrypoint can\'t be updated. For more information, see `ENTRYPOINT <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/builder/#entrypoint>`_ in the *Dockerfile reference* and `Define a command and arguments for a container <https://docs.aws.amazon.com/https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/>`_ and `Entrypoint <https://docs.aws.amazon.com/https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#entrypoint>`_ in the *Kubernetes documentation* .\n')
    env: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The environment variables to pass to a container. .. epigraph:: Environment variables cannot start with " ``AWS_BATCH`` ". This naming convention is reserved for variables that AWS Batch sets.\n')
    image_pull_policy: typing.Optional[str] = pydantic.Field(None, description='The image pull policy for the container. Supported values are ``Always`` , ``IfNotPresent`` , and ``Never`` . This parameter defaults to ``IfNotPresent`` . However, if the ``:latest`` tag is specified, it defaults to ``Always`` . For more information, see `Updating images <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/containers/images/#updating-images>`_ in the *Kubernetes documentation* .\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. If the name isn\'t specified, the default name " ``Default`` " is used. Each container in a pod must have a unique name.\n')
    resources: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type and amount of resources to assign to a container. The supported resources include ``memory`` , ``cpu`` , and ``nvidia.com/gpu`` . For more information, see `Resource management for pods and containers <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>`_ in the *Kubernetes documentation* .\n')
    security_context: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_SecurityContextPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The security context for a job. For more information, see `Configure a security context for a pod or container <https://docs.aws.amazon.com/https://kubernetes.io/docs/tasks/configure-pod-container/security-context/>`_ in the *Kubernetes documentation* .\n')
    volume_mounts: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksContainerVolumeMountPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The volume mounts for the container. AWS Batch supports ``emptyDir`` , ``hostPath`` , and ``secret`` volume types. For more information about volumes and volume mounts in Kubernetes, see `Volumes <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/>`_ in the *Kubernetes documentation* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainer.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # limits: Any\n    # requests: Any\n\n    eks_container_property = batch.CfnJobDefinition.EksContainerProperty(\n        image="image",\n\n        # the properties below are optional\n        args=["args"],\n        command=["command"],\n        env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n            name="name",\n\n            # the properties below are optional\n            value="value"\n        )],\n        image_pull_policy="imagePullPolicy",\n        name="name",\n        resources=batch.CfnJobDefinition.ResourcesProperty(\n            limits=limits,\n            requests=requests\n        ),\n        security_context=batch.CfnJobDefinition.SecurityContextProperty(\n            privileged=False,\n            read_only_root_filesystem=False,\n            run_as_group=123,\n            run_as_non_root=False,\n            run_as_user=123\n        ),\n        volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n            mount_path="mountPath",\n            name="name",\n            read_only=False\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'env', 'image_pull_policy', 'name', 'resources', 'security_context', 'volume_mounts']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksContainerProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksContainerVolumeMountProperty
class CfnJobDefinition_EksContainerVolumeMountPropertyDef(BaseStruct):
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name the volume mount. This must match the name of one of the volumes in the pod.\n')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If this value is ``true`` , the container has read-only access to the volume. Otherwise, the container can write to the volume. The default value is ``false`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainervolumemount.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_container_volume_mount_property = batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n        mount_path="mountPath",\n        name="name",\n        read_only=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['mount_path', 'name', 'read_only']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksContainerVolumeMountProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksPropertiesProperty
class CfnJobDefinition_EksPropertiesPropertyDef(BaseStruct):
    pod_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_PodPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The properties for the Kubernetes pod resources of a job.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-eksproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # labels: Any\n    # limits: Any\n    # requests: Any\n\n    eks_properties_property = batch.CfnJobDefinition.EksPropertiesProperty(\n        pod_properties=batch.CfnJobDefinition.PodPropertiesProperty(\n            containers=[batch.CfnJobDefinition.EksContainerProperty(\n                image="image",\n\n                # the properties below are optional\n                args=["args"],\n                command=["command"],\n                env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n                    name="name",\n\n                    # the properties below are optional\n                    value="value"\n                )],\n                image_pull_policy="imagePullPolicy",\n                name="name",\n                resources=batch.CfnJobDefinition.ResourcesProperty(\n                    limits=limits,\n                    requests=requests\n                ),\n                security_context=batch.CfnJobDefinition.SecurityContextProperty(\n                    privileged=False,\n                    read_only_root_filesystem=False,\n                    run_as_group=123,\n                    run_as_non_root=False,\n                    run_as_user=123\n                ),\n                volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n                    mount_path="mountPath",\n                    name="name",\n                    read_only=False\n                )]\n            )],\n            dns_policy="dnsPolicy",\n            host_network=False,\n            metadata=batch.CfnJobDefinition.MetadataProperty(\n                labels=labels\n            ),\n            service_account_name="serviceAccountName",\n            volumes=[batch.CfnJobDefinition.EksVolumeProperty(\n                name="name",\n\n                # the properties below are optional\n                empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n                    medium="medium",\n                    size_limit="sizeLimit"\n                ),\n                host_path=batch.CfnJobDefinition.HostPathProperty(\n                    path="path"\n                ),\n                secret=batch.CfnJobDefinition.EksSecretProperty(\n                    secret_name="secretName",\n\n                    # the properties below are optional\n                    optional=False\n                )\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['pod_properties']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksPropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksSecretProperty
class CfnJobDefinition_EksSecretPropertyDef(BaseStruct):
    secret_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the secret. The name must be allowed as a DNS subdomain name. For more information, see `DNS subdomain names <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names>`_ in the *Kubernetes documentation* .\n')
    optional: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the secret or the secret\'s keys must be defined.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekssecret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_secret_property = batch.CfnJobDefinition.EksSecretProperty(\n        secret_name="secretName",\n\n        # the properties below are optional\n        optional=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['secret_name', 'optional']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksSecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksVolumeProperty
class CfnJobDefinition_EksVolumePropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the volume. The name must be allowed as a DNS subdomain name. For more information, see `DNS subdomain names <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names>`_ in the *Kubernetes documentation* .\n')
    empty_dir: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EmptyDirPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the configuration of a Kubernetes ``emptyDir`` volume. For more information, see `emptyDir <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/#emptydir>`_ in the *Kubernetes documentation* .\n')
    host_path: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_HostPathPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the configuration of a Kubernetes ``hostPath`` volume. For more information, see `hostPath <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/#hostpath>`_ in the *Kubernetes documentation* .\n')
    secret: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the configuration of a Kubernetes ``secret`` volume. For more information, see `secret <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/#secret>`_ in the *Kubernetes documentation* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-eksvolume.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_volume_property = batch.CfnJobDefinition.EksVolumeProperty(\n        name="name",\n\n        # the properties below are optional\n        empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n            medium="medium",\n            size_limit="sizeLimit"\n        ),\n        host_path=batch.CfnJobDefinition.HostPathProperty(\n            path="path"\n        ),\n        secret=batch.CfnJobDefinition.EksSecretProperty(\n            secret_name="secretName",\n\n            # the properties below are optional\n            optional=False\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'empty_dir', 'host_path', 'secret']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksVolumeProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EmptyDirProperty
class CfnJobDefinition_EmptyDirPropertyDef(BaseStruct):
    medium: typing.Optional[str] = pydantic.Field(None, description='')
    size_limit: typing.Optional[str] = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['medium', 'size_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EmptyDirProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EnvironmentProperty
class CfnJobDefinition_EnvironmentPropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the environment variable.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value of the environment variable.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-environment.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    environment_property = batch.CfnJobDefinition.EnvironmentProperty(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EnvironmentProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EphemeralStorageProperty
class CfnJobDefinition_EphemeralStoragePropertyDef(BaseStruct):
    size_in_gib: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The total amount, in GiB, of ephemeral storage to set for the task. The minimum supported value is ``21`` GiB and the maximum supported value is ``200`` GiB.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ephemeralstorage.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    ephemeral_storage_property = batch.CfnJobDefinition.EphemeralStorageProperty(\n        size_in_gi_b=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['size_in_gib']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EphemeralStorageProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EvaluateOnExitProperty
class CfnJobDefinition_EvaluateOnExitPropertyDef(BaseStruct):
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Specifies the action to take if all of the specified conditions ( ``onStatusReason`` , ``onReason`` , and ``onExitCode`` ) are met. The values aren't case sensitive.\n")
    on_exit_code: typing.Optional[str] = pydantic.Field(None, description='Contains a glob pattern to match against the decimal representation of the ``ExitCode`` returned for a job. The pattern can be up to 512 characters long. It can contain only numbers, and can end with an asterisk (*) so that only the start of the string needs to be an exact match. The string can contain up to 512 characters.\n')
    on_reason: typing.Optional[str] = pydantic.Field(None, description='Contains a glob pattern to match against the ``Reason`` returned for a job. The pattern can contain up to 512 characters. It can contain letters, numbers, periods (.), colons (:), and white space (including spaces and tabs). It can optionally end with an asterisk (*) so that only the start of the string needs to be an exact match.\n')
    on_status_reason: typing.Optional[str] = pydantic.Field(None, description='Contains a glob pattern to match against the ``StatusReason`` returned for a job. The pattern can contain up to 512 characters. It can contain letters, numbers, periods (.), colons (:), and white spaces (including spaces or tabs). It can optionally end with an asterisk (*) so that only the start of the string needs to be an exact match.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-evaluateonexit.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    evaluate_on_exit_property = batch.CfnJobDefinition.EvaluateOnExitProperty(\n        action="action",\n\n        # the properties below are optional\n        on_exit_code="onExitCode",\n        on_reason="onReason",\n        on_status_reason="onStatusReason"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'on_exit_code', 'on_reason', 'on_status_reason']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EvaluateOnExitProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.FargatePlatformConfigurationProperty
class CfnJobDefinition_FargatePlatformConfigurationPropertyDef(BaseStruct):
    platform_version: typing.Optional[str] = pydantic.Field(None, description='The AWS Fargate platform version where the jobs are running. A platform version is specified only for jobs that are running on Fargate resources. If one isn\'t specified, the ``LATEST`` platform version is used by default. This uses a recent, approved version of the AWS Fargate platform for compute resources. For more information, see `AWS Fargate platform versions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-fargateplatformconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    fargate_platform_configuration_property = batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n        platform_version="platformVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['platform_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.FargatePlatformConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.HostPathProperty
class CfnJobDefinition_HostPathPropertyDef(BaseStruct):
    path: typing.Optional[str] = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.HostPathProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.LinuxParametersProperty
class CfnJobDefinition_LinuxParametersPropertyDef(BaseStruct):
    devices: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_DevicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Any of the host devices to expose to the container. This parameter maps to ``Devices`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--device`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    init_process_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If true, run an ``init`` process inside the container that forwards signals and reaps processes. This parameter maps to the ``--init`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . This parameter requires version 1.25 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"``\n')
    max_swap: typing.Union[int, float, None] = pydantic.Field(None, description="The total amount of swap memory (in MiB) a container can use. This parameter is translated to the ``--memory-swap`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ where the value is the sum of the container memory plus the ``maxSwap`` value. For more information, see ```--memory-swap`` details <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/resource_constraints/#--memory-swap-details>`_ in the Docker documentation. If a ``maxSwap`` value of ``0`` is specified, the container doesn't use swap. Accepted values are ``0`` or any positive integer. If the ``maxSwap`` parameter is omitted, the container doesn't use the swap configuration for the container instance that it's running on. A ``maxSwap`` value must be set for the ``swappiness`` parameter to be used. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description="The value for the size (in MiB) of the ``/dev/shm`` volume. This parameter maps to the ``--shm-size`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description="You can use this parameter to tune a container's memory swappiness behavior. A ``swappiness`` value of ``0`` causes swapping to not occur unless absolutely necessary. A ``swappiness`` value of ``100`` causes pages to be swapped aggressively. Valid values are whole numbers between ``0`` and ``100`` . If the ``swappiness`` parameter isn't specified, a default value of ``60`` is used. If a value isn't specified for ``maxSwap`` , then this parameter is ignored. If ``maxSwap`` is set to 0, the container doesn't use swap. This parameter maps to the ``--memory-swappiness`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . Consider the following when you use a per-container swap configuration. - Swap space must be enabled and allocated on the container instance for the containers to use. .. epigraph:: By default, the Amazon ECS optimized AMIs don't have swap enabled. You must enable swap on the instance to use this feature. For more information, see `Instance store swap volumes <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-store-swap-volumes.html>`_ in the *Amazon EC2 User Guide for Linux Instances* or `How do I allocate memory to work as swap space in an Amazon EC2 instance by using a swap file? <https://docs.aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/>`_ - The swap space parameters are only supported for job definitions using EC2 resources. - If the ``maxSwap`` and ``swappiness`` parameters are omitted from a job definition, each container has a default ``swappiness`` value of 60. Moreover, the total swap usage is limited to two times the memory reservation of the container. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    tmpfs: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_TmpfsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The container path, mount options, and size (in MiB) of the ``tmpfs`` mount. This parameter maps to the ``--tmpfs`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn\'t applicable to jobs that are running on Fargate resources. Don\'t provide this parameter for this resource type.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-linuxparameters.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    linux_parameters_property = batch.CfnJobDefinition.LinuxParametersProperty(\n        devices=[batch.CfnJobDefinition.DeviceProperty(\n            container_path="containerPath",\n            host_path="hostPath",\n            permissions=["permissions"]\n        )],\n        init_process_enabled=False,\n        max_swap=123,\n        shared_memory_size=123,\n        swappiness=123,\n        tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n            container_path="containerPath",\n            size=123,\n\n            # the properties below are optional\n            mount_options=["mountOptions"]\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['devices', 'init_process_enabled', 'max_swap', 'shared_memory_size', 'swappiness', 'tmpfs']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.LinuxParametersProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.LogConfigurationProperty
class CfnJobDefinition_LogConfigurationPropertyDef(BaseStruct):
    log_driver: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log driver to use for the container. The valid values that are listed for this parameter are log drivers that the Amazon ECS container agent can communicate with by default. The supported log drivers are ``awslogs`` , ``fluentd`` , ``gelf`` , ``json-file`` , ``journald`` , ``logentries`` , ``syslog`` , and ``splunk`` . .. epigraph:: Jobs that are running on Fargate resources are restricted to the ``awslogs`` and ``splunk`` log drivers. - **awslogs** - Specifies the Amazon CloudWatch Logs logging driver. For more information, see `Using the awslogs log driver <https://docs.aws.amazon.com/batch/latest/userguide/using_awslogs.html>`_ in the *AWS Batch User Guide* and `Amazon CloudWatch Logs logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/awslogs/>`_ in the Docker documentation. - **fluentd** - Specifies the Fluentd logging driver. For more information including usage and options, see `Fluentd logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/fluentd/>`_ in the *Docker documentation* . - **gelf** - Specifies the Graylog Extended Format (GELF) logging driver. For more information including usage and options, see `Graylog Extended Format logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/gelf/>`_ in the *Docker documentation* . - **journald** - Specifies the journald logging driver. For more information including usage and options, see `Journald logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/journald/>`_ in the *Docker documentation* . - **json-file** - Specifies the JSON file logging driver. For more information including usage and options, see `JSON File logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/json-file/>`_ in the *Docker documentation* . - **splunk** - Specifies the Splunk logging driver. For more information including usage and options, see `Splunk logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/splunk/>`_ in the *Docker documentation* . - **syslog** - Specifies the syslog logging driver. For more information including usage and options, see `Syslog logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/syslog/>`_ in the *Docker documentation* . .. epigraph:: If you have a custom driver that\'s not listed earlier that you want to work with the Amazon ECS container agent, you can fork the Amazon ECS container agent project that\'s `available on GitHub <https://docs.aws.amazon.com/https://github.com/aws/amazon-ecs-agent>`_ and customize it to work with that driver. We encourage you to submit pull requests for changes that you want to have included. However, Amazon Web Services doesn\'t currently support running modified copies of this software. This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"``\n')
    options: typing.Any = pydantic.Field(None, description='The configuration options to send to the log driver. This parameter requires version 1.19 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"``\n')
    secret_options: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The secrets to pass to the log configuration. For more information, see `Specifying sensitive data <https://docs.aws.amazon.com/batch/latest/userguide/specifying-sensitive-data.html>`_ in the *AWS Batch User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-logconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    log_configuration_property = batch.CfnJobDefinition.LogConfigurationProperty(\n        log_driver="logDriver",\n\n        # the properties below are optional\n        options=options,\n        secret_options=[batch.CfnJobDefinition.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.LogConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.MetadataProperty
class CfnJobDefinition_MetadataPropertyDef(BaseStruct):
    labels: typing.Any = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['labels']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.MetadataProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.MountPointsProperty
class CfnJobDefinition_MountPointsPropertyDef(BaseStruct):
    container_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the host volume is mounted.\n')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If this value is ``true`` , the container has read-only access to the volume. Otherwise, the container can write to the volume. The default value is ``false`` .\n')
    source_volume: typing.Optional[str] = pydantic.Field(None, description='The name of the volume to mount.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-mountpoints.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    mount_points_property = batch.CfnJobDefinition.MountPointsProperty(\n        container_path="containerPath",\n        read_only=False,\n        source_volume="sourceVolume"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'read_only', 'source_volume']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.MountPointsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.NetworkConfigurationProperty
class CfnJobDefinition_NetworkConfigurationPropertyDef(BaseStruct):
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description='Indicates whether the job has a public IP address. For a job that\'s running on Fargate resources in a private subnet to send outbound traffic to the internet (for example, to pull container images), the private subnet requires a NAT gateway be attached to route requests to the internet. For more information, see `Amazon ECS task networking <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html>`_ in the *Amazon Elastic Container Service Developer Guide* . The default value is " ``DISABLED`` ".\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-networkconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    network_configuration_property = batch.CfnJobDefinition.NetworkConfigurationProperty(\n        assign_public_ip="assignPublicIp"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['assign_public_ip']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.NetworkConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.NodePropertiesProperty
class CfnJobDefinition_NodePropertiesPropertyDef(BaseStruct):
    main_node: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the node index for the main node of a multi-node parallel job. This node index value must be fewer than the number of nodes.\n')
    node_range_properties: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_NodeRangePropertyPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='A list of node ranges and their properties that are associated with a multi-node parallel job.\n')
    num_nodes: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of nodes that are associated with a multi-node parallel job.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-nodeproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    node_properties_property = batch.CfnJobDefinition.NodePropertiesProperty(\n        main_node=123,\n        node_range_properties=[batch.CfnJobDefinition.NodeRangePropertyProperty(\n            target_nodes="targetNodes",\n\n            # the properties below are optional\n            container=batch.CfnJobDefinition.ContainerPropertiesProperty(\n                image="image",\n\n                # the properties below are optional\n                command=["command"],\n                environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                    name="name",\n                    value="value"\n                )],\n                ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                    size_in_gi_b=123\n                ),\n                execution_role_arn="executionRoleArn",\n                fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                    platform_version="platformVersion"\n                ),\n                instance_type="instanceType",\n                job_role_arn="jobRoleArn",\n                linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                    devices=[batch.CfnJobDefinition.DeviceProperty(\n                        container_path="containerPath",\n                        host_path="hostPath",\n                        permissions=["permissions"]\n                    )],\n                    init_process_enabled=False,\n                    max_swap=123,\n                    shared_memory_size=123,\n                    swappiness=123,\n                    tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                        container_path="containerPath",\n                        size=123,\n\n                        # the properties below are optional\n                        mount_options=["mountOptions"]\n                    )]\n                ),\n                log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                    log_driver="logDriver",\n\n                    # the properties below are optional\n                    options=options,\n                    secret_options=[batch.CfnJobDefinition.SecretProperty(\n                        name="name",\n                        value_from="valueFrom"\n                    )]\n                ),\n                memory=123,\n                mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                    container_path="containerPath",\n                    read_only=False,\n                    source_volume="sourceVolume"\n                )],\n                network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                    assign_public_ip="assignPublicIp"\n                ),\n                privileged=False,\n                readonly_root_filesystem=False,\n                resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                    type="type",\n                    value="value"\n                )],\n                runtime_platform=batch.CfnJobDefinition.RuntimePlatformProperty(\n                    cpu_architecture="cpuArchitecture",\n                    operating_system_family="operatingSystemFamily"\n                ),\n                secrets=[batch.CfnJobDefinition.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )],\n                ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                    hard_limit=123,\n                    name="name",\n                    soft_limit=123\n                )],\n                user="user",\n                vcpus=123,\n                volumes=[batch.CfnJobDefinition.VolumesProperty(\n                    efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                        file_system_id="fileSystemId",\n\n                        # the properties below are optional\n                        authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                            access_point_id="accessPointId",\n                            iam="iam"\n                        ),\n                        root_directory="rootDirectory",\n                        transit_encryption="transitEncryption",\n                        transit_encryption_port=123\n                    ),\n                    host=batch.CfnJobDefinition.VolumesHostProperty(\n                        source_path="sourcePath"\n                    ),\n                    name="name"\n                )]\n            )\n        )],\n        num_nodes=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['main_node', 'node_range_properties', 'num_nodes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.NodePropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.NodeRangePropertyProperty
class CfnJobDefinition_NodeRangePropertyPropertyDef(BaseStruct):
    target_nodes: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The range of nodes, using node index values. A range of ``0:3`` indicates nodes with index values of ``0`` through ``3`` . If the starting range value is omitted ( ``:n`` ), then ``0`` is used to start the range. If the ending range value is omitted ( ``n:`` ), then the highest possible node index is used to end the range. Your accumulative node ranges must account for all nodes ( ``0:n`` ). You can nest node ranges (for example, ``0:10`` and ``4:5`` ). In this case, the ``4:5`` range properties override the ``0:10`` properties.\n')
    container: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The container details for the node range.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-noderangeproperty.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    node_range_property_property = batch.CfnJobDefinition.NodeRangePropertyProperty(\n        target_nodes="targetNodes",\n\n        # the properties below are optional\n        container=batch.CfnJobDefinition.ContainerPropertiesProperty(\n            image="image",\n\n            # the properties below are optional\n            command=["command"],\n            environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                name="name",\n                value="value"\n            )],\n            ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                size_in_gi_b=123\n            ),\n            execution_role_arn="executionRoleArn",\n            fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                platform_version="platformVersion"\n            ),\n            instance_type="instanceType",\n            job_role_arn="jobRoleArn",\n            linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                devices=[batch.CfnJobDefinition.DeviceProperty(\n                    container_path="containerPath",\n                    host_path="hostPath",\n                    permissions=["permissions"]\n                )],\n                init_process_enabled=False,\n                max_swap=123,\n                shared_memory_size=123,\n                swappiness=123,\n                tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                    container_path="containerPath",\n                    size=123,\n\n                    # the properties below are optional\n                    mount_options=["mountOptions"]\n                )]\n            ),\n            log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                log_driver="logDriver",\n\n                # the properties below are optional\n                options=options,\n                secret_options=[batch.CfnJobDefinition.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )]\n            ),\n            memory=123,\n            mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                container_path="containerPath",\n                read_only=False,\n                source_volume="sourceVolume"\n            )],\n            network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                assign_public_ip="assignPublicIp"\n            ),\n            privileged=False,\n            readonly_root_filesystem=False,\n            resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                type="type",\n                value="value"\n            )],\n            runtime_platform=batch.CfnJobDefinition.RuntimePlatformProperty(\n                cpu_architecture="cpuArchitecture",\n                operating_system_family="operatingSystemFamily"\n            ),\n            secrets=[batch.CfnJobDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )],\n            ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                hard_limit=123,\n                name="name",\n                soft_limit=123\n            )],\n            user="user",\n            vcpus=123,\n            volumes=[batch.CfnJobDefinition.VolumesProperty(\n                efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                    file_system_id="fileSystemId",\n\n                    # the properties below are optional\n                    authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                        access_point_id="accessPointId",\n                        iam="iam"\n                    ),\n                    root_directory="rootDirectory",\n                    transit_encryption="transitEncryption",\n                    transit_encryption_port=123\n                ),\n                host=batch.CfnJobDefinition.VolumesHostProperty(\n                    source_path="sourcePath"\n                ),\n                name="name"\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['target_nodes', 'container']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.NodeRangePropertyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.PodPropertiesProperty
class CfnJobDefinition_PodPropertiesPropertyDef(BaseStruct):
    containers: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksContainerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The properties of the container that's used on the Amazon EKS pod.\n")
    dns_policy: typing.Optional[str] = pydantic.Field(None, description="The DNS policy for the pod. The default value is ``ClusterFirst`` . If the ``hostNetwork`` parameter is not specified, the default is ``ClusterFirstWithHostNet`` . ``ClusterFirst`` indicates that any DNS query that does not match the configured cluster domain suffix is forwarded to the upstream nameserver inherited from the node. If no value was specified for ``dnsPolicy`` in the `RegisterJobDefinition <https://docs.aws.amazon.com/batch/latest/APIReference/API_RegisterJobDefinition.html>`_ API operation, then no value will be returned for ``dnsPolicy`` by either of `DescribeJobDefinitions <https://docs.aws.amazon.com/batch/latest/APIReference/API_DescribeJobDefinitions.html>`_ or `DescribeJobs <https://docs.aws.amazon.com/batch/latest/APIReference/API_DescribeJobs.html>`_ API operations. The pod spec setting will contain either ``ClusterFirst`` or ``ClusterFirstWithHostNet`` , depending on the value of the ``hostNetwork`` parameter. For more information, see `Pod's DNS policy <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy>`_ in the *Kubernetes documentation* . Valid values: ``Default`` | ``ClusterFirst`` | ``ClusterFirstWithHostNet``\n")
    host_network: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Indicates if the pod uses the hosts' network IP address. The default value is ``true`` . Setting this to ``false`` enables the Kubernetes pod networking model. Most AWS Batch workloads are egress-only and don't require the overhead of IP allocation for each pod for incoming connections. For more information, see `Host namespaces <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/security/pod-security-policy/#host-namespaces>`_ and `Pod networking <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/workloads/pods/#pod-networking>`_ in the *Kubernetes documentation* .\n")
    metadata: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_MetadataPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    service_account_name: typing.Optional[str] = pydantic.Field(None, description="The name of the service account that's used to run the pod. For more information, see `Kubernetes service accounts <https://docs.aws.amazon.com/eks/latest/userguide/service-accounts.html>`_ and `Configure a Kubernetes service account to assume an IAM role <https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html>`_ in the *Amazon EKS User Guide* and `Configure service accounts for pods <https://docs.aws.amazon.com/https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/>`_ in the *Kubernetes documentation* .\n")
    volumes: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksVolumePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the volumes for a job definition that uses Amazon EKS resources.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-podproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # labels: Any\n    # limits: Any\n    # requests: Any\n\n    pod_properties_property = batch.CfnJobDefinition.PodPropertiesProperty(\n        containers=[batch.CfnJobDefinition.EksContainerProperty(\n            image="image",\n\n            # the properties below are optional\n            args=["args"],\n            command=["command"],\n            env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n                name="name",\n\n                # the properties below are optional\n                value="value"\n            )],\n            image_pull_policy="imagePullPolicy",\n            name="name",\n            resources=batch.CfnJobDefinition.ResourcesProperty(\n                limits=limits,\n                requests=requests\n            ),\n            security_context=batch.CfnJobDefinition.SecurityContextProperty(\n                privileged=False,\n                read_only_root_filesystem=False,\n                run_as_group=123,\n                run_as_non_root=False,\n                run_as_user=123\n            ),\n            volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n                mount_path="mountPath",\n                name="name",\n                read_only=False\n            )]\n        )],\n        dns_policy="dnsPolicy",\n        host_network=False,\n        metadata=batch.CfnJobDefinition.MetadataProperty(\n            labels=labels\n        ),\n        service_account_name="serviceAccountName",\n        volumes=[batch.CfnJobDefinition.EksVolumeProperty(\n            name="name",\n\n            # the properties below are optional\n            empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n                medium="medium",\n                size_limit="sizeLimit"\n            ),\n            host_path=batch.CfnJobDefinition.HostPathProperty(\n                path="path"\n            ),\n            secret=batch.CfnJobDefinition.EksSecretProperty(\n                secret_name="secretName",\n\n                # the properties below are optional\n                optional=False\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['containers', 'dns_policy', 'host_network', 'metadata', 'service_account_name', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.PodPropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.ResourceRequirementProperty
class CfnJobDefinition_ResourceRequirementPropertyDef(BaseStruct):
    type: typing.Optional[str] = pydantic.Field(None, description='The type of resource to assign to a container. The supported resources include ``GPU`` , ``MEMORY`` , and ``VCPU`` .\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The quantity of the specified resource to reserve for the container. The values vary based on the ``type`` specified. - **type="GPU"** - The number of physical GPUs to reserve for the container. Make sure that the number of GPUs reserved for all containers in a job doesn\'t exceed the number of available GPUs on the compute resource that the job is launched on. .. epigraph:: GPUs aren\'t available for jobs that are running on Fargate resources. - **type="MEMORY"** - The memory hard limit (in MiB) present to the container. This parameter is supported for jobs that are running on EC2 resources. If your container attempts to exceed the memory specified, the container is terminated. This parameter maps to ``Memory`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--memory`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . You must specify at least 4 MiB of memory for a job. This is required but can be specified in several places for multi-node parallel (MNP) jobs. It must be specified for each node at least once. This parameter maps to ``Memory`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--memory`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: If you\'re trying to maximize your resource utilization by providing your jobs as much memory as possible for a particular instance type, see `Memory management <https://docs.aws.amazon.com/batch/latest/userguide/memory-management.html>`_ in the *AWS Batch User Guide* . For jobs that are running on Fargate resources, then ``value`` is the hard limit (in MiB), and must match one of the supported values and the ``VCPU`` values must be one of the values supported for that memory value. - **value = 512** - ``VCPU`` = 0.25 - **value = 1024** - ``VCPU`` = 0.25 or 0.5 - **value = 2048** - ``VCPU`` = 0.25, 0.5, or 1 - **value = 3072** - ``VCPU`` = 0.5, or 1 - **value = 4096** - ``VCPU`` = 0.5, 1, or 2 - **value = 5120, 6144, or 7168** - ``VCPU`` = 1 or 2 - **value = 8192** - ``VCPU`` = 1, 2, or 4 - **value = 9216, 10240, 11264, 12288, 13312, 14336, or 15360** - ``VCPU`` = 2 or 4 - **value = 16384** - ``VCPU`` = 2, 4, or 8 - **value = 17408, 18432, 19456, 21504, 22528, 23552, 25600, 26624, 27648, 29696, or 30720** - ``VCPU`` = 4 - **value = 20480, 24576, or 28672** - ``VCPU`` = 4 or 8 - **value = 36864, 45056, 53248, or 61440** - ``VCPU`` = 8 - **value = 32768, 40960, 49152, or 57344** - ``VCPU`` = 8 or 16 - **value = 65536, 73728, 81920, 90112, 98304, 106496, 114688, or 122880** - ``VCPU`` = 16 - **type="VCPU"** - The number of vCPUs reserved for the container. This parameter maps to ``CpuShares`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--cpu-shares`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . Each vCPU is equivalent to 1,024 CPU shares. For EC2 resources, you must specify at least one vCPU. This is required but can be specified in several places; it must be specified for each node at least once. The default for the Fargate On-Demand vCPU resource count quota is 6 vCPUs. For more information about Fargate quotas, see `AWS Fargate quotas <https://docs.aws.amazon.com/general/latest/gr/ecs-service.html#service-quotas-fargate>`_ in the *AWS General Reference* . For jobs that are running on Fargate resources, then ``value`` must match one of the supported values and the ``MEMORY`` values must be one of the values supported for that ``VCPU`` value. The supported values are 0.25, 0.5, 1, 2, 4, 8, and 16 - **value = 0.25** - ``MEMORY`` = 512, 1024, or 2048 - **value = 0.5** - ``MEMORY`` = 1024, 2048, 3072, or 4096 - **value = 1** - ``MEMORY`` = 2048, 3072, 4096, 5120, 6144, 7168, or 8192 - **value = 2** - ``MEMORY`` = 4096, 5120, 6144, 7168, 8192, 9216, 10240, 11264, 12288, 13312, 14336, 15360, or 16384 - **value = 4** - ``MEMORY`` = 8192, 9216, 10240, 11264, 12288, 13312, 14336, 15360, 16384, 17408, 18432, 19456, 20480, 21504, 22528, 23552, 24576, 25600, 26624, 27648, 28672, 29696, or 30720 - **value = 8** - ``MEMORY`` = 16384, 20480, 24576, 28672, 32768, 36864, 40960, 45056, 49152, 53248, 57344, or 61440 - **value = 16** - ``MEMORY`` = 32768, 40960, 49152, 57344, 65536, 73728, 81920, 90112, 98304, 106496, 114688, or 122880\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-resourcerequirement.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    resource_requirement_property = batch.CfnJobDefinition.ResourceRequirementProperty(\n        type="type",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.ResourceRequirementProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.ResourcesProperty
class CfnJobDefinition_ResourcesPropertyDef(BaseStruct):
    limits: typing.Any = pydantic.Field(None, description='')
    requests: typing.Any = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['limits', 'requests']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.ResourcesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.RetryStrategyProperty
class CfnJobDefinition_RetryStrategyPropertyDef(BaseStruct):
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to move a job to the ``RUNNABLE`` status. You can specify between 1 and 10 attempts. If the value of ``attempts`` is greater than one, the job is retried on failure the same number of attempts as the value.\n')
    evaluate_on_exit: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EvaluateOnExitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Array of up to 5 objects that specify the conditions where jobs are retried or failed. If this parameter is specified, then the ``attempts`` parameter must also be specified. If none of the listed conditions match, then the job is retried.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-retrystrategy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    retry_strategy_property = batch.CfnJobDefinition.RetryStrategyProperty(\n        attempts=123,\n        evaluate_on_exit=[batch.CfnJobDefinition.EvaluateOnExitProperty(\n            action="action",\n\n            # the properties below are optional\n            on_exit_code="onExitCode",\n            on_reason="onReason",\n            on_status_reason="onStatusReason"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['attempts', 'evaluate_on_exit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.RetryStrategyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.RuntimePlatformProperty
class CfnJobDefinition_RuntimePlatformPropertyDef(BaseStruct):
    cpu_architecture: typing.Optional[str] = pydantic.Field(None, description='The vCPU architecture. The default value is ``X86_64`` . Valid values are ``X86_64`` and ``ARM64`` . .. epigraph:: This parameter must be set to ``X86_64`` for Windows containers. > Fargate Spot is not supported for ``ARM64`` and Windows-based containers on Fargate. A job queue will be blocked if a Fargate ``ARM64`` or Windows job is submitted to a job queue with only Fargate Spot compute environments. However, you can attach both ``FARGATE`` and ``FARGATE_SPOT`` compute environments to the same job queue.\n')
    operating_system_family: typing.Optional[str] = pydantic.Field(None, description='The operating system for the compute environment. Valid values are: ``LINUX`` (default), ``WINDOWS_SERVER_2019_CORE`` , ``WINDOWS_SERVER_2019_FULL`` , ``WINDOWS_SERVER_2022_CORE`` , and ``WINDOWS_SERVER_2022_FULL`` . .. epigraph:: The following parameters cant be set for Windows containers: ``linuxParameters`` , ``privileged`` , ``user`` , ``ulimits`` , ``readonlyRootFilesystem`` , and ``efsVolumeConfiguration`` . > The AWS Batch Scheduler checks the compute environments that are attached to the job queue before registering a task definition with Fargate. In this scenario, the job queue is where the job is submitted. If the job requires a Windows container and the first compute environment is ``LINUX`` , the compute environment is skipped and the next compute environment is checked until a Windows-based compute environment is found. > Fargate Spot is not supported for ``ARM64`` and Windows-based containers on Fargate. A job queue will be blocked if a Fargate ``ARM64`` or Windows job is submitted to a job queue with only Fargate Spot compute environments. However, you can attach both ``FARGATE`` and ``FARGATE_SPOT`` compute environments to the same job queue.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-runtimeplatform.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    runtime_platform_property = batch.CfnJobDefinition.RuntimePlatformProperty(\n        cpu_architecture="cpuArchitecture",\n        operating_system_family="operatingSystemFamily"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu_architecture', 'operating_system_family']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.RuntimePlatformProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.SecretProperty
class CfnJobDefinition_SecretPropertyDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the secret.\n')
    value_from: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The secret to expose to the container. The supported values are either the full Amazon Resource Name (ARN) of the AWS Secrets Manager secret or the full ARN of the parameter in the AWS Systems Manager Parameter Store. .. epigraph:: If the AWS Systems Manager Parameter Store parameter exists in the same Region as the job you\'re launching, then you can use either the full Amazon Resource Name (ARN) or name of the parameter. If the parameter exists in a different Region, then the full ARN must be specified.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-secret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    secret_property = batch.CfnJobDefinition.SecretProperty(\n        name="name",\n        value_from="valueFrom"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value_from']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.SecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.SecurityContextProperty
class CfnJobDefinition_SecurityContextPropertyDef(BaseStruct):
    privileged: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    read_only_root_filesystem: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    run_as_group: typing.Union[int, float, None] = pydantic.Field(None, description='')
    run_as_non_root: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    run_as_user: typing.Union[int, float, None] = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['privileged', 'read_only_root_filesystem', 'run_as_group', 'run_as_non_root', 'run_as_user']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.SecurityContextProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.TimeoutProperty
class CfnJobDefinition_TimeoutPropertyDef(BaseStruct):
    attempt_duration_seconds: typing.Union[int, float, None] = pydantic.Field(None, description="The job timeout time (in seconds) that's measured from the job attempt's ``startedAt`` timestamp. After this time passes, AWS Batch terminates your jobs if they aren't finished. The minimum value for the timeout is 60 seconds. For array jobs, the timeout applies to the child jobs, not to the parent array job. For multi-node parallel (MNP) jobs, the timeout applies to the whole job, not to the individual nodes.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-timeout.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    timeout_property = batch.CfnJobDefinition.TimeoutProperty(\n        attempt_duration_seconds=123\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['attempt_duration_seconds']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.TimeoutProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.TmpfsProperty
class CfnJobDefinition_TmpfsPropertyDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The absolute file path in the container where the ``tmpfs`` volume is mounted.\n')
    size: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The size (in MiB) of the ``tmpfs`` volume.\n')
    mount_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of ``tmpfs`` volume mount options. Valid values: " ``defaults`` " | " ``ro`` " | " ``rw`` " | " ``suid`` " | " ``nosuid`` " | " ``dev`` " | " ``nodev`` " | " ``exec`` " | " ``noexec`` " | " ``sync`` " | " ``async`` " | " ``dirsync`` " | " ``remount`` " | " ``mand`` " | " ``nomand`` " | " ``atime`` " | " ``noatime`` " | " ``diratime`` " | " ``nodiratime`` " | " ``bind`` " | " ``rbind" | "unbindable" | "runbindable" | "private" | "rprivate" | "shared" | "rshared" | "slave" | "rslave" | "relatime`` " | " ``norelatime`` " | " ``strictatime`` " | " ``nostrictatime`` " | " ``mode`` " | " ``uid`` " | " ``gid`` " | " ``nr_inodes`` " | " ``nr_blocks`` " | " ``mpol`` "\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-tmpfs.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    tmpfs_property = batch.CfnJobDefinition.TmpfsProperty(\n        container_path="containerPath",\n        size=123,\n\n        # the properties below are optional\n        mount_options=["mountOptions"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'size', 'mount_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.TmpfsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.UlimitProperty
class CfnJobDefinition_UlimitPropertyDef(BaseStruct):
    hard_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The hard limit for the ``ulimit`` type.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ``type`` of the ``ulimit`` . Valid values are: ``core`` | ``cpu`` | ``data`` | ``fsize`` | ``locks`` | ``memlock`` | ``msgqueue`` | ``nice`` | ``nofile`` | ``nproc`` | ``rss`` | ``rtprio`` | ``rttime`` | ``sigpending`` | ``stack`` .\n')
    soft_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The soft limit for the ``ulimit`` type.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ulimit.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    ulimit_property = batch.CfnJobDefinition.UlimitProperty(\n        hard_limit=123,\n        name="name",\n        soft_limit=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['hard_limit', 'name', 'soft_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.UlimitProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.VolumesHostProperty
class CfnJobDefinition_VolumesHostPropertyDef(BaseStruct):
    source_path: typing.Optional[str] = pydantic.Field(None, description='The path on the host container instance that\'s presented to the container. If this parameter is empty, then the Docker daemon has assigned a host path for you. If this parameter contains a file location, then the data volume persists at the specified location on the host container instance until you delete it manually. If the source path location doesn\'t exist on the host container instance, the Docker daemon creates it. If the location does exist, the contents of the source path folder are exported. .. epigraph:: This parameter isn\'t applicable to jobs that run on Fargate resources. Don\'t provide this for these jobs.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-volumeshost.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    volumes_host_property = batch.CfnJobDefinition.VolumesHostProperty(\n        source_path="sourcePath"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['source_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.VolumesHostProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.VolumesProperty
class CfnJobDefinition_VolumesPropertyDef(BaseStruct):
    efs_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EfsVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This is used when you're using an Amazon Elastic File System file system for job storage. For more information, see `Amazon EFS Volumes <https://docs.aws.amazon.com/batch/latest/userguide/efs-volumes.html>`_ in the *AWS Batch User Guide* .\n")
    host: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_VolumesHostPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The contents of the ``host`` parameter determine whether your data volume persists on the host container instance and where it's stored. If the host parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data isn't guaranteed to persist after the containers that are associated with it stop running. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources and shouldn't be provided.\n")
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the volume. It can be up to 255 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_). This name is referenced in the ``sourceVolume`` parameter of container definition ``mountPoints`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-volumes.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    volumes_property = batch.CfnJobDefinition.VolumesProperty(\n        efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n            file_system_id="fileSystemId",\n\n            # the properties below are optional\n            authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                access_point_id="accessPointId",\n                iam="iam"\n            ),\n            root_directory="rootDirectory",\n            transit_encryption="transitEncryption",\n            transit_encryption_port=123\n        ),\n        host=batch.CfnJobDefinition.VolumesHostProperty(\n            source_path="sourcePath"\n        ),\n        name="name"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['efs_volume_configuration', 'host', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.VolumesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobQueue.ComputeEnvironmentOrderProperty
class CfnJobQueue_ComputeEnvironmentOrderPropertyDef(BaseStruct):
    compute_environment: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the compute environment.\n')
    order: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The order of the compute environment. Compute environments are tried in ascending order. For example, if two compute environments are associated with a job queue, the compute environment with a lower ``order`` integer value is tried for job placement first.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobqueue-computeenvironmentorder.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    compute_environment_order_property = batch.CfnJobQueue.ComputeEnvironmentOrderProperty(\n        compute_environment="computeEnvironment",\n        order=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment', 'order']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobQueue.ComputeEnvironmentOrderProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicy.FairsharePolicyProperty
class CfnSchedulingPolicy_FairsharePolicyPropertyDef(BaseStruct):
    compute_reservation: typing.Union[int, float, None] = pydantic.Field(None, description="A value used to reserve some of the available maximum vCPU for fair share identifiers that aren't already used. The reserved ratio is ``( *computeReservation* /100)^ *ActiveFairShares*`` where ``*ActiveFairShares*`` is the number of active fair share identifiers. For example, a ``computeReservation`` value of 50 indicates that AWS Batch reserves 50% of the maximum available vCPU if there's only one fair share identifier. It reserves 25% if there are two fair share identifiers. It reserves 12.5% if there are three fair share identifiers. A ``computeReservation`` value of 25 indicates that AWS Batch should reserve 25% of the maximum available vCPU if there's only one fair share identifier, 6.25% if there are two fair share identifiers, and 1.56% if there are three fair share identifiers. The minimum value is 0 and the maximum value is 99.\n")
    share_decay_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time (in seconds) to use to calculate a fair share percentage for each fair share identifier in use. A value of zero (0) indicates that only current usage is measured. The decay allows for more recently run jobs to have more weight than jobs that ran earlier. The maximum supported value is 604800 (1 week).\n')
    share_distribution: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnSchedulingPolicy_ShareAttributesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``SharedIdentifier`` objects that contain the weights for the fair share identifiers for the fair share policy. Fair share identifiers that aren\'t included have a default weight of ``1.0`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-schedulingpolicy-fairsharepolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    fairshare_policy_property = batch.CfnSchedulingPolicy.FairsharePolicyProperty(\n        compute_reservation=123,\n        share_decay_seconds=123,\n        share_distribution=[batch.CfnSchedulingPolicy.ShareAttributesProperty(\n            share_identifier="shareIdentifier",\n            weight_factor=123\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_reservation', 'share_decay_seconds', 'share_distribution']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicy.FairsharePolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicy.ShareAttributesProperty
class CfnSchedulingPolicy_ShareAttributesPropertyDef(BaseStruct):
    share_identifier: typing.Optional[str] = pydantic.Field(None, description="A fair share identifier or fair share identifier prefix. If the string ends with an asterisk (*), this entry specifies the weight factor to use for fair share identifiers that start with that prefix. The list of fair share identifiers in a fair share policy can't overlap. For example, you can't have one that specifies a ``shareIdentifier`` of ``UserA*`` and another that specifies a ``shareIdentifier`` of ``UserA-1`` . There can be no more than 500 fair share identifiers active in a job queue. The string is limited to 255 alphanumeric characters, and can be followed by an asterisk (*).\n")
    weight_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The weight factor for the fair share identifier. The default value is 1.0. A lower value has a higher priority for compute resources. For example, jobs that use a share identifier with a weight factor of 0.125 (1/8) get 8 times the compute resources of jobs that use a share identifier with a weight factor of 1. The smallest supported value is 0.0001, and the largest supported value is 999.9999.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-schedulingpolicy-shareattributes.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    share_attributes_property = batch.CfnSchedulingPolicy.ShareAttributesProperty(\n        share_identifier="shareIdentifier",\n        weight_factor=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['share_identifier', 'weight_factor']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicy.ShareAttributesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.ComputeEnvironmentProps
class ComputeEnvironmentPropsDef(BaseStruct):
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    compute_environment_props = batch.ComputeEnvironmentProps(\n        compute_environment_name="computeEnvironmentName",\n        enabled=False,\n        service_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_name', 'enabled', 'service_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.ComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CustomReason
class CustomReasonDef(BaseStruct):
    on_exit_code: typing.Optional[str] = pydantic.Field(None, description="A glob string that will match on the job exit code. For example, ``'40*'`` will match 400, 404, 40123456789012 Default: - will not match on the exit code\n")
    on_reason: typing.Optional[str] = pydantic.Field(None, description="A glob string that will match on the reason returned by the exiting job For example, ``'CannotPullContainerError*'`` indicates that container needed to start the job could not be pulled. Default: - will not match on the reason\n")
    on_status_reason: typing.Optional[str] = pydantic.Field(None, description='A glob string that will match on the statusReason returned by the exiting job. For example, ``\'Host EC2*\'`` indicates that the spot instance has been reclaimed. Default: - will not match on the status reason\n\n:exampleMetadata: infused\n\nExample::\n\n    job_defn = batch.EcsJobDefinition(self, "JobDefn",\n        container=batch.EcsEc2ContainerDefinition(self, "containerDefn",\n            image=ecs.ContainerImage.from_registry("public.ecr.aws/amazonlinux/amazonlinux:latest"),\n            memory=cdk.Size.mebibytes(2048),\n            cpu=256\n        ),\n        retry_attempts=5,\n        retry_strategies=[\n            batch.RetryStrategy.of(batch.Action.EXIT, batch.Reason.CANNOT_PULL_CONTAINER)\n        ]\n    )\n    job_defn.add_retry_strategy(\n        batch.RetryStrategy.of(batch.Action.EXIT, batch.Reason.SPOT_INSTANCE_RECLAIMED))\n    job_defn.add_retry_strategy(\n        batch.RetryStrategy.of(batch.Action.EXIT, batch.Reason.CANNOT_PULL_CONTAINER))\n    job_defn.add_retry_strategy(\n        batch.RetryStrategy.of(batch.Action.EXIT, batch.Reason.custom(\n            on_exit_code="40*",\n            on_reason="some reason"\n        )))\n')
    _init_params: typing.ClassVar[list[str]] = ['on_exit_code', 'on_reason', 'on_status_reason']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CustomReason'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.Device
class DeviceDef(BaseStruct):
    host_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path for the device on the host container instance.\n')
    container_path: typing.Optional[str] = pydantic.Field(None, description='The path inside the container at which to expose the host device. Default: Same path as the host\n')
    permissions: typing.Optional[typing.Sequence[aws_cdk.aws_batch.DevicePermission]] = pydantic.Field(None, description='The explicit permissions to provide to the container for the device. By default, the container has permissions for read, write, and mknod for the device. Default: Readonly\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    device = batch.Device(\n        host_path="hostPath",\n\n        # the properties below are optional\n        container_path="containerPath",\n        permissions=[batch.DevicePermission.READ]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['host_path', 'container_path', 'permissions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.Device'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.EcsContainerDefinitionProps
class EcsContainerDefinitionPropsDef(BaseStruct):
    cpu: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of vCPUs reserved for the container. Each vCPU is equivalent to 1,024 CPU shares. For containers running on EC2 resources, you must specify at least one vCPU.\n')
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image that this container will run.\n')
    memory: typing.Union[models.SizeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The memory hard limit present to the container. If your container attempts to exceed the memory specified, the container is terminated. You must specify at least 4 MiB of memory for a job.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. Default: - no command\n")
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description="The environment variables to pass to a container. Cannot start with ``AWS_BATCH``. We don't recommend using plaintext environment variables for sensitive information, such as credential data. Default: - no environment variables\n")
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used by Amazon ECS container and AWS Fargate agents to make AWS API calls on your behalf. Default: - a Role will be created\n')
    job_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that the container can assume. Default: - no job role\n')
    linux_parameters: typing.Optional[models.aws_batch.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as details for device mappings. Default: none\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The loging configuration for this Job. Default: - the log configuration of the Docker daemon\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='Gives the container readonly access to its root filesystem. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_batch.SecretDef]] = pydantic.Field(None, description='A map from environment variable names to the secrets for the container. Allows your job definitions to reference the secret by the environment variable name defined in this property. Default: - no secrets\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user name to use inside the container. Default: - no user\n')
    volumes: typing.Optional[typing.Sequence[models.aws_batch.EcsVolumeDef]] = pydantic.Field(None, description='The volumes to mount to this container. Automatically added to the job definition. Default: - no volumes\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # container_image: ecs.ContainerImage\n    # ecs_volume: batch.EcsVolume\n    # linux_parameters: batch.LinuxParameters\n    # log_driver: ecs.LogDriver\n    # role: iam.Role\n    # secret: batch.Secret\n    # size: cdk.Size\n\n    ecs_container_definition_props = batch.EcsContainerDefinitionProps(\n        cpu=123,\n        image=container_image,\n        memory=size,\n\n        # the properties below are optional\n        command=["command"],\n        environment={\n            "environment_key": "environment"\n        },\n        execution_role=role,\n        job_role=role,\n        linux_parameters=linux_parameters,\n        logging=log_driver,\n        readonly_root_filesystem=False,\n        secrets={\n            "secrets_key": secret\n        },\n        user="user",\n        volumes=[ecs_volume]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu', 'image', 'memory', 'command', 'environment', 'execution_role', 'job_role', 'linux_parameters', 'logging', 'readonly_root_filesystem', 'secrets', 'user', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsContainerDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EcsContainerDefinitionPropsDefConfig] = pydantic.Field(None)


class EcsContainerDefinitionPropsDefConfig(pydantic.BaseModel):
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)
    memory_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.EcsEc2ContainerDefinitionProps
class EcsEc2ContainerDefinitionPropsDef(BaseStruct):
    cpu: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of vCPUs reserved for the container. Each vCPU is equivalent to 1,024 CPU shares. For containers running on EC2 resources, you must specify at least one vCPU.\n')
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image that this container will run.\n')
    memory: typing.Union[models.SizeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The memory hard limit present to the container. If your container attempts to exceed the memory specified, the container is terminated. You must specify at least 4 MiB of memory for a job.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. Default: - no command\n")
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description="The environment variables to pass to a container. Cannot start with ``AWS_BATCH``. We don't recommend using plaintext environment variables for sensitive information, such as credential data. Default: - no environment variables\n")
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used by Amazon ECS container and AWS Fargate agents to make AWS API calls on your behalf. Default: - a Role will be created\n')
    job_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that the container can assume. Default: - no job role\n')
    linux_parameters: typing.Optional[models.aws_batch.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as details for device mappings. Default: none\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The loging configuration for this Job. Default: - the log configuration of the Docker daemon\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='Gives the container readonly access to its root filesystem. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_batch.SecretDef]] = pydantic.Field(None, description='A map from environment variable names to the secrets for the container. Allows your job definitions to reference the secret by the environment variable name defined in this property. Default: - no secrets\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user name to use inside the container. Default: - no user\n')
    volumes: typing.Optional[typing.Sequence[models.aws_batch.EcsVolumeDef]] = pydantic.Field(None, description='The volumes to mount to this container. Automatically added to the job definition. Default: - no volumes\n')
    gpu: typing.Union[int, float, None] = pydantic.Field(None, description="The number of physical GPUs to reserve for the container. Make sure that the number of GPUs reserved for all containers in a job doesn't exceed the number of available GPUs on the compute resource that the job is launched on. Default: - no gpus\n")
    privileged: typing.Optional[bool] = pydantic.Field(None, description='When this parameter is true, the container is given elevated permissions on the host container instance (similar to the root user). Default: false\n')
    ulimits: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.UlimitDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Limits to set for the user this docker container will run as. Default: - no ulimits\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.IVpc\n\n\n    ecs_job = batch.EcsJobDefinition(self, "JobDefn",\n        container=batch.EcsEc2ContainerDefinition(self, "containerDefn",\n            image=ecs.ContainerImage.from_registry("public.ecr.aws/amazonlinux/amazonlinux:latest"),\n            memory=cdk.Size.mebibytes(2048),\n            cpu=256\n        )\n    )\n\n    queue = batch.JobQueue(self, "JobQueue",\n        compute_environments=[batch.OrderedComputeEnvironment(\n            compute_environment=batch.ManagedEc2EcsComputeEnvironment(self, "managedEc2CE",\n                vpc=vpc\n            ),\n            order=1\n        )],\n        priority=10\n    )\n\n    user = iam.User(self, "MyUser")\n    ecs_job.grant_submit_job(user, queue)\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu', 'image', 'memory', 'command', 'environment', 'execution_role', 'job_role', 'linux_parameters', 'logging', 'readonly_root_filesystem', 'secrets', 'user', 'volumes', 'gpu', 'privileged', 'ulimits']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsEc2ContainerDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EcsEc2ContainerDefinitionPropsDefConfig] = pydantic.Field(None)


class EcsEc2ContainerDefinitionPropsDefConfig(pydantic.BaseModel):
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)
    memory_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.EcsFargateContainerDefinitionProps
class EcsFargateContainerDefinitionPropsDef(BaseStruct):
    cpu: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of vCPUs reserved for the container. Each vCPU is equivalent to 1,024 CPU shares. For containers running on EC2 resources, you must specify at least one vCPU.\n')
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image that this container will run.\n')
    memory: typing.Union[models.SizeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The memory hard limit present to the container. If your container attempts to exceed the memory specified, the container is terminated. You must specify at least 4 MiB of memory for a job.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. Default: - no command\n")
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description="The environment variables to pass to a container. Cannot start with ``AWS_BATCH``. We don't recommend using plaintext environment variables for sensitive information, such as credential data. Default: - no environment variables\n")
    execution_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used by Amazon ECS container and AWS Fargate agents to make AWS API calls on your behalf. Default: - a Role will be created\n')
    job_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role that the container can assume. Default: - no job role\n')
    linux_parameters: typing.Optional[models.aws_batch.LinuxParametersDef] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as details for device mappings. Default: none\n')
    logging: typing.Optional[models.aws_ecs.LogDriverDef] = pydantic.Field(None, description='The loging configuration for this Job. Default: - the log configuration of the Docker daemon\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='Gives the container readonly access to its root filesystem. Default: false\n')
    secrets: typing.Optional[typing.Mapping[str, models.aws_batch.SecretDef]] = pydantic.Field(None, description='A map from environment variable names to the secrets for the container. Allows your job definitions to reference the secret by the environment variable name defined in this property. Default: - no secrets\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user name to use inside the container. Default: - no user\n')
    volumes: typing.Optional[typing.Sequence[models.aws_batch.EcsVolumeDef]] = pydantic.Field(None, description='The volumes to mount to this container. Automatically added to the job definition. Default: - no volumes\n')
    assign_public_ip: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether the job has a public IP address. For a job that's running on Fargate resources in a private subnet to send outbound traffic to the internet (for example, to pull container images), the private subnet requires a NAT gateway be attached to route requests to the internet. Default: false\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size for ephemeral storage. Default: - 20 GiB\n')
    fargate_platform_version: typing.Optional[aws_cdk.aws_ecs.FargatePlatformVersion] = pydantic.Field(None, description='Which version of Fargate to use when running this container. Default: LATEST\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_ecs as ecs\n    from aws_cdk import aws_iam as iam\n\n    # container_image: ecs.ContainerImage\n    # ecs_volume: batch.EcsVolume\n    # linux_parameters: batch.LinuxParameters\n    # log_driver: ecs.LogDriver\n    # role: iam.Role\n    # secret: batch.Secret\n    # size: cdk.Size\n\n    ecs_fargate_container_definition_props = batch.EcsFargateContainerDefinitionProps(\n        cpu=123,\n        image=container_image,\n        memory=size,\n\n        # the properties below are optional\n        assign_public_ip=False,\n        command=["command"],\n        environment={\n            "environment_key": "environment"\n        },\n        ephemeral_storage_size=size,\n        execution_role=role,\n        fargate_platform_version=ecs.FargatePlatformVersion.LATEST,\n        job_role=role,\n        linux_parameters=linux_parameters,\n        logging=log_driver,\n        readonly_root_filesystem=False,\n        secrets={\n            "secrets_key": secret\n        },\n        user="user",\n        volumes=[ecs_volume]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cpu', 'image', 'memory', 'command', 'environment', 'execution_role', 'job_role', 'linux_parameters', 'logging', 'readonly_root_filesystem', 'secrets', 'user', 'volumes', 'assign_public_ip', 'ephemeral_storage_size', 'fargate_platform_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsFargateContainerDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EcsFargateContainerDefinitionPropsDefConfig] = pydantic.Field(None)


class EcsFargateContainerDefinitionPropsDefConfig(pydantic.BaseModel):
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)
    memory_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.EcsJobDefinitionProps
class EcsJobDefinitionPropsDef(BaseStruct):
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of this job definition. Default: - generated by CloudFormation\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The default parameters passed to the container These parameters can be referenced in the ``command`` that you give to the container. Default: none\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a job. The job is retried on failure the same number of attempts as the value. Default: 1\n')
    retry_strategies: typing.Optional[typing.Sequence[models.aws_batch.RetryStrategyDef]] = pydantic.Field(None, description='Defines the retry behavior for this job. Default: - no ``RetryStrategy``\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of this Job. Only used in Fairshare Scheduling to decide which job to run first when there are multiple jobs with the same share identifier. Default: none\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, Batch terminates your jobs if they aren't finished. Default: - no timeout\n")
    container: typing.Union[_REQUIRED_INIT_PARAM, models.aws_batch.EcsEc2ContainerDefinitionDef, models.aws_batch.EcsFargateContainerDefinitionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The container that this job will run.\n')
    propagate_tags: typing.Optional[bool] = pydantic.Field(None, description='Whether to propogate tags from the JobDefinition to the ECS task that Batch spawns. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.IVpc\n\n\n    ecs_job = batch.EcsJobDefinition(self, "JobDefn",\n        container=batch.EcsEc2ContainerDefinition(self, "containerDefn",\n            image=ecs.ContainerImage.from_registry("public.ecr.aws/amazonlinux/amazonlinux:latest"),\n            memory=cdk.Size.mebibytes(2048),\n            cpu=256\n        )\n    )\n\n    queue = batch.JobQueue(self, "JobQueue",\n        compute_environments=[batch.OrderedComputeEnvironment(\n            compute_environment=batch.ManagedEc2EcsComputeEnvironment(self, "managedEc2CE",\n                vpc=vpc\n            ),\n            order=1\n        )],\n        priority=10\n    )\n\n    user = iam.User(self, "MyUser")\n    ecs_job.grant_submit_job(user, queue)\n')
    _init_params: typing.ClassVar[list[str]] = ['job_definition_name', 'parameters', 'retry_attempts', 'retry_strategies', 'scheduling_priority', 'timeout', 'container', 'propagate_tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsJobDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EcsJobDefinitionPropsDefConfig] = pydantic.Field(None)


class EcsJobDefinitionPropsDefConfig(pydantic.BaseModel):
    container_config: typing.Optional[models._interface_methods.AwsBatchIEcsContainerDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.EcsMachineImage
class EcsMachineImageDef(BaseStruct):
    image: typing.Optional[typing.Union[models.aws_ec2.AmazonLinux2022ImageSsmParameterDef, models.aws_ec2.AmazonLinux2023ImageSsmParameterDef, models.aws_ec2.AmazonLinux2ImageSsmParameterDef, models.aws_ec2.AmazonLinuxImageDef, models.aws_ec2.AmazonLinuxImageSsmParameterBaseDef, models.aws_ec2.GenericLinuxImageDef, models.aws_ec2.GenericSSMParameterImageDef, models.aws_ec2.GenericWindowsImageDef, models.aws_ec2.LookupMachineImageDef, models.aws_ec2.NatInstanceImageDef, models.aws_ec2.ResolveSsmParameterAtLaunchImageDef, models.aws_ec2.WindowsImageDef, models.aws_ecs.BottleRocketImageDef, models.aws_ecs.EcsOptimizedImageDef, models.aws_eks.EksOptimizedImageDef]] = pydantic.Field(None, description='The machine image to use. Default: - chosen by batch\n')
    image_type: typing.Optional[aws_cdk.aws_batch.EcsMachineImageType] = pydantic.Field(None, description="Tells Batch which instance type to launch this image on. Default: - 'ECS_AL2' for non-gpu instances, 'ECS_AL2_NVIDIA' for gpu instances\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_ec2 as ec2\n\n    # machine_image: ec2.IMachineImage\n\n    ecs_machine_image = batch.EcsMachineImage(\n        image=machine_image,\n        image_type=batch.EcsMachineImageType.ECS_AL2\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['image', 'image_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsMachineImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.EcsVolumeOptions
class EcsVolumeOptionsDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the path on the container where this volume is mounted.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    ecs_volume_options = batch.EcsVolumeOptions(\n        container_path="containerPath",\n        name="name",\n\n        # the properties below are optional\n        readonly=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'name', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EcsVolumeOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.EfsVolumeOptions
class EfsVolumeOptionsDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the path on the container where this volume is mounted.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false\n')
    file_system: typing.Union[_REQUIRED_INIT_PARAM, models.aws_efs.FileSystemDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The EFS File System that supports this volume.\n')
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, ``rootDirectory`` must either be omitted or set to ``/`` which enforces the path set on the EFS access point. If an access point is used, ``enableTransitEncryption`` must be ``true``. Default: - no accessPointId\n')
    enable_transit_encryption: typing.Optional[bool] = pydantic.Field(None, description='Enables encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Default: false\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume is used instead. Specifying ``/`` has the same effect as omitting this parameter. The maximum length is 4,096 characters. Default: - root of the EFS File System\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. The value must be between 0 and 65,535. Default: - chosen by the EFS Mount Helper\n')
    use_job_role: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use the AWS Batch job IAM role defined in a job definition when mounting the Amazon EFS file system. If specified, ``enableTransitEncryption`` must be ``true``. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    # my_file_system: efs.IFileSystem\n    # my_job_role: iam.Role\n\n    my_file_system.grant_read(my_job_role)\n\n    job_defn = batch.EcsJobDefinition(self, "JobDefn",\n        container=batch.EcsEc2ContainerDefinition(self, "containerDefn",\n            image=ecs.ContainerImage.from_registry("public.ecr.aws/amazonlinux/amazonlinux:latest"),\n            memory=cdk.Size.mebibytes(2048),\n            cpu=256,\n            volumes=[batch.EcsVolume.efs(\n                name="myVolume",\n                file_system=my_file_system,\n                container_path="/Volumes/myVolume",\n                use_job_role=True\n            )],\n            job_role=my_job_role\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'name', 'readonly', 'file_system', 'access_point_id', 'enable_transit_encryption', 'root_directory', 'transit_encryption_port', 'use_job_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EfsVolumeOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EfsVolumeOptionsDefConfig] = pydantic.Field(None)


class EfsVolumeOptionsDefConfig(pydantic.BaseModel):
    file_system_config: typing.Optional[models._interface_methods.AwsEfsIFileSystemDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.EksContainerDefinitionProps
class EksContainerDefinitionPropsDef(BaseStruct):
    image: typing.Union[models.aws_ecs.ContainerImageDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The image that this container will run.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The entrypoint for the container. This isn\'t run within a shell. If this isn\'t specified, the ``ENTRYPOINT`` of the container image is used. Environment variable references are expanded using the container\'s environment. If the referenced environment variable doesn\'t exist, the reference in the command isn\'t changed. For example, if the reference is to ``"$(NAME1)"`` and the ``NAME1`` environment variable doesn\'t exist, the command string will remain ``"$(NAME1)."`` ``$$`` is replaced with ``$`` and the resulting string isn\'t expanded. For example, ``$$(VAR_NAME)`` will be passed as ``$(VAR_NAME)`` whether or not the ``VAR_NAME`` environment variable exists. The entrypoint can\'t be updated. Default: - no command\n')
    cpu_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The hard limit of CPUs to present to this container. Must be an even multiple of 0.25. If your container attempts to exceed this limit, it will be terminated. At least one of ``cpuReservation`` and ``cpuLimit`` is required. If both are specified, then ``cpuLimit`` must be at least as large as ``cpuReservation``. Default: - No CPU limit\n')
    cpu_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit of CPUs to reserve for the container Must be an even multiple of 0.25. The container will given at least this many CPUs, but may consume more. At least one of ``cpuReservation`` and ``cpuLimit`` is required. If both are specified, then ``cpuLimit`` must be at least as large as ``cpuReservation``. Default: - No CPUs reserved\n')
    env: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to this container. *Note*: Environment variables cannot start with "AWS_BATCH". This naming convention is reserved for variables that AWS Batch sets. Default: - no environment variables\n')
    gpu_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The hard limit of GPUs to present to this container. If your container attempts to exceed this limit, it will be terminated. If both ``gpuReservation`` and ``gpuLimit`` are specified, then ``gpuLimit`` must be equal to ``gpuReservation``. Default: - No GPU limit\n')
    gpu_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='The soft limit of CPUs to reserve for the container Must be an even multiple of 0.25. The container will given at least this many CPUs, but may consume more. If both ``gpuReservation`` and ``gpuLimit`` are specified, then ``gpuLimit`` must be equal to ``gpuReservation``. Default: - No GPUs reserved\n')
    image_pull_policy: typing.Optional[aws_cdk.aws_batch.ImagePullPolicy] = pydantic.Field(None, description='The image pull policy for this container. Default: - ``ALWAYS`` if the ``:latest`` tag is specified, ``IF_NOT_PRESENT`` otherwise\n')
    memory_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The amount (in MiB) of memory to present to the container. If your container attempts to exceed the allocated memory, it will be terminated. Must be larger that 4 MiB At least one of ``memoryLimit`` and ``memoryReservation`` is required *Note*: To maximize your resource utilization, provide your jobs with as much memory as possible for the specific instance type that you are using. Default: - No memory limit\n')
    memory_reservation: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The soft limit (in MiB) of memory to reserve for the container. Your container will be given at least this much memory, but may consume more. Must be larger that 4 MiB When system memory is under heavy contention, Docker attempts to keep the container memory to this soft limit. However, your container can consume more memory when it needs to, up to either the hard limit specified with the memory parameter (if applicable), or all of the available memory on the container instance, whichever comes first. At least one of ``memoryLimit`` and ``memoryReservation`` is required. If both are specified, then ``memoryLimit`` must be equal to ``memoryReservation`` *Note*: To maximize your resource utilization, provide your jobs with as much memory as possible for the specific instance type that you are using. Default: - No memory reserved\n')
    name: typing.Optional[str] = pydantic.Field(None, description="The name of this container. Default: : ``'Default'``\n")
    privileged: typing.Optional[bool] = pydantic.Field(None, description='If specified, gives this container elevated permissions on the host container instance. The level of permissions are similar to the root user permissions. This parameter maps to ``privileged`` policy in the Privileged pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: false\n')
    readonly_root_filesystem: typing.Optional[bool] = pydantic.Field(None, description='If specified, gives this container readonly access to its root file system. This parameter maps to ``ReadOnlyRootFilesystem`` policy in the Volumes and file systems pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: false\n')
    run_as_group: typing.Union[int, float, None] = pydantic.Field(None, description="If specified, the container is run as the specified group ID (``gid``). If this parameter isn't specified, the default is the group that's specified in the image metadata. This parameter maps to ``RunAsGroup`` and ``MustRunAs`` policy in the Users and groups pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: none\n")
    run_as_root: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container is run as a user with a ``uid`` other than 0. Otherwise, no such rule is enforced. This parameter maps to ``RunAsUser`` and ``MustRunAsNonRoot`` policy in the Users and groups pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: - the container is *not* required to run as a non-root user\n')
    run_as_user: typing.Union[int, float, None] = pydantic.Field(None, description='If specified, this container is run as the specified user ID (``uid``). This parameter maps to ``RunAsUser`` and ``MustRunAs`` policy in the Users and groups pod security policies in the Kubernetes documentation. *Note*: this is only compatible with Kubernetes < v1.25 Default: - the user that is specified in the image metadata.\n')
    volumes: typing.Optional[typing.Sequence[models.aws_batch.EksVolumeDef]] = pydantic.Field(None, description='The Volumes to mount to this container. Automatically added to the Pod. Default: - no volumes\n\n:exampleMetadata: infused\n\nExample::\n\n    job_defn = batch.EksJobDefinition(self, "eksf2",\n        container=batch.EksContainerDefinition(self, "container",\n            image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n            volumes=[batch.EksVolume.empty_dir(\n                name="myEmptyDirVolume",\n                mount_path="/mount/path",\n                medium=batch.EmptyDirMediumType.MEMORY,\n                readonly=True,\n                size_limit=cdk.Size.mebibytes(2048)\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'cpu_limit', 'cpu_reservation', 'env', 'gpu_limit', 'gpu_reservation', 'image_pull_policy', 'memory_limit', 'memory_reservation', 'name', 'privileged', 'readonly_root_filesystem', 'run_as_group', 'run_as_root', 'run_as_user', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EksContainerDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EksContainerDefinitionPropsDefConfig] = pydantic.Field(None)


class EksContainerDefinitionPropsDefConfig(pydantic.BaseModel):
    image_config: typing.Optional[models.aws_ecs.ContainerImageDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.EksJobDefinitionProps
class EksJobDefinitionPropsDef(BaseStruct):
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of this job definition. Default: - generated by CloudFormation\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The default parameters passed to the container These parameters can be referenced in the ``command`` that you give to the container. Default: none\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a job. The job is retried on failure the same number of attempts as the value. Default: 1\n')
    retry_strategies: typing.Optional[typing.Sequence[models.aws_batch.RetryStrategyDef]] = pydantic.Field(None, description='Defines the retry behavior for this job. Default: - no ``RetryStrategy``\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of this Job. Only used in Fairshare Scheduling to decide which job to run first when there are multiple jobs with the same share identifier. Default: none\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, Batch terminates your jobs if they aren't finished. Default: - no timeout\n")
    container: typing.Union[models.aws_batch.EksContainerDefinitionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The container this Job Definition will run.\n')
    dns_policy: typing.Optional[aws_cdk.aws_batch.DnsPolicy] = pydantic.Field(None, description='The DNS Policy of the pod used by this Job Definition. Default: ``DnsPolicy.CLUSTER_FIRST``\n')
    service_account: typing.Optional[str] = pydantic.Field(None, description="The name of the service account that's used to run the container. service accounts are Kubernetes method of identification and authentication, roughly analogous to IAM users. Default: - the default service account of the container\n")
    use_host_network: typing.Optional[bool] = pydantic.Field(None, description='If specified, the Pod used by this Job Definition will use the host\'s network IP address. Otherwise, the Kubernetes pod networking model is enabled. Most AWS Batch workloads are egress-only and don\'t require the overhead of IP allocation for each pod for incoming connections. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    job_defn = batch.EksJobDefinition(self, "eksf2",\n        container=batch.EksContainerDefinition(self, "container",\n            image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n            volumes=[batch.EksVolume.empty_dir(\n                name="myEmptyDirVolume",\n                mount_path="/mount/path",\n                medium=batch.EmptyDirMediumType.MEMORY,\n                readonly=True,\n                size_limit=cdk.Size.mebibytes(2048)\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['job_definition_name', 'parameters', 'retry_attempts', 'retry_strategies', 'scheduling_priority', 'timeout', 'container', 'dns_policy', 'service_account', 'use_host_network']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EksJobDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.EksJobDefinitionPropsDefConfig] = pydantic.Field(None)


class EksJobDefinitionPropsDefConfig(pydantic.BaseModel):
    container_config: typing.Optional[models.aws_batch.EksContainerDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.EksMachineImage
class EksMachineImageDef(BaseStruct):
    image: typing.Optional[typing.Union[models.aws_ec2.AmazonLinux2022ImageSsmParameterDef, models.aws_ec2.AmazonLinux2023ImageSsmParameterDef, models.aws_ec2.AmazonLinux2ImageSsmParameterDef, models.aws_ec2.AmazonLinuxImageDef, models.aws_ec2.AmazonLinuxImageSsmParameterBaseDef, models.aws_ec2.GenericLinuxImageDef, models.aws_ec2.GenericSSMParameterImageDef, models.aws_ec2.GenericWindowsImageDef, models.aws_ec2.LookupMachineImageDef, models.aws_ec2.NatInstanceImageDef, models.aws_ec2.ResolveSsmParameterAtLaunchImageDef, models.aws_ec2.WindowsImageDef, models.aws_ecs.BottleRocketImageDef, models.aws_ecs.EcsOptimizedImageDef, models.aws_eks.EksOptimizedImageDef]] = pydantic.Field(None, description='The machine image to use. Default: - chosen by batch\n')
    image_type: typing.Optional[aws_cdk.aws_batch.EksMachineImageType] = pydantic.Field(None, description="Tells Batch which instance type to launch this image on. Default: - 'EKS_AL2' for non-gpu instances, 'EKS_AL2_NVIDIA' for gpu instances\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_ec2 as ec2\n\n    # machine_image: ec2.IMachineImage\n\n    eks_machine_image = batch.EksMachineImage(\n        image=machine_image,\n        image_type=batch.EksMachineImageType.EKS_AL2\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['image', 'image_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EksMachineImage'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.EksVolumeOptions
class EksVolumeOptionsDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_volume_options = batch.EksVolumeOptions(\n        name="name",\n\n        # the properties below are optional\n        mount_path="mountPath",\n        readonly=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'mount_path', 'readonly']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EksVolumeOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.EmptyDirVolumeOptions
class EmptyDirVolumeOptionsDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n')
    medium: typing.Optional[aws_cdk.aws_batch.EmptyDirMediumType] = pydantic.Field(None, description='The storage type to use for this Volume. Default: ``EmptyDirMediumType.DISK``\n')
    size_limit: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The maximum size for this Volume. Default: - no size limit\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\n:exampleMetadata: infused\n\nExample::\n\n    job_defn = batch.EksJobDefinition(self, "eksf2",\n        container=batch.EksContainerDefinition(self, "container",\n            image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n            volumes=[batch.EksVolume.empty_dir(\n                name="myEmptyDirVolume",\n                mount_path="/mount/path",\n                medium=batch.EmptyDirMediumType.MEMORY,\n                readonly=True,\n                size_limit=cdk.Size.mebibytes(2048)\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'mount_path', 'readonly', 'medium', 'size_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.EmptyDirVolumeOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.FairshareSchedulingPolicyProps
class FairshareSchedulingPolicyPropsDef(BaseStruct):
    compute_reservation: typing.Union[int, float, None] = pydantic.Field(None, description="Used to calculate the percentage of the maximum available vCPU to reserve for share identifiers not present in the Queue. The percentage reserved is defined by the Scheduler as: ``(computeReservation/100)^ActiveFairShares`` where ``ActiveFairShares`` is the number of active fair share identifiers. For example, a computeReservation value of 50 indicates that AWS Batch reserves 50% of the maximum available vCPU if there's only one fair share identifier. It reserves 25% if there are two fair share identifiers. It reserves 12.5% if there are three fair share identifiers. A computeReservation value of 25 indicates that AWS Batch should reserve 25% of the maximum available vCPU if there's only one fair share identifier, 6.25% if there are two fair share identifiers, and 1.56% if there are three fair share identifiers. Default: - no vCPU is reserved\n")
    scheduling_policy_name: typing.Optional[str] = pydantic.Field(None, description='The name of this SchedulingPolicy. Default: - generated by CloudFormation\n')
    share_decay: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The amount of time to use to measure the usage of each job. The usage is used to calculate a fair share percentage for each fair share identifier currently in the Queue. A value of zero (0) indicates that only current usage is measured. The decay is linear and gives preference to newer jobs. The maximum supported value is 604800 seconds (1 week). Default: - 0: only the current job usage is considered\n')
    shares: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.ShareDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The shares that this Scheduling Policy applies to. *Note*: It is possible to submit Jobs to the queue with Share Identifiers that are not recognized by the Scheduling Policy. Default: - no shares\n\n:exampleMetadata: infused\n\nExample::\n\n    fairshare_policy = batch.FairshareSchedulingPolicy(self, "myFairsharePolicy",\n        share_decay=cdk.Duration.minutes(5)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_reservation', 'scheduling_policy_name', 'share_decay', 'shares']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.FairshareSchedulingPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.FargateComputeEnvironmentProps
class FargateComputeEnvironmentPropsDef(BaseStruct):
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='VPC in which this Compute Environment will launch Instances.\n')
    maxv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum vCpus this ``ManagedComputeEnvironment`` can scale up to. Each vCPU is equivalent to 1024 CPU shares. *Note*: if this Compute Environment uses EC2 resources (not Fargate) with either ``AllocationStrategy.BEST_FIT_PROGRESSIVE`` or ``AllocationStrategy.SPOT_CAPACITY_OPTIMIZED``, or ``AllocationStrategy.BEST_FIT`` with Spot instances, The scheduler may exceed this number by at most one of the instances specified in ``instanceTypes`` or ``instanceClasses``. Default: 256\n')
    replace_compute_environment: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether this Compute Environment is replaced if an update is made that requires replacing its instances. To enable more properties to be updated, set this property to ``false``. When changing the value of this property to false, do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the UPDATE_ROLLBACK_FAILED state. You can't update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. The properties which require a replacement of the Compute Environment are: Default: false\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups this Compute Environment will launch instances in. Default: new security groups will be created\n')
    spot: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use spot instances. Spot instances are less expensive EC2 instances that can be reclaimed by EC2 at any time; your job will be given two minutes of notice before reclamation. Default: false\n')
    terminate_on_update: typing.Optional[bool] = pydantic.Field(None, description="Whether or not any running jobs will be immediately terminated when an infrastructure update occurs. If this is enabled, any terminated jobs may be retried, depending on the job's retry policy. Default: false\n")
    update_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Only meaningful if ``terminateOnUpdate`` is ``false``. If so, when an infrastructure update is triggered, any running jobs will be allowed to run until ``updateTimeout`` has expired. Default: 30 minutes\n')
    update_to_latest_image_version: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the AMI is updated to the latest one supported by Batch when an infrastructure update occurs. If you specify a specific AMI, this property will be ignored. Default: true\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC Subnets this Compute Environment will launch instances in. Default: new subnets will be created\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.IVpc\n\n    shared_compute_env = batch.FargateComputeEnvironment(self, "spotEnv",\n        vpc=vpc,\n        spot=True\n    )\n    low_priority_queue = batch.JobQueue(self, "JobQueue",\n        priority=1\n    )\n    high_priority_queue = batch.JobQueue(self, "JobQueue",\n        priority=10\n    )\n    low_priority_queue.add_compute_environment(shared_compute_env, 1)\n    high_priority_queue.add_compute_environment(shared_compute_env, 1)\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_name', 'enabled', 'service_role', 'vpc', 'maxv_cpus', 'replace_compute_environment', 'security_groups', 'spot', 'terminate_on_update', 'update_timeout', 'update_to_latest_image_version', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.FargateComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.FargateComputeEnvironmentPropsDefConfig] = pydantic.Field(None)


class FargateComputeEnvironmentPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.HostPathVolumeOptions
class HostPathVolumeOptionsDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n')
    host_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path of the file or directory on the host to mount into containers on the pod. *Note*: HothPath Volumes present many security risks, and should be avoided when possible.\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\n:exampleMetadata: infused\n\nExample::\n\n    # job_defn: batch.EksJobDefinition\n\n    job_defn.container.add_volume(batch.EksVolume.empty_dir(\n        name="emptyDir",\n        mount_path="/Volumes/emptyDir"\n    ))\n    job_defn.container.add_volume(batch.EksVolume.host_path(\n        name="hostPath",\n        host_path="/sys",\n        mount_path="/Volumes/hostPath"\n    ))\n    job_defn.container.add_volume(batch.EksVolume.secret(\n        name="secret",\n        optional=True,\n        mount_path="/Volumes/secret",\n        secret_name="mySecret"\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'mount_path', 'readonly', 'host_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.HostPathVolumeOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.HostVolumeOptions
class HostVolumeOptionsDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the path on the container where this volume is mounted.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='the name of this volume.\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='if set, the container will have readonly access to the volume. Default: false\n')
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path on the host machine this container will have access to. Default: - Docker will choose the host path. The data may not persist after the containers that use it stop running.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    host_volume_options = batch.HostVolumeOptions(\n        container_path="containerPath",\n        name="name",\n\n        # the properties below are optional\n        host_path="hostPath",\n        readonly=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'name', 'readonly', 'host_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.HostVolumeOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.JobDefinitionProps
class JobDefinitionPropsDef(BaseStruct):
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of this job definition. Default: - generated by CloudFormation\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The default parameters passed to the container These parameters can be referenced in the ``command`` that you give to the container. Default: none\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a job. The job is retried on failure the same number of attempts as the value. Default: 1\n')
    retry_strategies: typing.Optional[typing.Sequence[models.aws_batch.RetryStrategyDef]] = pydantic.Field(None, description='Defines the retry behavior for this job. Default: - no ``RetryStrategy``\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of this Job. Only used in Fairshare Scheduling to decide which job to run first when there are multiple jobs with the same share identifier. Default: none\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, Batch terminates your jobs if they aren\'t finished. Default: - no timeout\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_batch as batch\n\n    # parameters: Any\n    # retry_strategy: batch.RetryStrategy\n\n    job_definition_props = batch.JobDefinitionProps(\n        job_definition_name="jobDefinitionName",\n        parameters={\n            "parameters_key": parameters\n        },\n        retry_attempts=123,\n        retry_strategies=[retry_strategy],\n        scheduling_priority=123,\n        timeout=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['job_definition_name', 'parameters', 'retry_attempts', 'retry_strategies', 'scheduling_priority', 'timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.JobDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.JobQueueProps
class JobQueuePropsDef(BaseStruct):
    compute_environments: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.OrderedComputeEnvironmentDef, dict[str, typing.Any]]]] = pydantic.Field(None, description="The set of compute environments mapped to a job queue and their order relative to each other. The job scheduler uses this parameter to determine which compute environment runs a specific job. Compute environments must be in the VALID state before you can associate them with a job queue. You can associate up to three compute environments with a job queue. All of the compute environments must be either EC2 (EC2 or SPOT) or Fargate (FARGATE or FARGATE_SPOT); EC2 and Fargate compute environments can't be mixed. *Note*: All compute environments that are associated with a job queue must share the same architecture. AWS Batch doesn't support mixing compute environment architecture types in a single job queue. Default: none\n")
    enabled: typing.Optional[bool] = pydantic.Field(None, description="If the job queue is enabled, it is able to accept jobs. Otherwise, new jobs can't be added to the queue, but jobs already in the queue can finish. Default: true\n")
    job_queue_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job queue. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_) Default: - no name\n')
    priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of the job queue. Job queues with a higher priority are evaluated first when associated with the same compute environment. Priority is determined in descending order. For example, a job queue with a priority of 10 is given scheduling preference over a job queue with a priority of 1. Default: 1\n')
    scheduling_policy: typing.Optional[typing.Union[models.aws_batch.FairshareSchedulingPolicyDef]] = pydantic.Field(None, description='The SchedulingPolicy for this JobQueue. Instructs the Scheduler how to schedule different jobs. Default: - no scheduling policy\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.IVpc\n\n\n    ecs_job = batch.EcsJobDefinition(self, "JobDefn",\n        container=batch.EcsEc2ContainerDefinition(self, "containerDefn",\n            image=ecs.ContainerImage.from_registry("public.ecr.aws/amazonlinux/amazonlinux:latest"),\n            memory=cdk.Size.mebibytes(2048),\n            cpu=256\n        )\n    )\n\n    queue = batch.JobQueue(self, "JobQueue",\n        compute_environments=[batch.OrderedComputeEnvironment(\n            compute_environment=batch.ManagedEc2EcsComputeEnvironment(self, "managedEc2CE",\n                vpc=vpc\n            ),\n            order=1\n        )],\n        priority=10\n    )\n\n    user = iam.User(self, "MyUser")\n    ecs_job.grant_submit_job(user, queue)\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environments', 'enabled', 'job_queue_name', 'priority', 'scheduling_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.JobQueueProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.LinuxParametersProps
class LinuxParametersPropsDef(BaseStruct):
    init_process_enabled: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to run an init process inside the container that forwards signals and reaps processes. Default: false\n')
    max_swap: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The total amount of swap memory a container can use. This parameter will be translated to the --memory-swap option to docker run. This parameter is only supported when you are using the EC2 launch type. Accepted values are positive integers. Default: No swap.\n')
    shared_memory_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The value for the size of the /dev/shm volume. Default: No shared memory.\n')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description="This allows you to tune a container's memory swappiness behavior. This parameter maps to the --memory-swappiness option to docker run. The swappiness relates to the kernel's tendency to swap memory. A value of 0 will cause swapping to not happen unless absolutely necessary. A value of 100 will cause pages to be swapped very aggressively. This parameter is only supported when you are using the EC2 launch type. Accepted values are whole numbers between 0 and 100. If a value is not specified for maxSwap then this parameter is ignored. Default: 60\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_batch as batch\n\n    # size: cdk.Size\n\n    linux_parameters_props = batch.LinuxParametersProps(\n        init_process_enabled=False,\n        max_swap=size,\n        shared_memory_size=size,\n        swappiness=123\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['init_process_enabled', 'max_swap', 'shared_memory_size', 'swappiness']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.LinuxParametersProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.ManagedComputeEnvironmentProps
class ManagedComputeEnvironmentPropsDef(BaseStruct):
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='VPC in which this Compute Environment will launch Instances.\n')
    maxv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum vCpus this ``ManagedComputeEnvironment`` can scale up to. Each vCPU is equivalent to 1024 CPU shares. *Note*: if this Compute Environment uses EC2 resources (not Fargate) with either ``AllocationStrategy.BEST_FIT_PROGRESSIVE`` or ``AllocationStrategy.SPOT_CAPACITY_OPTIMIZED``, or ``AllocationStrategy.BEST_FIT`` with Spot instances, The scheduler may exceed this number by at most one of the instances specified in ``instanceTypes`` or ``instanceClasses``. Default: 256\n')
    replace_compute_environment: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether this Compute Environment is replaced if an update is made that requires replacing its instances. To enable more properties to be updated, set this property to ``false``. When changing the value of this property to false, do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the UPDATE_ROLLBACK_FAILED state. You can't update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. The properties which require a replacement of the Compute Environment are: Default: false\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups this Compute Environment will launch instances in. Default: new security groups will be created\n')
    spot: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use spot instances. Spot instances are less expensive EC2 instances that can be reclaimed by EC2 at any time; your job will be given two minutes of notice before reclamation. Default: false\n')
    terminate_on_update: typing.Optional[bool] = pydantic.Field(None, description="Whether or not any running jobs will be immediately terminated when an infrastructure update occurs. If this is enabled, any terminated jobs may be retried, depending on the job's retry policy. Default: false\n")
    update_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Only meaningful if ``terminateOnUpdate`` is ``false``. If so, when an infrastructure update is triggered, any running jobs will be allowed to run until ``updateTimeout`` has expired. Default: 30 minutes\n')
    update_to_latest_image_version: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the AMI is updated to the latest one supported by Batch when an infrastructure update occurs. If you specify a specific AMI, this property will be ignored. Default: true\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC Subnets this Compute Environment will launch instances in. Default: new subnets will be created\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n    # security_group: ec2.SecurityGroup\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # vpc: ec2.Vpc\n\n    managed_compute_environment_props = batch.ManagedComputeEnvironmentProps(\n        vpc=vpc,\n\n        # the properties below are optional\n        compute_environment_name="computeEnvironmentName",\n        enabled=False,\n        maxv_cpus=123,\n        replace_compute_environment=False,\n        security_groups=[security_group],\n        service_role=role,\n        spot=False,\n        terminate_on_update=False,\n        update_timeout=cdk.Duration.minutes(30),\n        update_to_latest_image_version=False,\n        vpc_subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_name', 'enabled', 'service_role', 'vpc', 'maxv_cpus', 'replace_compute_environment', 'security_groups', 'spot', 'terminate_on_update', 'update_timeout', 'update_to_latest_image_version', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.ManagedComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.ManagedComputeEnvironmentPropsDefConfig] = pydantic.Field(None)


class ManagedComputeEnvironmentPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.ManagedEc2EcsComputeEnvironmentProps
class ManagedEc2EcsComputeEnvironmentPropsDef(BaseStruct):
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='VPC in which this Compute Environment will launch Instances.\n')
    maxv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum vCpus this ``ManagedComputeEnvironment`` can scale up to. Each vCPU is equivalent to 1024 CPU shares. *Note*: if this Compute Environment uses EC2 resources (not Fargate) with either ``AllocationStrategy.BEST_FIT_PROGRESSIVE`` or ``AllocationStrategy.SPOT_CAPACITY_OPTIMIZED``, or ``AllocationStrategy.BEST_FIT`` with Spot instances, The scheduler may exceed this number by at most one of the instances specified in ``instanceTypes`` or ``instanceClasses``. Default: 256\n')
    replace_compute_environment: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether this Compute Environment is replaced if an update is made that requires replacing its instances. To enable more properties to be updated, set this property to ``false``. When changing the value of this property to false, do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the UPDATE_ROLLBACK_FAILED state. You can't update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. The properties which require a replacement of the Compute Environment are: Default: false\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups this Compute Environment will launch instances in. Default: new security groups will be created\n')
    spot: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use spot instances. Spot instances are less expensive EC2 instances that can be reclaimed by EC2 at any time; your job will be given two minutes of notice before reclamation. Default: false\n')
    terminate_on_update: typing.Optional[bool] = pydantic.Field(None, description="Whether or not any running jobs will be immediately terminated when an infrastructure update occurs. If this is enabled, any terminated jobs may be retried, depending on the job's retry policy. Default: false\n")
    update_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Only meaningful if ``terminateOnUpdate`` is ``false``. If so, when an infrastructure update is triggered, any running jobs will be allowed to run until ``updateTimeout`` has expired. Default: 30 minutes\n')
    update_to_latest_image_version: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the AMI is updated to the latest one supported by Batch when an infrastructure update occurs. If you specify a specific AMI, this property will be ignored. Default: true\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC Subnets this Compute Environment will launch instances in. Default: new subnets will be created\n')
    allocation_strategy: typing.Optional[aws_cdk.aws_batch.AllocationStrategy] = pydantic.Field(None, description='The allocation strategy to use if not enough instances of the best fitting instance type can be allocated. Default: - ``BEST_FIT_PROGRESSIVE`` if not using Spot instances, ``SPOT_CAPACITY_OPTIMIZED`` if using Spot instances.\n')
    images: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.EcsMachineImageDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configure which AMIs this Compute Environment can launch. If you specify this property with only ``image`` specified, then the ``imageType`` will default to ``ECS_AL2``. *If your image needs GPU resources, specify ``ECS_AL2_NVIDIA``; otherwise, the instances will not be able to properly join the ComputeEnvironment*. Default: - ECS_AL2 for non-GPU instances, ECS_AL2_NVIDIA for GPU instances\n')
    instance_classes: typing.Optional[typing.Sequence[aws_cdk.aws_ec2.InstanceClass]] = pydantic.Field(None, description='The instance classes that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Batch will automatically choose the instance size. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    instance_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The execution Role that instances launched by this Compute Environment will use. Default: - a role will be created\n')
    instance_types: typing.Optional[typing.Sequence[models.aws_ec2.InstanceTypeDef]] = pydantic.Field(None, description='The instance types that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    launch_template: typing.Optional[typing.Union[models.aws_ec2.LaunchTemplateDef]] = pydantic.Field(None, description='The Launch Template that this Compute Environment will use to provision EC2 Instances. *Note*: if ``securityGroups`` is specified on both your launch template and this Compute Environment, **the ``securityGroup``s on the Compute Environment override the ones on the launch template. Default: no launch template\n')
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum vCPUs that an environment should maintain, even if the compute environment is DISABLED. Default: 0\n')
    placement_group: typing.Optional[typing.Union[models.aws_ec2.PlacementGroupDef]] = pydantic.Field(None, description='The EC2 placement group to associate with your compute resources. If you intend to submit multi-node parallel jobs to this Compute Environment, you should consider creating a cluster placement group and associate it with your compute resources. This keeps your multi-node parallel job on a logical grouping of instances within a single Availability Zone with high network flow potential. Default: - no placement group\n')
    spot_bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage that a Spot Instance price can be when compared with the On-Demand price for that instance type before instances are launched. For example, if your maximum percentage is 20%, the Spot price must be less than 20% of the current On-Demand price for that Instance. You always pay the lowest market price and never more than your maximum percentage. For most use cases, Batch recommends leaving this field empty. Implies ``spot == true`` if set Default: 100%\n')
    spot_fleet_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The service-linked role that Spot Fleet needs to launch instances on your behalf. Default: - a new role will be created\n')
    use_optimal_instance_classes: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use batch\'s optimal instance type. The optimal instance type is equivalent to adding the C4, M4, and R4 instance classes. You can specify other instance classes (of the same architecture) in addition to the optimal instance classes. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    # compute_env: batch.IManagedEc2EcsComputeEnvironment\n    vpc = ec2.Vpc(self, "VPC")\n    compute_env.add_instance_class(ec2.InstanceClass.M5AD)\n    # Or, specify it on the constructor:\n    batch.ManagedEc2EcsComputeEnvironment(self, "myEc2ComputeEnv",\n        vpc=vpc,\n        instance_classes=[ec2.InstanceClass.R4]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_name', 'enabled', 'service_role', 'vpc', 'maxv_cpus', 'replace_compute_environment', 'security_groups', 'spot', 'terminate_on_update', 'update_timeout', 'update_to_latest_image_version', 'vpc_subnets', 'allocation_strategy', 'images', 'instance_classes', 'instance_role', 'instance_types', 'launch_template', 'minv_cpus', 'placement_group', 'spot_bid_percentage', 'spot_fleet_role', 'use_optimal_instance_classes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.ManagedEc2EcsComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.ManagedEc2EcsComputeEnvironmentPropsDefConfig] = pydantic.Field(None)


class ManagedEc2EcsComputeEnvironmentPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.ManagedEc2EksComputeEnvironmentProps
class ManagedEc2EksComputeEnvironmentPropsDef(BaseStruct):
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs\n')
    vpc: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ec2.VpcDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='VPC in which this Compute Environment will launch Instances.\n')
    maxv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum vCpus this ``ManagedComputeEnvironment`` can scale up to. Each vCPU is equivalent to 1024 CPU shares. *Note*: if this Compute Environment uses EC2 resources (not Fargate) with either ``AllocationStrategy.BEST_FIT_PROGRESSIVE`` or ``AllocationStrategy.SPOT_CAPACITY_OPTIMIZED``, or ``AllocationStrategy.BEST_FIT`` with Spot instances, The scheduler may exceed this number by at most one of the instances specified in ``instanceTypes`` or ``instanceClasses``. Default: 256\n')
    replace_compute_environment: typing.Optional[bool] = pydantic.Field(None, description="Specifies whether this Compute Environment is replaced if an update is made that requires replacing its instances. To enable more properties to be updated, set this property to ``false``. When changing the value of this property to false, do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the UPDATE_ROLLBACK_FAILED state. You can't update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. The properties which require a replacement of the Compute Environment are: Default: false\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups this Compute Environment will launch instances in. Default: new security groups will be created\n')
    spot: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use spot instances. Spot instances are less expensive EC2 instances that can be reclaimed by EC2 at any time; your job will be given two minutes of notice before reclamation. Default: false\n')
    terminate_on_update: typing.Optional[bool] = pydantic.Field(None, description="Whether or not any running jobs will be immediately terminated when an infrastructure update occurs. If this is enabled, any terminated jobs may be retried, depending on the job's retry policy. Default: false\n")
    update_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Only meaningful if ``terminateOnUpdate`` is ``false``. If so, when an infrastructure update is triggered, any running jobs will be allowed to run until ``updateTimeout`` has expired. Default: 30 minutes\n')
    update_to_latest_image_version: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the AMI is updated to the latest one supported by Batch when an infrastructure update occurs. If you specify a specific AMI, this property will be ignored. Default: true\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The VPC Subnets this Compute Environment will launch instances in. Default: new subnets will be created\n')
    eks_cluster: typing.Union[_REQUIRED_INIT_PARAM, models.aws_eks.ClusterDef, models.aws_eks.FargateClusterDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The cluster that backs this Compute Environment. Required for Compute Environments running Kubernetes jobs. Please ensure that you have followed the steps at https://docs.aws.amazon.com/batch/latest/userguide/getting-started-eks.html before attempting to deploy a ``ManagedEc2EksComputeEnvironment`` that uses this cluster. If you do not follow the steps in the link, the deployment fail with a message that the compute environment did not stabilize.\n')
    kubernetes_namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The namespace of the Cluster.\n')
    allocation_strategy: typing.Optional[aws_cdk.aws_batch.AllocationStrategy] = pydantic.Field(None, description='The allocation strategy to use if not enough instances of the best fitting instance type can be allocated. Default: - ``BEST_FIT_PROGRESSIVE`` if not using Spot instances, ``SPOT_CAPACITY_OPTIMIZED`` if using Spot instances.\n')
    images: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.EksMachineImageDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Configure which AMIs this Compute Environment can launch. Default: If ``imageKubernetesVersion`` is specified, - EKS_AL2 for non-GPU instances, EKS_AL2_NVIDIA for GPU instances, Otherwise, - ECS_AL2 for non-GPU instances, ECS_AL2_NVIDIA for GPU instances,\n')
    instance_classes: typing.Optional[typing.Sequence[aws_cdk.aws_ec2.InstanceClass]] = pydantic.Field(None, description='The instance types that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Batch will automatically choose the instance size. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    instance_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The execution Role that instances launched by this Compute Environment will use. Default: - a role will be created\n')
    instance_types: typing.Optional[typing.Sequence[models.aws_ec2.InstanceTypeDef]] = pydantic.Field(None, description='The instance types that this Compute Environment can launch. Which one is chosen depends on the ``AllocationStrategy`` used. Default: - the instances Batch considers will be used (currently C4, M4, and R4)\n')
    launch_template: typing.Optional[typing.Union[models.aws_ec2.LaunchTemplateDef]] = pydantic.Field(None, description='The Launch Template that this Compute Environment will use to provision EC2 Instances. *Note*: if ``securityGroups`` is specified on both your launch template and this Compute Environment, **the ``securityGroup``s on the Compute Environment override the ones on the launch template.** Default: - no launch template\n')
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum vCPUs that an environment should maintain, even if the compute environment is DISABLED. Default: 0\n')
    placement_group: typing.Optional[typing.Union[models.aws_ec2.PlacementGroupDef]] = pydantic.Field(None, description='The EC2 placement group to associate with your compute resources. If you intend to submit multi-node parallel jobs to this Compute Environment, you should consider creating a cluster placement group and associate it with your compute resources. This keeps your multi-node parallel job on a logical grouping of instances within a single Availability Zone with high network flow potential. Default: - no placement group\n')
    spot_bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage that a Spot Instance price can be when compared with the On-Demand price for that instance type before instances are launched. For example, if your maximum percentage is 20%, the Spot price must be less than 20% of the current On-Demand price for that Instance. You always pay the lowest market price and never more than your maximum percentage. For most use cases, Batch recommends leaving this field empty. Implies ``spot == true`` if set Default: - 100%\n')
    use_optimal_instance_classes: typing.Optional[bool] = pydantic.Field(None, description='Whether or not to use batch\'s optimal instance type. The optimal instance type is equivalent to adding the C4, M4, and R4 instance classes. You can specify other instance classes (of the same architecture) in addition to the optimal instance classes. Default: true\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_eks as eks\n    from aws_cdk import aws_iam as iam\n\n    # cluster: eks.Cluster\n    # instance_type: ec2.InstanceType\n    # launch_template: ec2.LaunchTemplate\n    # machine_image: ec2.IMachineImage\n    # placement_group: ec2.PlacementGroup\n    # role: iam.Role\n    # security_group: ec2.SecurityGroup\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # vpc: ec2.Vpc\n\n    managed_ec2_eks_compute_environment_props = batch.ManagedEc2EksComputeEnvironmentProps(\n        eks_cluster=cluster,\n        kubernetes_namespace="kubernetesNamespace",\n        vpc=vpc,\n\n        # the properties below are optional\n        allocation_strategy=batch.AllocationStrategy.BEST_FIT,\n        compute_environment_name="computeEnvironmentName",\n        enabled=False,\n        images=[batch.EksMachineImage(\n            image=machine_image,\n            image_type=batch.EksMachineImageType.EKS_AL2\n        )],\n        instance_classes=[ec2.InstanceClass.STANDARD3],\n        instance_role=role,\n        instance_types=[instance_type],\n        launch_template=launch_template,\n        maxv_cpus=123,\n        minv_cpus=123,\n        placement_group=placement_group,\n        replace_compute_environment=False,\n        security_groups=[security_group],\n        service_role=role,\n        spot=False,\n        spot_bid_percentage=123,\n        terminate_on_update=False,\n        update_timeout=cdk.Duration.minutes(30),\n        update_to_latest_image_version=False,\n        use_optimal_instance_classes=False,\n        vpc_subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_name', 'enabled', 'service_role', 'vpc', 'maxv_cpus', 'replace_compute_environment', 'security_groups', 'spot', 'terminate_on_update', 'update_timeout', 'update_to_latest_image_version', 'vpc_subnets', 'eks_cluster', 'kubernetes_namespace', 'allocation_strategy', 'images', 'instance_classes', 'instance_role', 'instance_types', 'launch_template', 'minv_cpus', 'placement_group', 'spot_bid_percentage', 'use_optimal_instance_classes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.ManagedEc2EksComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.ManagedEc2EksComputeEnvironmentPropsDefConfig] = pydantic.Field(None)


class ManagedEc2EksComputeEnvironmentPropsDefConfig(pydantic.BaseModel):
    eks_cluster_config: typing.Optional[models._interface_methods.AwsEksIClusterDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.MultiNodeContainer
class MultiNodeContainerDef(BaseStruct):
    container: typing.Union[_REQUIRED_INIT_PARAM, models.aws_batch.EcsEc2ContainerDefinitionDef, models.aws_batch.EcsFargateContainerDefinitionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The container that this node range will run.\n')
    end_node: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The index of the last node to run this container. The container is run on all nodes in the range [startNode, endNode] (inclusive)\n')
    start_node: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The index of the first node to run this container. The container is run on all nodes in the range [startNode, endNode] (inclusive)\n\n:exampleMetadata: infused\n\nExample::\n\n    multi_node_job = batch.MultiNodeJobDefinition(self, "JobDefinition",\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.R4, ec2.InstanceSize.LARGE),  # optional, omit to let Batch choose the type for you\n        containers=[batch.MultiNodeContainer(\n            container=batch.EcsEc2ContainerDefinition(self, "mainMPIContainer",\n                image=ecs.ContainerImage.from_registry("yourregsitry.com/yourMPIImage:latest"),\n                cpu=256,\n                memory=cdk.Size.mebibytes(2048)\n            ),\n            start_node=0,\n            end_node=5\n        )]\n    )\n    # convenience method\n    multi_node_job.add_container(\n        start_node=6,\n        end_node=10,\n        container=batch.EcsEc2ContainerDefinition(self, "multiContainer",\n            image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n            cpu=256,\n            memory=cdk.Size.mebibytes(2048)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container', 'end_node', 'start_node']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.MultiNodeContainer'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.MultiNodeContainerDefConfig] = pydantic.Field(None)


class MultiNodeContainerDefConfig(pydantic.BaseModel):
    container_config: typing.Optional[models._interface_methods.AwsBatchIEcsContainerDefinitionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.MultiNodeJobDefinitionProps
class MultiNodeJobDefinitionPropsDef(BaseStruct):
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of this job definition. Default: - generated by CloudFormation\n')
    parameters: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='The default parameters passed to the container These parameters can be referenced in the ``command`` that you give to the container. Default: none\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to retry a job. The job is retried on failure the same number of attempts as the value. Default: 1\n')
    retry_strategies: typing.Optional[typing.Sequence[models.aws_batch.RetryStrategyDef]] = pydantic.Field(None, description='Defines the retry behavior for this job. Default: - no ``RetryStrategy``\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The priority of this Job. Only used in Fairshare Scheduling to decide which job to run first when there are multiple jobs with the same share identifier. Default: none\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, Batch terminates your jobs if they aren't finished. Default: - no timeout\n")
    containers: typing.Optional[typing.Sequence[typing.Union[models.aws_batch.MultiNodeContainerDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='The containers that this multinode job will run. Default: none\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The instance type that this job definition will run. Default: - optimal instance, selected by Batch\n')
    main_node: typing.Union[int, float, None] = pydantic.Field(None, description='The index of the main node in this job. The main node is responsible for orchestration. Default: 0\n')
    propagate_tags: typing.Optional[bool] = pydantic.Field(None, description='Whether to propogate tags from the JobDefinition to the ECS task that Batch spawns. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    multi_node_job = batch.MultiNodeJobDefinition(self, "JobDefinition",\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.R4, ec2.InstanceSize.LARGE),  # optional, omit to let Batch choose the type for you\n        containers=[batch.MultiNodeContainer(\n            container=batch.EcsEc2ContainerDefinition(self, "mainMPIContainer",\n                image=ecs.ContainerImage.from_registry("yourregsitry.com/yourMPIImage:latest"),\n                cpu=256,\n                memory=cdk.Size.mebibytes(2048)\n            ),\n            start_node=0,\n            end_node=5\n        )]\n    )\n    # convenience method\n    multi_node_job.add_container(\n        start_node=6,\n        end_node=10,\n        container=batch.EcsEc2ContainerDefinition(self, "multiContainer",\n            image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample"),\n            cpu=256,\n            memory=cdk.Size.mebibytes(2048)\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['job_definition_name', 'parameters', 'retry_attempts', 'retry_strategies', 'scheduling_priority', 'timeout', 'containers', 'instance_type', 'main_node', 'propagate_tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.MultiNodeJobDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.OrderedComputeEnvironment
class OrderedComputeEnvironmentDef(BaseStruct):
    compute_environment: typing.Union[_REQUIRED_INIT_PARAM, models.aws_batch.FargateComputeEnvironmentDef, models.aws_batch.ManagedEc2EcsComputeEnvironmentDef, models.aws_batch.ManagedEc2EksComputeEnvironmentDef, models.aws_batch.UnmanagedComputeEnvironmentDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ComputeEnvironment to link to this JobQueue.\n')
    order: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The order associated with ``computeEnvironment``.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # compute_environment: batch.IComputeEnvironment\n\n    ordered_compute_environment = batch.OrderedComputeEnvironment(\n        compute_environment=compute_environment,\n        order=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment', 'order']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.OrderedComputeEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.OrderedComputeEnvironmentDefConfig] = pydantic.Field(None)


class OrderedComputeEnvironmentDefConfig(pydantic.BaseModel):
    compute_environment_config: typing.Optional[models._interface_methods.AwsBatchIComputeEnvironmentDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.SecretPathVolumeOptions
class SecretPathVolumeOptionsDef(BaseStruct):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of this volume. The name must be a valid DNS subdomain name.\n')
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted. Default: - the volume is not mounted\n')
    readonly: typing.Optional[bool] = pydantic.Field(None, description='If specified, the container has readonly access to the volume. Otherwise, the container has read/write access. Default: false\n')
    secret_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the secret. Must be a valid DNS subdomain name.\n')
    optional: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the secret or the secret\'s keys must be defined. Default: true\n\n:see: https://kubernetes.io/docs/concepts/storage/volumes/#secret\n:exampleMetadata: infused\n\nExample::\n\n    # job_defn: batch.EksJobDefinition\n\n    job_defn.container.add_volume(batch.EksVolume.empty_dir(\n        name="emptyDir",\n        mount_path="/Volumes/emptyDir"\n    ))\n    job_defn.container.add_volume(batch.EksVolume.host_path(\n        name="hostPath",\n        host_path="/sys",\n        mount_path="/Volumes/hostPath"\n    ))\n    job_defn.container.add_volume(batch.EksVolume.secret(\n        name="secret",\n        optional=True,\n        mount_path="/Volumes/secret",\n        secret_name="mySecret"\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'mount_path', 'readonly', 'secret_name', 'optional']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.SecretPathVolumeOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.SecretVersionInfo
class SecretVersionInfoDef(BaseStruct):
    version_id: typing.Optional[str] = pydantic.Field(None, description='version id of the secret. Default: - use default version id\n')
    version_stage: typing.Optional[str] = pydantic.Field(None, description='version stage of the secret. Default: - use default version stage\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    secret_version_info = batch.SecretVersionInfo(\n        version_id="versionId",\n        version_stage="versionStage"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version_id', 'version_stage']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.SecretVersionInfo'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.Share
class ShareDef(BaseStruct):
    share_identifier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The identifier of this Share. All jobs that specify this share identifier when submitted to the queue will be considered as part of this Share.\n')
    weight_factor: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The weight factor given to this Share. The Scheduler decides which jobs to put in the Compute Environment such that the following ratio is equal for each job: ``sharevCpu / weightFactor``, where ``sharevCpu`` is the total amount of vCPU given to that particular share; that is, the sum of the vCPU of each job currently in the Compute Environment for that share. See the readme of this module for a detailed example that shows how these are used, how it relates to ``computeReservation``, and how ``shareDecay`` affects these calculations.\n\n:exampleMetadata: infused\n\nExample::\n\n    fairshare_policy = batch.FairshareSchedulingPolicy(self, "myFairsharePolicy")\n\n    fairshare_policy.add_share(\n        share_identifier="A",\n        weight_factor=1\n    )\n    fairshare_policy.add_share(\n        share_identifier="B",\n        weight_factor=1\n    )\n    batch.JobQueue(self, "JobQueue",\n        scheduling_policy=fairshare_policy\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['share_identifier', 'weight_factor']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.Share'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.Tmpfs
class TmpfsDef(BaseStruct):
    container_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The absolute file path where the tmpfs volume is to be mounted.\n')
    size: typing.Union[models.SizeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The size (in MiB) of the tmpfs volume.\n')
    mount_options: typing.Optional[typing.Sequence[aws_cdk.aws_batch.TmpfsMountOption]] = pydantic.Field(None, description='The list of tmpfs volume mount options. For more information, see `TmpfsMountOptions <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_Tmpfs.html>`_. Default: none\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_batch as batch\n\n    # size: cdk.Size\n\n    tmpfs = batch.Tmpfs(\n        container_path="containerPath",\n        size=size,\n\n        # the properties below are optional\n        mount_options=[batch.TmpfsMountOption.DEFAULTS]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'size', 'mount_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.Tmpfs'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.TmpfsDefConfig] = pydantic.Field(None)


class TmpfsDefConfig(pydantic.BaseModel):
    size_config: typing.Optional[models.core.SizeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_batch.Ulimit
class UlimitDef(BaseStruct):
    hard_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The hard limit for this resource. The container will be terminated if it exceeds this limit.\n')
    name: typing.Union[aws_cdk.aws_batch.UlimitName, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The resource to limit.\n')
    soft_limit: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The reservation for this resource. The container will not be terminated if it exceeds this limit.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    ulimit = batch.Ulimit(\n        hard_limit=123,\n        name=batch.UlimitName.CORE,\n        soft_limit=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['hard_limit', 'name', 'soft_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.Ulimit'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.UnmanagedComputeEnvironmentProps
class UnmanagedComputeEnvironmentPropsDef(BaseStruct):
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name of the ComputeEnvironment. Default: - generated by CloudFormation\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description="Whether or not this ComputeEnvironment can accept jobs from a Queue. Enabled ComputeEnvironments can accept jobs from a Queue and can scale instances up or down. Disabled ComputeEnvironments cannot accept jobs from a Queue or scale instances up or down. If you change a ComputeEnvironment from enabled to disabled while it is executing jobs, Jobs in the ``STARTED`` or ``RUNNING`` states will not be interrupted. As jobs complete, the ComputeEnvironment will scale instances down to ``minvCpus``. To ensure you aren't billed for unused capacity, set ``minvCpus`` to ``0``. Default: true\n")
    service_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role Batch uses to perform actions on your behalf in your account, such as provision instances to run your jobs. Default: - a serviceRole will be created for managed CEs, none for unmanaged CEs\n')
    unmanagedv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='The vCPUs this Compute Environment provides. Used only by the scheduler to schedule jobs in ``Queue``s that use ``FairshareSchedulingPolicy``s. **If this parameter is not provided on a fairshare queue, no capacity is reserved**; that is, the ``FairshareSchedulingPolicy`` is ignored. Default: 0\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n    from aws_cdk import aws_iam as iam\n\n    # role: iam.Role\n\n    unmanaged_compute_environment_props = batch.UnmanagedComputeEnvironmentProps(\n        compute_environment_name="computeEnvironmentName",\n        enabled=False,\n        service_role=role,\n        unmanagedv_cpus=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_name', 'enabled', 'service_role', 'unmanagedv_cpus']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.UnmanagedComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.Action
# skipping emum

#  autogenerated from aws_cdk.aws_batch.AllocationStrategy
# skipping emum

#  autogenerated from aws_cdk.aws_batch.DevicePermission
# skipping emum

#  autogenerated from aws_cdk.aws_batch.DnsPolicy
# skipping emum

#  autogenerated from aws_cdk.aws_batch.EcsMachineImageType
# skipping emum

#  autogenerated from aws_cdk.aws_batch.EksMachineImageType
# skipping emum

#  autogenerated from aws_cdk.aws_batch.EmptyDirMediumType
# skipping emum

#  autogenerated from aws_cdk.aws_batch.ImagePullPolicy
# skipping emum

#  autogenerated from aws_cdk.aws_batch.TmpfsMountOption
# skipping emum

#  autogenerated from aws_cdk.aws_batch.UlimitName
# skipping emum

#  autogenerated from aws_cdk.aws_batch.IComputeEnvironment
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IEcsContainerDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IEcsEc2ContainerDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IEcsFargateContainerDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IEksContainerDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IEksJobDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IFairshareSchedulingPolicy
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IFargateComputeEnvironment
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IJobDefinition
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IJobQueue
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IManagedComputeEnvironment
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IManagedEc2EcsComputeEnvironment
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.ISchedulingPolicy
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.IUnmanagedComputeEnvironment
#  skipping Interface

#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment
class CfnComputeEnvironmentDef(BaseCfnResource):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of the compute environment: ``MANAGED`` or ``UNMANAGED`` . For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the *AWS Batch User Guide* .\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name for your compute environment. It can be up to 128 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    compute_resources: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_ComputeResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ComputeResources property type specifies details of the compute resources managed by the compute environment. This parameter is required for managed compute environments. For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the ** .\n')
    eks_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_EksConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The details for the Amazon EKS cluster that supports the compute environment.\n')
    replace_compute_environment: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether the compute environment is replaced if an update is made that requires replacing the instances in the compute environment. The default value is ``true`` . To enable more properties to be updated, set this property to ``false`` . When changing the value of this property to ``false`` , do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the ``UPDATE_ROLLBACK_FAILED`` state. You can't update a stack that is in the ``UPDATE_ROLLBACK_FAILED`` state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. For more information, see `Continue rolling back an update <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html>`_ in the *AWS CloudFormation User Guide* . The properties that can't be changed without replacing the compute environment are in the ```ComputeResources`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html>`_ property type: ```AllocationStrategy`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-allocationstrategy>`_ , ```BidPercentage`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-bidpercentage>`_ , ```Ec2Configuration`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2configuration>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```ImageId`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-imageid>`_ , ```InstanceRole`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancerole>`_ , ```InstanceTypes`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancetypes>`_ , ```LaunchTemplate`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-launchtemplate>`_ , ```MaxvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-maxvcpus>`_ , ```MinvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-minvcpus>`_ , ```PlacementGroup`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-placementgroup>`_ , ```SecurityGroupIds`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-securitygroupids>`_ , ```Subnets`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-subnets>`_ , `Tags <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-tags>`_ , ```Type`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-type>`_ , and ```UpdateToLatestImageVersion`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-updatetolatestimageversion>`_ . Default: - true\n")
    service_role: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that allows AWS Batch to make calls to other AWS services on your behalf. For more information, see `AWS Batch service IAM role <https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If your account already created the AWS Batch service-linked role, that role is used by default for your compute environment unless you specify a different role here. If the AWS Batch service-linked role doesn't exist in your account, and no role is specified here, the service attempts to create the AWS Batch service-linked role in your account. If your specified role has a path other than ``/`` , then you must specify either the full role ARN (recommended) or prefix the role name with the path. For example, if a role with the name ``bar`` has a path of ``/foo/`` , specify ``/foo/bar`` as the role name. For more information, see `Friendly names and paths <https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-friendly-names>`_ in the *IAM User Guide* . .. epigraph:: Depending on how you created your AWS Batch service role, its ARN might contain the ``service-role`` path prefix. When you only specify the name of the service role, AWS Batch assumes that your ARN doesn't use the ``service-role`` path prefix. Because of this, we recommend that you specify the full ARN of your service role when you create compute environments.\n")
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the compute environment. If the state is ``ENABLED`` , then the compute environment accepts jobs from a queue and can scale out automatically based on queues. If the state is ``ENABLED`` , then the AWS Batch scheduler can attempt to place jobs from an associated job queue on the compute resources within the environment. If the compute environment is managed, then it can scale its instances out or in automatically, based on the job queue demand. If the state is ``DISABLED`` , then the AWS Batch scheduler doesn't attempt to place jobs within the environment. Jobs in a ``STARTING`` or ``RUNNING`` state continue to progress normally. Managed compute environments in the ``DISABLED`` state don't scale out. .. epigraph:: Compute environments in a ``DISABLED`` state may continue to incur billing charges. To prevent additional charges, turn off and then delete the compute environment. For more information, see `State <https://docs.aws.amazon.com/batch/latest/userguide/compute_environment_parameters.html#compute_environment_state>`_ in the *AWS Batch User Guide* . When an instance is idle, the instance scales down to the ``minvCpus`` value. However, the instance size doesn't change. For example, consider a ``c5.8xlarge`` instance with a ``minvCpus`` value of ``4`` and a ``desiredvCpus`` value of ``36`` . This instance doesn't scale down to a ``c5.large`` instance.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags applied to the compute environment.\n')
    unmanagedv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of vCPUs for an unmanaged compute environment. This parameter is only used for fair share scheduling to reserve vCPU capacity for new share identifiers. If this parameter isn't provided for a fair share job queue, no vCPU capacity is reserved. .. epigraph:: This parameter is only supported when the ``type`` parameter is set to ``UNMANAGED`` .\n")
    update_policy: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_UpdatePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the infrastructure update policy for the compute environment. For more information about infrastructure updates, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* .')
    _init_params: typing.ClassVar[list[str]] = ['type', 'compute_environment_name', 'compute_resources', 'eks_configuration', 'replace_compute_environment', 'service_role', 'state', 'tags', 'unmanagedv_cpus', 'update_policy']
    _method_names: typing.ClassVar[list[str]] = ['ComputeResourcesProperty', 'Ec2ConfigurationObjectProperty', 'EksConfigurationProperty', 'LaunchTemplateSpecificationProperty', 'UpdatePolicyProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.CfnComputeEnvironmentDefConfig] = pydantic.Field(None)


class CfnComputeEnvironmentDefConfig(pydantic.BaseModel):
    ComputeResourcesProperty: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefComputeresourcespropertyParams]] = pydantic.Field(None, description='')
    Ec2ConfigurationObjectProperty: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefEc2ConfigurationobjectpropertyParams]] = pydantic.Field(None, description='')
    EksConfigurationProperty: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefEksconfigurationpropertyParams]] = pydantic.Field(None, description='')
    LaunchTemplateSpecificationProperty: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefLaunchtemplatespecificationpropertyParams]] = pydantic.Field(None, description='')
    UpdatePolicyProperty: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefUpdatepolicypropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_batch.CfnComputeEnvironmentDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnComputeEnvironmentDefComputeresourcespropertyParams(pydantic.BaseModel):
    maxv_cpus: typing.Union[int, float] = pydantic.Field(..., description='')
    subnets: typing.Sequence[str] = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    allocation_strategy: typing.Optional[str] = pydantic.Field(None, description='')
    bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='')
    desiredv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ec2_configuration: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ec2_key_pair: typing.Optional[str] = pydantic.Field(None, description='')
    image_id: typing.Optional[str] = pydantic.Field(None, description='')
    instance_role: typing.Optional[str] = pydantic.Field(None, description='')
    instance_types: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    launch_template: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='')
    placement_group: typing.Optional[str] = pydantic.Field(None, description='')
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    spot_iam_fleet_role: typing.Optional[str] = pydantic.Field(None, description='')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='')
    update_to_latest_image_version: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefEc2ConfigurationobjectpropertyParams(pydantic.BaseModel):
    image_type: str = pydantic.Field(..., description='')
    image_id_override: typing.Optional[str] = pydantic.Field(None, description='')
    image_kubernetes_version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefEksconfigurationpropertyParams(pydantic.BaseModel):
    eks_cluster_arn: str = pydantic.Field(..., description='')
    kubernetes_namespace: str = pydantic.Field(..., description='')
    ...

class CfnComputeEnvironmentDefLaunchtemplatespecificationpropertyParams(pydantic.BaseModel):
    launch_template_id: typing.Optional[str] = pydantic.Field(None, description='')
    launch_template_name: typing.Optional[str] = pydantic.Field(None, description='')
    version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefUpdatepolicypropertyParams(pydantic.BaseModel):
    job_execution_timeout_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='')
    terminate_jobs_on_update: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnComputeEnvironmentDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnComputeEnvironmentDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnComputeEnvironmentDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnComputeEnvironmentDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnComputeEnvironmentDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnComputeEnvironmentDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnComputeEnvironmentDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnComputeEnvironmentDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnComputeEnvironmentDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnComputeEnvironmentDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnComputeEnvironmentDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnComputeEnvironmentDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnComputeEnvironmentDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition
class CfnJobDefinitionDef(BaseCfnResource):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The type of job definition. For more information about multi-node parallel jobs, see `Creating a multi-node parallel job definition <https://docs.aws.amazon.com/batch/latest/userguide/multi-node-job-def.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If the job is run on Fargate resources, then ``multinode`` isn't supported.\n")
    container_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties specific to Amazon ECS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    eks_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties that are specific to Amazon EKS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job definition.\n')
    node_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_NodePropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="An object with various properties that are specific to multi-node parallel jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified. .. epigraph:: If the job runs on Fargate resources, don't specify ``nodeProperties`` . Use ``containerProperties`` instead.\n")
    parameters: typing.Any = pydantic.Field(None, description='Default parameters or parameter substitution placeholders that are set in the job definition. Parameters are specified as a key-value pair mapping. Parameters in a ``SubmitJob`` request override any corresponding parameter defaults from the job definition. For more information about specifying parameters, see `Job definition parameters <https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html>`_ in the *AWS Batch User Guide* .\n')
    platform_capabilities: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The platform capabilities required by the job definition. If no value is specified, it defaults to ``EC2`` . Jobs run on Fargate resources specify ``FARGATE`` .\n')
    propagate_tags: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether to propagate the tags from the job or job definition to the corresponding Amazon ECS task. If no value is specified, the tags aren't propagated. Tags can only be propagated to the tasks when the tasks are created. For tags with the same name, job tags are given priority over job definitions tags. If the total number of combined tags from the job and job definition is over 50, the job is moved to the ``FAILED`` state.\n")
    retry_strategy: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_RetryStrategyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The retry strategy to use for failed jobs that are submitted with this job definition.\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The scheduling priority of the job definition. This only affects jobs in job queues with a fair share policy. Jobs with a higher scheduling priority are scheduled before jobs with a lower scheduling priority.\n')
    tags: typing.Any = pydantic.Field(None, description='The tags that are applied to the job definition.\n')
    timeout: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_TimeoutPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, AWS Batch terminates your jobs if they aren't finished.")
    _init_params: typing.ClassVar[list[str]] = ['type', 'container_properties', 'eks_properties', 'job_definition_name', 'node_properties', 'parameters', 'platform_capabilities', 'propagate_tags', 'retry_strategy', 'scheduling_priority', 'tags', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['AuthorizationConfigProperty', 'ContainerPropertiesProperty', 'DeviceProperty', 'EfsVolumeConfigurationProperty', 'EksContainerEnvironmentVariableProperty', 'EksContainerProperty', 'EksContainerVolumeMountProperty', 'EksPropertiesProperty', 'EksSecretProperty', 'EksVolumeProperty', 'EmptyDirProperty', 'EnvironmentProperty', 'EphemeralStorageProperty', 'EvaluateOnExitProperty', 'FargatePlatformConfigurationProperty', 'HostPathProperty', 'LinuxParametersProperty', 'LogConfigurationProperty', 'MetadataProperty', 'MountPointsProperty', 'NetworkConfigurationProperty', 'NodePropertiesProperty', 'NodeRangePropertyProperty', 'PodPropertiesProperty', 'ResourceRequirementProperty', 'ResourcesProperty', 'RetryStrategyProperty', 'RuntimePlatformProperty', 'SecretProperty', 'SecurityContextProperty', 'TimeoutProperty', 'TmpfsProperty', 'UlimitProperty', 'VolumesHostProperty', 'VolumesProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.CfnJobDefinitionDefConfig] = pydantic.Field(None)


class CfnJobDefinitionDefConfig(pydantic.BaseModel):
    AuthorizationConfigProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAuthorizationconfigpropertyParams]] = pydantic.Field(None, description='')
    ContainerPropertiesProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefContainerpropertiespropertyParams]] = pydantic.Field(None, description='')
    DeviceProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefDevicepropertyParams]] = pydantic.Field(None, description='')
    EfsVolumeConfigurationProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEfsvolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    EksContainerEnvironmentVariableProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEkscontainerenvironmentvariablepropertyParams]] = pydantic.Field(None, description='')
    EksContainerProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEkscontainerpropertyParams]] = pydantic.Field(None, description='')
    EksContainerVolumeMountProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEkscontainervolumemountpropertyParams]] = pydantic.Field(None, description='')
    EksPropertiesProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEkspropertiespropertyParams]] = pydantic.Field(None, description='')
    EksSecretProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEkssecretpropertyParams]] = pydantic.Field(None, description='')
    EksVolumeProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEksvolumepropertyParams]] = pydantic.Field(None, description='')
    EmptyDirProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEmptydirpropertyParams]] = pydantic.Field(None, description='')
    EnvironmentProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEnvironmentpropertyParams]] = pydantic.Field(None, description='')
    EphemeralStorageProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEphemeralstoragepropertyParams]] = pydantic.Field(None, description='')
    EvaluateOnExitProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefEvaluateonexitpropertyParams]] = pydantic.Field(None, description='')
    FargatePlatformConfigurationProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefFargateplatformconfigurationpropertyParams]] = pydantic.Field(None, description='')
    HostPathProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefHostpathpropertyParams]] = pydantic.Field(None, description='')
    LinuxParametersProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefLinuxparameterspropertyParams]] = pydantic.Field(None, description='')
    LogConfigurationProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefLogconfigurationpropertyParams]] = pydantic.Field(None, description='')
    MetadataProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefMetadatapropertyParams]] = pydantic.Field(None, description='')
    MountPointsProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefMountpointspropertyParams]] = pydantic.Field(None, description='')
    NetworkConfigurationProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefNetworkconfigurationpropertyParams]] = pydantic.Field(None, description='')
    NodePropertiesProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefNodepropertiespropertyParams]] = pydantic.Field(None, description='')
    NodeRangePropertyProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefNoderangepropertypropertyParams]] = pydantic.Field(None, description='')
    PodPropertiesProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefPodpropertiespropertyParams]] = pydantic.Field(None, description='')
    ResourceRequirementProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefResourcerequirementpropertyParams]] = pydantic.Field(None, description='')
    ResourcesProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefResourcespropertyParams]] = pydantic.Field(None, description='')
    RetryStrategyProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefRetrystrategypropertyParams]] = pydantic.Field(None, description='')
    RuntimePlatformProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefRuntimeplatformpropertyParams]] = pydantic.Field(None, description='')
    SecretProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefSecretpropertyParams]] = pydantic.Field(None, description='')
    SecurityContextProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefSecuritycontextpropertyParams]] = pydantic.Field(None, description='')
    TimeoutProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefTimeoutpropertyParams]] = pydantic.Field(None, description='')
    TmpfsProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefTmpfspropertyParams]] = pydantic.Field(None, description='')
    UlimitProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefUlimitpropertyParams]] = pydantic.Field(None, description='')
    VolumesHostProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefVolumeshostpropertyParams]] = pydantic.Field(None, description='')
    VolumesProperty: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefVolumespropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_batch.CfnJobDefinitionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnJobDefinitionDefAuthorizationconfigpropertyParams(pydantic.BaseModel):
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='')
    iam: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefContainerpropertiespropertyParams(pydantic.BaseModel):
    image: str = pydantic.Field(..., description='')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    environment: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EnvironmentPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ephemeral_storage: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    execution_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    fargate_platform_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_FargatePlatformConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    instance_type: typing.Optional[str] = pydantic.Field(None, description='')
    job_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    linux_parameters: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_LinuxParametersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    log_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    memory: typing.Union[int, float, None] = pydantic.Field(None, description='')
    mount_points: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_MountPointsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    network_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    privileged: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    readonly_root_filesystem: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    resource_requirements: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ResourceRequirementPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    runtime_platform: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_RuntimePlatformPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    secrets: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ulimits: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_UlimitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    user: typing.Optional[str] = pydantic.Field(None, description='')
    vcpus: typing.Union[int, float, None] = pydantic.Field(None, description='')
    volumes: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_VolumesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefDevicepropertyParams(pydantic.BaseModel):
    container_path: typing.Optional[str] = pydantic.Field(None, description='')
    host_path: typing.Optional[str] = pydantic.Field(None, description='')
    permissions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEfsvolumeconfigurationpropertyParams(pydantic.BaseModel):
    file_system_id: str = pydantic.Field(..., description='')
    authorization_config: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_AuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='')
    transit_encryption: typing.Optional[str] = pydantic.Field(None, description='')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkscontainerenvironmentvariablepropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkscontainerpropertyParams(pydantic.BaseModel):
    image: str = pydantic.Field(..., description='')
    args: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    env: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    image_pull_policy: typing.Optional[str] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    resources: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    security_context: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_SecurityContextPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    volume_mounts: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksContainerVolumeMountPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkscontainervolumemountpropertyParams(pydantic.BaseModel):
    mount_path: typing.Optional[str] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkspropertiespropertyParams(pydantic.BaseModel):
    pod_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_PodPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkssecretpropertyParams(pydantic.BaseModel):
    secret_name: str = pydantic.Field(..., description='')
    optional: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEksvolumepropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    empty_dir: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EmptyDirPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    host_path: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_HostPathPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    secret: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEmptydirpropertyParams(pydantic.BaseModel):
    medium: typing.Optional[str] = pydantic.Field(None, description='')
    size_limit: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEnvironmentpropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEphemeralstoragepropertyParams(pydantic.BaseModel):
    size_in_gib: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefEvaluateonexitpropertyParams(pydantic.BaseModel):
    action: str = pydantic.Field(..., description='')
    on_exit_code: typing.Optional[str] = pydantic.Field(None, description='')
    on_reason: typing.Optional[str] = pydantic.Field(None, description='')
    on_status_reason: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefFargateplatformconfigurationpropertyParams(pydantic.BaseModel):
    platform_version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefHostpathpropertyParams(pydantic.BaseModel):
    path: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefLinuxparameterspropertyParams(pydantic.BaseModel):
    devices: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_DevicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    init_process_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    max_swap: typing.Union[int, float, None] = pydantic.Field(None, description='')
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description='')
    tmpfs: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_TmpfsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefLogconfigurationpropertyParams(pydantic.BaseModel):
    log_driver: str = pydantic.Field(..., description='')
    options: typing.Any = pydantic.Field(None, description='')
    secret_options: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefMetadatapropertyParams(pydantic.BaseModel):
    labels: typing.Any = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefMountpointspropertyParams(pydantic.BaseModel):
    container_path: typing.Optional[str] = pydantic.Field(None, description='')
    read_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    source_volume: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefNetworkconfigurationpropertyParams(pydantic.BaseModel):
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefNodepropertiespropertyParams(pydantic.BaseModel):
    main_node: typing.Union[int, float] = pydantic.Field(..., description='')
    node_range_properties: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_NodeRangePropertyPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    num_nodes: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefNoderangepropertypropertyParams(pydantic.BaseModel):
    target_nodes: str = pydantic.Field(..., description='')
    container: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefPodpropertiespropertyParams(pydantic.BaseModel):
    containers: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksContainerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    dns_policy: typing.Optional[str] = pydantic.Field(None, description='')
    host_network: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    metadata: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_MetadataPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    service_account_name: typing.Optional[str] = pydantic.Field(None, description='')
    volumes: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksVolumePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefResourcerequirementpropertyParams(pydantic.BaseModel):
    type: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefResourcespropertyParams(pydantic.BaseModel):
    limits: typing.Any = pydantic.Field(None, description='')
    requests: typing.Any = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefRetrystrategypropertyParams(pydantic.BaseModel):
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='')
    evaluate_on_exit: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EvaluateOnExitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefRuntimeplatformpropertyParams(pydantic.BaseModel):
    cpu_architecture: typing.Optional[str] = pydantic.Field(None, description='')
    operating_system_family: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefSecretpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value_from: str = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefSecuritycontextpropertyParams(pydantic.BaseModel):
    privileged: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    read_only_root_filesystem: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    run_as_group: typing.Union[int, float, None] = pydantic.Field(None, description='')
    run_as_non_root: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    run_as_user: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefTimeoutpropertyParams(pydantic.BaseModel):
    attempt_duration_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefTmpfspropertyParams(pydantic.BaseModel):
    container_path: str = pydantic.Field(..., description='')
    size: typing.Union[int, float] = pydantic.Field(..., description='')
    mount_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefUlimitpropertyParams(pydantic.BaseModel):
    hard_limit: typing.Union[int, float] = pydantic.Field(..., description='')
    name: str = pydantic.Field(..., description='')
    soft_limit: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefVolumeshostpropertyParams(pydantic.BaseModel):
    source_path: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefVolumespropertyParams(pydantic.BaseModel):
    efs_volume_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EfsVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    host: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_VolumesHostPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnJobDefinitionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobDefinitionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnJobDefinitionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobDefinitionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnJobDefinitionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnJobDefinitionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnJobDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnJobDefinitionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnJobDefinitionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobDefinitionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnJobDefinitionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnJobDefinitionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobDefinitionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnJobQueue
class CfnJobQueueDef(BaseCfnResource):
    compute_environment_order: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobQueue_ComputeEnvironmentOrderPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description="The set of compute environments mapped to a job queue and their order relative to each other. The job scheduler uses this parameter to determine which compute environment runs a specific job. Compute environments must be in the ``VALID`` state before you can associate them with a job queue. You can associate up to three compute environments with a job queue. All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed. .. epigraph:: All compute environments that are associated with a job queue must share the same architecture. AWS Batch doesn't support mixing compute environment architecture types in a single job queue.\n")
    priority: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description="The priority of the job queue. Job queues with a higher priority (or a higher integer value for the ``priority`` parameter) are evaluated first when associated with the same compute environment. Priority is determined in descending order. For example, a job queue with a priority value of ``10`` is given scheduling preference over a job queue with a priority value of ``1`` . All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed.\n")
    job_queue_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job queue. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    scheduling_policy_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the scheduling policy. The format is ``aws: *Partition* :batch: *Region* : *Account* :scheduling-policy/ *Name*`` . For example, ``aws:aws:batch:us-west-2:123456789012:scheduling-policy/MySchedulingPolicy`` .\n')
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the job queue. If the job queue state is ``ENABLED`` , it is able to accept jobs. If the job queue state is ``DISABLED`` , new jobs can't be added to the queue, but jobs already in the queue can finish.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that are applied to the job queue. For more information, see `Tagging your AWS Batch resources <https://docs.aws.amazon.com/batch/latest/userguide/using-tags.html>`_ in *AWS Batch User Guide* .')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_order', 'priority', 'job_queue_name', 'scheduling_policy_arn', 'state', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['ComputeEnvironmentOrderProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobQueue'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.CfnJobQueueDefConfig] = pydantic.Field(None)


class CfnJobQueueDefConfig(pydantic.BaseModel):
    ComputeEnvironmentOrderProperty: typing.Optional[list[models.aws_batch.CfnJobQueueDefComputeenvironmentorderpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_batch.CfnJobQueueDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_batch.CfnJobQueueDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_batch.CfnJobQueueDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_batch.CfnJobQueueDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_batch.CfnJobQueueDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_batch.CfnJobQueueDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_batch.CfnJobQueueDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_batch.CfnJobQueueDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_batch.CfnJobQueueDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_batch.CfnJobQueueDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_batch.CfnJobQueueDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_batch.CfnJobQueueDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_batch.CfnJobQueueDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnJobQueueDefComputeenvironmentorderpropertyParams(pydantic.BaseModel):
    compute_environment: str = pydantic.Field(..., description='')
    order: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobQueueDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnJobQueueDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobQueueDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnJobQueueDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobQueueDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnJobQueueDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnJobQueueDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnJobQueueDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnJobQueueDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnJobQueueDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobQueueDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnJobQueueDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnJobQueueDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobQueueDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicy
class CfnSchedulingPolicyDef(BaseCfnResource):
    fairshare_policy: typing.Union[models.UnsupportedResource, models.aws_batch.CfnSchedulingPolicy_FairsharePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The fair share policy of the scheduling policy.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the scheduling policy. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that you apply to the scheduling policy to help you categorize and organize your resources. Each tag consists of a key and an optional value. For more information, see `Tagging AWS Resources <https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>`_ in *AWS General Reference* . These tags can be updated or removed using the `TagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_TagResource.html>`_ and `UntagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_UntagResource.html>`_ API operations.')
    _init_params: typing.ClassVar[list[str]] = ['fairshare_policy', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['FairsharePolicyProperty', 'ShareAttributesProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_batch.CfnSchedulingPolicyDefConfig] = pydantic.Field(None)


class CfnSchedulingPolicyDefConfig(pydantic.BaseModel):
    FairsharePolicyProperty: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefFairsharepolicypropertyParams]] = pydantic.Field(None, description='')
    ShareAttributesProperty: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefShareattributespropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_batch.CfnSchedulingPolicyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnSchedulingPolicyDefFairsharepolicypropertyParams(pydantic.BaseModel):
    compute_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='')
    share_decay_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='')
    share_distribution: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnSchedulingPolicy_ShareAttributesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnSchedulingPolicyDefShareattributespropertyParams(pydantic.BaseModel):
    share_identifier: typing.Optional[str] = pydantic.Field(None, description='')
    weight_factor: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnSchedulingPolicyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnSchedulingPolicyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSchedulingPolicyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnSchedulingPolicyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSchedulingPolicyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnSchedulingPolicyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnSchedulingPolicyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnSchedulingPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnSchedulingPolicyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnSchedulingPolicyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSchedulingPolicyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnSchedulingPolicyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnSchedulingPolicyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSchedulingPolicyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironmentProps
class CfnComputeEnvironmentPropsDef(BaseCfnProperty):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of the compute environment: ``MANAGED`` or ``UNMANAGED`` . For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the *AWS Batch User Guide* .\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name for your compute environment. It can be up to 128 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    compute_resources: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_ComputeResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ComputeResources property type specifies details of the compute resources managed by the compute environment. This parameter is required for managed compute environments. For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the ** .\n')
    eks_configuration: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_EksConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The details for the Amazon EKS cluster that supports the compute environment.\n')
    replace_compute_environment: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether the compute environment is replaced if an update is made that requires replacing the instances in the compute environment. The default value is ``true`` . To enable more properties to be updated, set this property to ``false`` . When changing the value of this property to ``false`` , do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the ``UPDATE_ROLLBACK_FAILED`` state. You can't update a stack that is in the ``UPDATE_ROLLBACK_FAILED`` state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. For more information, see `Continue rolling back an update <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html>`_ in the *AWS CloudFormation User Guide* . The properties that can't be changed without replacing the compute environment are in the ```ComputeResources`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html>`_ property type: ```AllocationStrategy`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-allocationstrategy>`_ , ```BidPercentage`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-bidpercentage>`_ , ```Ec2Configuration`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2configuration>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```ImageId`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-imageid>`_ , ```InstanceRole`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancerole>`_ , ```InstanceTypes`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancetypes>`_ , ```LaunchTemplate`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-launchtemplate>`_ , ```MaxvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-maxvcpus>`_ , ```MinvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-minvcpus>`_ , ```PlacementGroup`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-placementgroup>`_ , ```SecurityGroupIds`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-securitygroupids>`_ , ```Subnets`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-subnets>`_ , `Tags <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-tags>`_ , ```Type`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-type>`_ , and ```UpdateToLatestImageVersion`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-updatetolatestimageversion>`_ . Default: - true\n")
    service_role: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that allows AWS Batch to make calls to other AWS services on your behalf. For more information, see `AWS Batch service IAM role <https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If your account already created the AWS Batch service-linked role, that role is used by default for your compute environment unless you specify a different role here. If the AWS Batch service-linked role doesn't exist in your account, and no role is specified here, the service attempts to create the AWS Batch service-linked role in your account. If your specified role has a path other than ``/`` , then you must specify either the full role ARN (recommended) or prefix the role name with the path. For example, if a role with the name ``bar`` has a path of ``/foo/`` , specify ``/foo/bar`` as the role name. For more information, see `Friendly names and paths <https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-friendly-names>`_ in the *IAM User Guide* . .. epigraph:: Depending on how you created your AWS Batch service role, its ARN might contain the ``service-role`` path prefix. When you only specify the name of the service role, AWS Batch assumes that your ARN doesn't use the ``service-role`` path prefix. Because of this, we recommend that you specify the full ARN of your service role when you create compute environments.\n")
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the compute environment. If the state is ``ENABLED`` , then the compute environment accepts jobs from a queue and can scale out automatically based on queues. If the state is ``ENABLED`` , then the AWS Batch scheduler can attempt to place jobs from an associated job queue on the compute resources within the environment. If the compute environment is managed, then it can scale its instances out or in automatically, based on the job queue demand. If the state is ``DISABLED`` , then the AWS Batch scheduler doesn't attempt to place jobs within the environment. Jobs in a ``STARTING`` or ``RUNNING`` state continue to progress normally. Managed compute environments in the ``DISABLED`` state don't scale out. .. epigraph:: Compute environments in a ``DISABLED`` state may continue to incur billing charges. To prevent additional charges, turn off and then delete the compute environment. For more information, see `State <https://docs.aws.amazon.com/batch/latest/userguide/compute_environment_parameters.html#compute_environment_state>`_ in the *AWS Batch User Guide* . When an instance is idle, the instance scales down to the ``minvCpus`` value. However, the instance size doesn't change. For example, consider a ``c5.8xlarge`` instance with a ``minvCpus`` value of ``4`` and a ``desiredvCpus`` value of ``36`` . This instance doesn't scale down to a ``c5.large`` instance.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags applied to the compute environment.\n')
    unmanagedv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of vCPUs for an unmanaged compute environment. This parameter is only used for fair share scheduling to reserve vCPU capacity for new share identifiers. If this parameter isn't provided for a fair share job queue, no vCPU capacity is reserved. .. epigraph:: This parameter is only supported when the ``type`` parameter is set to ``UNMANAGED`` .\n")
    update_policy: typing.Union[models.UnsupportedResource, models.aws_batch.CfnComputeEnvironment_UpdatePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the infrastructure update policy for the compute environment. For more information about infrastructure updates, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-computeenvironment.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    cfn_compute_environment_props = batch.CfnComputeEnvironmentProps(\n        type="type",\n\n        # the properties below are optional\n        compute_environment_name="computeEnvironmentName",\n        compute_resources=batch.CfnComputeEnvironment.ComputeResourcesProperty(\n            maxv_cpus=123,\n            subnets=["subnets"],\n            type="type",\n\n            # the properties below are optional\n            allocation_strategy="allocationStrategy",\n            bid_percentage=123,\n            desiredv_cpus=123,\n            ec2_configuration=[batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty(\n                image_type="imageType",\n\n                # the properties below are optional\n                image_id_override="imageIdOverride",\n                image_kubernetes_version="imageKubernetesVersion"\n            )],\n            ec2_key_pair="ec2KeyPair",\n            image_id="imageId",\n            instance_role="instanceRole",\n            instance_types=["instanceTypes"],\n            launch_template=batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty(\n                launch_template_id="launchTemplateId",\n                launch_template_name="launchTemplateName",\n                version="version"\n            ),\n            minv_cpus=123,\n            placement_group="placementGroup",\n            security_group_ids=["securityGroupIds"],\n            spot_iam_fleet_role="spotIamFleetRole",\n            tags={\n                "tags_key": "tags"\n            },\n            update_to_latest_image_version=False\n        ),\n        eks_configuration=batch.CfnComputeEnvironment.EksConfigurationProperty(\n            eks_cluster_arn="eksClusterArn",\n            kubernetes_namespace="kubernetesNamespace"\n        ),\n        replace_compute_environment=False,\n        service_role="serviceRole",\n        state="state",\n        tags={\n            "tags_key": "tags"\n        },\n        unmanagedv_cpus=123,\n        update_policy=batch.CfnComputeEnvironment.UpdatePolicyProperty(\n            job_execution_timeout_minutes=123,\n            terminate_jobs_on_update=False\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'compute_environment_name', 'compute_resources', 'eks_configuration', 'replace_compute_environment', 'service_role', 'state', 'tags', 'unmanagedv_cpus', 'update_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinitionProps
class CfnJobDefinitionPropsDef(BaseCfnProperty):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The type of job definition. For more information about multi-node parallel jobs, see `Creating a multi-node parallel job definition <https://docs.aws.amazon.com/batch/latest/userguide/multi-node-job-def.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If the job is run on Fargate resources, then ``multinode`` isn't supported.\n")
    container_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties specific to Amazon ECS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    eks_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_EksPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties that are specific to Amazon EKS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job definition.\n')
    node_properties: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_NodePropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="An object with various properties that are specific to multi-node parallel jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified. .. epigraph:: If the job runs on Fargate resources, don't specify ``nodeProperties`` . Use ``containerProperties`` instead.\n")
    parameters: typing.Any = pydantic.Field(None, description='Default parameters or parameter substitution placeholders that are set in the job definition. Parameters are specified as a key-value pair mapping. Parameters in a ``SubmitJob`` request override any corresponding parameter defaults from the job definition. For more information about specifying parameters, see `Job definition parameters <https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html>`_ in the *AWS Batch User Guide* .\n')
    platform_capabilities: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The platform capabilities required by the job definition. If no value is specified, it defaults to ``EC2`` . Jobs run on Fargate resources specify ``FARGATE`` .\n')
    propagate_tags: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether to propagate the tags from the job or job definition to the corresponding Amazon ECS task. If no value is specified, the tags aren't propagated. Tags can only be propagated to the tasks when the tasks are created. For tags with the same name, job tags are given priority over job definitions tags. If the total number of combined tags from the job and job definition is over 50, the job is moved to the ``FAILED`` state.\n")
    retry_strategy: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_RetryStrategyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The retry strategy to use for failed jobs that are submitted with this job definition.\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The scheduling priority of the job definition. This only affects jobs in job queues with a fair share policy. Jobs with a higher scheduling priority are scheduled before jobs with a lower scheduling priority.\n')
    tags: typing.Any = pydantic.Field(None, description='The tags that are applied to the job definition.\n')
    timeout: typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobDefinition_TimeoutPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, AWS Batch terminates your jobs if they aren\'t finished.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-jobdefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # labels: Any\n    # limits: Any\n    # options: Any\n    # parameters: Any\n    # requests: Any\n    # tags: Any\n\n    cfn_job_definition_props = batch.CfnJobDefinitionProps(\n        type="type",\n\n        # the properties below are optional\n        container_properties=batch.CfnJobDefinition.ContainerPropertiesProperty(\n            image="image",\n\n            # the properties below are optional\n            command=["command"],\n            environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                name="name",\n                value="value"\n            )],\n            ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                size_in_gi_b=123\n            ),\n            execution_role_arn="executionRoleArn",\n            fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                platform_version="platformVersion"\n            ),\n            instance_type="instanceType",\n            job_role_arn="jobRoleArn",\n            linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                devices=[batch.CfnJobDefinition.DeviceProperty(\n                    container_path="containerPath",\n                    host_path="hostPath",\n                    permissions=["permissions"]\n                )],\n                init_process_enabled=False,\n                max_swap=123,\n                shared_memory_size=123,\n                swappiness=123,\n                tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                    container_path="containerPath",\n                    size=123,\n\n                    # the properties below are optional\n                    mount_options=["mountOptions"]\n                )]\n            ),\n            log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                log_driver="logDriver",\n\n                # the properties below are optional\n                options=options,\n                secret_options=[batch.CfnJobDefinition.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )]\n            ),\n            memory=123,\n            mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                container_path="containerPath",\n                read_only=False,\n                source_volume="sourceVolume"\n            )],\n            network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                assign_public_ip="assignPublicIp"\n            ),\n            privileged=False,\n            readonly_root_filesystem=False,\n            resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                type="type",\n                value="value"\n            )],\n            runtime_platform=batch.CfnJobDefinition.RuntimePlatformProperty(\n                cpu_architecture="cpuArchitecture",\n                operating_system_family="operatingSystemFamily"\n            ),\n            secrets=[batch.CfnJobDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )],\n            ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                hard_limit=123,\n                name="name",\n                soft_limit=123\n            )],\n            user="user",\n            vcpus=123,\n            volumes=[batch.CfnJobDefinition.VolumesProperty(\n                efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                    file_system_id="fileSystemId",\n\n                    # the properties below are optional\n                    authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                        access_point_id="accessPointId",\n                        iam="iam"\n                    ),\n                    root_directory="rootDirectory",\n                    transit_encryption="transitEncryption",\n                    transit_encryption_port=123\n                ),\n                host=batch.CfnJobDefinition.VolumesHostProperty(\n                    source_path="sourcePath"\n                ),\n                name="name"\n            )]\n        ),\n        eks_properties=batch.CfnJobDefinition.EksPropertiesProperty(\n            pod_properties=batch.CfnJobDefinition.PodPropertiesProperty(\n                containers=[batch.CfnJobDefinition.EksContainerProperty(\n                    image="image",\n\n                    # the properties below are optional\n                    args=["args"],\n                    command=["command"],\n                    env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n                        name="name",\n\n                        # the properties below are optional\n                        value="value"\n                    )],\n                    image_pull_policy="imagePullPolicy",\n                    name="name",\n                    resources=batch.CfnJobDefinition.ResourcesProperty(\n                        limits=limits,\n                        requests=requests\n                    ),\n                    security_context=batch.CfnJobDefinition.SecurityContextProperty(\n                        privileged=False,\n                        read_only_root_filesystem=False,\n                        run_as_group=123,\n                        run_as_non_root=False,\n                        run_as_user=123\n                    ),\n                    volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n                        mount_path="mountPath",\n                        name="name",\n                        read_only=False\n                    )]\n                )],\n                dns_policy="dnsPolicy",\n                host_network=False,\n                metadata=batch.CfnJobDefinition.MetadataProperty(\n                    labels=labels\n                ),\n                service_account_name="serviceAccountName",\n                volumes=[batch.CfnJobDefinition.EksVolumeProperty(\n                    name="name",\n\n                    # the properties below are optional\n                    empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n                        medium="medium",\n                        size_limit="sizeLimit"\n                    ),\n                    host_path=batch.CfnJobDefinition.HostPathProperty(\n                        path="path"\n                    ),\n                    secret=batch.CfnJobDefinition.EksSecretProperty(\n                        secret_name="secretName",\n\n                        # the properties below are optional\n                        optional=False\n                    )\n                )]\n            )\n        ),\n        job_definition_name="jobDefinitionName",\n        node_properties=batch.CfnJobDefinition.NodePropertiesProperty(\n            main_node=123,\n            node_range_properties=[batch.CfnJobDefinition.NodeRangePropertyProperty(\n                target_nodes="targetNodes",\n\n                # the properties below are optional\n                container=batch.CfnJobDefinition.ContainerPropertiesProperty(\n                    image="image",\n\n                    # the properties below are optional\n                    command=["command"],\n                    environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                        name="name",\n                        value="value"\n                    )],\n                    ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                        size_in_gi_b=123\n                    ),\n                    execution_role_arn="executionRoleArn",\n                    fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                        platform_version="platformVersion"\n                    ),\n                    instance_type="instanceType",\n                    job_role_arn="jobRoleArn",\n                    linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                        devices=[batch.CfnJobDefinition.DeviceProperty(\n                            container_path="containerPath",\n                            host_path="hostPath",\n                            permissions=["permissions"]\n                        )],\n                        init_process_enabled=False,\n                        max_swap=123,\n                        shared_memory_size=123,\n                        swappiness=123,\n                        tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                            container_path="containerPath",\n                            size=123,\n\n                            # the properties below are optional\n                            mount_options=["mountOptions"]\n                        )]\n                    ),\n                    log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                        log_driver="logDriver",\n\n                        # the properties below are optional\n                        options=options,\n                        secret_options=[batch.CfnJobDefinition.SecretProperty(\n                            name="name",\n                            value_from="valueFrom"\n                        )]\n                    ),\n                    memory=123,\n                    mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                        container_path="containerPath",\n                        read_only=False,\n                        source_volume="sourceVolume"\n                    )],\n                    network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                        assign_public_ip="assignPublicIp"\n                    ),\n                    privileged=False,\n                    readonly_root_filesystem=False,\n                    resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                        type="type",\n                        value="value"\n                    )],\n                    runtime_platform=batch.CfnJobDefinition.RuntimePlatformProperty(\n                        cpu_architecture="cpuArchitecture",\n                        operating_system_family="operatingSystemFamily"\n                    ),\n                    secrets=[batch.CfnJobDefinition.SecretProperty(\n                        name="name",\n                        value_from="valueFrom"\n                    )],\n                    ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                        hard_limit=123,\n                        name="name",\n                        soft_limit=123\n                    )],\n                    user="user",\n                    vcpus=123,\n                    volumes=[batch.CfnJobDefinition.VolumesProperty(\n                        efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                            file_system_id="fileSystemId",\n\n                            # the properties below are optional\n                            authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                                access_point_id="accessPointId",\n                                iam="iam"\n                            ),\n                            root_directory="rootDirectory",\n                            transit_encryption="transitEncryption",\n                            transit_encryption_port=123\n                        ),\n                        host=batch.CfnJobDefinition.VolumesHostProperty(\n                            source_path="sourcePath"\n                        ),\n                        name="name"\n                    )]\n                )\n            )],\n            num_nodes=123\n        ),\n        parameters=parameters,\n        platform_capabilities=["platformCapabilities"],\n        propagate_tags=False,\n        retry_strategy=batch.CfnJobDefinition.RetryStrategyProperty(\n            attempts=123,\n            evaluate_on_exit=[batch.CfnJobDefinition.EvaluateOnExitProperty(\n                action="action",\n\n                # the properties below are optional\n                on_exit_code="onExitCode",\n                on_reason="onReason",\n                on_status_reason="onStatusReason"\n            )]\n        ),\n        scheduling_priority=123,\n        tags=tags,\n        timeout=batch.CfnJobDefinition.TimeoutProperty(\n            attempt_duration_seconds=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'container_properties', 'eks_properties', 'job_definition_name', 'node_properties', 'parameters', 'platform_capabilities', 'propagate_tags', 'retry_strategy', 'scheduling_priority', 'tags', 'timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobQueueProps
class CfnJobQueuePropsDef(BaseCfnProperty):
    compute_environment_order: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_batch.CfnJobQueue_ComputeEnvironmentOrderPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description="The set of compute environments mapped to a job queue and their order relative to each other. The job scheduler uses this parameter to determine which compute environment runs a specific job. Compute environments must be in the ``VALID`` state before you can associate them with a job queue. You can associate up to three compute environments with a job queue. All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed. .. epigraph:: All compute environments that are associated with a job queue must share the same architecture. AWS Batch doesn't support mixing compute environment architecture types in a single job queue.\n")
    priority: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description="The priority of the job queue. Job queues with a higher priority (or a higher integer value for the ``priority`` parameter) are evaluated first when associated with the same compute environment. Priority is determined in descending order. For example, a job queue with a priority value of ``10`` is given scheduling preference over a job queue with a priority value of ``1`` . All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed.\n")
    job_queue_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job queue. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    scheduling_policy_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the scheduling policy. The format is ``aws: *Partition* :batch: *Region* : *Account* :scheduling-policy/ *Name*`` . For example, ``aws:aws:batch:us-west-2:123456789012:scheduling-policy/MySchedulingPolicy`` .\n')
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the job queue. If the job queue state is ``ENABLED`` , it is able to accept jobs. If the job queue state is ``DISABLED`` , new jobs can't be added to the queue, but jobs already in the queue can finish.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that are applied to the job queue. For more information, see `Tagging your AWS Batch resources <https://docs.aws.amazon.com/batch/latest/userguide/using-tags.html>`_ in *AWS Batch User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-jobqueue.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    cfn_job_queue_props = batch.CfnJobQueueProps(\n        compute_environment_order=[batch.CfnJobQueue.ComputeEnvironmentOrderProperty(\n            compute_environment="computeEnvironment",\n            order=123\n        )],\n        priority=123,\n\n        # the properties below are optional\n        job_queue_name="jobQueueName",\n        scheduling_policy_arn="schedulingPolicyArn",\n        state="state",\n        tags={\n            "tags_key": "tags"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_order', 'priority', 'job_queue_name', 'scheduling_policy_arn', 'state', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobQueueProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicyProps
class CfnSchedulingPolicyPropsDef(BaseCfnProperty):
    fairshare_policy: typing.Union[models.UnsupportedResource, models.aws_batch.CfnSchedulingPolicy_FairsharePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The fair share policy of the scheduling policy.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the scheduling policy. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that you apply to the scheduling policy to help you categorize and organize your resources. Each tag consists of a key and an optional value. For more information, see `Tagging AWS Resources <https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>`_ in *AWS General Reference* . These tags can be updated or removed using the `TagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_TagResource.html>`_ and `UntagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_UntagResource.html>`_ API operations.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-schedulingpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    cfn_scheduling_policy_props = batch.CfnSchedulingPolicyProps(\n        fairshare_policy=batch.CfnSchedulingPolicy.FairsharePolicyProperty(\n            compute_reservation=123,\n            share_decay_seconds=123,\n            share_distribution=[batch.CfnSchedulingPolicy.ShareAttributesProperty(\n                share_identifier="shareIdentifier",\n                weight_factor=123\n            )]\n        ),\n        name="name",\n        tags={\n            "tags_key": "tags"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['fairshare_policy', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    EcsVolume: typing.Optional[dict[str, models.aws_batch.EcsVolumeDef]] = pydantic.Field(None)
    EfsVolume: typing.Optional[dict[str, models.aws_batch.EfsVolumeDef]] = pydantic.Field(None)
    EksVolume: typing.Optional[dict[str, models.aws_batch.EksVolumeDef]] = pydantic.Field(None)
    EmptyDirVolume: typing.Optional[dict[str, models.aws_batch.EmptyDirVolumeDef]] = pydantic.Field(None)
    HostPathVolume: typing.Optional[dict[str, models.aws_batch.HostPathVolumeDef]] = pydantic.Field(None)
    HostVolume: typing.Optional[dict[str, models.aws_batch.HostVolumeDef]] = pydantic.Field(None)
    OptimalInstanceType: typing.Optional[dict[str, models.aws_batch.OptimalInstanceTypeDef]] = pydantic.Field(None)
    Reason: typing.Optional[dict[str, models.aws_batch.ReasonDef]] = pydantic.Field(None)
    RetryStrategy: typing.Optional[dict[str, models.aws_batch.RetryStrategyDef]] = pydantic.Field(None)
    Secret: typing.Optional[dict[str, models.aws_batch.SecretDef]] = pydantic.Field(None)
    SecretPathVolume: typing.Optional[dict[str, models.aws_batch.SecretPathVolumeDef]] = pydantic.Field(None)
    EcsEc2ContainerDefinition: typing.Optional[dict[str, models.aws_batch.EcsEc2ContainerDefinitionDef]] = pydantic.Field(None)
    EcsFargateContainerDefinition: typing.Optional[dict[str, models.aws_batch.EcsFargateContainerDefinitionDef]] = pydantic.Field(None)
    EcsJobDefinition: typing.Optional[dict[str, models.aws_batch.EcsJobDefinitionDef]] = pydantic.Field(None)
    EksContainerDefinition: typing.Optional[dict[str, models.aws_batch.EksContainerDefinitionDef]] = pydantic.Field(None)
    EksJobDefinition: typing.Optional[dict[str, models.aws_batch.EksJobDefinitionDef]] = pydantic.Field(None)
    FairshareSchedulingPolicy: typing.Optional[dict[str, models.aws_batch.FairshareSchedulingPolicyDef]] = pydantic.Field(None)
    FargateComputeEnvironment: typing.Optional[dict[str, models.aws_batch.FargateComputeEnvironmentDef]] = pydantic.Field(None)
    JobQueue: typing.Optional[dict[str, models.aws_batch.JobQueueDef]] = pydantic.Field(None)
    LinuxParameters: typing.Optional[dict[str, models.aws_batch.LinuxParametersDef]] = pydantic.Field(None)
    ManagedEc2EcsComputeEnvironment: typing.Optional[dict[str, models.aws_batch.ManagedEc2EcsComputeEnvironmentDef]] = pydantic.Field(None)
    ManagedEc2EksComputeEnvironment: typing.Optional[dict[str, models.aws_batch.ManagedEc2EksComputeEnvironmentDef]] = pydantic.Field(None)
    MultiNodeJobDefinition: typing.Optional[dict[str, models.aws_batch.MultiNodeJobDefinitionDef]] = pydantic.Field(None)
    UnmanagedComputeEnvironment: typing.Optional[dict[str, models.aws_batch.UnmanagedComputeEnvironmentDef]] = pydantic.Field(None)
    CfnComputeEnvironment_ComputeResourcesProperty: typing.Optional[dict[str, models.aws_batch.CfnComputeEnvironment_ComputeResourcesPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_Ec2ConfigurationObjectProperty: typing.Optional[dict[str, models.aws_batch.CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_EksConfigurationProperty: typing.Optional[dict[str, models.aws_batch.CfnComputeEnvironment_EksConfigurationPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_LaunchTemplateSpecificationProperty: typing.Optional[dict[str, models.aws_batch.CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_UpdatePolicyProperty: typing.Optional[dict[str, models.aws_batch.CfnComputeEnvironment_UpdatePolicyPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_AuthorizationConfigProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_AuthorizationConfigPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_ContainerPropertiesProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_DeviceProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_DevicePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EfsVolumeConfigurationProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EfsVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksContainerEnvironmentVariableProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksContainerProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EksContainerPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksContainerVolumeMountProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EksContainerVolumeMountPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksPropertiesProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EksPropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksSecretProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EksSecretPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksVolumeProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EksVolumePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EmptyDirProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EmptyDirPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EnvironmentProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EnvironmentPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EphemeralStorageProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EphemeralStoragePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EvaluateOnExitProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_EvaluateOnExitPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_FargatePlatformConfigurationProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_FargatePlatformConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_HostPathProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_HostPathPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_LinuxParametersProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_LinuxParametersPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_LogConfigurationProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_LogConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_MetadataProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_MetadataPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_MountPointsProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_MountPointsPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_NetworkConfigurationProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_NetworkConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_NodePropertiesProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_NodePropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_NodeRangePropertyProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_NodeRangePropertyPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_PodPropertiesProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_PodPropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_ResourceRequirementProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_ResourceRequirementPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_ResourcesProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_ResourcesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_RetryStrategyProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_RetryStrategyPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_RuntimePlatformProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_RuntimePlatformPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_SecretProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_SecretPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_SecurityContextProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_SecurityContextPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_TimeoutProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_TimeoutPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_TmpfsProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_TmpfsPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_UlimitProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_UlimitPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_VolumesHostProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_VolumesHostPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_VolumesProperty: typing.Optional[dict[str, models.aws_batch.CfnJobDefinition_VolumesPropertyDef]] = pydantic.Field(None)
    CfnJobQueue_ComputeEnvironmentOrderProperty: typing.Optional[dict[str, models.aws_batch.CfnJobQueue_ComputeEnvironmentOrderPropertyDef]] = pydantic.Field(None)
    CfnSchedulingPolicy_FairsharePolicyProperty: typing.Optional[dict[str, models.aws_batch.CfnSchedulingPolicy_FairsharePolicyPropertyDef]] = pydantic.Field(None)
    CfnSchedulingPolicy_ShareAttributesProperty: typing.Optional[dict[str, models.aws_batch.CfnSchedulingPolicy_ShareAttributesPropertyDef]] = pydantic.Field(None)
    ComputeEnvironmentProps: typing.Optional[dict[str, models.aws_batch.ComputeEnvironmentPropsDef]] = pydantic.Field(None)
    CustomReason: typing.Optional[dict[str, models.aws_batch.CustomReasonDef]] = pydantic.Field(None)
    Device: typing.Optional[dict[str, models.aws_batch.DeviceDef]] = pydantic.Field(None)
    EcsContainerDefinitionProps: typing.Optional[dict[str, models.aws_batch.EcsContainerDefinitionPropsDef]] = pydantic.Field(None)
    EcsEc2ContainerDefinitionProps: typing.Optional[dict[str, models.aws_batch.EcsEc2ContainerDefinitionPropsDef]] = pydantic.Field(None)
    EcsFargateContainerDefinitionProps: typing.Optional[dict[str, models.aws_batch.EcsFargateContainerDefinitionPropsDef]] = pydantic.Field(None)
    EcsJobDefinitionProps: typing.Optional[dict[str, models.aws_batch.EcsJobDefinitionPropsDef]] = pydantic.Field(None)
    EcsMachineImage: typing.Optional[dict[str, models.aws_batch.EcsMachineImageDef]] = pydantic.Field(None)
    EcsVolumeOptions: typing.Optional[dict[str, models.aws_batch.EcsVolumeOptionsDef]] = pydantic.Field(None)
    EfsVolumeOptions: typing.Optional[dict[str, models.aws_batch.EfsVolumeOptionsDef]] = pydantic.Field(None)
    EksContainerDefinitionProps: typing.Optional[dict[str, models.aws_batch.EksContainerDefinitionPropsDef]] = pydantic.Field(None)
    EksJobDefinitionProps: typing.Optional[dict[str, models.aws_batch.EksJobDefinitionPropsDef]] = pydantic.Field(None)
    EksMachineImage: typing.Optional[dict[str, models.aws_batch.EksMachineImageDef]] = pydantic.Field(None)
    EksVolumeOptions: typing.Optional[dict[str, models.aws_batch.EksVolumeOptionsDef]] = pydantic.Field(None)
    EmptyDirVolumeOptions: typing.Optional[dict[str, models.aws_batch.EmptyDirVolumeOptionsDef]] = pydantic.Field(None)
    FairshareSchedulingPolicyProps: typing.Optional[dict[str, models.aws_batch.FairshareSchedulingPolicyPropsDef]] = pydantic.Field(None)
    FargateComputeEnvironmentProps: typing.Optional[dict[str, models.aws_batch.FargateComputeEnvironmentPropsDef]] = pydantic.Field(None)
    HostPathVolumeOptions: typing.Optional[dict[str, models.aws_batch.HostPathVolumeOptionsDef]] = pydantic.Field(None)
    HostVolumeOptions: typing.Optional[dict[str, models.aws_batch.HostVolumeOptionsDef]] = pydantic.Field(None)
    JobDefinitionProps: typing.Optional[dict[str, models.aws_batch.JobDefinitionPropsDef]] = pydantic.Field(None)
    JobQueueProps: typing.Optional[dict[str, models.aws_batch.JobQueuePropsDef]] = pydantic.Field(None)
    LinuxParametersProps: typing.Optional[dict[str, models.aws_batch.LinuxParametersPropsDef]] = pydantic.Field(None)
    ManagedComputeEnvironmentProps: typing.Optional[dict[str, models.aws_batch.ManagedComputeEnvironmentPropsDef]] = pydantic.Field(None)
    ManagedEc2EcsComputeEnvironmentProps: typing.Optional[dict[str, models.aws_batch.ManagedEc2EcsComputeEnvironmentPropsDef]] = pydantic.Field(None)
    ManagedEc2EksComputeEnvironmentProps: typing.Optional[dict[str, models.aws_batch.ManagedEc2EksComputeEnvironmentPropsDef]] = pydantic.Field(None)
    MultiNodeContainer: typing.Optional[dict[str, models.aws_batch.MultiNodeContainerDef]] = pydantic.Field(None)
    MultiNodeJobDefinitionProps: typing.Optional[dict[str, models.aws_batch.MultiNodeJobDefinitionPropsDef]] = pydantic.Field(None)
    OrderedComputeEnvironment: typing.Optional[dict[str, models.aws_batch.OrderedComputeEnvironmentDef]] = pydantic.Field(None)
    SecretPathVolumeOptions: typing.Optional[dict[str, models.aws_batch.SecretPathVolumeOptionsDef]] = pydantic.Field(None)
    SecretVersionInfo: typing.Optional[dict[str, models.aws_batch.SecretVersionInfoDef]] = pydantic.Field(None)
    Share: typing.Optional[dict[str, models.aws_batch.ShareDef]] = pydantic.Field(None)
    Tmpfs: typing.Optional[dict[str, models.aws_batch.TmpfsDef]] = pydantic.Field(None)
    Ulimit: typing.Optional[dict[str, models.aws_batch.UlimitDef]] = pydantic.Field(None)
    UnmanagedComputeEnvironmentProps: typing.Optional[dict[str, models.aws_batch.UnmanagedComputeEnvironmentPropsDef]] = pydantic.Field(None)
    CfnComputeEnvironment: typing.Optional[dict[str, models.aws_batch.CfnComputeEnvironmentDef]] = pydantic.Field(None)
    CfnJobDefinition: typing.Optional[dict[str, models.aws_batch.CfnJobDefinitionDef]] = pydantic.Field(None)
    CfnJobQueue: typing.Optional[dict[str, models.aws_batch.CfnJobQueueDef]] = pydantic.Field(None)
    CfnSchedulingPolicy: typing.Optional[dict[str, models.aws_batch.CfnSchedulingPolicyDef]] = pydantic.Field(None)
    CfnComputeEnvironmentProps: typing.Optional[dict[str, models.aws_batch.CfnComputeEnvironmentPropsDef]] = pydantic.Field(None)
    CfnJobDefinitionProps: typing.Optional[dict[str, models.aws_batch.CfnJobDefinitionPropsDef]] = pydantic.Field(None)
    CfnJobQueueProps: typing.Optional[dict[str, models.aws_batch.CfnJobQueuePropsDef]] = pydantic.Field(None)
    CfnSchedulingPolicyProps: typing.Optional[dict[str, models.aws_batch.CfnSchedulingPolicyPropsDef]] = pydantic.Field(None)
    ...

import models
