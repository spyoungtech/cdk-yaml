from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams

#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.ComputeResourcesProperty
class CfnComputeEnvironment_ComputeResourcesPropertyDef(BaseStruct):
    maxv_cpus: typing.Union[int, float] = pydantic.Field(..., description='The maximum number of Amazon EC2 vCPUs that an environment can reach. .. epigraph:: With both ``BEST_FIT_PROGRESSIVE`` and ``SPOT_CAPACITY_OPTIMIZED`` allocation strategies using On-Demand or Spot Instances, and the ``BEST_FIT`` strategy using Spot Instances, AWS Batch might need to exceed ``maxvCpus`` to meet your capacity requirements. In this event, AWS Batch never exceeds ``maxvCpus`` by more than a single instance. That is, no more than a single instance from among those specified in your compute environment.\n')
    subnets: typing.Sequence[str] = pydantic.Field(..., description="The VPC subnets where the compute resources are launched. Fargate compute resources can contain up to 16 subnets. For Fargate compute resources, providing an empty list will be handled as if this parameter wasn't specified and no change is made. For EC2 compute resources, providing an empty list removes the VPC subnets from the compute resource. For more information, see `VPCs and subnets <https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html>`_ in the *Amazon VPC User Guide* . When updating a compute environment, changing the VPC subnets requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: AWS Batch on Amazon EC2 and AWS Batch on Amazon EKS support Local Zones. For more information, see `Local Zones <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-local-zones>`_ in the *Amazon EC2 User Guide for Linux Instances* , `Amazon EKS and AWS Local Zones <https://docs.aws.amazon.com/eks/latest/userguide/local-zones.html>`_ in the *Amazon EKS User Guide* and `Amazon ECS clusters in Local Zones, Wavelength Zones, and AWS Outposts <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-regions-zones.html#clusters-local-zones>`_ in the *Amazon ECS Developer Guide* . AWS Batch on Fargate doesn't currently support Local Zones.\n")
    type: str = pydantic.Field(..., description='The type of compute environment: ``EC2`` , ``SPOT`` , ``FARGATE`` , or ``FARGATE_SPOT`` . For more information, see `Compute environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the *AWS Batch User Guide* . If you choose ``SPOT`` , you must also specify an Amazon EC2 Spot Fleet role with the ``spotIamFleetRole`` parameter. For more information, see `Amazon EC2 spot fleet role <https://docs.aws.amazon.com/batch/latest/userguide/spot_fleet_IAM_role.html>`_ in the *AWS Batch User Guide* . When updating compute environment, changing the type of a compute environment requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . When updating the type of a compute environment, changing between ``EC2`` and ``SPOT`` or between ``FARGATE`` and ``FARGATE_SPOT`` will initiate an infrastructure update, but if you switch between ``EC2`` and ``FARGATE`` , AWS CloudFormation will create a new compute environment.\n')
    allocation_strategy: typing.Optional[str] = pydantic.Field(None, description="The allocation strategy to use for the compute resource if not enough instances of the best fitting instance type can be allocated. This might be because of availability of the instance type in the Region or `Amazon EC2 service limits <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html>`_ . For more information, see `Allocation strategies <https://docs.aws.amazon.com/batch/latest/userguide/allocation-strategies.html>`_ in the *AWS Batch User Guide* . When updating a compute environment, changing the allocation strategy requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . ``BEST_FIT`` is not supported when updating a compute environment. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources, and shouldn't be specified. - **BEST_FIT (default)** - AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren't available, AWS Batch waits for the additional instances to be available. If there aren't enough instances available, or if the user is reaching `Amazon EC2 service limits <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html>`_ then additional jobs aren't run until the currently running jobs have completed. This allocation strategy keeps costs lower but can limit scaling. If you are using Spot Fleets with ``BEST_FIT`` then the Spot Fleet IAM role must be specified. - **BEST_FIT_PROGRESSIVE** - AWS Batch will select additional instance types that are large enough to meet the requirements of the jobs in the queue, with a preference for instance types with a lower cost per unit vCPU. If additional instances of the previously selected instance types aren't available, AWS Batch will select new instance types. - **SPOT_CAPACITY_OPTIMIZED** - AWS Batch will select one or more instance types that are large enough to meet the requirements of the jobs in the queue, with a preference for instance types that are less likely to be interrupted. This allocation strategy is only available for Spot Instance compute resources. With both ``BEST_FIT_PROGRESSIVE`` and ``SPOT_CAPACITY_OPTIMIZED`` allocation strategies using On-Demand or Spot Instances, and the ``BEST_FIT`` strategy using Spot Instances, AWS Batch might need to go above ``maxvCpus`` to meet your capacity requirements. In this event, AWS Batch never exceeds ``maxvCpus`` by more than a single instance.\n")
    bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum percentage that a Spot Instance price can be when compared with the On-Demand price for that instance type before instances are launched. For example, if your maximum percentage is 20%, the Spot price must be less than 20% of the current On-Demand price for that Amazon EC2 instance. You always pay the lowest (market) price and never more than your maximum percentage. For most use cases, we recommend leaving this field empty. When updating a compute environment, changing the bid percentage requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    desiredv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The desired number of vCPUS in the compute environment. AWS Batch modifies this value between the minimum and maximum values based on job queue demand. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > AWS Batch doesn't support changing the desired number of vCPUs of an existing compute environment. Don't specify this parameter for compute environments using Amazon EKS clusters. > When you update the ``desiredvCpus`` setting, the value must be between the ``minvCpus`` and ``maxvCpus`` values. Additionally, the updated ``desiredvCpus`` value must be greater than or equal to the current ``desiredvCpus`` value. For more information, see `Troubleshooting AWS Batch <https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#error-desired-vcpus-update>`_ in the *AWS Batch User Guide* .\n")
    ec2_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Provides information used to select Amazon Machine Images (AMIs) for EC2 instances in the compute environment. If ``Ec2Configuration`` isn't specified, the default is ``ECS_AL2`` . When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . To remove the EC2 configuration and any custom AMI ID specified in ``imageIdOverride`` , set this value to an empty string. One or two values can be provided. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    ec2_key_pair: typing.Optional[str] = pydantic.Field(None, description="The Amazon EC2 key pair that's used for instances launched in the compute environment. You can use this key pair to log in to your instances with SSH. To remove the Amazon EC2 key pair, set this value to an empty string. When updating a compute environment, changing the EC2 key pair requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    image_id: typing.Optional[str] = pydantic.Field(None, description="The Amazon Machine Image (AMI) ID used for instances launched in the compute environment. This parameter is overridden by the ``imageIdOverride`` member of the ``Ec2Configuration`` structure. To remove the custom AMI ID and use the default AMI ID, set this value to an empty string. When updating a compute environment, changing the AMI ID requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > The AMI that you choose for a compute environment must match the architecture of the instance types that you intend to use for that compute environment. For example, if your compute environment uses A1 instance types, the compute resource AMI that you choose must support ARM instances. Amazon ECS vends both x86 and ARM versions of the Amazon ECS-optimized Amazon Linux 2 AMI. For more information, see `Amazon ECS-optimized Amazon Linux 2 AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#ecs-optimized-ami-linux-variants.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n")
    instance_role: typing.Optional[str] = pydantic.Field(None, description="The Amazon ECS instance profile applied to Amazon EC2 instances in a compute environment. You can specify the short name or full Amazon Resource Name (ARN) of an instance profile. For example, ``*ecsInstanceRole*`` or ``arn:aws:iam:: *<aws_account_id>* :instance-profile/ *ecsInstanceRole*`` . For more information, see `Amazon ECS instance role <https://docs.aws.amazon.com/batch/latest/userguide/instance_IAM_role.html>`_ in the *AWS Batch User Guide* . When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    instance_types: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The instances types that can be launched. You can specify instance families to launch any instance type within those families (for example, ``c5`` or ``p3`` ), or you can specify specific sizes within a family (such as ``c5.8xlarge`` ). You can also choose ``optimal`` to select instance types (from the C4, M4, and R4 instance families) that match the demand of your job queues. When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > When you create a compute environment, the instance types that you select for the compute environment must share the same architecture. For example, you can't mix x86 and ARM instances in the same compute environment. > Currently, ``optimal`` uses instance types from the C4, M4, and R4 instance families. In Regions that don't have instance types from those instance families, instance types from the C5, M5, and R5 instance families are used.\n")
    launch_template: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The launch template to use for your compute resources. Any other compute resource parameters that you specify in a `CreateComputeEnvironment <https://docs.aws.amazon.com/batch/latest/APIReference/API_CreateComputeEnvironment.html>`_ API operation override the same parameters in the launch template. You must specify either the launch template ID or launch template name in the request, but not both. For more information, see `Launch Template Support <https://docs.aws.amazon.com/batch/latest/userguide/launch-templates.html>`_ in the ** . Removing the launch template from a compute environment will not remove the AMI specified in the launch template. In order to update the AMI specified in a launch template, the ``updateToLatestImageVersion`` parameter must be set to ``true`` . When updating a compute environment, changing the launch template requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the ** . .. epigraph:: This parameter isn't applicable to jobs running on Fargate resources, and shouldn't be specified.\n")
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The minimum number of vCPUs that an environment should maintain (even if the compute environment is ``DISABLED`` ). .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    placement_group: typing.Optional[str] = pydantic.Field(None, description="The Amazon EC2 placement group to associate with your compute resources. If you intend to submit multi-node parallel jobs to your compute environment, you should consider creating a cluster placement group and associate it with your compute resources. This keeps your multi-node parallel job on a logical grouping of instances within a single Availability Zone with high network flow potential. For more information, see `Placement groups <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html>`_ in the *Amazon EC2 User Guide for Linux Instances* . When updating a compute environment, changing the placement group requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it.\n")
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The Amazon EC2 security groups that are associated with instances launched in the compute environment. This parameter is required for Fargate compute resources, where it can contain up to 5 security groups. For Fargate compute resources, providing an empty list is handled as if this parameter wasn't specified and no change is made. For EC2 compute resources, providing an empty list removes the security groups from the compute resource. When updating a compute environment, changing the EC2 security groups requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* .\n")
    spot_iam_fleet_role: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the Amazon EC2 Spot Fleet IAM role applied to a ``SPOT`` compute environment. This role is required if the allocation strategy set to ``BEST_FIT`` or if the allocation strategy isn't specified. For more information, see `Amazon EC2 spot fleet role <https://docs.aws.amazon.com/batch/latest/userguide/spot_fleet_IAM_role.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't specify it. > To tag your Spot Instances on creation, the Spot Fleet IAM role specified here must use the newer *AmazonEC2SpotFleetTaggingRole* managed policy. The previously recommended *AmazonEC2SpotFleetRole* managed policy doesn't have the required permissions to tag Spot Instances. For more information, see `Spot instances not tagged on creation <https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#spot-instance-no-tag>`_ in the *AWS Batch User Guide* .\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pair tags to be applied to EC2 resources that are launched in the compute environment. For AWS Batch , these take the form of ``"String1": "String2"`` , where ``String1`` is the tag key and ``String2`` is the tag value-for example, ``{ "Name": "Batch Instance - C4OnDemand" }`` . This is helpful for recognizing your AWS Batch instances in the Amazon EC2 console. These tags aren\'t seen when using the AWS Batch ``ListTagsForResource`` API operation. When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . .. epigraph:: This parameter isn\'t applicable to jobs that are running on Fargate resources. Don\'t specify it.\n')
    update_to_latest_image_version: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Specifies whether the AMI ID is updated to the latest one that\'s supported by AWS Batch when the compute environment has an infrastructure update. The default value is ``false`` . .. epigraph:: An AMI ID can either be specified in the ``imageId`` or ``imageIdOverride`` parameters or be determined by the launch template that\'s specified in the ``launchTemplate`` parameter. If an AMI ID is specified any of these ways, this parameter is ignored. For more information about to update AMI IDs during an infrastructure update, see `Updating the AMI ID <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html#updating-compute-environments-ami>`_ in the *AWS Batch User Guide* . When updating a compute environment, changing this setting requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    compute_resources_property = batch.CfnComputeEnvironment.ComputeResourcesProperty(\n        maxv_cpus=123,\n        subnets=["subnets"],\n        type="type",\n\n        # the properties below are optional\n        allocation_strategy="allocationStrategy",\n        bid_percentage=123,\n        desiredv_cpus=123,\n        ec2_configuration=[batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty(\n            image_type="imageType",\n\n            # the properties below are optional\n            image_id_override="imageIdOverride",\n            image_kubernetes_version="imageKubernetesVersion"\n        )],\n        ec2_key_pair="ec2KeyPair",\n        image_id="imageId",\n        instance_role="instanceRole",\n        instance_types=["instanceTypes"],\n        launch_template=batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty(\n            launch_template_id="launchTemplateId",\n            launch_template_name="launchTemplateName",\n            version="version"\n        ),\n        minv_cpus=123,\n        placement_group="placementGroup",\n        security_group_ids=["securityGroupIds"],\n        spot_iam_fleet_role="spotIamFleetRole",\n        tags={\n            "tags_key": "tags"\n        },\n        update_to_latest_image_version=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['maxv_cpus', 'subnets', 'type', 'allocation_strategy', 'bid_percentage', 'desiredv_cpus', 'ec2_configuration', 'ec2_key_pair', 'image_id', 'instance_role', 'instance_types', 'launch_template', 'minv_cpus', 'placement_group', 'security_group_ids', 'spot_iam_fleet_role', 'tags', 'update_to_latest_image_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.ComputeResourcesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty
class CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef(BaseStruct):
    image_type: str = pydantic.Field(..., description="The image type to match with the instance type to select an AMI. The supported values are different for ``ECS`` and ``EKS`` resources. - **ECS** - If the ``imageIdOverride`` parameter isn't specified, then a recent `Amazon ECS-optimized Amazon Linux 2 AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#al2ami>`_ ( ``ECS_AL2`` ) is used. If a new image type is specified in an update, but neither an ``imageId`` nor a ``imageIdOverride`` parameter is specified, then the latest Amazon ECS optimized AMI for that image type that's supported by AWS Batch is used. - **ECS_AL2** - `Amazon Linux 2 <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#al2ami>`_ : Default for all non-GPU instance families. - **ECS_AL2_NVIDIA** - `Amazon Linux 2 (GPU) <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#gpuami>`_ : Default for all GPU instance families (for example ``P4`` and ``G4`` ) and can be used for all non AWS Graviton-based instance types. - **ECS_AL1** - `Amazon Linux <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#alami>`_ . Amazon Linux has reached the end-of-life of standard support. For more information, see `Amazon Linux AMI <https://docs.aws.amazon.com/amazon-linux-ami/>`_ . - **EKS** - If the ``imageIdOverride`` parameter isn't specified, then a recent `Amazon EKS-optimized Amazon Linux AMI <https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html>`_ ( ``EKS_AL2`` ) is used. If a new image type is specified in an update, but neither an ``imageId`` nor a ``imageIdOverride`` parameter is specified, then the latest Amazon EKS optimized AMI for that image type that AWS Batch supports is used. - **EKS_AL2** - `Amazon Linux 2 <https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html>`_ : Default for all non-GPU instance families. - **EKS_AL2_NVIDIA** - `Amazon Linux 2 (accelerated) <https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html>`_ : Default for all GPU instance families (for example, ``P4`` and ``G4`` ) and can be used for all non AWS Graviton-based instance types.\n")
    image_id_override: typing.Optional[str] = pydantic.Field(None, description='The AMI ID used for instances launched in the compute environment that match the image type. This setting overrides the ``imageId`` set in the ``computeResource`` object. .. epigraph:: The AMI that you choose for a compute environment must match the architecture of the instance types that you intend to use for that compute environment. For example, if your compute environment uses A1 instance types, the compute resource AMI that you choose must support ARM instances. Amazon ECS vends both x86 and ARM versions of the Amazon ECS-optimized Amazon Linux 2 AMI. For more information, see `Amazon ECS-optimized Amazon Linux 2 AMI <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#ecs-optimized-ami-linux-variants.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    image_kubernetes_version: typing.Optional[str] = pydantic.Field(None, description='The Kubernetes version for the compute environment. If you don\'t specify a value, the latest version that AWS Batch supports is used.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-ec2configurationobject.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    ec2_configuration_object_property = batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty(\n        image_type="imageType",\n\n        # the properties below are optional\n        image_id_override="imageIdOverride",\n        image_kubernetes_version="imageKubernetesVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image_type', 'image_id_override', 'image_kubernetes_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.EksConfigurationProperty
class CfnComputeEnvironment_EksConfigurationPropertyDef(BaseStruct):
    eks_cluster_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the Amazon EKS cluster. An example is ``arn: *aws* :eks: *us-east-1* : *123456789012* :cluster/ *ClusterForBatch*`` .\n')
    kubernetes_namespace: str = pydantic.Field(..., description='The namespace of the Amazon EKS cluster. AWS Batch manages pods in this namespace. The value can\'t left empty or null. It must be fewer than 64 characters long, can\'t be set to ``default`` , can\'t start with " ``kube-`` ," and must match this regular expression: ``^[a-z0-9]([-a-z0-9]*[a-z0-9])?$`` . For more information, see `Namespaces <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/>`_ in the Kubernetes documentation.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-eksconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_configuration_property = batch.CfnComputeEnvironment.EksConfigurationProperty(\n        eks_cluster_arn="eksClusterArn",\n        kubernetes_namespace="kubernetesNamespace"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['eks_cluster_arn', 'kubernetes_namespace']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.EksConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty
class CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef(BaseStruct):
    launch_template_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the launch template.\n')
    launch_template_name: typing.Optional[str] = pydantic.Field(None, description='The name of the launch template.\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version number of the launch template, ``$Latest`` , or ``$Default`` . If the value is ``$Latest`` , the latest version of the launch template is used. If the value is ``$Default`` , the default version of the launch template is used. .. epigraph:: If the AMI ID that\'s used in a compute environment is from the launch template, the AMI isn\'t changed when the compute environment is updated. It\'s only changed if the ``updateToLatestImageVersion`` parameter for the compute environment is set to ``true`` . During an infrastructure update, if either ``$Latest`` or ``$Default`` is specified, AWS Batch re-evaluates the launch template version, and it might use a different version of the launch template. This is the case even if the launch template isn\'t specified in the update. When updating a compute environment, changing the launch template requires an infrastructure update of the compute environment. For more information, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* . Default: ``$Default`` .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-launchtemplatespecification.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    launch_template_specification_property = batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty(\n        launch_template_id="launchTemplateId",\n        launch_template_name="launchTemplateName",\n        version="version"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['launch_template_id', 'launch_template_name', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment.UpdatePolicyProperty
class CfnComputeEnvironment_UpdatePolicyPropertyDef(BaseStruct):
    job_execution_timeout_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the job timeout (in minutes) when the compute environment infrastructure is updated. The default value is 30.\n')
    terminate_jobs_on_update: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Specifies whether jobs are automatically terminated when the computer environment infrastructure is updated. The default value is ``false`` .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-updatepolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    update_policy_property = batch.CfnComputeEnvironment.UpdatePolicyProperty(\n        job_execution_timeout_minutes=123,\n        terminate_jobs_on_update=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['job_execution_timeout_minutes', 'terminate_jobs_on_update']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment.UpdatePolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.AuthorizationConfigProperty
class CfnJobDefinition_AuthorizationConfigPropertyDef(BaseStruct):
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='The Amazon EFS access point ID to use. If an access point is specified, the root directory value specified in the ``EFSVolumeConfiguration`` must either be omitted or set to ``/`` which enforces the path set on the EFS access point. If an access point is used, transit encryption must be enabled in the ``EFSVolumeConfiguration`` . For more information, see `Working with Amazon EFS access points <https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html>`_ in the *Amazon Elastic File System User Guide* .\n')
    iam: typing.Optional[str] = pydantic.Field(None, description='Whether or not to use the AWS Batch job IAM role defined in a job definition when mounting the Amazon EFS file system. If enabled, transit encryption must be enabled in the ``EFSVolumeConfiguration`` . If this parameter is omitted, the default value of ``DISABLED`` is used. For more information, see `Using Amazon EFS access points <https://docs.aws.amazon.com/batch/latest/userguide/efs-volumes.html#efs-volume-accesspoints>`_ in the *AWS Batch User Guide* . EFS IAM authorization requires that ``TransitEncryption`` be ``ENABLED`` and that a ``JobRoleArn`` is specified.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-authorizationconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    authorization_config_property = batch.CfnJobDefinition.AuthorizationConfigProperty(\n        access_point_id="accessPointId",\n        iam="iam"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_point_id', 'iam']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.AuthorizationConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.ContainerPropertiesProperty
class CfnJobDefinition_ContainerPropertiesPropertyDef(BaseStruct):
    image: str = pydantic.Field(..., description="The image used to start a container. This string is passed directly to the Docker daemon. Images in the Docker Hub registry are available by default. Other repositories are specified with ``*repository-url* / *image* : *tag*`` . It can be 255 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), underscores (_), colons (:), periods (.), forward slashes (/), and number signs (#). This parameter maps to ``Image`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``IMAGE`` parameter of `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: Docker image architecture must match the processor architecture of the compute resources that they're scheduled on. For example, ARM-based Docker images can only run on ARM-based compute resources. - Images in Amazon ECR Public repositories use the full ``registry/repository[:tag]`` or ``registry/repository[@digest]`` naming conventions. For example, ``public.ecr.aws/ *registry_alias* / *my-web-app* : *latest*`` . - Images in Amazon ECR repositories use the full registry and repository URI (for example, ``123456789012.dkr.ecr.<region-name>.amazonaws.com/<repository-name>`` ). - Images in official repositories on Docker Hub use a single name (for example, ``ubuntu`` or ``mongo`` ). - Images in other repositories on Docker Hub are qualified with an organization name (for example, ``amazon/amazon-ecs-agent`` ). - Images in other online repositories are qualified further by a domain name (for example, ``quay.io/assemblyline/ubuntu`` ).\n")
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The command that's passed to the container. This parameter maps to ``Cmd`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``COMMAND`` parameter to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . For more information, see `https://docs.docker.com/engine/reference/builder/#cmd <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/builder/#cmd>`_ .\n")
    environment: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EnvironmentPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The environment variables to pass to a container. This parameter maps to ``Env`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--env`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: We don\'t recommend using plaintext environment variables for sensitive information, such as credential data. > Environment variables cannot start with " ``AWS_BATCH`` ". This naming convention is reserved for variables that AWS Batch sets.\n')
    ephemeral_storage: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``CfnJobDefinition.ContainerPropertiesProperty.EphemeralStorage``.\n')
    execution_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the execution role that AWS Batch can assume. For jobs that run on Fargate resources, you must provide an execution role. For more information, see `AWS Batch execution IAM role <https://docs.aws.amazon.com/batch/latest/userguide/execution-IAM-role.html>`_ in the *AWS Batch User Guide* .\n')
    fargate_platform_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_FargatePlatformConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The platform configuration for jobs that are running on Fargate resources. Jobs that are running on EC2 resources must not specify this parameter.\n')
    instance_type: typing.Optional[str] = pydantic.Field(None, description="The instance type to use for a multi-node parallel job. All node groups in a multi-node parallel job must use the same instance type. .. epigraph:: This parameter isn't applicable to single-node container jobs or jobs that run on Fargate resources, and shouldn't be provided.\n")
    job_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the IAM role that the container can assume for AWS permissions. For more information, see `IAM roles for tasks <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    linux_parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_LinuxParametersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Linux-specific modifications that are applied to the container, such as details for device mappings.\n')
    log_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The log configuration specification for the container. This parameter maps to ``LogConfig`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--log-driver`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . By default, containers use the same logging driver that the Docker daemon uses. However the container might use a different logging driver than the Docker daemon by specifying a log driver with this parameter in the container definition. To use a different logging driver for a container, the log system must be configured properly on the container instance (or on a different log server for remote logging options). For more information on the options for different supported log drivers, see `Configure logging drivers <https://docs.aws.amazon.com/https://docs.docker.com/engine/admin/logging/overview/>`_ in the Docker documentation. .. epigraph:: AWS Batch currently supports a subset of the logging drivers available to the Docker daemon (shown in the ``LogConfiguration`` data type). This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"`` .. epigraph:: The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with the ``ECS_AVAILABLE_LOGGING_DRIVERS`` environment variable before containers placed on that instance can use these log configuration options. For more information, see `Amazon ECS container agent configuration <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n')
    memory: typing.Union[int, float, None] = pydantic.Field(None, description="This parameter is deprecated, use ``resourceRequirements`` to specify the memory requirements for the job definition. It's not supported for jobs running on Fargate resources. For jobs that run on EC2 resources, it specifies the memory hard limit (in MiB) for a container. If your container attempts to exceed the specified number, it's terminated. You must specify at least 4 MiB of memory for a job using this parameter. The memory hard limit can be specified in several places. It must be specified for each node at least once.\n")
    mount_points: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_MountPointsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The mount points for data volumes in your container. This parameter maps to ``Volumes`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--volume`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ .\n')
    network_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The network configuration for jobs that are running on Fargate resources. Jobs that are running on EC2 resources must not specify this parameter.\n')
    privileged: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="When this parameter is true, the container is given elevated permissions on the host container instance (similar to the ``root`` user). This parameter maps to ``Privileged`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--privileged`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . The default value is false. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources and shouldn't be provided, or specified as false.\n")
    readonly_root_filesystem: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='When this parameter is true, the container is given read-only access to its root file system. This parameter maps to ``ReadonlyRootfs`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--read-only`` option to ``docker run`` .\n')
    resource_requirements: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ResourceRequirementPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The type and amount of resources to assign to a container. The supported resources include ``GPU`` , ``MEMORY`` , and ``VCPU`` .\n')
    secrets: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The secrets for the container. For more information, see `Specifying sensitive data <https://docs.aws.amazon.com/batch/latest/userguide/specifying-sensitive-data.html>`_ in the *AWS Batch User Guide* .\n')
    ulimits: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_UlimitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="A list of ``ulimits`` to set in the container. This parameter maps to ``Ulimits`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--ulimit`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources and shouldn't be provided.\n")
    user: typing.Optional[str] = pydantic.Field(None, description='The user name to use inside the container. This parameter maps to ``User`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--user`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ .\n')
    vcpus: typing.Union[int, float, None] = pydantic.Field(None, description="This parameter is deprecated, use ``resourceRequirements`` to specify the vCPU requirements for the job definition. It's not supported for jobs running on Fargate resources. For jobs running on EC2 resources, it specifies the number of vCPUs reserved for the job. Each vCPU is equivalent to 1,024 CPU shares. This parameter maps to ``CpuShares`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--cpu-shares`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . The number of vCPUs must be specified but can be specified in several places. You must specify it at least once for each node.\n")
    volumes: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_VolumesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of data volumes used in a job.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-containerproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    container_properties_property = batch.CfnJobDefinition.ContainerPropertiesProperty(\n        image="image",\n\n        # the properties below are optional\n        command=["command"],\n        environment=[batch.CfnJobDefinition.EnvironmentProperty(\n            name="name",\n            value="value"\n        )],\n        ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n            size_in_gi_b=123\n        ),\n        execution_role_arn="executionRoleArn",\n        fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n            platform_version="platformVersion"\n        ),\n        instance_type="instanceType",\n        job_role_arn="jobRoleArn",\n        linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n            devices=[batch.CfnJobDefinition.DeviceProperty(\n                container_path="containerPath",\n                host_path="hostPath",\n                permissions=["permissions"]\n            )],\n            init_process_enabled=False,\n            max_swap=123,\n            shared_memory_size=123,\n            swappiness=123,\n            tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                container_path="containerPath",\n                size=123,\n\n                # the properties below are optional\n                mount_options=["mountOptions"]\n            )]\n        ),\n        log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n            log_driver="logDriver",\n\n            # the properties below are optional\n            options=options,\n            secret_options=[batch.CfnJobDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )]\n        ),\n        memory=123,\n        mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n            container_path="containerPath",\n            read_only=False,\n            source_volume="sourceVolume"\n        )],\n        network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n            assign_public_ip="assignPublicIp"\n        ),\n        privileged=False,\n        readonly_root_filesystem=False,\n        resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n            type="type",\n            value="value"\n        )],\n        secrets=[batch.CfnJobDefinition.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )],\n        ulimits=[batch.CfnJobDefinition.UlimitProperty(\n            hard_limit=123,\n            name="name",\n            soft_limit=123\n        )],\n        user="user",\n        vcpus=123,\n        volumes=[batch.CfnJobDefinition.VolumesProperty(\n            efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                file_system_id="fileSystemId",\n\n                # the properties below are optional\n                authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                    access_point_id="accessPointId",\n                    iam="iam"\n                ),\n                root_directory="rootDirectory",\n                transit_encryption="transitEncryption",\n                transit_encryption_port=123\n            ),\n            host=batch.CfnJobDefinition.VolumesHostProperty(\n                source_path="sourcePath"\n            ),\n            name="name"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'environment', 'ephemeral_storage', 'execution_role_arn', 'fargate_platform_configuration', 'instance_type', 'job_role_arn', 'linux_parameters', 'log_configuration', 'memory', 'mount_points', 'network_configuration', 'privileged', 'readonly_root_filesystem', 'resource_requirements', 'secrets', 'ulimits', 'user', 'vcpus', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.ContainerPropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.DeviceProperty
class CfnJobDefinition_DevicePropertyDef(BaseStruct):
    container_path: typing.Optional[str] = pydantic.Field(None, description="The path inside the container that's used to expose the host device. By default, the ``hostPath`` value is used.\n")
    host_path: typing.Optional[str] = pydantic.Field(None, description='The path for the device on the host container instance.\n')
    permissions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The explicit permissions to provide to the container for the device. By default, the container has permissions for ``read`` , ``write`` , and ``mknod`` for the device.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-device.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    device_property = batch.CfnJobDefinition.DeviceProperty(\n        container_path="containerPath",\n        host_path="hostPath",\n        permissions=["permissions"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'host_path', 'permissions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.DeviceProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EfsVolumeConfigurationProperty
class CfnJobDefinition_EfsVolumeConfigurationPropertyDef(BaseStruct):
    file_system_id: str = pydantic.Field(..., description='The Amazon EFS file system ID to use.\n')
    authorization_config: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_AuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The authorization configuration details for the Amazon EFS file system.\n')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='The directory within the Amazon EFS file system to mount as the root directory inside the host. If this parameter is omitted, the root of the Amazon EFS volume is used instead. Specifying ``/`` has the same effect as omitting this parameter. The maximum length is 4,096 characters. .. epigraph:: If an EFS access point is specified in the ``authorizationConfig`` , the root directory parameter must either be omitted or set to ``/`` , which enforces the path set on the Amazon EFS access point.\n')
    transit_encryption: typing.Optional[str] = pydantic.Field(None, description='Determines whether to enable encryption for Amazon EFS data in transit between the Amazon ECS host and the Amazon EFS server. Transit encryption must be enabled if Amazon EFS IAM authorization is used. If this parameter is omitted, the default value of ``DISABLED`` is used. For more information, see `Encrypting data in transit <https://docs.aws.amazon.com/efs/latest/ug/encryption-in-transit.html>`_ in the *Amazon Elastic File System User Guide* .\n')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use when sending encrypted data between the Amazon ECS host and the Amazon EFS server. If you don\'t specify a transit encryption port, it uses the port selection strategy that the Amazon EFS mount helper uses. The value must be between 0 and 65,535. For more information, see `EFS mount helper <https://docs.aws.amazon.com/efs/latest/ug/efs-mount-helper.html>`_ in the *Amazon Elastic File System User Guide* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-efsvolumeconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    efs_volume_configuration_property = batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n        file_system_id="fileSystemId",\n\n        # the properties below are optional\n        authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n            access_point_id="accessPointId",\n            iam="iam"\n        ),\n        root_directory="rootDirectory",\n        transit_encryption="transitEncryption",\n        transit_encryption_port=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['file_system_id', 'authorization_config', 'root_directory', 'transit_encryption', 'transit_encryption_port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EfsVolumeConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty
class CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef(BaseStruct):
    name: str = pydantic.Field(..., description='The name of the environment variable.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value of the environment variable.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainerenvironmentvariable.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_container_environment_variable_property = batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n        name="name",\n\n        # the properties below are optional\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksContainerProperty
class CfnJobDefinition_EksContainerPropertyDef(BaseStruct):
    image: str = pydantic.Field(..., description='The Docker image used to start the container.\n')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The entrypoint for the container. This isn\'t run within a shell. If this isn\'t specified, the ``ENTRYPOINT`` of the container image is used. Environment variable references are expanded using the container\'s environment. If the referenced environment variable doesn\'t exist, the reference in the command isn\'t changed. For example, if the reference is to " ``$(NAME1)`` " and the ``NAME1`` environment variable doesn\'t exist, the command string will remain " ``$(NAME1)`` ." ``$$`` is replaced with ``$`` and the resulting string isn\'t expanded. For example, ``$$(VAR_NAME)`` will be passed as ``$(VAR_NAME)`` whether or not the ``VAR_NAME`` environment variable exists. The entrypoint can\'t be updated. For more information, see `ENTRYPOINT <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/builder/#entrypoint>`_ in the *Dockerfile reference* and `Define a command and arguments for a container <https://docs.aws.amazon.com/https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/>`_ and `Entrypoint <https://docs.aws.amazon.com/https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#entrypoint>`_ in the *Kubernetes documentation* .\n')
    env: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The environment variables to pass to a container. .. epigraph:: Environment variables cannot start with " ``AWS_BATCH`` ". This naming convention is reserved for variables that AWS Batch sets.\n')
    image_pull_policy: typing.Optional[str] = pydantic.Field(None, description='The image pull policy for the container. Supported values are ``Always`` , ``IfNotPresent`` , and ``Never`` . This parameter defaults to ``IfNotPresent`` . However, if the ``:latest`` tag is specified, it defaults to ``Always`` . For more information, see `Updating images <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/containers/images/#updating-images>`_ in the *Kubernetes documentation* .\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the container. If the name isn\'t specified, the default name " ``Default`` " is used. Each container in a pod must have a unique name.\n')
    resources: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type and amount of resources to assign to a container. The supported resources include ``memory`` , ``cpu`` , and ``nvidia.com/gpu`` . For more information, see `Resource management for pods and containers <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>`_ in the *Kubernetes documentation* .\n')
    security_context: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_SecurityContextPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The security context for a job. For more information, see `Configure a security context for a pod or container <https://docs.aws.amazon.com/https://kubernetes.io/docs/tasks/configure-pod-container/security-context/>`_ in the *Kubernetes documentation* .\n')
    volume_mounts: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksContainerVolumeMountPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The volume mounts for the container. AWS Batch supports ``emptyDir`` , ``hostPath`` , and ``secret`` volume types. For more information about volumes and volume mounts in Kubernetes, see `Volumes <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/>`_ in the *Kubernetes documentation* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainer.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # limits: Any\n    # requests: Any\n\n    eks_container_property = batch.CfnJobDefinition.EksContainerProperty(\n        image="image",\n\n        # the properties below are optional\n        args=["args"],\n        command=["command"],\n        env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n            name="name",\n\n            # the properties below are optional\n            value="value"\n        )],\n        image_pull_policy="imagePullPolicy",\n        name="name",\n        resources=batch.CfnJobDefinition.ResourcesProperty(\n            limits=limits,\n            requests=requests\n        ),\n        security_context=batch.CfnJobDefinition.SecurityContextProperty(\n            privileged=False,\n            read_only_root_filesystem=False,\n            run_as_group=123,\n            run_as_non_root=False,\n            run_as_user=123\n        ),\n        volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n            mount_path="mountPath",\n            name="name",\n            read_only=False\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'command', 'env', 'image_pull_policy', 'name', 'resources', 'security_context', 'volume_mounts']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksContainerProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksContainerVolumeMountProperty
class CfnJobDefinition_EksContainerVolumeMountPropertyDef(BaseStruct):
    mount_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the volume is mounted.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name the volume mount. This must match the name of one of the volumes in the pod.\n')
    read_only: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='If this value is ``true`` , the container has read-only access to the volume. Otherwise, the container can write to the volume. The default value is ``false`` .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainervolumemount.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_container_volume_mount_property = batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n        mount_path="mountPath",\n        name="name",\n        read_only=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['mount_path', 'name', 'read_only']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksContainerVolumeMountProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksPropertiesProperty
class CfnJobDefinition_EksPropertiesPropertyDef(BaseStruct):
    pod_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_PodPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The properties for the Kubernetes pod resources of a job.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-eksproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # labels: Any\n    # limits: Any\n    # requests: Any\n\n    eks_properties_property = batch.CfnJobDefinition.EksPropertiesProperty(\n        pod_properties=batch.CfnJobDefinition.PodPropertiesProperty(\n            containers=[batch.CfnJobDefinition.EksContainerProperty(\n                image="image",\n\n                # the properties below are optional\n                args=["args"],\n                command=["command"],\n                env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n                    name="name",\n\n                    # the properties below are optional\n                    value="value"\n                )],\n                image_pull_policy="imagePullPolicy",\n                name="name",\n                resources=batch.CfnJobDefinition.ResourcesProperty(\n                    limits=limits,\n                    requests=requests\n                ),\n                security_context=batch.CfnJobDefinition.SecurityContextProperty(\n                    privileged=False,\n                    read_only_root_filesystem=False,\n                    run_as_group=123,\n                    run_as_non_root=False,\n                    run_as_user=123\n                ),\n                volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n                    mount_path="mountPath",\n                    name="name",\n                    read_only=False\n                )]\n            )],\n            dns_policy="dnsPolicy",\n            host_network=False,\n            metadata=batch.CfnJobDefinition.MetadataProperty(\n                labels=labels\n            ),\n            service_account_name="serviceAccountName",\n            volumes=[batch.CfnJobDefinition.EksVolumeProperty(\n                name="name",\n\n                # the properties below are optional\n                empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n                    medium="medium",\n                    size_limit="sizeLimit"\n                ),\n                host_path=batch.CfnJobDefinition.HostPathProperty(\n                    path="path"\n                ),\n                secret=batch.CfnJobDefinition.EksSecretProperty(\n                    secret_name="secretName",\n\n                    # the properties below are optional\n                    optional=False\n                )\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['pod_properties']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksPropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksSecretProperty
class CfnJobDefinition_EksSecretPropertyDef(BaseStruct):
    secret_name: str = pydantic.Field(..., description='``CfnJobDefinition.EksSecretProperty.SecretName``.')
    optional: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='``CfnJobDefinition.EksSecretProperty.Optional``.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekssecret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_secret_property = batch.CfnJobDefinition.EksSecretProperty(\n        secret_name="secretName",\n\n        # the properties below are optional\n        optional=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['secret_name', 'optional']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksSecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EksVolumeProperty
class CfnJobDefinition_EksVolumePropertyDef(BaseStruct):
    name: str = pydantic.Field(..., description='The name of the volume. The name must be allowed as a DNS subdomain name. For more information, see `DNS subdomain names <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names>`_ in the *Kubernetes documentation* .\n')
    empty_dir: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EmptyDirPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the configuration of a Kubernetes ``emptyDir`` volume. For more information, see `emptyDir <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/#emptydir>`_ in the *Kubernetes documentation* .\n')
    host_path: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_HostPathPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the configuration of a Kubernetes ``hostPath`` volume. For more information, see `hostPath <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/#hostpath>`_ in the *Kubernetes documentation* .\n')
    secret: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the configuration of a Kubernetes ``secret`` volume. For more information, see `secret <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/storage/volumes/#secret>`_ in the *Kubernetes documentation* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-eksvolume.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    eks_volume_property = batch.CfnJobDefinition.EksVolumeProperty(\n        name="name",\n\n        # the properties below are optional\n        empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n            medium="medium",\n            size_limit="sizeLimit"\n        ),\n        host_path=batch.CfnJobDefinition.HostPathProperty(\n            path="path"\n        ),\n        secret=batch.CfnJobDefinition.EksSecretProperty(\n            secret_name="secretName",\n\n            # the properties below are optional\n            optional=False\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'empty_dir', 'host_path', 'secret']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EksVolumeProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EmptyDirProperty
class CfnJobDefinition_EmptyDirPropertyDef(BaseStruct):
    medium: typing.Optional[str] = pydantic.Field(None, description='``CfnJobDefinition.EmptyDirProperty.Medium``.')
    size_limit: typing.Optional[str] = pydantic.Field(None, description='``CfnJobDefinition.EmptyDirProperty.SizeLimit``.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-eksemptydir.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    empty_dir_property = batch.CfnJobDefinition.EmptyDirProperty(\n        medium="medium",\n        size_limit="sizeLimit"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['medium', 'size_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EmptyDirProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EnvironmentProperty
class CfnJobDefinition_EnvironmentPropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the environment variable.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value of the environment variable.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-environment.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    environment_property = batch.CfnJobDefinition.EnvironmentProperty(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EnvironmentProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EphemeralStorageProperty
class CfnJobDefinition_EphemeralStoragePropertyDef(BaseStruct):
    size_in_gib: typing.Union[int, float] = pydantic.Field(..., description='``CfnJobDefinition.EphemeralStorageProperty.SizeInGiB``.')
    _init_params: typing.ClassVar[list[str]] = ['size_in_gib']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EphemeralStorageProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.EvaluateOnExitProperty
class CfnJobDefinition_EvaluateOnExitPropertyDef(BaseStruct):
    action: str = pydantic.Field(..., description="Specifies the action to take if all of the specified conditions ( ``onStatusReason`` , ``onReason`` , and ``onExitCode`` ) are met. The values aren't case sensitive.\n")
    on_exit_code: typing.Optional[str] = pydantic.Field(None, description='Contains a glob pattern to match against the decimal representation of the ``ExitCode`` returned for a job. The pattern can be up to 512 characters long. It can contain only numbers, and can end with an asterisk (*) so that only the start of the string needs to be an exact match. The string can contain up to 512 characters.\n')
    on_reason: typing.Optional[str] = pydantic.Field(None, description='Contains a glob pattern to match against the ``Reason`` returned for a job. The pattern can contain up to 512 characters. It can contain letters, numbers, periods (.), colons (:), and white space (including spaces and tabs). It can optionally end with an asterisk (*) so that only the start of the string needs to be an exact match.\n')
    on_status_reason: typing.Optional[str] = pydantic.Field(None, description='Contains a glob pattern to match against the ``StatusReason`` returned for a job. The pattern can contain up to 512 characters. It can contain letters, numbers, periods (.), colons (:), and white spaces (including spaces or tabs). It can optionally end with an asterisk (*) so that only the start of the string needs to be an exact match.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-evaluateonexit.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    evaluate_on_exit_property = batch.CfnJobDefinition.EvaluateOnExitProperty(\n        action="action",\n\n        # the properties below are optional\n        on_exit_code="onExitCode",\n        on_reason="onReason",\n        on_status_reason="onStatusReason"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'on_exit_code', 'on_reason', 'on_status_reason']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.EvaluateOnExitProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.FargatePlatformConfigurationProperty
class CfnJobDefinition_FargatePlatformConfigurationPropertyDef(BaseStruct):
    platform_version: typing.Optional[str] = pydantic.Field(None, description='The AWS Fargate platform version where the jobs are running. A platform version is specified only for jobs that are running on Fargate resources. If one isn\'t specified, the ``LATEST`` platform version is used by default. This uses a recent, approved version of the AWS Fargate platform for compute resources. For more information, see `AWS Fargate platform versions <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html>`_ in the *Amazon Elastic Container Service Developer Guide* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-containerproperties-fargateplatformconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    fargate_platform_configuration_property = batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n        platform_version="platformVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['platform_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.FargatePlatformConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.HostPathProperty
class CfnJobDefinition_HostPathPropertyDef(BaseStruct):
    path: typing.Optional[str] = pydantic.Field(None, description='``CfnJobDefinition.HostPathProperty.Path``.')
    _init_params: typing.ClassVar[list[str]] = ['path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.HostPathProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.LinuxParametersProperty
class CfnJobDefinition_LinuxParametersPropertyDef(BaseStruct):
    devices: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_DevicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Any of the host devices to expose to the container. This parameter maps to ``Devices`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--device`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    init_process_enabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='If true, run an ``init`` process inside the container that forwards signals and reaps processes. This parameter maps to the ``--init`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . This parameter requires version 1.25 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"``\n')
    max_swap: typing.Union[int, float, None] = pydantic.Field(None, description="The total amount of swap memory (in MiB) a container can use. This parameter is translated to the ``--memory-swap`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ where the value is the sum of the container memory plus the ``maxSwap`` value. For more information, see ```--memory-swap`` details <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/resource_constraints/#--memory-swap-details>`_ in the Docker documentation. If a ``maxSwap`` value of ``0`` is specified, the container doesn't use swap. Accepted values are ``0`` or any positive integer. If the ``maxSwap`` parameter is omitted, the container doesn't use the swap configuration for the container instance that it's running on. A ``maxSwap`` value must be set for the ``swappiness`` parameter to be used. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description="The value for the size (in MiB) of the ``/dev/shm`` volume. This parameter maps to the ``--shm-size`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description="You can use this parameter to tune a container's memory swappiness behavior. A ``swappiness`` value of ``0`` causes swapping to not occur unless absolutely necessary. A ``swappiness`` value of ``100`` causes pages to be swapped aggressively. Valid values are whole numbers between ``0`` and ``100`` . If the ``swappiness`` parameter isn't specified, a default value of ``60`` is used. If a value isn't specified for ``maxSwap`` , then this parameter is ignored. If ``maxSwap`` is set to 0, the container doesn't use swap. This parameter maps to the ``--memory-swappiness`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . Consider the following when you use a per-container swap configuration. - Swap space must be enabled and allocated on the container instance for the containers to use. .. epigraph:: By default, the Amazon ECS optimized AMIs don't have swap enabled. You must enable swap on the instance to use this feature. For more information, see `Instance store swap volumes <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-store-swap-volumes.html>`_ in the *Amazon EC2 User Guide for Linux Instances* or `How do I allocate memory to work as swap space in an Amazon EC2 instance by using a swap file? <https://docs.aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/>`_ - The swap space parameters are only supported for job definitions using EC2 resources. - If the ``maxSwap`` and ``swappiness`` parameters are omitted from a job definition, each container has a default ``swappiness`` value of 60. Moreover, the total swap usage is limited to two times the memory reservation of the container. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources. Don't provide it for these jobs.\n")
    tmpfs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_TmpfsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The container path, mount options, and size (in MiB) of the ``tmpfs`` mount. This parameter maps to the ``--tmpfs`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: This parameter isn\'t applicable to jobs that are running on Fargate resources. Don\'t provide this parameter for this resource type.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-containerproperties-linuxparameters.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    linux_parameters_property = batch.CfnJobDefinition.LinuxParametersProperty(\n        devices=[batch.CfnJobDefinition.DeviceProperty(\n            container_path="containerPath",\n            host_path="hostPath",\n            permissions=["permissions"]\n        )],\n        init_process_enabled=False,\n        max_swap=123,\n        shared_memory_size=123,\n        swappiness=123,\n        tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n            container_path="containerPath",\n            size=123,\n\n            # the properties below are optional\n            mount_options=["mountOptions"]\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['devices', 'init_process_enabled', 'max_swap', 'shared_memory_size', 'swappiness', 'tmpfs']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.LinuxParametersProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.LogConfigurationProperty
class CfnJobDefinition_LogConfigurationPropertyDef(BaseStruct):
    log_driver: str = pydantic.Field(..., description='The log driver to use for the container. The valid values that are listed for this parameter are log drivers that the Amazon ECS container agent can communicate with by default. The supported log drivers are ``awslogs`` , ``fluentd`` , ``gelf`` , ``json-file`` , ``journald`` , ``logentries`` , ``syslog`` , and ``splunk`` . .. epigraph:: Jobs that are running on Fargate resources are restricted to the ``awslogs`` and ``splunk`` log drivers. - **awslogs** - Specifies the Amazon CloudWatch Logs logging driver. For more information, see `Using the awslogs log driver <https://docs.aws.amazon.com/batch/latest/userguide/using_awslogs.html>`_ in the *AWS Batch User Guide* and `Amazon CloudWatch Logs logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/awslogs/>`_ in the Docker documentation. - **fluentd** - Specifies the Fluentd logging driver. For more information including usage and options, see `Fluentd logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/fluentd/>`_ in the *Docker documentation* . - **gelf** - Specifies the Graylog Extended Format (GELF) logging driver. For more information including usage and options, see `Graylog Extended Format logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/gelf/>`_ in the *Docker documentation* . - **journald** - Specifies the journald logging driver. For more information including usage and options, see `Journald logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/journald/>`_ in the *Docker documentation* . - **json-file** - Specifies the JSON file logging driver. For more information including usage and options, see `JSON File logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/json-file/>`_ in the *Docker documentation* . - **splunk** - Specifies the Splunk logging driver. For more information including usage and options, see `Splunk logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/splunk/>`_ in the *Docker documentation* . - **syslog** - Specifies the syslog logging driver. For more information including usage and options, see `Syslog logging driver <https://docs.aws.amazon.com/https://docs.docker.com/config/containers/logging/syslog/>`_ in the *Docker documentation* . .. epigraph:: If you have a custom driver that\'s not listed earlier that you want to work with the Amazon ECS container agent, you can fork the Amazon ECS container agent project that\'s `available on GitHub <https://docs.aws.amazon.com/https://github.com/aws/amazon-ecs-agent>`_ and customize it to work with that driver. We encourage you to submit pull requests for changes that you want to have included. However, Amazon Web Services doesn\'t currently support running modified copies of this software. This parameter requires version 1.18 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"``\n')
    options: typing.Any = pydantic.Field(None, description='The configuration options to send to the log driver. This parameter requires version 1.19 of the Docker Remote API or greater on your container instance. To check the Docker Remote API version on your container instance, log in to your container instance and run the following command: ``sudo docker version | grep "Server API version"``\n')
    secret_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The secrets to pass to the log configuration. For more information, see `Specifying sensitive data <https://docs.aws.amazon.com/batch/latest/userguide/specifying-sensitive-data.html>`_ in the *AWS Batch User Guide* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-containerproperties-logconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    log_configuration_property = batch.CfnJobDefinition.LogConfigurationProperty(\n        log_driver="logDriver",\n\n        # the properties below are optional\n        options=options,\n        secret_options=[batch.CfnJobDefinition.SecretProperty(\n            name="name",\n            value_from="valueFrom"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_driver', 'options', 'secret_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.LogConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.MetadataProperty
class CfnJobDefinition_MetadataPropertyDef(BaseStruct):
    labels: typing.Any = pydantic.Field(None, description='``CfnJobDefinition.MetadataProperty.Labels``.')
    _init_params: typing.ClassVar[list[str]] = ['labels']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.MetadataProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.MountPointsProperty
class CfnJobDefinition_MountPointsPropertyDef(BaseStruct):
    container_path: typing.Optional[str] = pydantic.Field(None, description='The path on the container where the host volume is mounted.\n')
    read_only: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='If this value is ``true`` , the container has read-only access to the volume. Otherwise, the container can write to the volume. The default value is ``false`` .\n')
    source_volume: typing.Optional[str] = pydantic.Field(None, description='The name of the volume to mount.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-mountpoints.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    mount_points_property = batch.CfnJobDefinition.MountPointsProperty(\n        container_path="containerPath",\n        read_only=False,\n        source_volume="sourceVolume"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'read_only', 'source_volume']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.MountPointsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.NetworkConfigurationProperty
class CfnJobDefinition_NetworkConfigurationPropertyDef(BaseStruct):
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description='Indicates whether the job has a public IP address. For a job that\'s running on Fargate resources in a private subnet to send outbound traffic to the internet (for example, to pull container images), the private subnet requires a NAT gateway be attached to route requests to the internet. For more information, see `Amazon ECS task networking <https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html>`_ in the *Amazon Elastic Container Service Developer Guide* . The default value is " ``DISABLED`` ".\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-containerproperties-networkconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    network_configuration_property = batch.CfnJobDefinition.NetworkConfigurationProperty(\n        assign_public_ip="assignPublicIp"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['assign_public_ip']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.NetworkConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.NodePropertiesProperty
class CfnJobDefinition_NodePropertiesPropertyDef(BaseStruct):
    main_node: typing.Union[int, float] = pydantic.Field(..., description='Specifies the node index for the main node of a multi-node parallel job. This node index value must be fewer than the number of nodes.\n')
    node_range_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_NodeRangePropertyPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='A list of node ranges and their properties that are associated with a multi-node parallel job.\n')
    num_nodes: typing.Union[int, float] = pydantic.Field(..., description='The number of nodes that are associated with a multi-node parallel job.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-nodeproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    node_properties_property = batch.CfnJobDefinition.NodePropertiesProperty(\n        main_node=123,\n        node_range_properties=[batch.CfnJobDefinition.NodeRangePropertyProperty(\n            target_nodes="targetNodes",\n\n            # the properties below are optional\n            container=batch.CfnJobDefinition.ContainerPropertiesProperty(\n                image="image",\n\n                # the properties below are optional\n                command=["command"],\n                environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                    name="name",\n                    value="value"\n                )],\n                ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                    size_in_gi_b=123\n                ),\n                execution_role_arn="executionRoleArn",\n                fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                    platform_version="platformVersion"\n                ),\n                instance_type="instanceType",\n                job_role_arn="jobRoleArn",\n                linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                    devices=[batch.CfnJobDefinition.DeviceProperty(\n                        container_path="containerPath",\n                        host_path="hostPath",\n                        permissions=["permissions"]\n                    )],\n                    init_process_enabled=False,\n                    max_swap=123,\n                    shared_memory_size=123,\n                    swappiness=123,\n                    tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                        container_path="containerPath",\n                        size=123,\n\n                        # the properties below are optional\n                        mount_options=["mountOptions"]\n                    )]\n                ),\n                log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                    log_driver="logDriver",\n\n                    # the properties below are optional\n                    options=options,\n                    secret_options=[batch.CfnJobDefinition.SecretProperty(\n                        name="name",\n                        value_from="valueFrom"\n                    )]\n                ),\n                memory=123,\n                mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                    container_path="containerPath",\n                    read_only=False,\n                    source_volume="sourceVolume"\n                )],\n                network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                    assign_public_ip="assignPublicIp"\n                ),\n                privileged=False,\n                readonly_root_filesystem=False,\n                resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                    type="type",\n                    value="value"\n                )],\n                secrets=[batch.CfnJobDefinition.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )],\n                ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                    hard_limit=123,\n                    name="name",\n                    soft_limit=123\n                )],\n                user="user",\n                vcpus=123,\n                volumes=[batch.CfnJobDefinition.VolumesProperty(\n                    efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                        file_system_id="fileSystemId",\n\n                        # the properties below are optional\n                        authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                            access_point_id="accessPointId",\n                            iam="iam"\n                        ),\n                        root_directory="rootDirectory",\n                        transit_encryption="transitEncryption",\n                        transit_encryption_port=123\n                    ),\n                    host=batch.CfnJobDefinition.VolumesHostProperty(\n                        source_path="sourcePath"\n                    ),\n                    name="name"\n                )]\n            )\n        )],\n        num_nodes=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['main_node', 'node_range_properties', 'num_nodes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.NodePropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.NodeRangePropertyProperty
class CfnJobDefinition_NodeRangePropertyPropertyDef(BaseStruct):
    target_nodes: str = pydantic.Field(..., description='The range of nodes, using node index values. A range of ``0:3`` indicates nodes with index values of ``0`` through ``3`` . If the starting range value is omitted ( ``:n`` ), then ``0`` is used to start the range. If the ending range value is omitted ( ``n:`` ), then the highest possible node index is used to end the range. Your accumulative node ranges must account for all nodes ( ``0:n`` ). You can nest node ranges (for example, ``0:10`` and ``4:5`` ). In this case, the ``4:5`` range properties override the ``0:10`` properties.\n')
    container: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The container details for the node range.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-noderangeproperty.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # options: Any\n\n    node_range_property_property = batch.CfnJobDefinition.NodeRangePropertyProperty(\n        target_nodes="targetNodes",\n\n        # the properties below are optional\n        container=batch.CfnJobDefinition.ContainerPropertiesProperty(\n            image="image",\n\n            # the properties below are optional\n            command=["command"],\n            environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                name="name",\n                value="value"\n            )],\n            ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                size_in_gi_b=123\n            ),\n            execution_role_arn="executionRoleArn",\n            fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                platform_version="platformVersion"\n            ),\n            instance_type="instanceType",\n            job_role_arn="jobRoleArn",\n            linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                devices=[batch.CfnJobDefinition.DeviceProperty(\n                    container_path="containerPath",\n                    host_path="hostPath",\n                    permissions=["permissions"]\n                )],\n                init_process_enabled=False,\n                max_swap=123,\n                shared_memory_size=123,\n                swappiness=123,\n                tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                    container_path="containerPath",\n                    size=123,\n\n                    # the properties below are optional\n                    mount_options=["mountOptions"]\n                )]\n            ),\n            log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                log_driver="logDriver",\n\n                # the properties below are optional\n                options=options,\n                secret_options=[batch.CfnJobDefinition.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )]\n            ),\n            memory=123,\n            mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                container_path="containerPath",\n                read_only=False,\n                source_volume="sourceVolume"\n            )],\n            network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                assign_public_ip="assignPublicIp"\n            ),\n            privileged=False,\n            readonly_root_filesystem=False,\n            resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                type="type",\n                value="value"\n            )],\n            secrets=[batch.CfnJobDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )],\n            ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                hard_limit=123,\n                name="name",\n                soft_limit=123\n            )],\n            user="user",\n            vcpus=123,\n            volumes=[batch.CfnJobDefinition.VolumesProperty(\n                efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                    file_system_id="fileSystemId",\n\n                    # the properties below are optional\n                    authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                        access_point_id="accessPointId",\n                        iam="iam"\n                    ),\n                    root_directory="rootDirectory",\n                    transit_encryption="transitEncryption",\n                    transit_encryption_port=123\n                ),\n                host=batch.CfnJobDefinition.VolumesHostProperty(\n                    source_path="sourcePath"\n                ),\n                name="name"\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['target_nodes', 'container']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.NodeRangePropertyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.PodPropertiesProperty
class CfnJobDefinition_PodPropertiesPropertyDef(BaseStruct):
    containers: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksContainerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The properties of the container that's used on the Amazon EKS pod.\n")
    dns_policy: typing.Optional[str] = pydantic.Field(None, description="The DNS policy for the pod. The default value is ``ClusterFirst`` . If the ``hostNetwork`` parameter is not specified, the default is ``ClusterFirstWithHostNet`` . ``ClusterFirst`` indicates that any DNS query that does not match the configured cluster domain suffix is forwarded to the upstream nameserver inherited from the node. If no value was specified for ``dnsPolicy`` in the `RegisterJobDefinition <https://docs.aws.amazon.com/batch/latest/APIReference/API_RegisterJobDefinition.html>`_ API operation, then no value will be returned for ``dnsPolicy`` by either of `DescribeJobDefinitions <https://docs.aws.amazon.com/batch/latest/APIReference/API_DescribeJobDefinitions.html>`_ or `DescribeJobs <https://docs.aws.amazon.com/batch/latest/APIReference/API_DescribeJobs.html>`_ API operations. The pod spec setting will contain either ``ClusterFirst`` or ``ClusterFirstWithHostNet`` , depending on the value of the ``hostNetwork`` parameter. For more information, see `Pod's DNS policy <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy>`_ in the *Kubernetes documentation* . Valid values: ``Default`` | ``ClusterFirst`` | ``ClusterFirstWithHostNet``\n")
    host_network: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Indicates if the pod uses the hosts' network IP address. The default value is ``true`` . Setting this to ``false`` enables the Kubernetes pod networking model. Most AWS Batch workloads are egress-only and don't require the overhead of IP allocation for each pod for incoming connections. For more information, see `Host namespaces <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/security/pod-security-policy/#host-namespaces>`_ and `Pod networking <https://docs.aws.amazon.com/https://kubernetes.io/docs/concepts/workloads/pods/#pod-networking>`_ in the *Kubernetes documentation* .\n")
    metadata: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_MetadataPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='``CfnJobDefinition.PodPropertiesProperty.Metadata``.\n')
    service_account_name: typing.Optional[str] = pydantic.Field(None, description="The name of the service account that's used to run the pod. For more information, see `Kubernetes service accounts <https://docs.aws.amazon.com/eks/latest/userguide/service-accounts.html>`_ and `Configure a Kubernetes service account to assume an IAM role <https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html>`_ in the *Amazon EKS User Guide* and `Configure service accounts for pods <https://docs.aws.amazon.com/https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/>`_ in the *Kubernetes documentation* .\n")
    volumes: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksVolumePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the volumes for a job definition that uses Amazon EKS resources.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-podproperties.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # labels: Any\n    # limits: Any\n    # requests: Any\n\n    pod_properties_property = batch.CfnJobDefinition.PodPropertiesProperty(\n        containers=[batch.CfnJobDefinition.EksContainerProperty(\n            image="image",\n\n            # the properties below are optional\n            args=["args"],\n            command=["command"],\n            env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n                name="name",\n\n                # the properties below are optional\n                value="value"\n            )],\n            image_pull_policy="imagePullPolicy",\n            name="name",\n            resources=batch.CfnJobDefinition.ResourcesProperty(\n                limits=limits,\n                requests=requests\n            ),\n            security_context=batch.CfnJobDefinition.SecurityContextProperty(\n                privileged=False,\n                read_only_root_filesystem=False,\n                run_as_group=123,\n                run_as_non_root=False,\n                run_as_user=123\n            ),\n            volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n                mount_path="mountPath",\n                name="name",\n                read_only=False\n            )]\n        )],\n        dns_policy="dnsPolicy",\n        host_network=False,\n        metadata=batch.CfnJobDefinition.MetadataProperty(\n            labels=labels\n        ),\n        service_account_name="serviceAccountName",\n        volumes=[batch.CfnJobDefinition.EksVolumeProperty(\n            name="name",\n\n            # the properties below are optional\n            empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n                medium="medium",\n                size_limit="sizeLimit"\n            ),\n            host_path=batch.CfnJobDefinition.HostPathProperty(\n                path="path"\n            ),\n            secret=batch.CfnJobDefinition.EksSecretProperty(\n                secret_name="secretName",\n\n                # the properties below are optional\n                optional=False\n            )\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['containers', 'dns_policy', 'host_network', 'metadata', 'service_account_name', 'volumes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.PodPropertiesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.ResourceRequirementProperty
class CfnJobDefinition_ResourceRequirementPropertyDef(BaseStruct):
    type: typing.Optional[str] = pydantic.Field(None, description='The type of resource to assign to a container. The supported resources include ``GPU`` , ``MEMORY`` , and ``VCPU`` .\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The quantity of the specified resource to reserve for the container. The values vary based on the ``type`` specified. - **type="GPU"** - The number of physical GPUs to reserve for the container. Make sure that the number of GPUs reserved for all containers in a job doesn\'t exceed the number of available GPUs on the compute resource that the job is launched on. .. epigraph:: GPUs aren\'t available for jobs that are running on Fargate resources. - **type="MEMORY"** - The memory hard limit (in MiB) present to the container. This parameter is supported for jobs that are running on EC2 resources. If your container attempts to exceed the memory specified, the container is terminated. This parameter maps to ``Memory`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--memory`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . You must specify at least 4 MiB of memory for a job. This is required but can be specified in several places for multi-node parallel (MNP) jobs. It must be specified for each node at least once. This parameter maps to ``Memory`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--memory`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . .. epigraph:: If you\'re trying to maximize your resource utilization by providing your jobs as much memory as possible for a particular instance type, see `Memory management <https://docs.aws.amazon.com/batch/latest/userguide/memory-management.html>`_ in the *AWS Batch User Guide* . For jobs that are running on Fargate resources, then ``value`` is the hard limit (in MiB), and must match one of the supported values and the ``VCPU`` values must be one of the values supported for that memory value. - **value = 512** - ``VCPU`` = 0.25 - **value = 1024** - ``VCPU`` = 0.25 or 0.5 - **value = 2048** - ``VCPU`` = 0.25, 0.5, or 1 - **value = 3072** - ``VCPU`` = 0.5, or 1 - **value = 4096** - ``VCPU`` = 0.5, 1, or 2 - **value = 5120, 6144, or 7168** - ``VCPU`` = 1 or 2 - **value = 8192** - ``VCPU`` = 1, 2, or 4 - **value = 9216, 10240, 11264, 12288, 13312, 14336, or 15360** - ``VCPU`` = 2 or 4 - **value = 16384** - ``VCPU`` = 2, 4, or 8 - **value = 17408, 18432, 19456, 21504, 22528, 23552, 25600, 26624, 27648, 29696, or 30720** - ``VCPU`` = 4 - **value = 20480, 24576, or 28672** - ``VCPU`` = 4 or 8 - **value = 36864, 45056, 53248, or 61440** - ``VCPU`` = 8 - **value = 32768, 40960, 49152, or 57344** - ``VCPU`` = 8 or 16 - **value = 65536, 73728, 81920, 90112, 98304, 106496, 114688, or 122880** - ``VCPU`` = 16 - **type="VCPU"** - The number of vCPUs reserved for the container. This parameter maps to ``CpuShares`` in the `Create a container <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/#create-a-container>`_ section of the `Docker Remote API <https://docs.aws.amazon.com/https://docs.docker.com/engine/api/v1.23/>`_ and the ``--cpu-shares`` option to `docker run <https://docs.aws.amazon.com/https://docs.docker.com/engine/reference/run/>`_ . Each vCPU is equivalent to 1,024 CPU shares. For EC2 resources, you must specify at least one vCPU. This is required but can be specified in several places; it must be specified for each node at least once. The default for the Fargate On-Demand vCPU resource count quota is 6 vCPUs. For more information about Fargate quotas, see `AWS Fargate quotas <https://docs.aws.amazon.com/general/latest/gr/ecs-service.html#service-quotas-fargate>`_ in the *AWS General Reference* . For jobs that are running on Fargate resources, then ``value`` must match one of the supported values and the ``MEMORY`` values must be one of the values supported for that ``VCPU`` value. The supported values are 0.25, 0.5, 1, 2, 4, 8, and 16 - **value = 0.25** - ``MEMORY`` = 512, 1024, or 2048 - **value = 0.5** - ``MEMORY`` = 1024, 2048, 3072, or 4096 - **value = 1** - ``MEMORY`` = 2048, 3072, 4096, 5120, 6144, 7168, or 8192 - **value = 2** - ``MEMORY`` = 4096, 5120, 6144, 7168, 8192, 9216, 10240, 11264, 12288, 13312, 14336, 15360, or 16384 - **value = 4** - ``MEMORY`` = 8192, 9216, 10240, 11264, 12288, 13312, 14336, 15360, 16384, 17408, 18432, 19456, 20480, 21504, 22528, 23552, 24576, 25600, 26624, 27648, 28672, 29696, or 30720 - **value = 8** - ``MEMORY`` = 16384, 20480, 24576, 28672, 32768, 36864, 40960, 45056, 49152, 53248, 57344, or 61440 - **value = 16** - ``MEMORY`` = 32768, 40960, 49152, 57344, 65536, 73728, 81920, 90112, 98304, 106496, 114688, or 122880\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-resourcerequirement.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    resource_requirement_property = batch.CfnJobDefinition.ResourceRequirementProperty(\n        type="type",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.ResourceRequirementProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.ResourcesProperty
class CfnJobDefinition_ResourcesPropertyDef(BaseStruct):
    limits: typing.Any = pydantic.Field(None, description='``CfnJobDefinition.ResourcesProperty.Limits``.')
    requests: typing.Any = pydantic.Field(None, description='``CfnJobDefinition.ResourcesProperty.Requests``.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainerresourcerequirements.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # limits: Any\n    # requests: Any\n\n    resources_property = batch.CfnJobDefinition.ResourcesProperty(\n        limits=limits,\n        requests=requests\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['limits', 'requests']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.ResourcesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.RetryStrategyProperty
class CfnJobDefinition_RetryStrategyPropertyDef(BaseStruct):
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times to move a job to the ``RUNNABLE`` status. You can specify between 1 and 10 attempts. If the value of ``attempts`` is greater than one, the job is retried on failure the same number of attempts as the value.\n')
    evaluate_on_exit: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EvaluateOnExitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Array of up to 5 objects that specify the conditions where jobs are retried or failed. If this parameter is specified, then the ``attempts`` parameter must also be specified. If none of the listed conditions match, then the job is retried.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-retrystrategy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    retry_strategy_property = batch.CfnJobDefinition.RetryStrategyProperty(\n        attempts=123,\n        evaluate_on_exit=[batch.CfnJobDefinition.EvaluateOnExitProperty(\n            action="action",\n\n            # the properties below are optional\n            on_exit_code="onExitCode",\n            on_reason="onReason",\n            on_status_reason="onStatusReason"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['attempts', 'evaluate_on_exit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.RetryStrategyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.SecretProperty
class CfnJobDefinition_SecretPropertyDef(BaseStruct):
    name: str = pydantic.Field(..., description='The name of the secret.\n')
    value_from: str = pydantic.Field(..., description='The secret to expose to the container. The supported values are either the full Amazon Resource Name (ARN) of the AWS Secrets Manager secret or the full ARN of the parameter in the AWS Systems Manager Parameter Store. .. epigraph:: If the AWS Systems Manager Parameter Store parameter exists in the same Region as the job you\'re launching, then you can use either the full Amazon Resource Name (ARN) or name of the parameter. If the parameter exists in a different Region, then the full ARN must be specified.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-secret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    secret_property = batch.CfnJobDefinition.SecretProperty(\n        name="name",\n        value_from="valueFrom"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value_from']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.SecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.SecurityContextProperty
class CfnJobDefinition_SecurityContextPropertyDef(BaseStruct):
    privileged: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='``CfnJobDefinition.SecurityContextProperty.Privileged``.')
    read_only_root_filesystem: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='``CfnJobDefinition.SecurityContextProperty.ReadOnlyRootFilesystem``.\n')
    run_as_group: typing.Union[int, float, None] = pydantic.Field(None, description='``CfnJobDefinition.SecurityContextProperty.RunAsGroup``.\n')
    run_as_non_root: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='``CfnJobDefinition.SecurityContextProperty.RunAsNonRoot``.\n')
    run_as_user: typing.Union[int, float, None] = pydantic.Field(None, description='``CfnJobDefinition.SecurityContextProperty.RunAsUser``.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ekscontainersecuritycontext.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    security_context_property = batch.CfnJobDefinition.SecurityContextProperty(\n        privileged=False,\n        read_only_root_filesystem=False,\n        run_as_group=123,\n        run_as_non_root=False,\n        run_as_user=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['privileged', 'read_only_root_filesystem', 'run_as_group', 'run_as_non_root', 'run_as_user']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.SecurityContextProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.TimeoutProperty
class CfnJobDefinition_TimeoutPropertyDef(BaseStruct):
    attempt_duration_seconds: typing.Union[int, float, None] = pydantic.Field(None, description="The job timeout time (in seconds) that's measured from the job attempt's ``startedAt`` timestamp. After this time passes, AWS Batch terminates your jobs if they aren't finished. The minimum value for the timeout is 60 seconds. For array jobs, the timeout applies to the child jobs, not to the parent array job. For multi-node parallel (MNP) jobs, the timeout applies to the whole job, not to the individual nodes.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-timeout.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    timeout_property = batch.CfnJobDefinition.TimeoutProperty(\n        attempt_duration_seconds=123\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['attempt_duration_seconds']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.TimeoutProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.TmpfsProperty
class CfnJobDefinition_TmpfsPropertyDef(BaseStruct):
    container_path: str = pydantic.Field(..., description='The absolute file path in the container where the ``tmpfs`` volume is mounted.\n')
    size: typing.Union[int, float] = pydantic.Field(..., description='The size (in MiB) of the ``tmpfs`` volume.\n')
    mount_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of ``tmpfs`` volume mount options. Valid values: " ``defaults`` " | " ``ro`` " | " ``rw`` " | " ``suid`` " | " ``nosuid`` " | " ``dev`` " | " ``nodev`` " | " ``exec`` " | " ``noexec`` " | " ``sync`` " | " ``async`` " | " ``dirsync`` " | " ``remount`` " | " ``mand`` " | " ``nomand`` " | " ``atime`` " | " ``noatime`` " | " ``diratime`` " | " ``nodiratime`` " | " ``bind`` " | " ``rbind" | "unbindable" | "runbindable" | "private" | "rprivate" | "shared" | "rshared" | "slave" | "rslave" | "relatime`` " | " ``norelatime`` " | " ``strictatime`` " | " ``nostrictatime`` " | " ``mode`` " | " ``uid`` " | " ``gid`` " | " ``nr_inodes`` " | " ``nr_blocks`` " | " ``mpol`` "\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-tmpfs.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    tmpfs_property = batch.CfnJobDefinition.TmpfsProperty(\n        container_path="containerPath",\n        size=123,\n\n        # the properties below are optional\n        mount_options=["mountOptions"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['container_path', 'size', 'mount_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.TmpfsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.UlimitProperty
class CfnJobDefinition_UlimitPropertyDef(BaseStruct):
    hard_limit: typing.Union[int, float] = pydantic.Field(..., description='The hard limit for the ``ulimit`` type.\n')
    name: str = pydantic.Field(..., description='The ``type`` of the ``ulimit`` .\n')
    soft_limit: typing.Union[int, float] = pydantic.Field(..., description='The soft limit for the ``ulimit`` type.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-ulimit.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    ulimit_property = batch.CfnJobDefinition.UlimitProperty(\n        hard_limit=123,\n        name="name",\n        soft_limit=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['hard_limit', 'name', 'soft_limit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.UlimitProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.VolumesHostProperty
class CfnJobDefinition_VolumesHostPropertyDef(BaseStruct):
    source_path: typing.Optional[str] = pydantic.Field(None, description='The path on the host container instance that\'s presented to the container. If this parameter is empty, then the Docker daemon has assigned a host path for you. If this parameter contains a file location, then the data volume persists at the specified location on the host container instance until you delete it manually. If the source path location doesn\'t exist on the host container instance, the Docker daemon creates it. If the location does exist, the contents of the source path folder are exported. .. epigraph:: This parameter isn\'t applicable to jobs that run on Fargate resources. Don\'t provide this for these jobs.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-volumeshost.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    volumes_host_property = batch.CfnJobDefinition.VolumesHostProperty(\n        source_path="sourcePath"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['source_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.VolumesHostProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition.VolumesProperty
class CfnJobDefinition_VolumesPropertyDef(BaseStruct):
    efs_volume_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EfsVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="This is used when you're using an Amazon Elastic File System file system for job storage. For more information, see `Amazon EFS Volumes <https://docs.aws.amazon.com/batch/latest/userguide/efs-volumes.html>`_ in the *AWS Batch User Guide* .\n")
    host: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_VolumesHostPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The contents of the ``host`` parameter determine whether your data volume persists on the host container instance and where it's stored. If the host parameter is empty, then the Docker daemon assigns a host path for your data volume. However, the data isn't guaranteed to persist after the containers that are associated with it stop running. .. epigraph:: This parameter isn't applicable to jobs that are running on Fargate resources and shouldn't be provided.\n")
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the volume. It can be up to 255 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_). This name is referenced in the ``sourceVolume`` parameter of container definition ``mountPoints`` .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobdefinition-volumes.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    volumes_property = batch.CfnJobDefinition.VolumesProperty(\n        efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n            file_system_id="fileSystemId",\n\n            # the properties below are optional\n            authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                access_point_id="accessPointId",\n                iam="iam"\n            ),\n            root_directory="rootDirectory",\n            transit_encryption="transitEncryption",\n            transit_encryption_port=123\n        ),\n        host=batch.CfnJobDefinition.VolumesHostProperty(\n            source_path="sourcePath"\n        ),\n        name="name"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['efs_volume_configuration', 'host', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition.VolumesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobQueue.ComputeEnvironmentOrderProperty
class CfnJobQueue_ComputeEnvironmentOrderPropertyDef(BaseStruct):
    compute_environment: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the compute environment.\n')
    order: typing.Union[int, float] = pydantic.Field(..., description='The order of the compute environment. Compute environments are tried in ascending order. For example, if two compute environments are associated with a job queue, the compute environment with a lower ``order`` integer value is tried for job placement first.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-jobqueue-computeenvironmentorder.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    compute_environment_order_property = batch.CfnJobQueue.ComputeEnvironmentOrderProperty(\n        compute_environment="computeEnvironment",\n        order=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment', 'order']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobQueue.ComputeEnvironmentOrderProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicy.FairsharePolicyProperty
class CfnSchedulingPolicy_FairsharePolicyPropertyDef(BaseStruct):
    compute_reservation: typing.Union[int, float, None] = pydantic.Field(None, description="A value used to reserve some of the available maximum vCPU for fair share identifiers that aren't already used. The reserved ratio is ``( *computeReservation* /100)^ *ActiveFairShares*`` where ``*ActiveFairShares*`` is the number of active fair share identifiers. For example, a ``computeReservation`` value of 50 indicates that AWS Batch reserves 50% of the maximum available vCPU if there's only one fair share identifier. It reserves 25% if there are two fair share identifiers. It reserves 12.5% if there are three fair share identifiers. A ``computeReservation`` value of 25 indicates that AWS Batch should reserve 25% of the maximum available vCPU if there's only one fair share identifier, 6.25% if there are two fair share identifiers, and 1.56% if there are three fair share identifiers. The minimum value is 0 and the maximum value is 99.\n")
    share_decay_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time (in seconds) to use to calculate a fair share percentage for each fair share identifier in use. A value of zero (0) indicates that only current usage is measured. The decay allows for more recently run jobs to have more weight than jobs that ran earlier. The maximum supported value is 604800 (1 week).\n')
    share_distribution: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnSchedulingPolicy_ShareAttributesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of ``SharedIdentifier`` objects that contain the weights for the fair share identifiers for the fair share policy. Fair share identifiers that aren\'t included have a default weight of ``1.0`` .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-schedulingpolicy-fairsharepolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    fairshare_policy_property = batch.CfnSchedulingPolicy.FairsharePolicyProperty(\n        compute_reservation=123,\n        share_decay_seconds=123,\n        share_distribution=[batch.CfnSchedulingPolicy.ShareAttributesProperty(\n            share_identifier="shareIdentifier",\n            weight_factor=123\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_reservation', 'share_decay_seconds', 'share_distribution']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicy.FairsharePolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicy.ShareAttributesProperty
class CfnSchedulingPolicy_ShareAttributesPropertyDef(BaseStruct):
    share_identifier: typing.Optional[str] = pydantic.Field(None, description="A fair share identifier or fair share identifier prefix. If the string ends with an asterisk (*), this entry specifies the weight factor to use for fair share identifiers that start with that prefix. The list of fair share identifiers in a fair share policy can't overlap. For example, you can't have one that specifies a ``shareIdentifier`` of ``UserA*`` and another that specifies a ``shareIdentifier`` of ``UserA-1`` . There can be no more than 500 fair share identifiers active in a job queue. The string is limited to 255 alphanumeric characters, and can be followed by an asterisk (*).\n")
    weight_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The weight factor for the fair share identifier. The default value is 1.0. A lower value has a higher priority for compute resources. For example, jobs that use a share identifier with a weight factor of 0.125 (1/8) get 8 times the compute resources of jobs that use a share identifier with a weight factor of 1. The smallest supported value is 0.0001, and the largest supported value is 999.9999.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-schedulingpolicy-shareattributes.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    share_attributes_property = batch.CfnSchedulingPolicy.ShareAttributesProperty(\n        share_identifier="shareIdentifier",\n        weight_factor=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['share_identifier', 'weight_factor']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicy.ShareAttributesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironment
class CfnComputeEnvironmentDef(BaseCfnResource):
    type: str = pydantic.Field(..., description='The type of the compute environment: ``MANAGED`` or ``UNMANAGED`` . For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the *AWS Batch User Guide* .\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name for your compute environment. It can be up to 128 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    compute_resources: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_ComputeResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ComputeResources property type specifies details of the compute resources managed by the compute environment. This parameter is required for managed compute environments. For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the ** .\n')
    eks_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_EksConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The details for the Amazon EKS cluster that supports the compute environment.\n')
    replace_compute_environment: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Specifies whether the compute environment is replaced if an update is made that requires replacing the instances in the compute environment. The default value is ``true`` . To enable more properties to be updated, set this property to ``false`` . When changing the value of this property to ``false`` , do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the ``UPDATE_ROLLBACK_FAILED`` state. You can't update a stack that is in the ``UPDATE_ROLLBACK_FAILED`` state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. For more information, see `Continue rolling back an update <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html>`_ in the *AWS CloudFormation User Guide* . The properties that can't be changed without replacing the compute environment are in the ```ComputeResources`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html>`_ property type: ```AllocationStrategy`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-allocationstrategy>`_ , ```BidPercentage`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-bidpercentage>`_ , ```Ec2Configuration`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2configuration>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```ImageId`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-imageid>`_ , ```InstanceRole`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancerole>`_ , ```InstanceTypes`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancetypes>`_ , ```LaunchTemplate`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-launchtemplate>`_ , ```MaxvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-maxvcpus>`_ , ```MinvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-minvcpus>`_ , ```PlacementGroup`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-placementgroup>`_ , ```SecurityGroupIds`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-securitygroupids>`_ , ```Subnets`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-subnets>`_ , `Tags <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-tags>`_ , ```Type`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-type>`_ , and ```UpdateToLatestImageVersion`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-updatetolatestimageversion>`_ .\n")
    service_role: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that allows AWS Batch to make calls to other AWS services on your behalf. For more information, see `AWS Batch service IAM role <https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If your account already created the AWS Batch service-linked role, that role is used by default for your compute environment unless you specify a different role here. If the AWS Batch service-linked role doesn't exist in your account, and no role is specified here, the service attempts to create the AWS Batch service-linked role in your account. If your specified role has a path other than ``/`` , then you must specify either the full role ARN (recommended) or prefix the role name with the path. For example, if a role with the name ``bar`` has a path of ``/foo/`` , specify ``/foo/bar`` as the role name. For more information, see `Friendly names and paths <https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-friendly-names>`_ in the *IAM User Guide* . .. epigraph:: Depending on how you created your AWS Batch service role, its ARN might contain the ``service-role`` path prefix. When you only specify the name of the service role, AWS Batch assumes that your ARN doesn't use the ``service-role`` path prefix. Because of this, we recommend that you specify the full ARN of your service role when you create compute environments.\n")
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the compute environment. If the state is ``ENABLED`` , then the compute environment accepts jobs from a queue and can scale out automatically based on queues. If the state is ``ENABLED`` , then the AWS Batch scheduler can attempt to place jobs from an associated job queue on the compute resources within the environment. If the compute environment is managed, then it can scale its instances out or in automatically, based on the job queue demand. If the state is ``DISABLED`` , then the AWS Batch scheduler doesn't attempt to place jobs within the environment. Jobs in a ``STARTING`` or ``RUNNING`` state continue to progress normally. Managed compute environments in the ``DISABLED`` state don't scale out. .. epigraph:: Compute environments in a ``DISABLED`` state may continue to incur billing charges. To prevent additional charges, turn off and then delete the compute environment. For more information, see `State <https://docs.aws.amazon.com/batch/latest/userguide/compute_environment_parameters.html#compute_environment_state>`_ in the *AWS Batch User Guide* . When an instance is idle, the instance scales down to the ``minvCpus`` value. However, the instance size doesn't change. For example, consider a ``c5.8xlarge`` instance with a ``minvCpus`` value of ``4`` and a ``desiredvCpus`` value of ``36`` . This instance doesn't scale down to a ``c5.large`` instance.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags applied to the compute environment.\n')
    unmanagedv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of vCPUs for an unmanaged compute environment. This parameter is only used for fair share scheduling to reserve vCPU capacity for new share identifiers. If this parameter isn't provided for a fair share job queue, no vCPU capacity is reserved. .. epigraph:: This parameter is only supported when the ``type`` parameter is set to ``UNMANAGED`` .\n")
    update_policy: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_UpdatePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the infrastructure update policy for the compute environment. For more information about infrastructure updates, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* .')
    _init_params: typing.ClassVar[list[str]] = ['type', 'compute_environment_name', 'compute_resources', 'eks_configuration', 'replace_compute_environment', 'service_role', 'state', 'tags', 'unmanagedv_cpus', 'update_policy']
    _method_names: typing.ClassVar[list[str]] = ['ComputeResourcesProperty', 'Ec2ConfigurationObjectProperty', 'EksConfigurationProperty', 'LaunchTemplateSpecificationProperty', 'UpdatePolicyProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironment'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnComputeEnvironmentDefConfig] = pydantic.Field(None)


class CfnComputeEnvironmentDefConfig(pydantic.BaseModel):
    ComputeResourcesProperty: typing.Optional[list[CfnComputeEnvironmentDefComputeresourcespropertyParams]] = pydantic.Field(None, description='')
    Ec2ConfigurationObjectProperty: typing.Optional[list[CfnComputeEnvironmentDefEc2ConfigurationobjectpropertyParams]] = pydantic.Field(None, description='')
    EksConfigurationProperty: typing.Optional[list[CfnComputeEnvironmentDefEksconfigurationpropertyParams]] = pydantic.Field(None, description='')
    LaunchTemplateSpecificationProperty: typing.Optional[list[CfnComputeEnvironmentDefLaunchtemplatespecificationpropertyParams]] = pydantic.Field(None, description='')
    UpdatePolicyProperty: typing.Optional[list[CfnComputeEnvironmentDefUpdatepolicypropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnComputeEnvironmentDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnComputeEnvironmentDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnComputeEnvironmentDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnComputeEnvironmentDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnComputeEnvironmentDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnComputeEnvironmentDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnComputeEnvironmentDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnComputeEnvironmentDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnComputeEnvironmentDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnComputeEnvironmentDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnComputeEnvironmentDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnComputeEnvironmentDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnComputeEnvironmentDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnComputeEnvironmentDefComputeresourcespropertyParams(pydantic.BaseModel):
    maxv_cpus: typing.Union[int, float] = pydantic.Field(..., description='')
    subnets: typing.Sequence[str] = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    allocation_strategy: typing.Optional[str] = pydantic.Field(None, description='')
    bid_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='')
    desiredv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ec2_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ec2_key_pair: typing.Optional[str] = pydantic.Field(None, description='')
    image_id: typing.Optional[str] = pydantic.Field(None, description='')
    instance_role: typing.Optional[str] = pydantic.Field(None, description='')
    instance_types: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    launch_template: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    minv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description='')
    placement_group: typing.Optional[str] = pydantic.Field(None, description='')
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    spot_iam_fleet_role: typing.Optional[str] = pydantic.Field(None, description='')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='')
    update_to_latest_image_version: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefEc2ConfigurationobjectpropertyParams(pydantic.BaseModel):
    image_type: str = pydantic.Field(..., description='')
    image_id_override: typing.Optional[str] = pydantic.Field(None, description='')
    image_kubernetes_version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefEksconfigurationpropertyParams(pydantic.BaseModel):
    eks_cluster_arn: str = pydantic.Field(..., description='')
    kubernetes_namespace: str = pydantic.Field(..., description='')
    ...

class CfnComputeEnvironmentDefLaunchtemplatespecificationpropertyParams(pydantic.BaseModel):
    launch_template_id: typing.Optional[str] = pydantic.Field(None, description='')
    launch_template_name: typing.Optional[str] = pydantic.Field(None, description='')
    version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefUpdatepolicypropertyParams(pydantic.BaseModel):
    job_execution_timeout_minutes: typing.Union[int, float, None] = pydantic.Field(None, description='')
    terminate_jobs_on_update: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    ...

class CfnComputeEnvironmentDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnComputeEnvironmentDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnComputeEnvironmentDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnComputeEnvironmentDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnComputeEnvironmentDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnComputeEnvironmentDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnComputeEnvironmentDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnComputeEnvironmentDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnComputeEnvironmentDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnComputeEnvironmentDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnComputeEnvironmentDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnComputeEnvironmentDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnComputeEnvironmentDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnComputeEnvironmentDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnJobDefinition
class CfnJobDefinitionDef(BaseCfnResource):
    type: str = pydantic.Field(..., description="The type of job definition. For more information about multi-node parallel jobs, see `Creating a multi-node parallel job definition <https://docs.aws.amazon.com/batch/latest/userguide/multi-node-job-def.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If the job is run on Fargate resources, then ``multinode`` isn't supported.\n")
    container_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties specific to Amazon ECS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    eks_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties that are specific to Amazon EKS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job definition.\n')
    node_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_NodePropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="An object with various properties that are specific to multi-node parallel jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified. .. epigraph:: If the job runs on Fargate resources, don't specify ``nodeProperties`` . Use ``containerProperties`` instead.\n")
    parameters: typing.Any = pydantic.Field(None, description='Default parameters or parameter substitution placeholders that are set in the job definition. Parameters are specified as a key-value pair mapping. Parameters in a ``SubmitJob`` request override any corresponding parameter defaults from the job definition. For more information about specifying parameters, see `Job definition parameters <https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html>`_ in the *AWS Batch User Guide* .\n')
    platform_capabilities: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The platform capabilities required by the job definition. If no value is specified, it defaults to ``EC2`` . Jobs run on Fargate resources specify ``FARGATE`` .\n')
    propagate_tags: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Specifies whether to propagate the tags from the job or job definition to the corresponding Amazon ECS task. If no value is specified, the tags aren't propagated. Tags can only be propagated to the tasks when the tasks are created. For tags with the same name, job tags are given priority over job definitions tags. If the total number of combined tags from the job and job definition is over 50, the job is moved to the ``FAILED`` state.\n")
    retry_strategy: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_RetryStrategyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The retry strategy to use for failed jobs that are submitted with this job definition.\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The scheduling priority of the job definition. This only affects jobs in job queues with a fair share policy. Jobs with a higher scheduling priority are scheduled before jobs with a lower scheduling priority.\n')
    tags: typing.Any = pydantic.Field(None, description='The tags that are applied to the job definition.\n')
    timeout: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_TimeoutPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, AWS Batch terminates your jobs if they aren't finished.")
    _init_params: typing.ClassVar[list[str]] = ['type', 'container_properties', 'eks_properties', 'job_definition_name', 'node_properties', 'parameters', 'platform_capabilities', 'propagate_tags', 'retry_strategy', 'scheduling_priority', 'tags', 'timeout']
    _method_names: typing.ClassVar[list[str]] = ['AuthorizationConfigProperty', 'ContainerPropertiesProperty', 'DeviceProperty', 'EfsVolumeConfigurationProperty', 'EksContainerEnvironmentVariableProperty', 'EksContainerProperty', 'EksContainerVolumeMountProperty', 'EksPropertiesProperty', 'EksSecretProperty', 'EksVolumeProperty', 'EmptyDirProperty', 'EnvironmentProperty', 'EphemeralStorageProperty', 'EvaluateOnExitProperty', 'FargatePlatformConfigurationProperty', 'HostPathProperty', 'LinuxParametersProperty', 'LogConfigurationProperty', 'MetadataProperty', 'MountPointsProperty', 'NetworkConfigurationProperty', 'NodePropertiesProperty', 'NodeRangePropertyProperty', 'PodPropertiesProperty', 'ResourceRequirementProperty', 'ResourcesProperty', 'RetryStrategyProperty', 'SecretProperty', 'SecurityContextProperty', 'TimeoutProperty', 'TmpfsProperty', 'UlimitProperty', 'VolumesHostProperty', 'VolumesProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnJobDefinitionDefConfig] = pydantic.Field(None)


class CfnJobDefinitionDefConfig(pydantic.BaseModel):
    AuthorizationConfigProperty: typing.Optional[list[CfnJobDefinitionDefAuthorizationconfigpropertyParams]] = pydantic.Field(None, description='')
    ContainerPropertiesProperty: typing.Optional[list[CfnJobDefinitionDefContainerpropertiespropertyParams]] = pydantic.Field(None, description='')
    DeviceProperty: typing.Optional[list[CfnJobDefinitionDefDevicepropertyParams]] = pydantic.Field(None, description='')
    EfsVolumeConfigurationProperty: typing.Optional[list[CfnJobDefinitionDefEfsvolumeconfigurationpropertyParams]] = pydantic.Field(None, description='')
    EksContainerEnvironmentVariableProperty: typing.Optional[list[CfnJobDefinitionDefEkscontainerenvironmentvariablepropertyParams]] = pydantic.Field(None, description='')
    EksContainerProperty: typing.Optional[list[CfnJobDefinitionDefEkscontainerpropertyParams]] = pydantic.Field(None, description='')
    EksContainerVolumeMountProperty: typing.Optional[list[CfnJobDefinitionDefEkscontainervolumemountpropertyParams]] = pydantic.Field(None, description='')
    EksPropertiesProperty: typing.Optional[list[CfnJobDefinitionDefEkspropertiespropertyParams]] = pydantic.Field(None, description='')
    EksSecretProperty: typing.Optional[list[CfnJobDefinitionDefEkssecretpropertyParams]] = pydantic.Field(None, description='')
    EksVolumeProperty: typing.Optional[list[CfnJobDefinitionDefEksvolumepropertyParams]] = pydantic.Field(None, description='')
    EmptyDirProperty: typing.Optional[list[CfnJobDefinitionDefEmptydirpropertyParams]] = pydantic.Field(None, description='')
    EnvironmentProperty: typing.Optional[list[CfnJobDefinitionDefEnvironmentpropertyParams]] = pydantic.Field(None, description='')
    EphemeralStorageProperty: typing.Optional[list[CfnJobDefinitionDefEphemeralstoragepropertyParams]] = pydantic.Field(None, description='')
    EvaluateOnExitProperty: typing.Optional[list[CfnJobDefinitionDefEvaluateonexitpropertyParams]] = pydantic.Field(None, description='')
    FargatePlatformConfigurationProperty: typing.Optional[list[CfnJobDefinitionDefFargateplatformconfigurationpropertyParams]] = pydantic.Field(None, description='')
    HostPathProperty: typing.Optional[list[CfnJobDefinitionDefHostpathpropertyParams]] = pydantic.Field(None, description='')
    LinuxParametersProperty: typing.Optional[list[CfnJobDefinitionDefLinuxparameterspropertyParams]] = pydantic.Field(None, description='')
    LogConfigurationProperty: typing.Optional[list[CfnJobDefinitionDefLogconfigurationpropertyParams]] = pydantic.Field(None, description='')
    MetadataProperty: typing.Optional[list[CfnJobDefinitionDefMetadatapropertyParams]] = pydantic.Field(None, description='')
    MountPointsProperty: typing.Optional[list[CfnJobDefinitionDefMountpointspropertyParams]] = pydantic.Field(None, description='')
    NetworkConfigurationProperty: typing.Optional[list[CfnJobDefinitionDefNetworkconfigurationpropertyParams]] = pydantic.Field(None, description='')
    NodePropertiesProperty: typing.Optional[list[CfnJobDefinitionDefNodepropertiespropertyParams]] = pydantic.Field(None, description='')
    NodeRangePropertyProperty: typing.Optional[list[CfnJobDefinitionDefNoderangepropertypropertyParams]] = pydantic.Field(None, description='')
    PodPropertiesProperty: typing.Optional[list[CfnJobDefinitionDefPodpropertiespropertyParams]] = pydantic.Field(None, description='')
    ResourceRequirementProperty: typing.Optional[list[CfnJobDefinitionDefResourcerequirementpropertyParams]] = pydantic.Field(None, description='')
    ResourcesProperty: typing.Optional[list[CfnJobDefinitionDefResourcespropertyParams]] = pydantic.Field(None, description='')
    RetryStrategyProperty: typing.Optional[list[CfnJobDefinitionDefRetrystrategypropertyParams]] = pydantic.Field(None, description='')
    SecretProperty: typing.Optional[list[CfnJobDefinitionDefSecretpropertyParams]] = pydantic.Field(None, description='')
    SecurityContextProperty: typing.Optional[list[CfnJobDefinitionDefSecuritycontextpropertyParams]] = pydantic.Field(None, description='')
    TimeoutProperty: typing.Optional[list[CfnJobDefinitionDefTimeoutpropertyParams]] = pydantic.Field(None, description='')
    TmpfsProperty: typing.Optional[list[CfnJobDefinitionDefTmpfspropertyParams]] = pydantic.Field(None, description='')
    UlimitProperty: typing.Optional[list[CfnJobDefinitionDefUlimitpropertyParams]] = pydantic.Field(None, description='')
    VolumesHostProperty: typing.Optional[list[CfnJobDefinitionDefVolumeshostpropertyParams]] = pydantic.Field(None, description='')
    VolumesProperty: typing.Optional[list[CfnJobDefinitionDefVolumespropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnJobDefinitionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnJobDefinitionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnJobDefinitionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnJobDefinitionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnJobDefinitionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnJobDefinitionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnJobDefinitionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnJobDefinitionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnJobDefinitionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnJobDefinitionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnJobDefinitionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnJobDefinitionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnJobDefinitionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnJobDefinitionDefAuthorizationconfigpropertyParams(pydantic.BaseModel):
    access_point_id: typing.Optional[str] = pydantic.Field(None, description='')
    iam: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefContainerpropertiespropertyParams(pydantic.BaseModel):
    image: str = pydantic.Field(..., description='')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    environment: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EnvironmentPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ephemeral_storage: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    execution_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    fargate_platform_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_FargatePlatformConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    instance_type: typing.Optional[str] = pydantic.Field(None, description='')
    job_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    linux_parameters: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_LinuxParametersPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    log_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_LogConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    memory: typing.Union[int, float, None] = pydantic.Field(None, description='')
    mount_points: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_MountPointsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    network_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_NetworkConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    privileged: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    readonly_root_filesystem: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    resource_requirements: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ResourceRequirementPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    secrets: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ulimits: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_UlimitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    user: typing.Optional[str] = pydantic.Field(None, description='')
    vcpus: typing.Union[int, float, None] = pydantic.Field(None, description='')
    volumes: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_VolumesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefDevicepropertyParams(pydantic.BaseModel):
    container_path: typing.Optional[str] = pydantic.Field(None, description='')
    host_path: typing.Optional[str] = pydantic.Field(None, description='')
    permissions: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEfsvolumeconfigurationpropertyParams(pydantic.BaseModel):
    file_system_id: str = pydantic.Field(..., description='')
    authorization_config: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_AuthorizationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    root_directory: typing.Optional[str] = pydantic.Field(None, description='')
    transit_encryption: typing.Optional[str] = pydantic.Field(None, description='')
    transit_encryption_port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkscontainerenvironmentvariablepropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkscontainerpropertyParams(pydantic.BaseModel):
    image: str = pydantic.Field(..., description='')
    args: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    env: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    image_pull_policy: typing.Optional[str] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    resources: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    security_context: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_SecurityContextPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    volume_mounts: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksContainerVolumeMountPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkscontainervolumemountpropertyParams(pydantic.BaseModel):
    mount_path: typing.Optional[str] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    read_only: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkspropertiespropertyParams(pydantic.BaseModel):
    pod_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_PodPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEkssecretpropertyParams(pydantic.BaseModel):
    secret_name: str = pydantic.Field(..., description='')
    optional: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEksvolumepropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    empty_dir: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EmptyDirPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    host_path: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_HostPathPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    secret: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEmptydirpropertyParams(pydantic.BaseModel):
    medium: typing.Optional[str] = pydantic.Field(None, description='')
    size_limit: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEnvironmentpropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefEphemeralstoragepropertyParams(pydantic.BaseModel):
    size_in_gib: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefEvaluateonexitpropertyParams(pydantic.BaseModel):
    action: str = pydantic.Field(..., description='')
    on_exit_code: typing.Optional[str] = pydantic.Field(None, description='')
    on_reason: typing.Optional[str] = pydantic.Field(None, description='')
    on_status_reason: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefFargateplatformconfigurationpropertyParams(pydantic.BaseModel):
    platform_version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefHostpathpropertyParams(pydantic.BaseModel):
    path: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefLinuxparameterspropertyParams(pydantic.BaseModel):
    devices: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_DevicePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    init_process_enabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    max_swap: typing.Union[int, float, None] = pydantic.Field(None, description='')
    shared_memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    swappiness: typing.Union[int, float, None] = pydantic.Field(None, description='')
    tmpfs: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_TmpfsPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefLogconfigurationpropertyParams(pydantic.BaseModel):
    log_driver: str = pydantic.Field(..., description='')
    options: typing.Any = pydantic.Field(None, description='')
    secret_options: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_SecretPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefMetadatapropertyParams(pydantic.BaseModel):
    labels: typing.Any = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefMountpointspropertyParams(pydantic.BaseModel):
    container_path: typing.Optional[str] = pydantic.Field(None, description='')
    read_only: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    source_volume: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefNetworkconfigurationpropertyParams(pydantic.BaseModel):
    assign_public_ip: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefNodepropertiespropertyParams(pydantic.BaseModel):
    main_node: typing.Union[int, float] = pydantic.Field(..., description='')
    node_range_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_NodeRangePropertyPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    num_nodes: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefNoderangepropertypropertyParams(pydantic.BaseModel):
    target_nodes: str = pydantic.Field(..., description='')
    container: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefPodpropertiespropertyParams(pydantic.BaseModel):
    containers: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksContainerPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    dns_policy: typing.Optional[str] = pydantic.Field(None, description='')
    host_network: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    metadata: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_MetadataPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    service_account_name: typing.Optional[str] = pydantic.Field(None, description='')
    volumes: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksVolumePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefResourcerequirementpropertyParams(pydantic.BaseModel):
    type: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefResourcespropertyParams(pydantic.BaseModel):
    limits: typing.Any = pydantic.Field(None, description='')
    requests: typing.Any = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefRetrystrategypropertyParams(pydantic.BaseModel):
    attempts: typing.Union[int, float, None] = pydantic.Field(None, description='')
    evaluate_on_exit: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EvaluateOnExitPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefSecretpropertyParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='')
    value_from: str = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefSecuritycontextpropertyParams(pydantic.BaseModel):
    privileged: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    read_only_root_filesystem: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    run_as_group: typing.Union[int, float, None] = pydantic.Field(None, description='')
    run_as_non_root: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    run_as_user: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefTimeoutpropertyParams(pydantic.BaseModel):
    attempt_duration_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefTmpfspropertyParams(pydantic.BaseModel):
    container_path: str = pydantic.Field(..., description='')
    size: typing.Union[int, float] = pydantic.Field(..., description='')
    mount_options: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefUlimitpropertyParams(pydantic.BaseModel):
    hard_limit: typing.Union[int, float] = pydantic.Field(..., description='')
    name: str = pydantic.Field(..., description='')
    soft_limit: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobDefinitionDefVolumeshostpropertyParams(pydantic.BaseModel):
    source_path: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefVolumespropertyParams(pydantic.BaseModel):
    efs_volume_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EfsVolumeConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    host: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_VolumesHostPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnJobDefinitionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnJobDefinitionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobDefinitionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnJobDefinitionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobDefinitionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnJobDefinitionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnJobDefinitionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnJobDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnJobDefinitionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnJobDefinitionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobDefinitionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnJobDefinitionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnJobDefinitionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobDefinitionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnJobQueue
class CfnJobQueueDef(BaseCfnResource):
    compute_environment_order: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobQueue_ComputeEnvironmentOrderPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description="The set of compute environments mapped to a job queue and their order relative to each other. The job scheduler uses this parameter to determine which compute environment runs a specific job. Compute environments must be in the ``VALID`` state before you can associate them with a job queue. You can associate up to three compute environments with a job queue. All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed. .. epigraph:: All compute environments that are associated with a job queue must share the same architecture. AWS Batch doesn't support mixing compute environment architecture types in a single job queue.\n")
    priority: typing.Union[int, float] = pydantic.Field(..., description="The priority of the job queue. Job queues with a higher priority (or a higher integer value for the ``priority`` parameter) are evaluated first when associated with the same compute environment. Priority is determined in descending order. For example, a job queue with a priority value of ``10`` is given scheduling preference over a job queue with a priority value of ``1`` . All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed.\n")
    job_queue_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job queue. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    scheduling_policy_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the scheduling policy. The format is ``aws: *Partition* :batch: *Region* : *Account* :scheduling-policy/ *Name*`` . For example, ``aws:aws:batch:us-west-2:123456789012:scheduling-policy/MySchedulingPolicy`` .\n')
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the job queue. If the job queue state is ``ENABLED`` , it is able to accept jobs. If the job queue state is ``DISABLED`` , new jobs can't be added to the queue, but jobs already in the queue can finish.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that are applied to the job queue. For more information, see `Tagging your AWS Batch resources <https://docs.aws.amazon.com/batch/latest/userguide/using-tags.html>`_ in *AWS Batch User Guide* .')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_order', 'priority', 'job_queue_name', 'scheduling_policy_arn', 'state', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['ComputeEnvironmentOrderProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobQueue'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnJobQueueDefConfig] = pydantic.Field(None)


class CfnJobQueueDefConfig(pydantic.BaseModel):
    ComputeEnvironmentOrderProperty: typing.Optional[list[CfnJobQueueDefComputeenvironmentorderpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnJobQueueDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnJobQueueDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnJobQueueDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnJobQueueDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnJobQueueDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnJobQueueDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnJobQueueDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnJobQueueDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnJobQueueDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnJobQueueDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnJobQueueDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnJobQueueDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnJobQueueDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnJobQueueDefComputeenvironmentorderpropertyParams(pydantic.BaseModel):
    compute_environment: str = pydantic.Field(..., description='')
    order: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnJobQueueDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnJobQueueDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobQueueDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnJobQueueDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobQueueDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnJobQueueDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnJobQueueDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnJobQueueDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnJobQueueDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnJobQueueDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnJobQueueDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnJobQueueDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnJobQueueDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnJobQueueDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicy
class CfnSchedulingPolicyDef(BaseCfnResource):
    fairshare_policy: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnSchedulingPolicy_FairsharePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The fair share policy of the scheduling policy.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the scheduling policy. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that you apply to the scheduling policy to help you categorize and organize your resources. Each tag consists of a key and an optional value. For more information, see `Tagging AWS Resources <https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>`_ in *AWS General Reference* . These tags can be updated or removed using the `TagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_TagResource.html>`_ and `UntagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_UntagResource.html>`_ API operations.')
    _init_params: typing.ClassVar[list[str]] = ['fairshare_policy', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['FairsharePolicyProperty', 'ShareAttributesProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnSchedulingPolicyDefConfig] = pydantic.Field(None)


class CfnSchedulingPolicyDefConfig(pydantic.BaseModel):
    FairsharePolicyProperty: typing.Optional[list[CfnSchedulingPolicyDefFairsharepolicypropertyParams]] = pydantic.Field(None, description='')
    ShareAttributesProperty: typing.Optional[list[CfnSchedulingPolicyDefShareattributespropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnSchedulingPolicyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnSchedulingPolicyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnSchedulingPolicyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnSchedulingPolicyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnSchedulingPolicyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnSchedulingPolicyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnSchedulingPolicyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnSchedulingPolicyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnSchedulingPolicyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnSchedulingPolicyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnSchedulingPolicyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnSchedulingPolicyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnSchedulingPolicyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnSchedulingPolicyDefFairsharepolicypropertyParams(pydantic.BaseModel):
    compute_reservation: typing.Union[int, float, None] = pydantic.Field(None, description='')
    share_decay_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='')
    share_distribution: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnSchedulingPolicy_ShareAttributesPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnSchedulingPolicyDefShareattributespropertyParams(pydantic.BaseModel):
    share_identifier: typing.Optional[str] = pydantic.Field(None, description='')
    weight_factor: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnSchedulingPolicyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnSchedulingPolicyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSchedulingPolicyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnSchedulingPolicyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSchedulingPolicyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnSchedulingPolicyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnSchedulingPolicyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnSchedulingPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnSchedulingPolicyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnSchedulingPolicyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSchedulingPolicyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnSchedulingPolicyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnSchedulingPolicyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSchedulingPolicyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_batch.CfnComputeEnvironmentProps
class CfnComputeEnvironmentPropsDef(BaseCfnProperty):
    type: str = pydantic.Field(..., description='The type of the compute environment: ``MANAGED`` or ``UNMANAGED`` . For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the *AWS Batch User Guide* .\n')
    compute_environment_name: typing.Optional[str] = pydantic.Field(None, description='The name for your compute environment. It can be up to 128 characters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    compute_resources: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_ComputeResourcesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ComputeResources property type specifies details of the compute resources managed by the compute environment. This parameter is required for managed compute environments. For more information, see `Compute Environments <https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html>`_ in the ** .\n')
    eks_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_EksConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The details for the Amazon EKS cluster that supports the compute environment.\n')
    replace_compute_environment: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Specifies whether the compute environment is replaced if an update is made that requires replacing the instances in the compute environment. The default value is ``true`` . To enable more properties to be updated, set this property to ``false`` . When changing the value of this property to ``false`` , do not change any other properties at the same time. If other properties are changed at the same time, and the change needs to be rolled back but it can't, it's possible for the stack to go into the ``UPDATE_ROLLBACK_FAILED`` state. You can't update a stack that is in the ``UPDATE_ROLLBACK_FAILED`` state. However, if you can continue to roll it back, you can return the stack to its original settings and then try to update it again. For more information, see `Continue rolling back an update <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html>`_ in the *AWS CloudFormation User Guide* . The properties that can't be changed without replacing the compute environment are in the ```ComputeResources`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html>`_ property type: ```AllocationStrategy`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-allocationstrategy>`_ , ```BidPercentage`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-bidpercentage>`_ , ```Ec2Configuration`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2configuration>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```Ec2KeyPair`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-ec2keypair>`_ , ```ImageId`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-imageid>`_ , ```InstanceRole`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancerole>`_ , ```InstanceTypes`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-instancetypes>`_ , ```LaunchTemplate`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-launchtemplate>`_ , ```MaxvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-maxvcpus>`_ , ```MinvCpus`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-minvcpus>`_ , ```PlacementGroup`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-placementgroup>`_ , ```SecurityGroupIds`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-securitygroupids>`_ , ```Subnets`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-subnets>`_ , `Tags <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-tags>`_ , ```Type`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-type>`_ , and ```UpdateToLatestImageVersion`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-batch-computeenvironment-computeresources.html#cfn-batch-computeenvironment-computeresources-updatetolatestimageversion>`_ .\n")
    service_role: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that allows AWS Batch to make calls to other AWS services on your behalf. For more information, see `AWS Batch service IAM role <https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If your account already created the AWS Batch service-linked role, that role is used by default for your compute environment unless you specify a different role here. If the AWS Batch service-linked role doesn't exist in your account, and no role is specified here, the service attempts to create the AWS Batch service-linked role in your account. If your specified role has a path other than ``/`` , then you must specify either the full role ARN (recommended) or prefix the role name with the path. For example, if a role with the name ``bar`` has a path of ``/foo/`` , specify ``/foo/bar`` as the role name. For more information, see `Friendly names and paths <https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-friendly-names>`_ in the *IAM User Guide* . .. epigraph:: Depending on how you created your AWS Batch service role, its ARN might contain the ``service-role`` path prefix. When you only specify the name of the service role, AWS Batch assumes that your ARN doesn't use the ``service-role`` path prefix. Because of this, we recommend that you specify the full ARN of your service role when you create compute environments.\n")
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the compute environment. If the state is ``ENABLED`` , then the compute environment accepts jobs from a queue and can scale out automatically based on queues. If the state is ``ENABLED`` , then the AWS Batch scheduler can attempt to place jobs from an associated job queue on the compute resources within the environment. If the compute environment is managed, then it can scale its instances out or in automatically, based on the job queue demand. If the state is ``DISABLED`` , then the AWS Batch scheduler doesn't attempt to place jobs within the environment. Jobs in a ``STARTING`` or ``RUNNING`` state continue to progress normally. Managed compute environments in the ``DISABLED`` state don't scale out. .. epigraph:: Compute environments in a ``DISABLED`` state may continue to incur billing charges. To prevent additional charges, turn off and then delete the compute environment. For more information, see `State <https://docs.aws.amazon.com/batch/latest/userguide/compute_environment_parameters.html#compute_environment_state>`_ in the *AWS Batch User Guide* . When an instance is idle, the instance scales down to the ``minvCpus`` value. However, the instance size doesn't change. For example, consider a ``c5.8xlarge`` instance with a ``minvCpus`` value of ``4`` and a ``desiredvCpus`` value of ``36`` . This instance doesn't scale down to a ``c5.large`` instance.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags applied to the compute environment.\n')
    unmanagedv_cpus: typing.Union[int, float, None] = pydantic.Field(None, description="The maximum number of vCPUs for an unmanaged compute environment. This parameter is only used for fair share scheduling to reserve vCPU capacity for new share identifiers. If this parameter isn't provided for a fair share job queue, no vCPU capacity is reserved. .. epigraph:: This parameter is only supported when the ``type`` parameter is set to ``UNMANAGED`` .\n")
    update_policy: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnComputeEnvironment_UpdatePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the infrastructure update policy for the compute environment. For more information about infrastructure updates, see `Updating compute environments <https://docs.aws.amazon.com/batch/latest/userguide/updating-compute-environments.html>`_ in the *AWS Batch User Guide* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-computeenvironment.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    cfn_compute_environment_props = batch.CfnComputeEnvironmentProps(\n        type="type",\n\n        # the properties below are optional\n        compute_environment_name="computeEnvironmentName",\n        compute_resources=batch.CfnComputeEnvironment.ComputeResourcesProperty(\n            maxv_cpus=123,\n            subnets=["subnets"],\n            type="type",\n\n            # the properties below are optional\n            allocation_strategy="allocationStrategy",\n            bid_percentage=123,\n            desiredv_cpus=123,\n            ec2_configuration=[batch.CfnComputeEnvironment.Ec2ConfigurationObjectProperty(\n                image_type="imageType",\n\n                # the properties below are optional\n                image_id_override="imageIdOverride",\n                image_kubernetes_version="imageKubernetesVersion"\n            )],\n            ec2_key_pair="ec2KeyPair",\n            image_id="imageId",\n            instance_role="instanceRole",\n            instance_types=["instanceTypes"],\n            launch_template=batch.CfnComputeEnvironment.LaunchTemplateSpecificationProperty(\n                launch_template_id="launchTemplateId",\n                launch_template_name="launchTemplateName",\n                version="version"\n            ),\n            minv_cpus=123,\n            placement_group="placementGroup",\n            security_group_ids=["securityGroupIds"],\n            spot_iam_fleet_role="spotIamFleetRole",\n            tags={\n                "tags_key": "tags"\n            },\n            update_to_latest_image_version=False\n        ),\n        eks_configuration=batch.CfnComputeEnvironment.EksConfigurationProperty(\n            eks_cluster_arn="eksClusterArn",\n            kubernetes_namespace="kubernetesNamespace"\n        ),\n        replace_compute_environment=False,\n        service_role="serviceRole",\n        state="state",\n        tags={\n            "tags_key": "tags"\n        },\n        unmanagedv_cpus=123,\n        update_policy=batch.CfnComputeEnvironment.UpdatePolicyProperty(\n            job_execution_timeout_minutes=123,\n            terminate_jobs_on_update=False\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'compute_environment_name', 'compute_resources', 'eks_configuration', 'replace_compute_environment', 'service_role', 'state', 'tags', 'unmanagedv_cpus', 'update_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnComputeEnvironmentProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobDefinitionProps
class CfnJobDefinitionPropsDef(BaseCfnProperty):
    type: str = pydantic.Field(..., description="The type of job definition. For more information about multi-node parallel jobs, see `Creating a multi-node parallel job definition <https://docs.aws.amazon.com/batch/latest/userguide/multi-node-job-def.html>`_ in the *AWS Batch User Guide* . .. epigraph:: If the job is run on Fargate resources, then ``multinode`` isn't supported.\n")
    container_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_ContainerPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties specific to Amazon ECS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    eks_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_EksPropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object with various properties that are specific to Amazon EKS based jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified.\n')
    job_definition_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job definition.\n')
    node_properties: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_NodePropertiesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="An object with various properties that are specific to multi-node parallel jobs. Valid values are ``containerProperties`` , ``eksProperties`` , and ``nodeProperties`` . Only one can be specified. .. epigraph:: If the job runs on Fargate resources, don't specify ``nodeProperties`` . Use ``containerProperties`` instead.\n")
    parameters: typing.Any = pydantic.Field(None, description='Default parameters or parameter substitution placeholders that are set in the job definition. Parameters are specified as a key-value pair mapping. Parameters in a ``SubmitJob`` request override any corresponding parameter defaults from the job definition. For more information about specifying parameters, see `Job definition parameters <https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html>`_ in the *AWS Batch User Guide* .\n')
    platform_capabilities: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The platform capabilities required by the job definition. If no value is specified, it defaults to ``EC2`` . Jobs run on Fargate resources specify ``FARGATE`` .\n')
    propagate_tags: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Specifies whether to propagate the tags from the job or job definition to the corresponding Amazon ECS task. If no value is specified, the tags aren't propagated. Tags can only be propagated to the tasks when the tasks are created. For tags with the same name, job tags are given priority over job definitions tags. If the total number of combined tags from the job and job definition is over 50, the job is moved to the ``FAILED`` state.\n")
    retry_strategy: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_RetryStrategyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The retry strategy to use for failed jobs that are submitted with this job definition.\n')
    scheduling_priority: typing.Union[int, float, None] = pydantic.Field(None, description='The scheduling priority of the job definition. This only affects jobs in job queues with a fair share policy. Jobs with a higher scheduling priority are scheduled before jobs with a lower scheduling priority.\n')
    tags: typing.Any = pydantic.Field(None, description='The tags that are applied to the job definition.\n')
    timeout: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobDefinition_TimeoutPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The timeout time for jobs that are submitted with this job definition. After the amount of time you specify passes, AWS Batch terminates your jobs if they aren\'t finished.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-jobdefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    # labels: Any\n    # limits: Any\n    # options: Any\n    # parameters: Any\n    # requests: Any\n    # tags: Any\n\n    cfn_job_definition_props = batch.CfnJobDefinitionProps(\n        type="type",\n\n        # the properties below are optional\n        container_properties=batch.CfnJobDefinition.ContainerPropertiesProperty(\n            image="image",\n\n            # the properties below are optional\n            command=["command"],\n            environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                name="name",\n                value="value"\n            )],\n            ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                size_in_gi_b=123\n            ),\n            execution_role_arn="executionRoleArn",\n            fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                platform_version="platformVersion"\n            ),\n            instance_type="instanceType",\n            job_role_arn="jobRoleArn",\n            linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                devices=[batch.CfnJobDefinition.DeviceProperty(\n                    container_path="containerPath",\n                    host_path="hostPath",\n                    permissions=["permissions"]\n                )],\n                init_process_enabled=False,\n                max_swap=123,\n                shared_memory_size=123,\n                swappiness=123,\n                tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                    container_path="containerPath",\n                    size=123,\n\n                    # the properties below are optional\n                    mount_options=["mountOptions"]\n                )]\n            ),\n            log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                log_driver="logDriver",\n\n                # the properties below are optional\n                options=options,\n                secret_options=[batch.CfnJobDefinition.SecretProperty(\n                    name="name",\n                    value_from="valueFrom"\n                )]\n            ),\n            memory=123,\n            mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                container_path="containerPath",\n                read_only=False,\n                source_volume="sourceVolume"\n            )],\n            network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                assign_public_ip="assignPublicIp"\n            ),\n            privileged=False,\n            readonly_root_filesystem=False,\n            resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                type="type",\n                value="value"\n            )],\n            secrets=[batch.CfnJobDefinition.SecretProperty(\n                name="name",\n                value_from="valueFrom"\n            )],\n            ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                hard_limit=123,\n                name="name",\n                soft_limit=123\n            )],\n            user="user",\n            vcpus=123,\n            volumes=[batch.CfnJobDefinition.VolumesProperty(\n                efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                    file_system_id="fileSystemId",\n\n                    # the properties below are optional\n                    authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                        access_point_id="accessPointId",\n                        iam="iam"\n                    ),\n                    root_directory="rootDirectory",\n                    transit_encryption="transitEncryption",\n                    transit_encryption_port=123\n                ),\n                host=batch.CfnJobDefinition.VolumesHostProperty(\n                    source_path="sourcePath"\n                ),\n                name="name"\n            )]\n        ),\n        eks_properties=batch.CfnJobDefinition.EksPropertiesProperty(\n            pod_properties=batch.CfnJobDefinition.PodPropertiesProperty(\n                containers=[batch.CfnJobDefinition.EksContainerProperty(\n                    image="image",\n\n                    # the properties below are optional\n                    args=["args"],\n                    command=["command"],\n                    env=[batch.CfnJobDefinition.EksContainerEnvironmentVariableProperty(\n                        name="name",\n\n                        # the properties below are optional\n                        value="value"\n                    )],\n                    image_pull_policy="imagePullPolicy",\n                    name="name",\n                    resources=batch.CfnJobDefinition.ResourcesProperty(\n                        limits=limits,\n                        requests=requests\n                    ),\n                    security_context=batch.CfnJobDefinition.SecurityContextProperty(\n                        privileged=False,\n                        read_only_root_filesystem=False,\n                        run_as_group=123,\n                        run_as_non_root=False,\n                        run_as_user=123\n                    ),\n                    volume_mounts=[batch.CfnJobDefinition.EksContainerVolumeMountProperty(\n                        mount_path="mountPath",\n                        name="name",\n                        read_only=False\n                    )]\n                )],\n                dns_policy="dnsPolicy",\n                host_network=False,\n                metadata=batch.CfnJobDefinition.MetadataProperty(\n                    labels=labels\n                ),\n                service_account_name="serviceAccountName",\n                volumes=[batch.CfnJobDefinition.EksVolumeProperty(\n                    name="name",\n\n                    # the properties below are optional\n                    empty_dir=batch.CfnJobDefinition.EmptyDirProperty(\n                        medium="medium",\n                        size_limit="sizeLimit"\n                    ),\n                    host_path=batch.CfnJobDefinition.HostPathProperty(\n                        path="path"\n                    ),\n                    secret=batch.CfnJobDefinition.EksSecretProperty(\n                        secret_name="secretName",\n\n                        # the properties below are optional\n                        optional=False\n                    )\n                )]\n            )\n        ),\n        job_definition_name="jobDefinitionName",\n        node_properties=batch.CfnJobDefinition.NodePropertiesProperty(\n            main_node=123,\n            node_range_properties=[batch.CfnJobDefinition.NodeRangePropertyProperty(\n                target_nodes="targetNodes",\n\n                # the properties below are optional\n                container=batch.CfnJobDefinition.ContainerPropertiesProperty(\n                    image="image",\n\n                    # the properties below are optional\n                    command=["command"],\n                    environment=[batch.CfnJobDefinition.EnvironmentProperty(\n                        name="name",\n                        value="value"\n                    )],\n                    ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty(\n                        size_in_gi_b=123\n                    ),\n                    execution_role_arn="executionRoleArn",\n                    fargate_platform_configuration=batch.CfnJobDefinition.FargatePlatformConfigurationProperty(\n                        platform_version="platformVersion"\n                    ),\n                    instance_type="instanceType",\n                    job_role_arn="jobRoleArn",\n                    linux_parameters=batch.CfnJobDefinition.LinuxParametersProperty(\n                        devices=[batch.CfnJobDefinition.DeviceProperty(\n                            container_path="containerPath",\n                            host_path="hostPath",\n                            permissions=["permissions"]\n                        )],\n                        init_process_enabled=False,\n                        max_swap=123,\n                        shared_memory_size=123,\n                        swappiness=123,\n                        tmpfs=[batch.CfnJobDefinition.TmpfsProperty(\n                            container_path="containerPath",\n                            size=123,\n\n                            # the properties below are optional\n                            mount_options=["mountOptions"]\n                        )]\n                    ),\n                    log_configuration=batch.CfnJobDefinition.LogConfigurationProperty(\n                        log_driver="logDriver",\n\n                        # the properties below are optional\n                        options=options,\n                        secret_options=[batch.CfnJobDefinition.SecretProperty(\n                            name="name",\n                            value_from="valueFrom"\n                        )]\n                    ),\n                    memory=123,\n                    mount_points=[batch.CfnJobDefinition.MountPointsProperty(\n                        container_path="containerPath",\n                        read_only=False,\n                        source_volume="sourceVolume"\n                    )],\n                    network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty(\n                        assign_public_ip="assignPublicIp"\n                    ),\n                    privileged=False,\n                    readonly_root_filesystem=False,\n                    resource_requirements=[batch.CfnJobDefinition.ResourceRequirementProperty(\n                        type="type",\n                        value="value"\n                    )],\n                    secrets=[batch.CfnJobDefinition.SecretProperty(\n                        name="name",\n                        value_from="valueFrom"\n                    )],\n                    ulimits=[batch.CfnJobDefinition.UlimitProperty(\n                        hard_limit=123,\n                        name="name",\n                        soft_limit=123\n                    )],\n                    user="user",\n                    vcpus=123,\n                    volumes=[batch.CfnJobDefinition.VolumesProperty(\n                        efs_volume_configuration=batch.CfnJobDefinition.EfsVolumeConfigurationProperty(\n                            file_system_id="fileSystemId",\n\n                            # the properties below are optional\n                            authorization_config=batch.CfnJobDefinition.AuthorizationConfigProperty(\n                                access_point_id="accessPointId",\n                                iam="iam"\n                            ),\n                            root_directory="rootDirectory",\n                            transit_encryption="transitEncryption",\n                            transit_encryption_port=123\n                        ),\n                        host=batch.CfnJobDefinition.VolumesHostProperty(\n                            source_path="sourcePath"\n                        ),\n                        name="name"\n                    )]\n                )\n            )],\n            num_nodes=123\n        ),\n        parameters=parameters,\n        platform_capabilities=["platformCapabilities"],\n        propagate_tags=False,\n        retry_strategy=batch.CfnJobDefinition.RetryStrategyProperty(\n            attempts=123,\n            evaluate_on_exit=[batch.CfnJobDefinition.EvaluateOnExitProperty(\n                action="action",\n\n                # the properties below are optional\n                on_exit_code="onExitCode",\n                on_reason="onReason",\n                on_status_reason="onStatusReason"\n            )]\n        ),\n        scheduling_priority=123,\n        tags=tags,\n        timeout=batch.CfnJobDefinition.TimeoutProperty(\n            attempt_duration_seconds=123\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'container_properties', 'eks_properties', 'job_definition_name', 'node_properties', 'parameters', 'platform_capabilities', 'propagate_tags', 'retry_strategy', 'scheduling_priority', 'tags', 'timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnJobQueueProps
class CfnJobQueuePropsDef(BaseCfnProperty):
    compute_environment_order: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnJobQueue_ComputeEnvironmentOrderPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description="The set of compute environments mapped to a job queue and their order relative to each other. The job scheduler uses this parameter to determine which compute environment runs a specific job. Compute environments must be in the ``VALID`` state before you can associate them with a job queue. You can associate up to three compute environments with a job queue. All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed. .. epigraph:: All compute environments that are associated with a job queue must share the same architecture. AWS Batch doesn't support mixing compute environment architecture types in a single job queue.\n")
    priority: typing.Union[int, float] = pydantic.Field(..., description="The priority of the job queue. Job queues with a higher priority (or a higher integer value for the ``priority`` parameter) are evaluated first when associated with the same compute environment. Priority is determined in descending order. For example, a job queue with a priority value of ``10`` is given scheduling preference over a job queue with a priority value of ``1`` . All of the compute environments must be either EC2 ( ``EC2`` or ``SPOT`` ) or Fargate ( ``FARGATE`` or ``FARGATE_SPOT`` ); EC2 and Fargate compute environments can't be mixed.\n")
    job_queue_name: typing.Optional[str] = pydantic.Field(None, description='The name of the job queue. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    scheduling_policy_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the scheduling policy. The format is ``aws: *Partition* :batch: *Region* : *Account* :scheduling-policy/ *Name*`` . For example, ``aws:aws:batch:us-west-2:123456789012:scheduling-policy/MySchedulingPolicy`` .\n')
    state: typing.Optional[str] = pydantic.Field(None, description="The state of the job queue. If the job queue state is ``ENABLED`` , it is able to accept jobs. If the job queue state is ``DISABLED`` , new jobs can't be added to the queue, but jobs already in the queue can finish.\n")
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that are applied to the job queue. For more information, see `Tagging your AWS Batch resources <https://docs.aws.amazon.com/batch/latest/userguide/using-tags.html>`_ in *AWS Batch User Guide* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-jobqueue.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    cfn_job_queue_props = batch.CfnJobQueueProps(\n        compute_environment_order=[batch.CfnJobQueue.ComputeEnvironmentOrderProperty(\n            compute_environment="computeEnvironment",\n            order=123\n        )],\n        priority=123,\n\n        # the properties below are optional\n        job_queue_name="jobQueueName",\n        scheduling_policy_arn="schedulingPolicyArn",\n        state="state",\n        tags={\n            "tags_key": "tags"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_environment_order', 'priority', 'job_queue_name', 'scheduling_policy_arn', 'state', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnJobQueueProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_batch.CfnSchedulingPolicyProps
class CfnSchedulingPolicyPropsDef(BaseCfnProperty):
    fairshare_policy: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_batch.CfnSchedulingPolicy_FairsharePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The fair share policy of the scheduling policy.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the scheduling policy. It can be up to 128 letters long. It can contain uppercase and lowercase letters, numbers, hyphens (-), and underscores (_).\n')
    tags: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The tags that you apply to the scheduling policy to help you categorize and organize your resources. Each tag consists of a key and an optional value. For more information, see `Tagging AWS Resources <https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>`_ in *AWS General Reference* . These tags can be updated or removed using the `TagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_TagResource.html>`_ and `UntagResource <https://docs.aws.amazon.com/batch/latest/APIReference/API_UntagResource.html>`_ API operations.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-batch-schedulingpolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_batch as batch\n\n    cfn_scheduling_policy_props = batch.CfnSchedulingPolicyProps(\n        fairshare_policy=batch.CfnSchedulingPolicy.FairsharePolicyProperty(\n            compute_reservation=123,\n            share_decay_seconds=123,\n            share_distribution=[batch.CfnSchedulingPolicy.ShareAttributesProperty(\n                share_identifier="shareIdentifier",\n                weight_factor=123\n            )]\n        ),\n        name="name",\n        tags={\n            "tags_key": "tags"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['fairshare_policy', 'name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_batch.CfnSchedulingPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




import models

class ModuleModel(pydantic.BaseModel):
    CfnComputeEnvironment_ComputeResourcesProperty: typing.Optional[dict[str, CfnComputeEnvironment_ComputeResourcesPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_Ec2ConfigurationObjectProperty: typing.Optional[dict[str, CfnComputeEnvironment_Ec2ConfigurationObjectPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_EksConfigurationProperty: typing.Optional[dict[str, CfnComputeEnvironment_EksConfigurationPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_LaunchTemplateSpecificationProperty: typing.Optional[dict[str, CfnComputeEnvironment_LaunchTemplateSpecificationPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment_UpdatePolicyProperty: typing.Optional[dict[str, CfnComputeEnvironment_UpdatePolicyPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_AuthorizationConfigProperty: typing.Optional[dict[str, CfnJobDefinition_AuthorizationConfigPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_ContainerPropertiesProperty: typing.Optional[dict[str, CfnJobDefinition_ContainerPropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_DeviceProperty: typing.Optional[dict[str, CfnJobDefinition_DevicePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EfsVolumeConfigurationProperty: typing.Optional[dict[str, CfnJobDefinition_EfsVolumeConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksContainerEnvironmentVariableProperty: typing.Optional[dict[str, CfnJobDefinition_EksContainerEnvironmentVariablePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksContainerProperty: typing.Optional[dict[str, CfnJobDefinition_EksContainerPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksContainerVolumeMountProperty: typing.Optional[dict[str, CfnJobDefinition_EksContainerVolumeMountPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksPropertiesProperty: typing.Optional[dict[str, CfnJobDefinition_EksPropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksSecretProperty: typing.Optional[dict[str, CfnJobDefinition_EksSecretPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EksVolumeProperty: typing.Optional[dict[str, CfnJobDefinition_EksVolumePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EmptyDirProperty: typing.Optional[dict[str, CfnJobDefinition_EmptyDirPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EnvironmentProperty: typing.Optional[dict[str, CfnJobDefinition_EnvironmentPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EphemeralStorageProperty: typing.Optional[dict[str, CfnJobDefinition_EphemeralStoragePropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_EvaluateOnExitProperty: typing.Optional[dict[str, CfnJobDefinition_EvaluateOnExitPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_FargatePlatformConfigurationProperty: typing.Optional[dict[str, CfnJobDefinition_FargatePlatformConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_HostPathProperty: typing.Optional[dict[str, CfnJobDefinition_HostPathPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_LinuxParametersProperty: typing.Optional[dict[str, CfnJobDefinition_LinuxParametersPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_LogConfigurationProperty: typing.Optional[dict[str, CfnJobDefinition_LogConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_MetadataProperty: typing.Optional[dict[str, CfnJobDefinition_MetadataPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_MountPointsProperty: typing.Optional[dict[str, CfnJobDefinition_MountPointsPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_NetworkConfigurationProperty: typing.Optional[dict[str, CfnJobDefinition_NetworkConfigurationPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_NodePropertiesProperty: typing.Optional[dict[str, CfnJobDefinition_NodePropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_NodeRangePropertyProperty: typing.Optional[dict[str, CfnJobDefinition_NodeRangePropertyPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_PodPropertiesProperty: typing.Optional[dict[str, CfnJobDefinition_PodPropertiesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_ResourceRequirementProperty: typing.Optional[dict[str, CfnJobDefinition_ResourceRequirementPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_ResourcesProperty: typing.Optional[dict[str, CfnJobDefinition_ResourcesPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_RetryStrategyProperty: typing.Optional[dict[str, CfnJobDefinition_RetryStrategyPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_SecretProperty: typing.Optional[dict[str, CfnJobDefinition_SecretPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_SecurityContextProperty: typing.Optional[dict[str, CfnJobDefinition_SecurityContextPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_TimeoutProperty: typing.Optional[dict[str, CfnJobDefinition_TimeoutPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_TmpfsProperty: typing.Optional[dict[str, CfnJobDefinition_TmpfsPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_UlimitProperty: typing.Optional[dict[str, CfnJobDefinition_UlimitPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_VolumesHostProperty: typing.Optional[dict[str, CfnJobDefinition_VolumesHostPropertyDef]] = pydantic.Field(None)
    CfnJobDefinition_VolumesProperty: typing.Optional[dict[str, CfnJobDefinition_VolumesPropertyDef]] = pydantic.Field(None)
    CfnJobQueue_ComputeEnvironmentOrderProperty: typing.Optional[dict[str, CfnJobQueue_ComputeEnvironmentOrderPropertyDef]] = pydantic.Field(None)
    CfnSchedulingPolicy_FairsharePolicyProperty: typing.Optional[dict[str, CfnSchedulingPolicy_FairsharePolicyPropertyDef]] = pydantic.Field(None)
    CfnSchedulingPolicy_ShareAttributesProperty: typing.Optional[dict[str, CfnSchedulingPolicy_ShareAttributesPropertyDef]] = pydantic.Field(None)
    CfnComputeEnvironment: typing.Optional[dict[str, CfnComputeEnvironmentDef]] = pydantic.Field(None)
    CfnJobDefinition: typing.Optional[dict[str, CfnJobDefinitionDef]] = pydantic.Field(None)
    CfnJobQueue: typing.Optional[dict[str, CfnJobQueueDef]] = pydantic.Field(None)
    CfnSchedulingPolicy: typing.Optional[dict[str, CfnSchedulingPolicyDef]] = pydantic.Field(None)
    CfnComputeEnvironmentProps: typing.Optional[dict[str, CfnComputeEnvironmentPropsDef]] = pydantic.Field(None)
    CfnJobDefinitionProps: typing.Optional[dict[str, CfnJobDefinitionPropsDef]] = pydantic.Field(None)
    CfnJobQueueProps: typing.Optional[dict[str, CfnJobQueuePropsDef]] = pydantic.Field(None)
    CfnSchedulingPolicyProps: typing.Optional[dict[str, CfnSchedulingPolicyPropsDef]] = pydantic.Field(None)
    ...
