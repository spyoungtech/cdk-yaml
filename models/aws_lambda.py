from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_lambda.AdotLambdaLayerGenericVersion
class AdotLambdaLayerGenericVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['layer_arn']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AdotLambdaLayerGenericVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.AdotLambdaLayerGenericVersionDefConfig] = pydantic.Field(None)


class AdotLambdaLayerGenericVersionDefConfig(pydantic.BaseModel):
    layer_arn: typing.Optional[list[models.aws_lambda.AdotLambdaLayerGenericVersionDefLayerArnParams]] = pydantic.Field(None, description='The ARN of the Lambda layer.')

class AdotLambdaLayerGenericVersionDefLayerArnParams(pydantic.BaseModel):
    scope: models.AnyResource = pydantic.Field(..., description='The binding scope. Usually this is the stack where the Lambda layer is bound to\n')
    architecture: models.aws_lambda.ArchitectureDef = pydantic.Field(..., description='The architecture of the Lambda layer (either X86_64 or ARM_64).')
    ...


#  autogenerated from aws_cdk.aws_lambda.AdotLambdaLayerJavaAutoInstrumentationVersion
class AdotLambdaLayerJavaAutoInstrumentationVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['layer_arn']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AdotLambdaLayerJavaAutoInstrumentationVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.AdotLambdaLayerJavaAutoInstrumentationVersionDefConfig] = pydantic.Field(None)


class AdotLambdaLayerJavaAutoInstrumentationVersionDefConfig(pydantic.BaseModel):
    layer_arn: typing.Optional[list[models.aws_lambda.AdotLambdaLayerJavaAutoInstrumentationVersionDefLayerArnParams]] = pydantic.Field(None, description='The ARN of the Lambda layer.')

class AdotLambdaLayerJavaAutoInstrumentationVersionDefLayerArnParams(pydantic.BaseModel):
    scope: models.AnyResource = pydantic.Field(..., description='The binding scope. Usually this is the stack where the Lambda layer is bound to\n')
    architecture: models.aws_lambda.ArchitectureDef = pydantic.Field(..., description='The architecture of the Lambda layer (either X86_64 or ARM_64).')
    ...


#  autogenerated from aws_cdk.aws_lambda.AdotLambdaLayerJavaScriptSdkVersion
class AdotLambdaLayerJavaScriptSdkVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['layer_arn']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AdotLambdaLayerJavaScriptSdkVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.AdotLambdaLayerJavaScriptSdkVersionDefConfig] = pydantic.Field(None)


class AdotLambdaLayerJavaScriptSdkVersionDefConfig(pydantic.BaseModel):
    layer_arn: typing.Optional[list[models.aws_lambda.AdotLambdaLayerJavaScriptSdkVersionDefLayerArnParams]] = pydantic.Field(None, description='The ARN of the Lambda layer.')

class AdotLambdaLayerJavaScriptSdkVersionDefLayerArnParams(pydantic.BaseModel):
    scope: models.AnyResource = pydantic.Field(..., description='The binding scope. Usually this is the stack where the Lambda layer is bound to\n')
    architecture: models.aws_lambda.ArchitectureDef = pydantic.Field(..., description='The architecture of the Lambda layer (either X86_64 or ARM_64).')
    ...


#  autogenerated from aws_cdk.aws_lambda.AdotLambdaLayerJavaSdkVersion
class AdotLambdaLayerJavaSdkVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['layer_arn']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AdotLambdaLayerJavaSdkVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.AdotLambdaLayerJavaSdkVersionDefConfig] = pydantic.Field(None)


class AdotLambdaLayerJavaSdkVersionDefConfig(pydantic.BaseModel):
    layer_arn: typing.Optional[list[models.aws_lambda.AdotLambdaLayerJavaSdkVersionDefLayerArnParams]] = pydantic.Field(None, description='The ARN of the Lambda layer.')

class AdotLambdaLayerJavaSdkVersionDefLayerArnParams(pydantic.BaseModel):
    scope: models.AnyResource = pydantic.Field(..., description='The binding scope. Usually this is the stack where the Lambda layer is bound to\n')
    architecture: models.aws_lambda.ArchitectureDef = pydantic.Field(..., description='The architecture of the Lambda layer (either X86_64 or ARM_64).')
    ...


#  autogenerated from aws_cdk.aws_lambda.AdotLambdaLayerPythonSdkVersion
class AdotLambdaLayerPythonSdkVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['layer_arn']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AdotLambdaLayerPythonSdkVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.AdotLambdaLayerPythonSdkVersionDefConfig] = pydantic.Field(None)


class AdotLambdaLayerPythonSdkVersionDefConfig(pydantic.BaseModel):
    layer_arn: typing.Optional[list[models.aws_lambda.AdotLambdaLayerPythonSdkVersionDefLayerArnParams]] = pydantic.Field(None, description='The ARN of the Lambda layer.')

class AdotLambdaLayerPythonSdkVersionDefLayerArnParams(pydantic.BaseModel):
    scope: models.AnyResource = pydantic.Field(..., description='The binding scope. Usually this is the stack where the Lambda layer is bound to\n')
    architecture: models.aws_lambda.ArchitectureDef = pydantic.Field(..., description='The architecture of the Lambda layer (either X86_64 or ARM_64).')
    ...


#  autogenerated from aws_cdk.aws_lambda.AdotLayerVersion
class AdotLayerVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_generic_layer_version', 'from_java_auto_instrumentation_layer_version', 'from_java_script_sdk_layer_version', 'from_java_sdk_layer_version', 'from_python_sdk_layer_version']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AdotLayerVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_generic_layer_version', 'from_java_auto_instrumentation_layer_version', 'from_java_script_sdk_layer_version', 'from_java_sdk_layer_version', 'from_python_sdk_layer_version']
    ...


    from_generic_layer_version: typing.Optional[models.aws_lambda.AdotLayerVersionDefFromGenericLayerVersionParams] = pydantic.Field(None, description='The ADOT Lambda layer for generic use cases.')
    from_java_auto_instrumentation_layer_version: typing.Optional[models.aws_lambda.AdotLayerVersionDefFromJavaAutoInstrumentationLayerVersionParams] = pydantic.Field(None, description='The ADOT Lambda layer for Java auto instrumentation.')
    from_java_script_sdk_layer_version: typing.Optional[models.aws_lambda.AdotLayerVersionDefFromJavaScriptSdkLayerVersionParams] = pydantic.Field(None, description='The ADOT Lambda layer for JavaScript SDK.')
    from_java_sdk_layer_version: typing.Optional[models.aws_lambda.AdotLayerVersionDefFromJavaSdkLayerVersionParams] = pydantic.Field(None, description='The ADOT Lambda layer for Java SDK.')
    from_python_sdk_layer_version: typing.Optional[models.aws_lambda.AdotLayerVersionDefFromPythonSdkLayerVersionParams] = pydantic.Field(None, description='The ADOT Lambda layer for Python SDK.')

class AdotLayerVersionDefFromGenericLayerVersionParams(pydantic.BaseModel):
    version: models.aws_lambda.AdotLambdaLayerGenericVersionDef = pydantic.Field(..., description='The version of the Lambda layer to use.')
    ...

class AdotLayerVersionDefFromJavaAutoInstrumentationLayerVersionParams(pydantic.BaseModel):
    version: models.aws_lambda.AdotLambdaLayerJavaAutoInstrumentationVersionDef = pydantic.Field(..., description='The version of the Lambda layer to use.')
    ...

class AdotLayerVersionDefFromJavaScriptSdkLayerVersionParams(pydantic.BaseModel):
    version: models.aws_lambda.AdotLambdaLayerJavaScriptSdkVersionDef = pydantic.Field(..., description='The version of the Lambda layer to use.')
    ...

class AdotLayerVersionDefFromJavaSdkLayerVersionParams(pydantic.BaseModel):
    version: models.aws_lambda.AdotLambdaLayerJavaSdkVersionDef = pydantic.Field(..., description='The version of the Lambda layer to use.')
    ...

class AdotLayerVersionDefFromPythonSdkLayerVersionParams(pydantic.BaseModel):
    version: models.aws_lambda.AdotLambdaLayerPythonSdkVersionDef = pydantic.Field(..., description='The version of the Lambda layer to use.')
    ...


#  autogenerated from aws_cdk.aws_lambda.Architecture
class ArchitectureDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['custom']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Architecture'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)


class ArchitectureDefConfig(pydantic.BaseModel):
    custom: typing.Optional[list[models.aws_lambda.ArchitectureDefCustomParams]] = pydantic.Field(None, description='Used to specify a custom architecture name.\nUse this if the architecture name is not yet supported by the CDK.')

class ArchitectureDefCustomParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='the architecture name as recognized by AWS Lambda.\n')
    docker_platform: typing.Optional[str] = pydantic.Field(None, description='the platform to use for this architecture when building with Docker.')
    return_config: typing.Optional[list[models.aws_lambda.ArchitectureDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.AssetCode
class AssetCodeDef(BaseClass):
    path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path to the asset file or directory.')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    _init_params: typing.ClassVar[list[str]] = ['path', 'deploy_time', 'readers', 'asset_hash', 'asset_hash_type', 'bundling', 'exclude', 'follow_symlinks', 'ignore_mode']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'bind_to_resource']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AssetCode'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    ...


    from_asset: typing.Optional[models.aws_lambda.AssetCodeDefFromAssetParams] = pydantic.Field(None, description='Loads the function code from a local disk path.')
    from_asset_image: typing.Optional[models.aws_lambda.AssetCodeDefFromAssetImageParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')
    from_bucket: typing.Optional[models.aws_lambda.AssetCodeDefFromBucketParams] = pydantic.Field(None, description='Lambda handler code as an S3 object.')
    from_cfn_parameters: typing.Optional[models.aws_lambda.AssetCodeDefFromCfnParametersParams] = pydantic.Field(None, description='Creates a new Lambda source defined using CloudFormation parameters.')
    from_custom_command: typing.Optional[models.aws_lambda.AssetCodeDefFromCustomCommandParams] = pydantic.Field(None, description='Runs a command to build the code asset that will be used.')
    from_docker_build: typing.Optional[models.aws_lambda.AssetCodeDefFromDockerBuildParams] = pydantic.Field(None, description='Loads the function code from an asset created by a Docker build.\nBy default, the asset is expected to be located at ``/asset`` in the\nimage.')
    from_ecr_image: typing.Optional[models.aws_lambda.AssetCodeDefFromEcrImageParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_inline: typing.Optional[models.aws_lambda.AssetCodeDefFromInlineParams] = pydantic.Field(None, description='Inline code for Lambda handler.')
    resource_config: typing.Optional[models.aws_lambda.AssetCodeDefConfig] = pydantic.Field(None)


class AssetCodeDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_lambda.AssetCodeDefBindParams]] = pydantic.Field(None, description='Called when the lambda or layer is initialized to allow this object to bind to the stack, add resources and have fun.')
    bind_to_resource: typing.Optional[list[models.aws_lambda.AssetCodeDefBindToResourceParams]] = pydantic.Field(None, description="Called after the CFN function resource has been created to allow the code class to bind to it.\nSpecifically it's required to allow assets to add\nmetadata for tooling like SAM CLI to be able to find their origins.")

class AssetCodeDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    ...

class AssetCodeDefBindToResourceParams(pydantic.BaseModel):
    resource: models.CfnResourceDef = pydantic.Field(..., description='-\n')
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    ...

class AssetCodeDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Either a directory with the Lambda code bundle or a .zip file.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetCodeDefFromAssetImageParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetCodeDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.')
    ...

class AssetCodeDefFromCfnParametersParams(pydantic.BaseModel):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n")
    ...

class AssetCodeDefFromCustomCommandParams(pydantic.BaseModel):
    output: str = pydantic.Field(..., description="Where the output of the command will be directed, either a directory or a .zip file with the output Lambda code bundle * For example, if you use the command to run a build script (e.g., [ 'node', 'bundle_code.js' ]), and the build script generates a directory ``/my/lambda/code`` containing code that should be ran in a Lambda function, then output should be set to ``/my/lambda/code``.\n")
    command: typing.Sequence[str] = pydantic.Field(..., description="The command which will be executed to generate the output, for example, [ 'node', 'bundle_code.js' ].\n")
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetCodeDefFromDockerBuildParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to the directory containing the Docker file.\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile')
    ...

class AssetCodeDefFromEcrImageParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class AssetCodeDefFromInlineParams(pydantic.BaseModel):
    code: str = pydantic.Field(..., description='The actual handler code (limited to 4KiB).\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.AssetImageCode
class AssetImageCodeDef(BaseClass):
    directory: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    _init_params: typing.ClassVar[list[str]] = ['directory', 'cmd', 'entrypoint', 'working_directory', 'asset_name', 'build_args', 'build_secrets', 'build_ssh', 'cache_disabled', 'cache_from', 'cache_to', 'file', 'invalidation', 'network_mode', 'outputs', 'platform', 'target', 'extra_hash', 'exclude', 'follow_symlinks', 'ignore_mode']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'bind_to_resource']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AssetImageCode'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    ...


    from_asset: typing.Optional[models.aws_lambda.AssetImageCodeDefFromAssetParams] = pydantic.Field(None, description='Loads the function code from a local disk path.')
    from_asset_image: typing.Optional[models.aws_lambda.AssetImageCodeDefFromAssetImageParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')
    from_bucket: typing.Optional[models.aws_lambda.AssetImageCodeDefFromBucketParams] = pydantic.Field(None, description='Lambda handler code as an S3 object.')
    from_cfn_parameters: typing.Optional[models.aws_lambda.AssetImageCodeDefFromCfnParametersParams] = pydantic.Field(None, description='Creates a new Lambda source defined using CloudFormation parameters.')
    from_custom_command: typing.Optional[models.aws_lambda.AssetImageCodeDefFromCustomCommandParams] = pydantic.Field(None, description='Runs a command to build the code asset that will be used.')
    from_docker_build: typing.Optional[models.aws_lambda.AssetImageCodeDefFromDockerBuildParams] = pydantic.Field(None, description='Loads the function code from an asset created by a Docker build.\nBy default, the asset is expected to be located at ``/asset`` in the\nimage.')
    from_ecr_image: typing.Optional[models.aws_lambda.AssetImageCodeDefFromEcrImageParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_inline: typing.Optional[models.aws_lambda.AssetImageCodeDefFromInlineParams] = pydantic.Field(None, description='Inline code for Lambda handler.')
    resource_config: typing.Optional[models.aws_lambda.AssetImageCodeDefConfig] = pydantic.Field(None)


class AssetImageCodeDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_lambda.AssetImageCodeDefBindParams]] = pydantic.Field(None, description='Called when the lambda or layer is initialized to allow this object to bind to the stack, add resources and have fun.')
    bind_to_resource: typing.Optional[list[models.aws_lambda.AssetImageCodeDefBindToResourceParams]] = pydantic.Field(None, description="Called after the CFN function resource has been created to allow the code class to bind to it.\nSpecifically it's required to allow assets to add\nmetadata for tooling like SAM CLI to be able to find their origins.")

class AssetImageCodeDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    ...

class AssetImageCodeDefBindToResourceParams(pydantic.BaseModel):
    resource: models.CfnResourceDef = pydantic.Field(..., description='-\n')
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    ...

class AssetImageCodeDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Either a directory with the Lambda code bundle or a .zip file.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetImageCodeDefFromAssetImageParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetImageCodeDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.')
    ...

class AssetImageCodeDefFromCfnParametersParams(pydantic.BaseModel):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n")
    ...

class AssetImageCodeDefFromCustomCommandParams(pydantic.BaseModel):
    output: str = pydantic.Field(..., description="Where the output of the command will be directed, either a directory or a .zip file with the output Lambda code bundle * For example, if you use the command to run a build script (e.g., [ 'node', 'bundle_code.js' ]), and the build script generates a directory ``/my/lambda/code`` containing code that should be ran in a Lambda function, then output should be set to ``/my/lambda/code``.\n")
    command: typing.Sequence[str] = pydantic.Field(..., description="The command which will be executed to generate the output, for example, [ 'node', 'bundle_code.js' ].\n")
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class AssetImageCodeDefFromDockerBuildParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to the directory containing the Docker file.\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile')
    ...

class AssetImageCodeDefFromEcrImageParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class AssetImageCodeDefFromInlineParams(pydantic.BaseModel):
    code: str = pydantic.Field(..., description='The actual handler code (limited to 4KiB).\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnParametersCode
class CfnParametersCodeDef(BaseClass):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created")
    _init_params: typing.ClassVar[list[str]] = ['bucket_name_param', 'object_key_param']
    _method_names: typing.ClassVar[list[str]] = ['assign', 'bind', 'bind_to_resource']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnParametersCode'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    ...


    from_asset: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromAssetParams] = pydantic.Field(None, description='Loads the function code from a local disk path.')
    from_asset_image: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromAssetImageParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')
    from_bucket: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromBucketParams] = pydantic.Field(None, description='Lambda handler code as an S3 object.')
    from_cfn_parameters: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromCfnParametersParams] = pydantic.Field(None, description='Creates a new Lambda source defined using CloudFormation parameters.')
    from_custom_command: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromCustomCommandParams] = pydantic.Field(None, description='Runs a command to build the code asset that will be used.')
    from_docker_build: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromDockerBuildParams] = pydantic.Field(None, description='Loads the function code from an asset created by a Docker build.\nBy default, the asset is expected to be located at ``/asset`` in the\nimage.')
    from_ecr_image: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromEcrImageParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_inline: typing.Optional[models.aws_lambda.CfnParametersCodeDefFromInlineParams] = pydantic.Field(None, description='Inline code for Lambda handler.')
    resource_config: typing.Optional[models.aws_lambda.CfnParametersCodeDefConfig] = pydantic.Field(None)


class CfnParametersCodeDefConfig(pydantic.BaseModel):
    assign: typing.Optional[list[models.aws_lambda.CfnParametersCodeDefAssignParams]] = pydantic.Field(None, description="Create a parameters map from this instance's CloudFormation parameters.\nIt returns a map with 2 keys that correspond to the names of the parameters defined in this Lambda code,\nand as values it contains the appropriate expressions pointing at the provided S3 location\n(most likely, obtained from a CodePipeline Artifact by calling the ``artifact.s3Location`` method).\nThe result should be provided to the CloudFormation Action\nthat is deploying the Stack that the Lambda with this code is part of,\nin the ``parameterOverrides`` property.")
    bind: typing.Optional[list[models.aws_lambda.CfnParametersCodeDefBindParams]] = pydantic.Field(None, description='Called when the lambda or layer is initialized to allow this object to bind to the stack, add resources and have fun.')
    bind_to_resource: typing.Optional[list[models.aws_lambda.CfnParametersCodeDefBindToResourceParams]] = pydantic.Field(None, description="Called after the CFN function resource has been created to allow the code class to bind to it.\nSpecifically it's required to allow assets to add\nmetadata for tooling like SAM CLI to be able to find their origins.")

class CfnParametersCodeDefAssignParams(pydantic.BaseModel):
    bucket_name: str = pydantic.Field(..., description='The name of the S3 Bucket the object is in.\n')
    object_key: str = pydantic.Field(..., description='The path inside the Bucket where the object is located at.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='The S3 object version.')
    ...

class CfnParametersCodeDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    ...

class CfnParametersCodeDefBindToResourceParams(pydantic.BaseModel):
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    ...

class CfnParametersCodeDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Either a directory with the Lambda code bundle or a .zip file.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class CfnParametersCodeDefFromAssetImageParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class CfnParametersCodeDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.')
    ...

class CfnParametersCodeDefFromCfnParametersParams(pydantic.BaseModel):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n")
    ...

class CfnParametersCodeDefFromCustomCommandParams(pydantic.BaseModel):
    output: str = pydantic.Field(..., description="Where the output of the command will be directed, either a directory or a .zip file with the output Lambda code bundle * For example, if you use the command to run a build script (e.g., [ 'node', 'bundle_code.js' ]), and the build script generates a directory ``/my/lambda/code`` containing code that should be ran in a Lambda function, then output should be set to ``/my/lambda/code``.\n")
    command: typing.Sequence[str] = pydantic.Field(..., description="The command which will be executed to generate the output, for example, [ 'node', 'bundle_code.js' ].\n")
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class CfnParametersCodeDefFromDockerBuildParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to the directory containing the Docker file.\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile')
    ...

class CfnParametersCodeDefFromEcrImageParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class CfnParametersCodeDefFromInlineParams(pydantic.BaseModel):
    code: str = pydantic.Field(..., description='The actual handler code (limited to 4KiB).\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.Code
class CodeDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind', 'bind_to_resource']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Code'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    ...


    from_asset: typing.Optional[models.aws_lambda.CodeDefFromAssetParams] = pydantic.Field(None, description='Loads the function code from a local disk path.')
    from_asset_image: typing.Optional[models.aws_lambda.CodeDefFromAssetImageParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')
    from_bucket: typing.Optional[models.aws_lambda.CodeDefFromBucketParams] = pydantic.Field(None, description='Lambda handler code as an S3 object.')
    from_cfn_parameters: typing.Optional[models.aws_lambda.CodeDefFromCfnParametersParams] = pydantic.Field(None, description='Creates a new Lambda source defined using CloudFormation parameters.')
    from_custom_command: typing.Optional[models.aws_lambda.CodeDefFromCustomCommandParams] = pydantic.Field(None, description='Runs a command to build the code asset that will be used.')
    from_docker_build: typing.Optional[models.aws_lambda.CodeDefFromDockerBuildParams] = pydantic.Field(None, description='Loads the function code from an asset created by a Docker build.\nBy default, the asset is expected to be located at ``/asset`` in the\nimage.')
    from_ecr_image: typing.Optional[models.aws_lambda.CodeDefFromEcrImageParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_inline: typing.Optional[models.aws_lambda.CodeDefFromInlineParams] = pydantic.Field(None, description='Inline code for Lambda handler.')
    resource_config: typing.Optional[models.aws_lambda.CodeDefConfig] = pydantic.Field(None)


class CodeDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_lambda.CodeDefBindParams]] = pydantic.Field(None, description='Called when the lambda or layer is initialized to allow this object to bind to the stack, add resources and have fun.')
    bind_to_resource: typing.Optional[list[models.aws_lambda.CodeDefBindToResourceParams]] = pydantic.Field(None, description="Called after the CFN function resource has been created to allow the code class to bind to it.\nSpecifically it's required to allow assets to add\nmetadata for tooling like SAM CLI to be able to find their origins.")

class CodeDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description="The binding scope. Don't be smart about trying to down-cast or assume it's initialized. You may just use it as a construct scope.")
    ...

class CodeDefBindToResourceParams(pydantic.BaseModel):
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    ...

class CodeDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Either a directory with the Lambda code bundle or a .zip file.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class CodeDefFromAssetImageParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class CodeDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.')
    ...

class CodeDefFromCfnParametersParams(pydantic.BaseModel):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n")
    ...

class CodeDefFromCustomCommandParams(pydantic.BaseModel):
    output: str = pydantic.Field(..., description="Where the output of the command will be directed, either a directory or a .zip file with the output Lambda code bundle * For example, if you use the command to run a build script (e.g., [ 'node', 'bundle_code.js' ]), and the build script generates a directory ``/my/lambda/code`` containing code that should be ran in a Lambda function, then output should be set to ``/my/lambda/code``.\n")
    command: typing.Sequence[str] = pydantic.Field(..., description="The command which will be executed to generate the output, for example, [ 'node', 'bundle_code.js' ].\n")
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class CodeDefFromDockerBuildParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to the directory containing the Docker file.\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile')
    ...

class CodeDefFromEcrImageParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class CodeDefFromInlineParams(pydantic.BaseModel):
    code: str = pydantic.Field(..., description='The actual handler code (limited to 4KiB).\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.DockerImageCode
class DockerImageCodeDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_ecr', 'from_image_asset']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.DockerImageCode'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_ecr', 'from_image_asset']
    ...


    from_ecr: typing.Optional[models.aws_lambda.DockerImageCodeDefFromEcrParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_image_asset: typing.Optional[models.aws_lambda.DockerImageCodeDefFromImageAssetParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')

class DockerImageCodeDefFromEcrParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class DockerImageCodeDefFromImageAssetParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...


#  autogenerated from aws_cdk.aws_lambda.EcrImageCode
class EcrImageCodeDef(BaseClass):
    repository: typing.Union[_REQUIRED_INIT_PARAM, models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    _init_params: typing.ClassVar[list[str]] = ['repository', 'cmd', 'entrypoint', 'tag', 'tag_or_digest', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'bind_to_resource']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EcrImageCode'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    ...


    from_asset: typing.Optional[models.aws_lambda.EcrImageCodeDefFromAssetParams] = pydantic.Field(None, description='Loads the function code from a local disk path.')
    from_asset_image: typing.Optional[models.aws_lambda.EcrImageCodeDefFromAssetImageParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')
    from_bucket: typing.Optional[models.aws_lambda.EcrImageCodeDefFromBucketParams] = pydantic.Field(None, description='Lambda handler code as an S3 object.')
    from_cfn_parameters: typing.Optional[models.aws_lambda.EcrImageCodeDefFromCfnParametersParams] = pydantic.Field(None, description='Creates a new Lambda source defined using CloudFormation parameters.')
    from_custom_command: typing.Optional[models.aws_lambda.EcrImageCodeDefFromCustomCommandParams] = pydantic.Field(None, description='Runs a command to build the code asset that will be used.')
    from_docker_build: typing.Optional[models.aws_lambda.EcrImageCodeDefFromDockerBuildParams] = pydantic.Field(None, description='Loads the function code from an asset created by a Docker build.\nBy default, the asset is expected to be located at ``/asset`` in the\nimage.')
    from_ecr_image: typing.Optional[models.aws_lambda.EcrImageCodeDefFromEcrImageParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_inline: typing.Optional[models.aws_lambda.EcrImageCodeDefFromInlineParams] = pydantic.Field(None, description='Inline code for Lambda handler.')
    resource_config: typing.Optional[models.aws_lambda.EcrImageCodeDefConfig] = pydantic.Field(None)


class EcrImageCodeDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_lambda.EcrImageCodeDefBindParams]] = pydantic.Field(None, description='Called when the lambda or layer is initialized to allow this object to bind to the stack, add resources and have fun.')
    bind_to_resource: typing.Optional[list[models.aws_lambda.EcrImageCodeDefBindToResourceParams]] = pydantic.Field(None, description="Called after the CFN function resource has been created to allow the code class to bind to it.\nSpecifically it's required to allow assets to add\nmetadata for tooling like SAM CLI to be able to find their origins.")

class EcrImageCodeDefBindParams(pydantic.BaseModel):
    ...

class EcrImageCodeDefBindToResourceParams(pydantic.BaseModel):
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    ...

class EcrImageCodeDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Either a directory with the Lambda code bundle or a .zip file.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class EcrImageCodeDefFromAssetImageParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class EcrImageCodeDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.')
    ...

class EcrImageCodeDefFromCfnParametersParams(pydantic.BaseModel):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n")
    ...

class EcrImageCodeDefFromCustomCommandParams(pydantic.BaseModel):
    output: str = pydantic.Field(..., description="Where the output of the command will be directed, either a directory or a .zip file with the output Lambda code bundle * For example, if you use the command to run a build script (e.g., [ 'node', 'bundle_code.js' ]), and the build script generates a directory ``/my/lambda/code`` containing code that should be ran in a Lambda function, then output should be set to ``/my/lambda/code``.\n")
    command: typing.Sequence[str] = pydantic.Field(..., description="The command which will be executed to generate the output, for example, [ 'node', 'bundle_code.js' ].\n")
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class EcrImageCodeDefFromDockerBuildParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to the directory containing the Docker file.\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile')
    ...

class EcrImageCodeDefFromEcrImageParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class EcrImageCodeDefFromInlineParams(pydantic.BaseModel):
    code: str = pydantic.Field(..., description='The actual handler code (limited to 4KiB).\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.FileSystem
class FileSystemDef(BaseClass):
    arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='ARN of the access point.')
    local_mount_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='mount path in the lambda runtime environment.\n')
    connections: typing.Optional[models.aws_ec2.ConnectionsDef] = pydantic.Field(None, description='connections object used to allow ingress traffic from lambda function. Default: - no connections required to add extra ingress rules for Lambda function\n')
    dependency: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='array of IDependable that lambda function depends on. Default: - no dependency\n')
    policies: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='additional IAM policies required for the lambda function. Default: - no additional policies required')
    _init_params: typing.ClassVar[list[str]] = ['arn', 'local_mount_path', 'connections', 'dependency', 'policies']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_efs_access_point']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FileSystem'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_efs_access_point']
    ...


    from_efs_access_point: typing.Optional[models.aws_lambda.FileSystemDefFromEfsAccessPointParams] = pydantic.Field(None, description='mount the filesystem from Amazon EFS.')

class FileSystemDefFromEfsAccessPointParams(pydantic.BaseModel):
    ap: typing.Union[models.aws_efs.AccessPointDef] = pydantic.Field(..., description='the Amazon EFS access point.\n')
    mount_path: str = pydantic.Field(..., description='the target path in the lambda runtime environment.')
    ...


#  autogenerated from aws_cdk.aws_lambda.FilterCriteria
class FilterCriteriaDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['filter']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FilterCriteria'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.FilterCriteriaDefConfig] = pydantic.Field(None)


class FilterCriteriaDefConfig(pydantic.BaseModel):
    filter: typing.Optional[list[models.aws_lambda.FilterCriteriaDefFilterParams]] = pydantic.Field(None, description='Filter for event source.')

class FilterCriteriaDefFilterParams(pydantic.BaseModel):
    filter: typing.Mapping[str, typing.Any] = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda.FilterRule
class FilterRuleDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['begins_with', 'between', 'empty', 'exists', 'not_equals', 'not_exists', 'null', 'or_']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FilterRule'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.FilterRuleDefConfig] = pydantic.Field(None)


class FilterRuleDefConfig(pydantic.BaseModel):
    begins_with: typing.Optional[list[models.aws_lambda.FilterRuleDefBeginsWithParams]] = pydantic.Field(None, description='Begins with comparison operator.')
    between: typing.Optional[list[models.aws_lambda.FilterRuleDefBetweenParams]] = pydantic.Field(None, description='Numeric range comparison operator.')
    empty: typing.Optional[bool] = pydantic.Field(None, description='Empty comparison operator.')
    exists: typing.Optional[bool] = pydantic.Field(None, description='Exists comparison operator.')
    not_equals: typing.Optional[list[models.aws_lambda.FilterRuleDefNotEqualsParams]] = pydantic.Field(None, description='Not equals comparison operator.')
    not_exists: typing.Optional[bool] = pydantic.Field(None, description='Not exists comparison operator.')
    null: typing.Optional[bool] = pydantic.Field(None, description='Null comparison operator.')
    or_: typing.Optional[list[models.aws_lambda.FilterRuleDefOrParams]] = pydantic.Field(None, description='Or comparison operator.')

class FilterRuleDefBeginsWithParams(pydantic.BaseModel):
    elem: str = pydantic.Field(..., description='-')
    ...

class FilterRuleDefBetweenParams(pydantic.BaseModel):
    first: typing.Union[int, float] = pydantic.Field(..., description='-\n')
    second: typing.Union[int, float] = pydantic.Field(..., description='-')
    ...

class FilterRuleDefNotEqualsParams(pydantic.BaseModel):
    elem: str = pydantic.Field(..., description='-')
    ...

class FilterRuleDefOrParams(pydantic.BaseModel):
    elem: list[str] = pydantic.Field(...)
    ...


#  autogenerated from aws_cdk.aws_lambda.FunctionBase
class FunctionBaseDef(BaseClass):
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID this resource belongs to. Default: - the resource is in the same account as the stack it belongs to\n')
    environment_from_arn: typing.Optional[str] = pydantic.Field(None, description='ARN to deduce region and account from. The ARN is parsed and the account and region are taken from the ARN. This should be used for imported resources. Cannot be supplied together with either ``account`` or ``region``. Default: - take environment from ``account``, ``region`` parameters, or use Stack environment.\n')
    physical_name: typing.Optional[str] = pydantic.Field(None, description='The value passed in by users to the physical name prop of the resource. - ``undefined`` implies that a physical name will be allocated by CloudFormation during deployment. - a concrete value implies a specific physical name - ``PhysicalName.GENERATE_IF_NEEDED`` is a marker that indicates that a physical will only be generated by the CDK if it is needed for cross-environment references. Otherwise, it will be allocated by CloudFormation. Default: - The physical name will be allocated by CloudFormation at deployment time\n')
    region: typing.Optional[str] = pydantic.Field(None, description='The AWS region this resource belongs to. Default: - the resource is in the same region as the stack it belongs to')
    _init_params: typing.ClassVar[list[str]] = ['account', 'environment_from_arn', 'physical_name', 'region']
    _method_names: typing.ClassVar[list[str]] = ['add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionBase'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.FunctionBaseDefConfig] = pydantic.Field(None)


class FunctionBaseDefConfig(pydantic.BaseModel):
    add_event_source: typing.Optional[list[models.aws_lambda.FunctionBaseDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda.FunctionBaseDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda.FunctionBaseDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda.FunctionBaseDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda.FunctionBaseDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    configure_async_invoke: typing.Optional[list[models.aws_lambda.FunctionBaseDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[list[models.aws_lambda.FunctionBaseDefConsiderWarningOnInvokeFunctionPermissionsParams]] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    grant_invoke: typing.Optional[list[models.aws_lambda.FunctionBaseDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda.FunctionBaseDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_lambda.FunctionBaseDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda.FunctionBaseDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_lambda.FunctionBaseDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    metric: typing.Optional[list[models.aws_lambda.FunctionBaseDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda.FunctionBaseDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda.FunctionBaseDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda.FunctionBaseDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda.FunctionBaseDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)

class FunctionBaseDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class FunctionBaseDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class FunctionBaseDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class FunctionBaseDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class FunctionBaseDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class FunctionBaseDefConsiderWarningOnInvokeFunctionPermissionsParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    action: str = pydantic.Field(..., description='-')
    ...

class FunctionBaseDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class FunctionBaseDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionBaseDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.FunctionVersionUpgrade
class FunctionVersionUpgradeDef(BaseClass):
    feature_flag: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['feature_flag', 'enabled']
    _method_names: typing.ClassVar[list[str]] = ['visit']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionVersionUpgrade'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.FunctionVersionUpgradeDefConfig] = pydantic.Field(None)


class FunctionVersionUpgradeDefConfig(pydantic.BaseModel):
    visit: typing.Optional[list[models.aws_lambda.FunctionVersionUpgradeDefVisitParams]] = pydantic.Field(None, description='All aspects can visit an IConstruct.')

class FunctionVersionUpgradeDefVisitParams(pydantic.BaseModel):
    node: models.AnyResource = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda.Handler
class HandlerDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Handler'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.InlineCode
class InlineCodeDef(BaseClass):
    code: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['code']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'bind_to_resource']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.InlineCode'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    ...


    from_asset: typing.Optional[models.aws_lambda.InlineCodeDefFromAssetParams] = pydantic.Field(None, description='Loads the function code from a local disk path.')
    from_asset_image: typing.Optional[models.aws_lambda.InlineCodeDefFromAssetImageParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')
    from_bucket: typing.Optional[models.aws_lambda.InlineCodeDefFromBucketParams] = pydantic.Field(None, description='Lambda handler code as an S3 object.')
    from_cfn_parameters: typing.Optional[models.aws_lambda.InlineCodeDefFromCfnParametersParams] = pydantic.Field(None, description='Creates a new Lambda source defined using CloudFormation parameters.')
    from_custom_command: typing.Optional[models.aws_lambda.InlineCodeDefFromCustomCommandParams] = pydantic.Field(None, description='Runs a command to build the code asset that will be used.')
    from_docker_build: typing.Optional[models.aws_lambda.InlineCodeDefFromDockerBuildParams] = pydantic.Field(None, description='Loads the function code from an asset created by a Docker build.\nBy default, the asset is expected to be located at ``/asset`` in the\nimage.')
    from_ecr_image: typing.Optional[models.aws_lambda.InlineCodeDefFromEcrImageParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_inline: typing.Optional[models.aws_lambda.InlineCodeDefFromInlineParams] = pydantic.Field(None, description='Inline code for Lambda handler.')
    resource_config: typing.Optional[models.aws_lambda.InlineCodeDefConfig] = pydantic.Field(None)


class InlineCodeDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_lambda.InlineCodeDefBindParams]] = pydantic.Field(None, description='Called when the lambda or layer is initialized to allow this object to bind to the stack, add resources and have fun.')
    bind_to_resource: typing.Optional[list[models.aws_lambda.InlineCodeDefBindToResourceParams]] = pydantic.Field(None, description="Called after the CFN function resource has been created to allow the code class to bind to it.\nSpecifically it's required to allow assets to add\nmetadata for tooling like SAM CLI to be able to find their origins.")

class InlineCodeDefBindParams(pydantic.BaseModel):
    ...

class InlineCodeDefBindToResourceParams(pydantic.BaseModel):
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    ...

class InlineCodeDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Either a directory with the Lambda code bundle or a .zip file.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class InlineCodeDefFromAssetImageParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class InlineCodeDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.')
    ...

class InlineCodeDefFromCfnParametersParams(pydantic.BaseModel):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n")
    ...

class InlineCodeDefFromCustomCommandParams(pydantic.BaseModel):
    output: str = pydantic.Field(..., description="Where the output of the command will be directed, either a directory or a .zip file with the output Lambda code bundle * For example, if you use the command to run a build script (e.g., [ 'node', 'bundle_code.js' ]), and the build script generates a directory ``/my/lambda/code`` containing code that should be ran in a Lambda function, then output should be set to ``/my/lambda/code``.\n")
    command: typing.Sequence[str] = pydantic.Field(..., description="The command which will be executed to generate the output, for example, [ 'node', 'bundle_code.js' ].\n")
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class InlineCodeDefFromDockerBuildParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to the directory containing the Docker file.\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile')
    ...

class InlineCodeDefFromEcrImageParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class InlineCodeDefFromInlineParams(pydantic.BaseModel):
    code: str = pydantic.Field(..., description='The actual handler code (limited to 4KiB).\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.LambdaInsightsVersion
class LambdaInsightsVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_insight_version_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LambdaInsightsVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_insight_version_arn']
    ...


    from_insight_version_arn: typing.Optional[models.aws_lambda.LambdaInsightsVersionDefFromInsightVersionArnParams] = pydantic.Field(None, description='Use the insights extension associated with the provided ARN.\nMake sure the ARN is associated\nwith same region as your function')

class LambdaInsightsVersionDefFromInsightVersionArnParams(pydantic.BaseModel):
    arn: str = pydantic.Field(..., description='-\n\n:see: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights-extension-versions.html\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.ParamsAndSecretsLayerVersion
class ParamsAndSecretsLayerVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_version', 'from_version_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.ParamsAndSecretsLayerVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_version', 'from_version_arn']
    ...


    from_version: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDefFromVersionParams] = pydantic.Field(None, description='Use a specific version of the Parameters and Secrets Extension to generate a layer version.')
    from_version_arn: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDefFromVersionArnParams] = pydantic.Field(None, description='Use the Parameters and Secrets Extension associated with the provided ARN.\nMake sure the ARN is associated\nwith the same region and architecture as your function.')

class ParamsAndSecretsLayerVersionDefFromVersionParams(pydantic.BaseModel):
    version: aws_cdk.aws_lambda.ParamsAndSecretsVersions = pydantic.Field(..., description='-\n')
    cache_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether the Parameters and Secrets Extension will cache parameters and secrets. Default: true\n')
    cache_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of secrets and parameters to cache. Must be a value from 0 to 1000. A value of 0 means there is no caching. Note: This variable is ignored if parameterStoreTtl and secretsManagerTtl are 0. Default: 1000\n')
    http_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the local HTTP server. Valid port numbers are 1 - 65535. Default: 2773\n')
    log_level: typing.Optional[aws_cdk.aws_lambda.ParamsAndSecretsLogLevel] = pydantic.Field(None, description='The level of logging provided by the Parameters and Secrets Extension. Note: Set to debug to see the cache configuration. Default: - Logging level will be ``info``\n')
    max_connections: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of connection for HTTP clients that the Parameters and Secrets Extension uses to make requests to Parameter Store or Secrets Manager. There is no maximum limit. Minimum is 1. Note: Every running copy of this Lambda function may open the number of connections specified by this property. Thus, the total number of connections may exceed this number. Default: 3\n')
    parameter_store_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout for requests to Parameter Store. A value of 0 means that there is no timeout. Default: 0\n')
    parameter_store_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time-to-live of a parameter in the cache. A value of 0 means there is no caching. The maximum time-to-live is 300 seconds. Note: This variable is ignored if cacheSize is 0. Default: 300 seconds\n')
    secrets_manager_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout for requests to Secrets Manager. A value of 0 means that there is no timeout. Default: 0\n')
    secrets_manager_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time-to-live of a secret in the cache. A value of 0 means there is no caching. The maximum time-to-live is 300 seconds. Note: This variable is ignored if cacheSize is 0. Default: 300 seconds')
    ...

class ParamsAndSecretsLayerVersionDefFromVersionArnParams(pydantic.BaseModel):
    arn: str = pydantic.Field(..., description='-\n')
    cache_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether the Parameters and Secrets Extension will cache parameters and secrets. Default: true\n')
    cache_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of secrets and parameters to cache. Must be a value from 0 to 1000. A value of 0 means there is no caching. Note: This variable is ignored if parameterStoreTtl and secretsManagerTtl are 0. Default: 1000\n')
    http_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the local HTTP server. Valid port numbers are 1 - 65535. Default: 2773\n')
    log_level: typing.Optional[aws_cdk.aws_lambda.ParamsAndSecretsLogLevel] = pydantic.Field(None, description='The level of logging provided by the Parameters and Secrets Extension. Note: Set to debug to see the cache configuration. Default: - Logging level will be ``info``\n')
    max_connections: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of connection for HTTP clients that the Parameters and Secrets Extension uses to make requests to Parameter Store or Secrets Manager. There is no maximum limit. Minimum is 1. Note: Every running copy of this Lambda function may open the number of connections specified by this property. Thus, the total number of connections may exceed this number. Default: 3\n')
    parameter_store_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout for requests to Parameter Store. A value of 0 means that there is no timeout. Default: 0\n')
    parameter_store_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time-to-live of a parameter in the cache. A value of 0 means there is no caching. The maximum time-to-live is 300 seconds. Note: This variable is ignored if cacheSize is 0. Default: 300 seconds\n')
    secrets_manager_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout for requests to Secrets Manager. A value of 0 means that there is no timeout. Default: 0\n')
    secrets_manager_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time-to-live of a secret in the cache. A value of 0 means there is no caching. The maximum time-to-live is 300 seconds. Note: This variable is ignored if cacheSize is 0. Default: 300 seconds\n\n:see: https://docs.aws.amazon.com/secretsmanager/latest/userguide/retrieving-secrets_lambda.html#retrieving-secrets_lambda_ARNs\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.QualifiedFunctionBase
class QualifiedFunctionBaseDef(BaseClass):
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID this resource belongs to. Default: - the resource is in the same account as the stack it belongs to\n')
    environment_from_arn: typing.Optional[str] = pydantic.Field(None, description='ARN to deduce region and account from. The ARN is parsed and the account and region are taken from the ARN. This should be used for imported resources. Cannot be supplied together with either ``account`` or ``region``. Default: - take environment from ``account``, ``region`` parameters, or use Stack environment.\n')
    physical_name: typing.Optional[str] = pydantic.Field(None, description='The value passed in by users to the physical name prop of the resource. - ``undefined`` implies that a physical name will be allocated by CloudFormation during deployment. - a concrete value implies a specific physical name - ``PhysicalName.GENERATE_IF_NEEDED`` is a marker that indicates that a physical will only be generated by the CDK if it is needed for cross-environment references. Otherwise, it will be allocated by CloudFormation. Default: - The physical name will be allocated by CloudFormation at deployment time\n')
    region: typing.Optional[str] = pydantic.Field(None, description='The AWS region this resource belongs to. Default: - the resource is in the same region as the stack it belongs to')
    _init_params: typing.ClassVar[list[str]] = ['account', 'environment_from_arn', 'physical_name', 'region']
    _method_names: typing.ClassVar[list[str]] = ['add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.QualifiedFunctionBase'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.QualifiedFunctionBaseDefConfig] = pydantic.Field(None)


class QualifiedFunctionBaseDefConfig(pydantic.BaseModel):
    add_event_source: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    configure_async_invoke: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[bool] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    grant_invoke: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    metric: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda.QualifiedFunctionBaseDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    lambda__config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)

class QualifiedFunctionBaseDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class QualifiedFunctionBaseDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class QualifiedFunctionBaseDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class QualifiedFunctionBaseDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class QualifiedFunctionBaseDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class QualifiedFunctionBaseDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class QualifiedFunctionBaseDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class QualifiedFunctionBaseDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.Runtime
class RuntimeDef(BaseClass):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    family: typing.Optional[aws_cdk.aws_lambda.RuntimeFamily] = pydantic.Field(None, description='-\n')
    bundling_docker_image: typing.Optional[str] = pydantic.Field(None, description='The Docker image name to be used for bundling in this runtime. Default: - the latest docker image "amazon/public.ecr.aws/sam/build-" from https://gallery.ecr.aws\n')
    is_variable: typing.Optional[bool] = pydantic.Field(None, description='Whether the runtime enum is meant to change over time, IE NODEJS_LATEST. Default: false\n')
    supports_code_guru_profiling: typing.Optional[bool] = pydantic.Field(None, description='Whether this runtime is integrated with and supported for profiling using Amazon CodeGuru Profiler. Default: false\n')
    supports_inline_code: typing.Optional[bool] = pydantic.Field(None, description='Whether the ``ZipFile`` (aka inline code) property can be used with this runtime. Default: false\n')
    supports_snap_start: typing.Optional[bool] = pydantic.Field(None, description='Whether this runtime supports SnapStart. Default: false')
    _init_params: typing.ClassVar[list[str]] = ['name', 'family', 'bundling_docker_image', 'is_variable', 'supports_code_guru_profiling', 'supports_inline_code', 'supports_snap_start']
    _method_names: typing.ClassVar[list[str]] = ['runtime_equals']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Runtime'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)


class RuntimeDefConfig(pydantic.BaseModel):
    runtime_equals: typing.Optional[list[models.aws_lambda.RuntimeDefRuntimeEqualsParams]] = pydantic.Field(None, description='')
    bundling_image_config: typing.Optional[models.core.DockerImageDefConfig] = pydantic.Field(None)

class RuntimeDefRuntimeEqualsParams(pydantic.BaseModel):
    other: models.aws_lambda.RuntimeDef = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda.RuntimeManagementMode
class RuntimeManagementModeDef(BaseClass):
    mode: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    arn: typing.Optional[str] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['mode', 'arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['manual']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.RuntimeManagementMode'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.RuntimeManagementModeDefConfig] = pydantic.Field(None)


class RuntimeManagementModeDefConfig(pydantic.BaseModel):
    manual: typing.Optional[list[models.aws_lambda.RuntimeManagementModeDefManualParams]] = pydantic.Field(None, description='You specify a runtime version in your function configuration.\nThe function uses this runtime version indefinitely.\nIn the rare case in which a new runtime version is incompatible with an existing function,\nyou can use this mode to roll back your function to an earlier runtime version.')

class RuntimeManagementModeDefManualParams(pydantic.BaseModel):
    arn: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_lambda.RuntimeManagementModeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.S3Code
class S3CodeDef(BaseClass):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='-')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'key', 'object_version']
    _method_names: typing.ClassVar[list[str]] = ['bind', 'bind_to_resource']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.S3Code'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_asset', 'from_asset_image', 'from_bucket', 'from_cfn_parameters', 'from_custom_command', 'from_docker_build', 'from_ecr_image', 'from_inline']
    ...


    from_asset: typing.Optional[models.aws_lambda.S3CodeDefFromAssetParams] = pydantic.Field(None, description='Loads the function code from a local disk path.')
    from_asset_image: typing.Optional[models.aws_lambda.S3CodeDefFromAssetImageParams] = pydantic.Field(None, description='Create an ECR image from the specified asset and bind it as the Lambda code.')
    from_bucket: typing.Optional[models.aws_lambda.S3CodeDefFromBucketParams] = pydantic.Field(None, description='Lambda handler code as an S3 object.')
    from_cfn_parameters: typing.Optional[models.aws_lambda.S3CodeDefFromCfnParametersParams] = pydantic.Field(None, description='Creates a new Lambda source defined using CloudFormation parameters.')
    from_custom_command: typing.Optional[models.aws_lambda.S3CodeDefFromCustomCommandParams] = pydantic.Field(None, description='Runs a command to build the code asset that will be used.')
    from_docker_build: typing.Optional[models.aws_lambda.S3CodeDefFromDockerBuildParams] = pydantic.Field(None, description='Loads the function code from an asset created by a Docker build.\nBy default, the asset is expected to be located at ``/asset`` in the\nimage.')
    from_ecr_image: typing.Optional[models.aws_lambda.S3CodeDefFromEcrImageParams] = pydantic.Field(None, description='Use an existing ECR image as the Lambda code.')
    from_inline: typing.Optional[models.aws_lambda.S3CodeDefFromInlineParams] = pydantic.Field(None, description='Inline code for Lambda handler.')
    resource_config: typing.Optional[models.aws_lambda.S3CodeDefConfig] = pydantic.Field(None)


class S3CodeDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[models.aws_lambda.S3CodeDefBindParams]] = pydantic.Field(None, description='Called when the lambda or layer is initialized to allow this object to bind to the stack, add resources and have fun.')
    bind_to_resource: typing.Optional[list[models.aws_lambda.S3CodeDefBindToResourceParams]] = pydantic.Field(None, description="Called after the CFN function resource has been created to allow the code class to bind to it.\nSpecifically it's required to allow assets to add\nmetadata for tooling like SAM CLI to be able to find their origins.")

class S3CodeDefBindParams(pydantic.BaseModel):
    ...

class S3CodeDefBindToResourceParams(pydantic.BaseModel):
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    ...

class S3CodeDefFromAssetParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='Either a directory with the Lambda code bundle or a .zip file.\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class S3CodeDefFromAssetImageParams(pydantic.BaseModel):
    directory: str = pydantic.Field(..., description='the directory from which the asset must be created.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class S3CodeDefFromBucketParams(pydantic.BaseModel):
    bucket: typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(..., description='The S3 bucket.\n')
    key: str = pydantic.Field(..., description='The object key.\n')
    object_version: typing.Optional[str] = pydantic.Field(None, description='Optional S3 object version.')
    ...

class S3CodeDefFromCfnParametersParams(pydantic.BaseModel):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n")
    ...

class S3CodeDefFromCustomCommandParams(pydantic.BaseModel):
    output: str = pydantic.Field(..., description="Where the output of the command will be directed, either a directory or a .zip file with the output Lambda code bundle * For example, if you use the command to run a build script (e.g., [ 'node', 'bundle_code.js' ]), and the build script generates a directory ``/my/lambda/code`` containing code that should be ran in a Lambda function, then output should be set to ``/my/lambda/code``.\n")
    command: typing.Sequence[str] = pydantic.Field(..., description="The command which will be executed to generate the output, for example, [ 'node', 'bundle_code.js' ].\n")
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB')
    ...

class S3CodeDefFromDockerBuildParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path to the directory containing the Docker file.\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile')
    ...

class S3CodeDefFromEcrImageParams(pydantic.BaseModel):
    repository: typing.Union[models.aws_ecr.RepositoryBaseDef, models.aws_ecr.RepositoryDef] = pydantic.Field(..., description='the ECR repository that the image is in.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.')
    ...

class S3CodeDefFromInlineParams(pydantic.BaseModel):
    code: str = pydantic.Field(..., description='The actual handler code (limited to 4KiB).\n')
    ...


#  autogenerated from aws_cdk.aws_lambda.SnapStartConf
class SnapStartConfDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.SnapStartConf'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.SourceAccessConfigurationType
class SourceAccessConfigurationTypeDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.SourceAccessConfigurationType'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.SourceAccessConfigurationTypeDefConfig] = pydantic.Field(None)


class SourceAccessConfigurationTypeDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[models.aws_lambda.SourceAccessConfigurationTypeDefOfParams]] = pydantic.Field(None, description='A custom source access configuration property.')

class SourceAccessConfigurationTypeDefOfParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_lambda.SourceAccessConfigurationTypeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.Alias
class AliasDef(BaseConstruct):
    alias_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name of this alias.\n')
    version: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.VersionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Function version this alias refers to. Use lambda.currentVersion to reference a version with your latest changes.\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['alias_name', 'version', 'additional_versions', 'description', 'provisioned_concurrent_executions', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_auto_scaling', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_alias_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Alias'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_alias_attributes']
    ...


    from_alias_attributes: typing.Optional[models.aws_lambda.AliasDefFromAliasAttributesParams] = pydantic.Field(None, description='')
    resource_config: typing.Optional[models.aws_lambda.AliasDefConfig] = pydantic.Field(None)


class AliasDefConfig(pydantic.BaseModel):
    add_auto_scaling: typing.Optional[list[models.aws_lambda.AliasDefAddAutoScalingParams]] = pydantic.Field(None, description='Configure provisioned concurrency autoscaling on a function alias.\nReturns a scalable attribute that can call\n``scaleOnUtilization()`` and ``scaleOnSchedule()``.')
    add_event_source: typing.Optional[list[models.aws_lambda.AliasDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda.AliasDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda.AliasDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda.AliasDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda.AliasDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    configure_async_invoke: typing.Optional[list[models.aws_lambda.AliasDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[bool] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    grant_invoke: typing.Optional[list[models.aws_lambda.AliasDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda.AliasDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_lambda.AliasDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda.AliasDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_lambda.AliasDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    metric: typing.Optional[list[models.aws_lambda.AliasDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda.AliasDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda.AliasDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda.AliasDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda.AliasDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    lambda__config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)
    version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)

class AliasDefAddAutoScalingParams(pydantic.BaseModel):
    max_capacity: typing.Union[int, float] = pydantic.Field(..., description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1')
    return_config: typing.Optional[list[models._interface_methods.AwsLambdaIScalableFunctionAttributeDefConfig]] = pydantic.Field(None)
    ...

class AliasDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class AliasDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class AliasDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class AliasDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class AliasDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class AliasDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class AliasDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class AliasDefFromAliasAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    id: str = pydantic.Field(..., description='-\n')
    alias_name: str = pydantic.Field(..., description='')
    alias_version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='')
    ...

class AliasDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class AliasDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class AliasDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class AliasDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class AliasDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class AliasDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AliasDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AliasDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AliasDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class AliasDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.CodeSigningConfig
class CodeSigningConfigDef(BaseConstruct):
    signing_profiles: typing.Union[typing.Sequence[typing.Union[models.aws_signer.SigningProfileDef]], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='List of signing profiles that defines a trusted user who can sign a code package.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Code signing configuration description. Default: - No description.\n')
    untrusted_artifact_on_deployment: typing.Optional[aws_cdk.aws_lambda.UntrustedArtifactOnDeployment] = pydantic.Field(None, description='Code signing configuration policy for deployment validation failure. If you set the policy to Enforce, Lambda blocks the deployment request if signature validation checks fail. If you set the policy to Warn, Lambda allows the deployment and creates a CloudWatch log. Default: UntrustedArtifactOnDeployment.WARN')
    _init_params: typing.ClassVar[list[str]] = ['signing_profiles', 'description', 'untrusted_artifact_on_deployment']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_code_signing_config_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CodeSigningConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_code_signing_config_arn']
    ...


    from_code_signing_config_arn: typing.Optional[models.aws_lambda.CodeSigningConfigDefFromCodeSigningConfigArnParams] = pydantic.Field(None, description='Creates a Signing Profile construct that represents an external Signing Profile.')
    resource_config: typing.Optional[models.aws_lambda.CodeSigningConfigDefConfig] = pydantic.Field(None)


class CodeSigningConfigDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class CodeSigningConfigDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class CodeSigningConfigDefFromCodeSigningConfigArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The parent creating construct (usually ``this``).\n')
    id: str = pydantic.Field(..., description="The construct's name.\n")
    code_signing_config_arn: str = pydantic.Field(..., description='The ARN of code signing config.')
    ...


#  autogenerated from aws_cdk.aws_lambda.DockerImageFunction
class DockerImageFunctionDef(BaseConstruct):
    code: typing.Union[models.aws_lambda.DockerImageCodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functions /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['code', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_alias', 'add_environment', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_layers', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'invalidate_version_based_on', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = ['classify_version_property', 'from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.DockerImageFunction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    ...


    from_function_arn: typing.Optional[models.aws_lambda.DockerImageFunctionDefFromFunctionArnParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its ARN.\nFor ``Function.addPermissions()`` to work on this imported lambda, make sure that is\nin the same account and region as the stack you are importing it into.')
    from_function_attributes: typing.Optional[models.aws_lambda.DockerImageFunctionDefFromFunctionAttributesParams] = pydantic.Field(None, description='Creates a Lambda function object which represents a function not defined within this stack.\nFor ``Function.addPermissions()`` to work on this imported lambda, set the sameEnvironment property to true\nif this imported lambda is in the same account and region as the stack you are importing it into.')
    from_function_name: typing.Optional[models.aws_lambda.DockerImageFunctionDefFromFunctionNameParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its name.')
    metric_all: typing.Optional[models.aws_lambda.DockerImageFunctionDefMetricAllParams] = pydantic.Field(None, description='Return the given named metric for this Lambda.')
    metric_all_concurrent_executions: typing.Optional[models.aws_lambda.DockerImageFunctionDefMetricAllConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of concurrent executions across all Lambdas.')
    metric_all_duration: typing.Optional[models.aws_lambda.DockerImageFunctionDefMetricAllDurationParams] = pydantic.Field(None, description='Metric for the Duration executing all Lambdas.')
    metric_all_errors: typing.Optional[models.aws_lambda.DockerImageFunctionDefMetricAllErrorsParams] = pydantic.Field(None, description='Metric for the number of Errors executing all Lambdas.')
    metric_all_invocations: typing.Optional[models.aws_lambda.DockerImageFunctionDefMetricAllInvocationsParams] = pydantic.Field(None, description='Metric for the number of invocations of all Lambdas.')
    metric_all_throttles: typing.Optional[models.aws_lambda.DockerImageFunctionDefMetricAllThrottlesParams] = pydantic.Field(None, description='Metric for the number of throttled invocations of all Lambdas.')
    metric_all_unreserved_concurrent_executions: typing.Optional[models.aws_lambda.DockerImageFunctionDefMetricAllUnreservedConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of unreserved concurrent executions across all Lambdas.')
    resource_config: typing.Optional[models.aws_lambda.DockerImageFunctionDefConfig] = pydantic.Field(None)


class DockerImageFunctionDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddAliasParams]] = pydantic.Field(None, description='Defines an alias for this function.\nThe alias will automatically be updated to point to the latest version of\nthe function as it is being updated during a deployment::\n\n   # fn: lambda.Function\n\n\n   fn.add_alias("Live")\n\n   # Is equivalent to\n\n   lambda_.Alias(self, "AliasLive",\n       alias_name="Live",\n       version=fn.current_version\n   )')
    add_environment: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddEnvironmentParams]] = pydantic.Field(None, description='Adds an environment variable to this Lambda function.\nIf this is a ref to a Lambda function, this operation results in a no-op.')
    add_event_source: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_layers: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddLayersParams]] = pydantic.Field(None, description='Adds one or more Lambda Layers to this Lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    classify_version_property: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefClassifyVersionPropertyParams]] = pydantic.Field(None, description="Record whether specific properties in the ``AWS::Lambda::Function`` resource should also be associated to the Version resource.\nSee 'currentVersion' section in the module README for more details.")
    configure_async_invoke: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams]] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    grant_invoke: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    invalidate_version_based_on: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefInvalidateVersionBasedOnParams]] = pydantic.Field(None, description='Mix additional information into the hash of the Version object.\nThe Lambda Function construct does its best to automatically create a new\nVersion when anything about the Function changes (its code, its layers,\nany of the other properties).\n\nHowever, you can sometimes source information from places that the CDK cannot\nlook into, like the deploy-time values of SSM parameters. In those cases,\nthe CDK would not force the creation of a new Version object when it actually\nshould.\n\nThis method can be used to invalidate the current Version object. Pass in\nany string into this method, and make sure the string changes when you know\na new Version needs to be created.\n\nThis method may be called more than once.')
    metric: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda.DockerImageFunctionDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    current_version_config: typing.Optional[models.aws_lambda.VersionDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)

class DockerImageFunctionDefAddAliasParams(pydantic.BaseModel):
    alias_name: str = pydantic.Field(..., description='The name of the alias.\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    return_config: typing.Optional[list[models.aws_lambda.AliasDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefAddEnvironmentParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='The environment variable key.\n')
    value: str = pydantic.Field(..., description="The environment variable's value.\n")
    remove_in_edge: typing.Optional[bool] = pydantic.Field(None, description='When used in Lambda@Edge via edgeArn() API, these environment variables will be removed. If not set, an error will be thrown. Default: false - using the function in Lambda@Edge will throw')
    return_config: typing.Optional[list[models.aws_lambda.FunctionDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefAddLayersParams(pydantic.BaseModel):
    layers: list[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]] = pydantic.Field(...)
    ...

class DockerImageFunctionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class DockerImageFunctionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefClassifyVersionPropertyParams(pydantic.BaseModel):
    property_name: str = pydantic.Field(..., description='The property to classify.\n')
    locked: bool = pydantic.Field(..., description='whether the property should be associated to the version or not.')
    ...

class DockerImageFunctionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class DockerImageFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    action: str = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefFromFunctionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_arn: str = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefFromFunctionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The parent construct.\n')
    id: str = pydantic.Field(..., description='The name of the lambda construct.\n')
    function_arn: str = pydantic.Field(..., description='The ARN of the Lambda function. Format: arn::lambda:::function:\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The architecture of this Lambda Function (this is an optional attribute and defaults to X86_64). Default: - Architecture.X86_64\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM execution role associated with this function. If the role is not specified, any role-related operations will no-op.\n')
    same_environment: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function is in the same environment as the stack. This affects certain behaviours such as, whether this function's permission can be modified. When not configured, the CDK attempts to auto-determine this. For environment agnostic stacks, i.e., stacks where the account is not specified with the ``env`` property, this is determined to be false. Set this to property *ONLY IF* the imported function is in the same account as the stack it's imported in. Default: - depends: true, if the Stack is configured with an explicit ``env`` (account and region) and the account is the same as this function. For environment-agnostic stacks this will default to ``false``.\n")
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group of this Lambda, if in a VPC. This needs to be given in order to support allowing connections to this Lambda.\n')
    skip_permissions: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function ALREADY HAS the necessary permissions for what you are trying to do. When not configured, the CDK attempts to auto-determine whether or not additional permissions are necessary on the function when grant APIs are used. If the CDK tried to add permissions on an imported lambda, it will fail. Set this property *ONLY IF* you are committing to manage the imported function's permissions outside of CDK. You are acknowledging that your CDK code alone will have insufficient permissions to access the imported function. Default: false")
    ...

class DockerImageFunctionDefFromFunctionNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_name: str = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefInvalidateVersionBasedOnParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class DockerImageFunctionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefMetricAllParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    ...

class DockerImageFunctionDefMetricAllConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class DockerImageFunctionDefMetricAllDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    ...

class DockerImageFunctionDefMetricAllErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class DockerImageFunctionDefMetricAllInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class DockerImageFunctionDefMetricAllThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class DockerImageFunctionDefMetricAllUnreservedConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class DockerImageFunctionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DockerImageFunctionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.EventInvokeConfig
class EventInvokeConfigDef(BaseConstruct):
    function: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Lambda function.\n')
    qualifier: typing.Optional[str] = pydantic.Field(None, description='The qualifier. Default: - latest version\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['function', 'qualifier', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EventInvokeConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.EventInvokeConfigDefConfig] = pydantic.Field(None)


class EventInvokeConfigDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class EventInvokeConfigDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda.EventSourceMapping
class EventSourceMappingDef(BaseConstruct):
    target: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The target AWS Lambda function.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['target', 'batch_size', 'bisect_batch_on_error', 'enabled', 'event_source_arn', 'filters', 'kafka_bootstrap_servers', 'kafka_consumer_group_id', 'kafka_topic', 'max_batching_window', 'max_concurrency', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'source_access_configurations', 'starting_position', 'starting_position_timestamp', 'support_s3_on_failure_destination', 'tumbling_window']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_event_source_mapping_id']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EventSourceMapping'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_event_source_mapping_id']
    ...


    from_event_source_mapping_id: typing.Optional[models.aws_lambda.EventSourceMappingDefFromEventSourceMappingIdParams] = pydantic.Field(None, description='Import an event source into this stack from its event source id.')
    resource_config: typing.Optional[models.aws_lambda.EventSourceMappingDefConfig] = pydantic.Field(None)


class EventSourceMappingDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class EventSourceMappingDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class EventSourceMappingDefFromEventSourceMappingIdParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    event_source_mapping_id: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_lambda.Function
class FunctionDef(BaseConstruct):
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functions /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['code', 'handler', 'runtime', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_alias', 'add_environment', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_layers', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'invalidate_version_based_on', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = ['classify_version_property', 'from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Function'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    ...


    from_function_arn: typing.Optional[models.aws_lambda.FunctionDefFromFunctionArnParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its ARN.\nFor ``Function.addPermissions()`` to work on this imported lambda, make sure that is\nin the same account and region as the stack you are importing it into.')
    from_function_attributes: typing.Optional[models.aws_lambda.FunctionDefFromFunctionAttributesParams] = pydantic.Field(None, description='Creates a Lambda function object which represents a function not defined within this stack.\nFor ``Function.addPermissions()`` to work on this imported lambda, set the sameEnvironment property to true\nif this imported lambda is in the same account and region as the stack you are importing it into.')
    from_function_name: typing.Optional[models.aws_lambda.FunctionDefFromFunctionNameParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its name.')
    metric_all: typing.Optional[models.aws_lambda.FunctionDefMetricAllParams] = pydantic.Field(None, description='Return the given named metric for this Lambda.')
    metric_all_concurrent_executions: typing.Optional[models.aws_lambda.FunctionDefMetricAllConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of concurrent executions across all Lambdas.')
    metric_all_duration: typing.Optional[models.aws_lambda.FunctionDefMetricAllDurationParams] = pydantic.Field(None, description='Metric for the Duration executing all Lambdas.')
    metric_all_errors: typing.Optional[models.aws_lambda.FunctionDefMetricAllErrorsParams] = pydantic.Field(None, description='Metric for the number of Errors executing all Lambdas.')
    metric_all_invocations: typing.Optional[models.aws_lambda.FunctionDefMetricAllInvocationsParams] = pydantic.Field(None, description='Metric for the number of invocations of all Lambdas.')
    metric_all_throttles: typing.Optional[models.aws_lambda.FunctionDefMetricAllThrottlesParams] = pydantic.Field(None, description='Metric for the number of throttled invocations of all Lambdas.')
    metric_all_unreserved_concurrent_executions: typing.Optional[models.aws_lambda.FunctionDefMetricAllUnreservedConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of unreserved concurrent executions across all Lambdas.')
    resource_config: typing.Optional[models.aws_lambda.FunctionDefConfig] = pydantic.Field(None)


class FunctionDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[models.aws_lambda.FunctionDefAddAliasParams]] = pydantic.Field(None, description='Defines an alias for this function.\nThe alias will automatically be updated to point to the latest version of\nthe function as it is being updated during a deployment::\n\n   # fn: lambda.Function\n\n\n   fn.add_alias("Live")\n\n   # Is equivalent to\n\n   lambda_.Alias(self, "AliasLive",\n       alias_name="Live",\n       version=fn.current_version\n   )')
    add_environment: typing.Optional[list[models.aws_lambda.FunctionDefAddEnvironmentParams]] = pydantic.Field(None, description='Adds an environment variable to this Lambda function.\nIf this is a ref to a Lambda function, this operation results in a no-op.')
    add_event_source: typing.Optional[list[models.aws_lambda.FunctionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda.FunctionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda.FunctionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_layers: typing.Optional[list[models.aws_lambda.FunctionDefAddLayersParams]] = pydantic.Field(None, description='Adds one or more Lambda Layers to this Lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda.FunctionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda.FunctionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    classify_version_property: typing.Optional[list[models.aws_lambda.FunctionDefClassifyVersionPropertyParams]] = pydantic.Field(None, description="Record whether specific properties in the ``AWS::Lambda::Function`` resource should also be associated to the Version resource.\nSee 'currentVersion' section in the module README for more details.")
    configure_async_invoke: typing.Optional[list[models.aws_lambda.FunctionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[list[models.aws_lambda.FunctionDefConsiderWarningOnInvokeFunctionPermissionsParams]] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    grant_invoke: typing.Optional[list[models.aws_lambda.FunctionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda.FunctionDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_lambda.FunctionDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda.FunctionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_lambda.FunctionDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    invalidate_version_based_on: typing.Optional[list[models.aws_lambda.FunctionDefInvalidateVersionBasedOnParams]] = pydantic.Field(None, description='Mix additional information into the hash of the Version object.\nThe Lambda Function construct does its best to automatically create a new\nVersion when anything about the Function changes (its code, its layers,\nany of the other properties).\n\nHowever, you can sometimes source information from places that the CDK cannot\nlook into, like the deploy-time values of SSM parameters. In those cases,\nthe CDK would not force the creation of a new Version object when it actually\nshould.\n\nThis method can be used to invalidate the current Version object. Pass in\nany string into this method, and make sure the string changes when you know\na new Version needs to be created.\n\nThis method may be called more than once.')
    metric: typing.Optional[list[models.aws_lambda.FunctionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda.FunctionDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda.FunctionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda.FunctionDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda.FunctionDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    current_version_config: typing.Optional[models.aws_lambda.VersionDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)

class FunctionDefAddAliasParams(pydantic.BaseModel):
    alias_name: str = pydantic.Field(..., description='The name of the alias.\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    return_config: typing.Optional[list[models.aws_lambda.AliasDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefAddEnvironmentParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='The environment variable key.\n')
    value: str = pydantic.Field(..., description="The environment variable's value.\n")
    remove_in_edge: typing.Optional[bool] = pydantic.Field(None, description='When used in Lambda@Edge via edgeArn() API, these environment variables will be removed. If not set, an error will be thrown. Default: false - using the function in Lambda@Edge will throw')
    return_config: typing.Optional[list[models.aws_lambda.FunctionDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class FunctionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefAddLayersParams(pydantic.BaseModel):
    layers: list[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]] = pydantic.Field(...)
    ...

class FunctionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class FunctionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class FunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class FunctionDefClassifyVersionPropertyParams(pydantic.BaseModel):
    property_name: str = pydantic.Field(..., description='The property to classify.\n')
    locked: bool = pydantic.Field(..., description='whether the property should be associated to the version or not.')
    ...

class FunctionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class FunctionDefConsiderWarningOnInvokeFunctionPermissionsParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    action: str = pydantic.Field(..., description='-')
    ...

class FunctionDefFromFunctionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_arn: str = pydantic.Field(..., description='-')
    ...

class FunctionDefFromFunctionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The parent construct.\n')
    id: str = pydantic.Field(..., description='The name of the lambda construct.\n')
    function_arn: str = pydantic.Field(..., description='The ARN of the Lambda function. Format: arn::lambda:::function:\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The architecture of this Lambda Function (this is an optional attribute and defaults to X86_64). Default: - Architecture.X86_64\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM execution role associated with this function. If the role is not specified, any role-related operations will no-op.\n')
    same_environment: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function is in the same environment as the stack. This affects certain behaviours such as, whether this function's permission can be modified. When not configured, the CDK attempts to auto-determine this. For environment agnostic stacks, i.e., stacks where the account is not specified with the ``env`` property, this is determined to be false. Set this to property *ONLY IF* the imported function is in the same account as the stack it's imported in. Default: - depends: true, if the Stack is configured with an explicit ``env`` (account and region) and the account is the same as this function. For environment-agnostic stacks this will default to ``false``.\n")
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group of this Lambda, if in a VPC. This needs to be given in order to support allowing connections to this Lambda.\n')
    skip_permissions: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function ALREADY HAS the necessary permissions for what you are trying to do. When not configured, the CDK attempts to auto-determine whether or not additional permissions are necessary on the function when grant APIs are used. If the CDK tried to add permissions on an imported lambda, it will fail. Set this property *ONLY IF* you are committing to manage the imported function's permissions outside of CDK. You are acknowledging that your CDK code alone will have insufficient permissions to access the imported function. Default: false")
    ...

class FunctionDefFromFunctionNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_name: str = pydantic.Field(..., description='-')
    ...

class FunctionDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class FunctionDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefInvalidateVersionBasedOnParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class FunctionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefMetricAllParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    ...

class FunctionDefMetricAllConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class FunctionDefMetricAllDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    ...

class FunctionDefMetricAllErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class FunctionDefMetricAllInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class FunctionDefMetricAllThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class FunctionDefMetricAllUnreservedConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class FunctionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class FunctionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.FunctionUrl
class FunctionUrlDef(BaseConstruct):
    function: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The function to which this url refers. It can also be an ``Alias`` but not a ``Version``.\n')
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    _init_params: typing.ClassVar[list[str]] = ['function', 'auth_type', 'cors', 'invoke_mode']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy', 'grant_invoke_url']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionUrl'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.FunctionUrlDefConfig] = pydantic.Field(None)


class FunctionUrlDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_invoke_url: typing.Optional[list[models.aws_lambda.FunctionUrlDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')

class FunctionUrlDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class FunctionUrlDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.LayerVersion
class LayerVersionDef(BaseConstruct):
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The content of this Layer. Using ``Code.fromInline`` is not supported.\n')
    compatible_architectures: typing.Optional[typing.Sequence[models.aws_lambda.ArchitectureDef]] = pydantic.Field(None, description='The system architectures compatible with this layer. Default: [Architecture.X86_64]\n')
    compatible_runtimes: typing.Optional[typing.Sequence[models.aws_lambda.RuntimeDef]] = pydantic.Field(None, description='The runtimes compatible with this Layer. Default: - All runtimes are supported.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description the this Lambda Layer. Default: - No description.\n')
    layer_version_name: typing.Optional[str] = pydantic.Field(None, description='The name of the layer. Default: - A name will be generated.\n')
    license: typing.Optional[str] = pydantic.Field(None, description='The SPDX licence identifier or URL to the license file for this layer. Default: - No license information will be recorded.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Whether to retain this version of the layer when a new version is added or when the stack is deleted. Default: RemovalPolicy.DESTROY')
    _init_params: typing.ClassVar[list[str]] = ['code', 'compatible_architectures', 'compatible_runtimes', 'description', 'layer_version_name', 'license', 'removal_policy']
    _method_names: typing.ClassVar[list[str]] = ['add_permission', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_layer_version_arn', 'from_layer_version_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LayerVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_layer_version_arn', 'from_layer_version_attributes']
    ...


    from_layer_version_arn: typing.Optional[models.aws_lambda.LayerVersionDefFromLayerVersionArnParams] = pydantic.Field(None, description='Imports a layer version by ARN.\nAssumes it is compatible with all Lambda runtimes.')
    from_layer_version_attributes: typing.Optional[models.aws_lambda.LayerVersionDefFromLayerVersionAttributesParams] = pydantic.Field(None, description='Imports a Layer that has been defined externally.')
    resource_config: typing.Optional[models.aws_lambda.LayerVersionDefConfig] = pydantic.Field(None)


class LayerVersionDefConfig(pydantic.BaseModel):
    add_permission: typing.Optional[list[models.aws_lambda.LayerVersionDefAddPermissionParams]] = pydantic.Field(None, description='Add permission for this layer version to specific entities.\nUsage within\nthe same account where the layer is defined is always allowed and does not\nrequire calling this method. Note that the principal that creates the\nLambda function using the layer (for example, a CloudFormation changeset\nexecution role) also needs to have the ``lambda:GetLayerVersion``\npermission on the layer version.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class LayerVersionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    account_id: str = pydantic.Field(..., description='The AWS Account id of the account that is authorized to use a Lambda Layer Version. The wild-card ``\'*\'`` can be used to grant access to "any" account (or any account in an organization when ``organizationId`` is specified).\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description="The ID of the AWS Organization to which the grant is restricted. Can only be specified if ``accountId`` is ``'*'``")
    ...

class LayerVersionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class LayerVersionDefFromLayerVersionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    layer_version_arn: str = pydantic.Field(..., description='-')
    ...

class LayerVersionDefFromLayerVersionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='the parent Construct that will use the imported layer.\n')
    id: str = pydantic.Field(..., description='the id of the imported layer in the construct tree.\n')
    layer_version_arn: str = pydantic.Field(..., description='The ARN of the LayerVersion.\n')
    compatible_runtimes: typing.Optional[typing.Sequence[models.aws_lambda.RuntimeDef]] = pydantic.Field(None, description='The list of compatible runtimes with this Layer.')
    ...


#  autogenerated from aws_cdk.aws_lambda.SingletonFunction
class SingletonFunctionDef(BaseConstruct):
    uuid: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A unique identifier to identify this lambda. The identifier should be unique across all custom resource providers. We recommend generating a UUID per provider.\n')
    lambda_purpose: typing.Optional[str] = pydantic.Field(None, description='A descriptive name for the purpose of this Lambda. If the Lambda does not have a physical name, this string will be reflected its generated name. The combination of lambdaPurpose and uuid must be unique. Default: SingletonLambda\n')
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functions /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['uuid', 'lambda_purpose', 'code', 'handler', 'runtime', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_dependency', 'add_environment', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_layers', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'depend_on', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.SingletonFunction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.SingletonFunctionDefConfig] = pydantic.Field(None)


class SingletonFunctionDefConfig(pydantic.BaseModel):
    add_dependency: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddDependencyParams]] = pydantic.Field(None, description='Using node.addDependency() does not work on this method as the underlying lambda function is modeled as a singleton across the stack. Use this method instead to declare dependencies.')
    add_environment: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddEnvironmentParams]] = pydantic.Field(None, description='Adds an environment variable to this Lambda function.\nIf this is a ref to a Lambda function, this operation results in a no-op.')
    add_event_source: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_layers: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddLayersParams]] = pydantic.Field(None, description='Adds one or more Lambda Layers to this Lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda.SingletonFunctionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    configure_async_invoke: typing.Optional[list[models.aws_lambda.SingletonFunctionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[list[models.aws_lambda.SingletonFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams]] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    depend_on: typing.Optional[list[models.aws_lambda.SingletonFunctionDefDependOnParams]] = pydantic.Field(None, description='The SingletonFunction construct cannot be added as a dependency of another construct using node.addDependency(). Use this method instead to declare this as a dependency of another construct.')
    grant_invoke: typing.Optional[list[models.aws_lambda.SingletonFunctionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda.SingletonFunctionDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_lambda.SingletonFunctionDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda.SingletonFunctionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_lambda.SingletonFunctionDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    metric: typing.Optional[list[models.aws_lambda.SingletonFunctionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda.SingletonFunctionDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda.SingletonFunctionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda.SingletonFunctionDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda.SingletonFunctionDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)

class SingletonFunctionDefAddDependencyParams(pydantic.BaseModel):
    up: list[models.UnsupportedResource] = pydantic.Field(...)
    ...

class SingletonFunctionDefAddEnvironmentParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='The environment variable key.\n')
    value: str = pydantic.Field(..., description="The environment variable's value.\n")
    remove_in_edge: typing.Optional[bool] = pydantic.Field(None, description='When used in Lambda@Edge via edgeArn() API, these environment variables will be removed. If not set, an error will be thrown. Default: false - using the function in Lambda@Edge will throw')
    return_config: typing.Optional[list[models.aws_lambda.FunctionDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class SingletonFunctionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefAddLayersParams(pydantic.BaseModel):
    layers: list[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]] = pydantic.Field(...)
    ...

class SingletonFunctionDefAddPermissionParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='-\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.')
    ...

class SingletonFunctionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class SingletonFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class SingletonFunctionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class SingletonFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    action: str = pydantic.Field(..., description='-')
    ...

class SingletonFunctionDefDependOnParams(pydantic.BaseModel):
    down: models.AnyResource = pydantic.Field(..., description='-')
    ...

class SingletonFunctionDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class SingletonFunctionDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class SingletonFunctionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.Version
class VersionDef(BaseConstruct):
    lambda_: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Function to get the value of.\n')
    code_sha256: typing.Optional[str] = pydantic.Field(None, description="SHA256 of the version of the Lambda source code. Specify to validate that you're deploying the right version. Default: No validation is performed\n")
    description: typing.Optional[str] = pydantic.Field(None, description='Description of the version. Default: Description of the Lambda\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's version. Default: No provisioned concurrency\n")
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Whether to retain old versions of this function when a new version is created. Default: RemovalPolicy.DESTROY\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['lambda_', 'code_sha256', 'description', 'provisioned_concurrent_executions', 'removal_policy', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_alias', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_version_arn', 'from_version_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Version'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_version_arn', 'from_version_attributes']
    ...


    from_version_arn: typing.Optional[models.aws_lambda.VersionDefFromVersionArnParams] = pydantic.Field(None, description='Construct a Version object from a Version ARN.')
    from_version_attributes: typing.Optional[models.aws_lambda.VersionDefFromVersionAttributesParams] = pydantic.Field(None, description='')
    resource_config: typing.Optional[models.aws_lambda.VersionDefConfig] = pydantic.Field(None)


class VersionDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[models.aws_lambda.VersionDefAddAliasParams]] = pydantic.Field(None, description='(deprecated) Defines an alias for this version.')
    add_event_source: typing.Optional[list[models.aws_lambda.VersionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda.VersionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda.VersionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda.VersionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda.VersionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    configure_async_invoke: typing.Optional[list[models.aws_lambda.VersionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[bool] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    grant_invoke: typing.Optional[list[models.aws_lambda.VersionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda.VersionDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.aws_lambda.VersionDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda.VersionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.aws_lambda.VersionDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    metric: typing.Optional[list[models.aws_lambda.VersionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda.VersionDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda.VersionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda.VersionDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda.VersionDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    lambda__config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)

class VersionDefAddAliasParams(pydantic.BaseModel):
    alias_name: str = pydantic.Field(..., description='The name of the alias (e.g. "live").\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n\n:deprecated: Calling ``addAlias`` on a ``Version`` object will cause the Alias to be replaced on every function update. Call ``function.addAlias()`` or ``new Alias()`` instead.\n\n:stability: deprecated\n')
    return_config: typing.Optional[list[models.aws_lambda.AliasDefConfig]] = pydantic.Field(None)
    ...

class VersionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class VersionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class VersionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class VersionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class VersionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class VersionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class VersionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class VersionDefFromVersionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The cdk scope creating this resource.\n')
    id: str = pydantic.Field(..., description='The cdk id of this resource.\n')
    version_arn: str = pydantic.Field(..., description='The version ARN to create this version from.')
    ...

class VersionDefFromVersionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-')
    id: str = pydantic.Field(..., description='-\n')
    lambda_: typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(..., description='The lambda function.\n')
    version: str = pydantic.Field(..., description='The version.')
    ...

class VersionDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class VersionDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class VersionDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class VersionDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class VersionDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class VersionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class VersionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class VersionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class VersionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class VersionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda.AdotInstrumentationConfig
class AdotInstrumentationConfigDef(BaseStruct):
    exec_wrapper: typing.Union[aws_cdk.aws_lambda.AdotLambdaExecWrapper, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The startup script to run, see ADOT documentation to pick the right script for your use case: https://aws-otel.github.io/docs/getting-started/lambda.\n')
    layer_version: typing.Union[models.aws_lambda.AdotLayerVersionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ADOT Lambda layer.\n\n:exampleMetadata: infused\n\nExample::\n\n    from aws_cdk.aws_lambda import AdotLambdaExecWrapper, AdotLayerVersion, AdotLambdaLayerJavaScriptSdkVersion\n\n\n    fn = lambda_.Function(self, "MyFunction",\n        runtime=lambda_.Runtime.NODEJS_18_X,\n        handler="index.handler",\n        code=lambda_.Code.from_inline("exports.handler = function(event, ctx, cb) { return cb(null, "hi"); }"),\n        adot_instrumentation=lambda.AdotInstrumentationConfig(\n            layer_version=AdotLayerVersion.from_java_script_sdk_layer_version(AdotLambdaLayerJavaScriptSdkVersion.LATEST),\n            exec_wrapper=AdotLambdaExecWrapper.REGULAR_HANDLER\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['exec_wrapper', 'layer_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AdotInstrumentationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.AliasAttributes
class AliasAttributesDef(BaseStruct):
    alias_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    alias_version: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.VersionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='')
    _init_params: typing.ClassVar[list[str]] = ['alias_name', 'alias_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AliasAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.AliasOptions
class AliasOptionsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies a provisioned concurrency configuration for a function\'s alias. Default: No provisioned concurrency\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    # destination: lambda.IDestination\n    # version: lambda.Version\n\n    alias_options = lambda.AliasOptions(\n        additional_versions=[lambda.VersionWeight(\n            version=version,\n            weight=123\n        )],\n        description="description",\n        max_event_age=cdk.Duration.minutes(30),\n        on_failure=destination,\n        on_success=destination,\n        provisioned_concurrent_executions=123,\n        retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'additional_versions', 'description', 'provisioned_concurrent_executions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AliasOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.AliasProps
class AliasPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    alias_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name of this alias.\n')
    version: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.VersionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Function version this alias refers to. Use lambda.currentVersion to reference a version with your latest changes.\n\n:exampleMetadata: infused\n\nExample::\n\n    lambda_code = lambda_.Code.from_cfn_parameters()\n    func = lambda_.Function(self, "Lambda",\n        code=lambda_code,\n        handler="index.handler",\n        runtime=lambda_.Runtime.NODEJS_LATEST\n    )\n    # used to make sure each CDK synthesis produces a different Version\n    version = func.current_version\n    alias = lambda_.Alias(self, "LambdaAlias",\n        alias_name="Prod",\n        version=version\n    )\n\n    codedeploy.LambdaDeploymentGroup(self, "DeploymentGroup",\n        alias=alias,\n        deployment_config=codedeploy.LambdaDeploymentConfig.LINEAR_10PERCENT_EVERY_1MINUTE\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'additional_versions', 'description', 'provisioned_concurrent_executions', 'alias_name', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AliasProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.AliasPropsDefConfig] = pydantic.Field(None)


class AliasPropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.AssetImageCodeProps
class AssetImageCodePropsDef(BaseStruct):
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB\n')
    extra_hash: typing.Optional[str] = pydantic.Field(None, description='Extra information to encode into the fingerprint (e.g. build instructions and other inputs). Default: - hash is only based on source content\n')
    asset_name: typing.Optional[str] = pydantic.Field(None, description='Unique identifier of the docker image asset and its potential revisions. Required if using AppScopedStagingSynthesizer. Default: - no asset name\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args to pass to the ``docker build`` command. Since Docker build arguments are resolved before deployment, keys and values cannot refer to unresolved tokens (such as ``lambda.functionArn`` or ``queue.queueUrl``). Default: - no build args are passed\n')
    build_secrets: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build secrets. Docker BuildKit must be enabled to use build secrets. Default: - no build secrets\n')
    build_ssh: typing.Optional[str] = pydantic.Field(None, description='SSH agent socket or keys to pass to the ``docker build`` command. Docker BuildKit must be enabled to use the ssh flag Default: - no --ssh flag\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from options are passed to the build command\n')
    cache_to: typing.Union[models.aws_ecr_assets.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to options are passed to the build command\n')
    file: typing.Optional[str] = pydantic.Field(None, description="Path to the Dockerfile (relative to the directory). Default: 'Dockerfile'\n")
    invalidation: typing.Union[models.aws_ecr_assets.DockerImageAssetInvalidationOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options to control which parameters are used to invalidate the asset hash. Default: - hash all parameters\n')
    network_mode: typing.Optional[models.aws_ecr_assets.NetworkModeDef] = pydantic.Field(None, description='Networking mode for the RUN commands during build. Support docker API 1.25+. Default: - no networking mode specified (the default networking mode ``NetworkMode.DEFAULT`` will be used)\n')
    outputs: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Outputs to pass to the ``docker build`` command. Default: - no outputs are passed to the build command (default outputs are used)\n')
    platform: typing.Optional[models.aws_ecr_assets.PlatformDef] = pydantic.Field(None, description='Platform to build for. *Requires Docker Buildx*. Default: - no platform specified (the current machine architecture will be used)\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Docker target to build to. Default: - no target\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ecr_assets as ecr_assets\n    from aws_cdk import aws_lambda as lambda_\n\n    # network_mode: ecr_assets.NetworkMode\n    # platform: ecr_assets.Platform\n\n    asset_image_code_props = lambda.AssetImageCodeProps(\n        asset_name="assetName",\n        build_args={\n            "build_args_key": "buildArgs"\n        },\n        build_secrets={\n            "build_secrets_key": "buildSecrets"\n        },\n        build_ssh="buildSsh",\n        cache_disabled=False,\n        cache_from=[ecr_assets.DockerCacheOption(\n            type="type",\n\n            # the properties below are optional\n            params={\n                "params_key": "params"\n            }\n        )],\n        cache_to=ecr_assets.DockerCacheOption(\n            type="type",\n\n            # the properties below are optional\n            params={\n                "params_key": "params"\n            }\n        ),\n        cmd=["cmd"],\n        entrypoint=["entrypoint"],\n        exclude=["exclude"],\n        extra_hash="extraHash",\n        file="file",\n        follow_symlinks=cdk.SymlinkFollowMode.NEVER,\n        ignore_mode=cdk.IgnoreMode.GLOB,\n        invalidation=ecr_assets.DockerImageAssetInvalidationOptions(\n            build_args=False,\n            build_secrets=False,\n            build_ssh=False,\n            extra_hash=False,\n            file=False,\n            network_mode=False,\n            outputs=False,\n            platform=False,\n            repository_name=False,\n            target=False\n        ),\n        network_mode=network_mode,\n        outputs=["outputs"],\n        platform=platform,\n        target="target",\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['exclude', 'follow_symlinks', 'ignore_mode', 'extra_hash', 'asset_name', 'build_args', 'build_secrets', 'build_ssh', 'cache_disabled', 'cache_from', 'cache_to', 'file', 'invalidation', 'network_mode', 'outputs', 'platform', 'target', 'cmd', 'entrypoint', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AssetImageCodeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.AutoScalingOptions
class AutoScalingOptionsDef(BaseStruct):
    max_capacity: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Maximum capacity to scale to.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum capacity to scale to. Default: 1\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_autoscaling as autoscaling\n\n    # fn: lambda.Function\n\n    alias = fn.add_alias("prod")\n\n    # Create AutoScaling target\n    as = alias.add_auto_scaling(max_capacity=50)\n\n    # Configure Target Tracking\n    as.scale_on_utilization(\n        utilization_target=0.5\n    )\n\n    # Configure Scheduled Scaling\n    as.scale_on_schedule("ScaleUpInTheMorning",\n        schedule=autoscaling.Schedule.cron(hour="8", minute="0"),\n        min_capacity=20\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_capacity', 'min_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.AutoScalingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnAlias.AliasRoutingConfigurationProperty
class CfnAlias_AliasRoutingConfigurationPropertyDef(BaseStruct):
    additional_version_weights: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnAlias_VersionWeightPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The second version, and the percentage of traffic that\'s routed to it.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-alias-aliasroutingconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    alias_routing_configuration_property = lambda.CfnAlias.AliasRoutingConfigurationProperty(\n        additional_version_weights=[lambda.CfnAlias.VersionWeightProperty(\n            function_version="functionVersion",\n            function_weight=123\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['additional_version_weights']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnAlias.AliasRoutingConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnAlias.ProvisionedConcurrencyConfigurationProperty
class CfnAlias_ProvisionedConcurrencyConfigurationPropertyDef(BaseStruct):
    provisioned_concurrent_executions: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The amount of provisioned concurrency to allocate for the alias.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-alias-provisionedconcurrencyconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    provisioned_concurrency_configuration_property = lambda.CfnAlias.ProvisionedConcurrencyConfigurationProperty(\n        provisioned_concurrent_executions=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['provisioned_concurrent_executions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnAlias.ProvisionedConcurrencyConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnAlias.VersionWeightProperty
class CfnAlias_VersionWeightPropertyDef(BaseStruct):
    function_version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The qualifier of the second version.\n')
    function_weight: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The percentage of traffic that the alias routes to the second version.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-alias-versionweight.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    version_weight_property = lambda.CfnAlias.VersionWeightProperty(\n        function_version="functionVersion",\n        function_weight=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['function_version', 'function_weight']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnAlias.VersionWeightProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnCodeSigningConfig.AllowedPublishersProperty
class CfnCodeSigningConfig_AllowedPublishersPropertyDef(BaseStruct):
    signing_profile_version_arns: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) for each of the signing profiles. A signing profile defines a trusted user who can sign a code package.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-codesigningconfig-allowedpublishers.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    allowed_publishers_property = lambda.CfnCodeSigningConfig.AllowedPublishersProperty(\n        signing_profile_version_arns=["signingProfileVersionArns"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['signing_profile_version_arns']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnCodeSigningConfig.AllowedPublishersProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnCodeSigningConfig.CodeSigningPoliciesProperty
class CfnCodeSigningConfig_CodeSigningPoliciesPropertyDef(BaseStruct):
    untrusted_artifact_on_deployment: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Code signing configuration policy for deployment validation failure. If you set the policy to ``Enforce`` , Lambda blocks the deployment request if signature validation checks fail. If you set the policy to ``Warn`` , Lambda allows the deployment and creates a CloudWatch log. Default value: ``Warn`` Default: - "Warn"\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-codesigningconfig-codesigningpolicies.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    code_signing_policies_property = lambda.CfnCodeSigningConfig.CodeSigningPoliciesProperty(\n        untrusted_artifact_on_deployment="untrustedArtifactOnDeployment"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['untrusted_artifact_on_deployment']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnCodeSigningConfig.CodeSigningPoliciesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventInvokeConfig.DestinationConfigProperty
class CfnEventInvokeConfig_DestinationConfigPropertyDef(BaseStruct):
    on_failure: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventInvokeConfig_OnFailurePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The destination configuration for failed invocations.\n')
    on_success: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventInvokeConfig_OnSuccessPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The destination configuration for successful invocations.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventinvokeconfig-destinationconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    destination_config_property = lambda.CfnEventInvokeConfig.DestinationConfigProperty(\n        on_failure=lambda.CfnEventInvokeConfig.OnFailureProperty(\n            destination="destination"\n        ),\n        on_success=lambda.CfnEventInvokeConfig.OnSuccessProperty(\n            destination="destination"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['on_failure', 'on_success']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventInvokeConfig.DestinationConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventInvokeConfig.OnFailureProperty
class CfnEventInvokeConfig_OnFailurePropertyDef(BaseStruct):
    destination: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the destination resource. To retain records of `asynchronous invocations <https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-async-destinations>`_ , you can configure an Amazon SNS topic, Amazon SQS queue, Lambda function, or Amazon EventBridge event bus as the destination. To retain records of failed invocations from `Kinesis and DynamoDB event sources <https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html#event-source-mapping-destinations>`_ , you can configure an Amazon SNS topic or Amazon SQS queue as the destination. To retain records of failed invocations from `self-managed Kafka <https://docs.aws.amazon.com/lambda/latest/dg/with-kafka.html#services-smaa-onfailure-destination>`_ or `Amazon MSK <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-onfailure-destination>`_ , you can configure an Amazon SNS topic, Amazon SQS queue, or Amazon S3 bucket as the destination.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventinvokeconfig-onfailure.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    on_failure_property = lambda.CfnEventInvokeConfig.OnFailureProperty(\n        destination="destination"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventInvokeConfig.OnFailureProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventInvokeConfig.OnSuccessProperty
class CfnEventInvokeConfig_OnSuccessPropertyDef(BaseStruct):
    destination: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the destination resource.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventinvokeconfig-onsuccess.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    on_success_property = lambda.CfnEventInvokeConfig.OnSuccessProperty(\n        destination="destination"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventInvokeConfig.OnSuccessProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.AmazonManagedKafkaEventSourceConfigProperty
class CfnEventSourceMapping_AmazonManagedKafkaEventSourceConfigPropertyDef(BaseStruct):
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description='The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-amazonmanagedkafkaeventsourceconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    amazon_managed_kafka_event_source_config_property = lambda.CfnEventSourceMapping.AmazonManagedKafkaEventSourceConfigProperty(\n        consumer_group_id="consumerGroupId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['consumer_group_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.AmazonManagedKafkaEventSourceConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.DestinationConfigProperty
class CfnEventSourceMapping_DestinationConfigPropertyDef(BaseStruct):
    on_failure: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_OnFailurePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The destination configuration for failed invocations.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-destinationconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    destination_config_property = lambda.CfnEventSourceMapping.DestinationConfigProperty(\n        on_failure=lambda.CfnEventSourceMapping.OnFailureProperty(\n            destination="destination"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['on_failure']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.DestinationConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.DocumentDBEventSourceConfigProperty
class CfnEventSourceMapping_DocumentDBEventSourceConfigPropertyDef(BaseStruct):
    collection_name: typing.Optional[str] = pydantic.Field(None, description='The name of the collection to consume within the database. If you do not specify a collection, Lambda consumes all collections.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of the database to consume within the DocumentDB cluster.\n')
    full_document: typing.Optional[str] = pydantic.Field(None, description='Determines what DocumentDB sends to your event stream during document update operations. If set to UpdateLookup, DocumentDB sends a delta describing the changes, along with a copy of the entire document. Otherwise, DocumentDB sends only a partial document that contains the changes.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-documentdbeventsourceconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    document_dBEvent_source_config_property = lambda.CfnEventSourceMapping.DocumentDBEventSourceConfigProperty(\n        collection_name="collectionName",\n        database_name="databaseName",\n        full_document="fullDocument"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['collection_name', 'database_name', 'full_document']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.DocumentDBEventSourceConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.EndpointsProperty
class CfnEventSourceMapping_EndpointsPropertyDef(BaseStruct):
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of bootstrap servers for your Kafka brokers in the following format: ``"KafkaBootstrapServers": ["abc.xyz.com:xxxx","abc2.xyz.com:xxxx"]`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-endpoints.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    endpoints_property = lambda.CfnEventSourceMapping.EndpointsProperty(\n        kafka_bootstrap_servers=["kafkaBootstrapServers"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['kafka_bootstrap_servers']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.EndpointsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.FilterCriteriaProperty
class CfnEventSourceMapping_FilterCriteriaPropertyDef(BaseStruct):
    filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_FilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of filters.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-filtercriteria.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    filter_criteria_property = lambda.CfnEventSourceMapping.FilterCriteriaProperty(\n        filters=[lambda.CfnEventSourceMapping.FilterProperty(\n            pattern="pattern"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.FilterCriteriaProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.FilterProperty
class CfnEventSourceMapping_FilterPropertyDef(BaseStruct):
    pattern: typing.Optional[str] = pydantic.Field(None, description='A filter pattern. For more information on the syntax of a filter pattern, see `Filter rule syntax <https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventfiltering.html#filtering-syntax>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-filter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    filter_property = lambda.CfnEventSourceMapping.FilterProperty(\n        pattern="pattern"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['pattern']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.FilterProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.OnFailureProperty
class CfnEventSourceMapping_OnFailurePropertyDef(BaseStruct):
    destination: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the destination resource. To retain records of `asynchronous invocations <https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-async-destinations>`_ , you can configure an Amazon SNS topic, Amazon SQS queue, Lambda function, or Amazon EventBridge event bus as the destination. To retain records of failed invocations from `Kinesis and DynamoDB event sources <https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html#event-source-mapping-destinations>`_ , you can configure an Amazon SNS topic or Amazon SQS queue as the destination. To retain records of failed invocations from `self-managed Kafka <https://docs.aws.amazon.com/lambda/latest/dg/with-kafka.html#services-smaa-onfailure-destination>`_ or `Amazon MSK <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-onfailure-destination>`_ , you can configure an Amazon SNS topic, Amazon SQS queue, or Amazon S3 bucket as the destination.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-onfailure.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    on_failure_property = lambda.CfnEventSourceMapping.OnFailureProperty(\n        destination="destination"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.OnFailureProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.ScalingConfigProperty
class CfnEventSourceMapping_ScalingConfigPropertyDef(BaseStruct):
    maximum_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='Limits the number of concurrent instances that the Amazon SQS event source can invoke.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-scalingconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    scaling_config_property = lambda.CfnEventSourceMapping.ScalingConfigProperty(\n        maximum_concurrency=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['maximum_concurrency']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.ScalingConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.SelfManagedEventSourceProperty
class CfnEventSourceMapping_SelfManagedEventSourcePropertyDef(BaseStruct):
    endpoints: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_EndpointsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The list of bootstrap servers for your Kafka brokers in the following format: ``"KafkaBootstrapServers": ["abc.xyz.com:xxxx","abc2.xyz.com:xxxx"]`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-selfmanagedeventsource.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    self_managed_event_source_property = lambda.CfnEventSourceMapping.SelfManagedEventSourceProperty(\n        endpoints=lambda.CfnEventSourceMapping.EndpointsProperty(\n            kafka_bootstrap_servers=["kafkaBootstrapServers"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['endpoints']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.SelfManagedEventSourceProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.SelfManagedKafkaEventSourceConfigProperty
class CfnEventSourceMapping_SelfManagedKafkaEventSourceConfigPropertyDef(BaseStruct):
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description='The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-selfmanagedkafkaeventsourceconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    self_managed_kafka_event_source_config_property = lambda.CfnEventSourceMapping.SelfManagedKafkaEventSourceConfigProperty(\n        consumer_group_id="consumerGroupId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['consumer_group_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.SelfManagedKafkaEventSourceConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping.SourceAccessConfigurationProperty
class CfnEventSourceMapping_SourceAccessConfigurationPropertyDef(BaseStruct):
    type: typing.Optional[str] = pydantic.Field(None, description='The type of authentication protocol, VPC components, or virtual host for your event source. For example: ``"Type":"SASL_SCRAM_512_AUTH"`` . - ``BASIC_AUTH``  (Amazon MQ) The AWS Secrets Manager secret that stores your broker credentials. - ``BASIC_AUTH``  (Self-managed Apache Kafka) The Secrets Manager ARN of your secret key used for SASL/PLAIN authentication of your Apache Kafka brokers. - ``VPC_SUBNET``  (Self-managed Apache Kafka) The subnets associated with your VPC. Lambda connects to these subnets to fetch data from your self-managed Apache Kafka cluster. - ``VPC_SECURITY_GROUP``  (Self-managed Apache Kafka) The VPC security group used to manage access to your self-managed Apache Kafka brokers. - ``SASL_SCRAM_256_AUTH``  (Self-managed Apache Kafka) The Secrets Manager ARN of your secret key used for SASL SCRAM-256 authentication of your self-managed Apache Kafka brokers. - ``SASL_SCRAM_512_AUTH``  (Amazon MSK, Self-managed Apache Kafka) The Secrets Manager ARN of your secret key used for SASL SCRAM-512 authentication of your self-managed Apache Kafka brokers. - ``VIRTUAL_HOST`` - (RabbitMQ) The name of the virtual host in your RabbitMQ broker. Lambda uses this RabbitMQ host as the event source. This property cannot be specified in an UpdateEventSourceMapping API call. - ``CLIENT_CERTIFICATE_TLS_AUTH``  (Amazon MSK, self-managed Apache Kafka) The Secrets Manager ARN of your secret key containing the certificate chain (X.509 PEM), private key (PKCS#8 PEM), and private key password (optional) used for mutual TLS authentication of your MSK/Apache Kafka brokers. - ``SERVER_ROOT_CA_CERTIFICATE``  (Self-managed Apache Kafka) The Secrets Manager ARN of your secret key containing the root CA certificate (X.509 PEM) used for TLS encryption of your Apache Kafka brokers.\n')
    uri: typing.Optional[str] = pydantic.Field(None, description='The value for your chosen configuration in ``Type`` . For example: ``"URI": "arn:aws:secretsmanager:us-east-1:01234567890:secret:MyBrokerSecretName"`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-eventsourcemapping-sourceaccessconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    source_access_configuration_property = lambda.CfnEventSourceMapping.SourceAccessConfigurationProperty(\n        type="type",\n        uri="uri"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'uri']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping.SourceAccessConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.CodeProperty
class CfnFunction_CodePropertyDef(BaseStruct):
    image_uri: typing.Optional[str] = pydantic.Field(None, description='URI of a `container image <https://docs.aws.amazon.com/lambda/latest/dg/lambda-images.html>`_ in the Amazon ECR registry.\n')
    s3_bucket: typing.Optional[str] = pydantic.Field(None, description='An Amazon S3 bucket in the same AWS Region as your function. The bucket can be in a different AWS account .\n')
    s3_key: typing.Optional[str] = pydantic.Field(None, description='The Amazon S3 key of the deployment package.\n')
    s3_object_version: typing.Optional[str] = pydantic.Field(None, description='For versioned objects, the version of the deployment package object to use.\n')
    zip_file: typing.Optional[str] = pydantic.Field(None, description='(Node.js and Python) The source code of your Lambda function. If you include your function source inline with this parameter, AWS CloudFormation places it in a file named ``index`` and zips it to create a `deployment package <https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-package.html>`_ . This zip file cannot exceed 4MB. For the ``Handler`` property, the first part of the handler identifier must be ``index`` . For example, ``index.handler`` . For JSON, you must escape quotes and special characters such as newline ( ``\\n`` ) with a backslash. If you specify a function that interacts with an AWS CloudFormation custom resource, you don\'t have to write your own functions to send responses to the custom resource that invoked the function. AWS CloudFormation provides a response module ( `cfn-response <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html>`_ ) that simplifies sending responses. See `Using AWS Lambda with AWS CloudFormation <https://docs.aws.amazon.com/lambda/latest/dg/services-cloudformation.html>`_ for details.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    code_property = lambda.CfnFunction.CodeProperty(\n        image_uri="imageUri",\n        s3_bucket="s3Bucket",\n        s3_key="s3Key",\n        s3_object_version="s3ObjectVersion",\n        zip_file="zipFile"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image_uri', 's3_bucket', 's3_key', 's3_object_version', 'zip_file']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.CodeProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.DeadLetterConfigProperty
class CfnFunction_DeadLetterConfigPropertyDef(BaseStruct):
    target_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of an Amazon SQS queue or Amazon SNS topic.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-deadletterconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    dead_letter_config_property = lambda.CfnFunction.DeadLetterConfigProperty(\n        target_arn="targetArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['target_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.DeadLetterConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.EnvironmentProperty
class CfnFunction_EnvironmentPropertyDef(BaseStruct):
    variables: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='Environment variable key-value pairs. For more information, see `Using Lambda environment variables <https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-environment.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    environment_property = lambda.CfnFunction.EnvironmentProperty(\n        variables={\n            "variables_key": "variables"\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['variables']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.EnvironmentProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.EphemeralStorageProperty
class CfnFunction_EphemeralStoragePropertyDef(BaseStruct):
    size: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description="The size of the function's ``/tmp`` directory.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-ephemeralstorage.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    ephemeral_storage_property = lambda.CfnFunction.EphemeralStorageProperty(\n        size=123\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.EphemeralStorageProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.FileSystemConfigProperty
class CfnFunction_FileSystemConfigPropertyDef(BaseStruct):
    arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the Amazon EFS access point that provides access to the file system.\n')
    local_mount_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The path where the function can access the file system, starting with ``/mnt/`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-filesystemconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    file_system_config_property = lambda.CfnFunction.FileSystemConfigProperty(\n        arn="arn",\n        local_mount_path="localMountPath"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['arn', 'local_mount_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.FileSystemConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.ImageConfigProperty
class CfnFunction_ImageConfigPropertyDef(BaseStruct):
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Specifies parameters that you want to pass in with ENTRYPOINT. You can specify a maximum of 1,500 parameters in the list.\n')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Specifies the entry point to their application, which is typically the location of the runtime executable. You can specify a maximum of 1,500 string entries in the list.\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specifies the working directory. The length of the directory string cannot exceed 1,000 characters.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-imageconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    image_config_property = lambda.CfnFunction.ImageConfigProperty(\n        command=["command"],\n        entry_point=["entryPoint"],\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['command', 'entry_point', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.ImageConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.LoggingConfigProperty
class CfnFunction_LoggingConfigPropertyDef(BaseStruct):
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='Set this property to filter the application logs for your function that Lambda sends to CloudWatch. Lambda only sends application logs at the selected level of detail and lower, where ``TRACE`` is the highest level and ``FATAL`` is the lowest.\n')
    log_format: typing.Optional[str] = pydantic.Field(None, description="The format in which Lambda sends your function's application and system logs to CloudWatch. Select between plain text and structured JSON.\n")
    log_group: typing.Optional[str] = pydantic.Field(None, description='The name of the Amazon CloudWatch log group the function sends logs to. By default, Lambda functions send logs to a default log group named ``/aws/lambda/<function name>`` . To use a different log group, enter an existing log group or enter a new log group name.\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='Set this property to filter the system logs for your function that Lambda sends to CloudWatch. Lambda only sends system logs at the selected level of detail and lower, where ``DEBUG`` is the highest level and ``WARN`` is the lowest.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-loggingconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    logging_config_property = lambda.CfnFunction.LoggingConfigProperty(\n        application_log_level="applicationLogLevel",\n        log_format="logFormat",\n        log_group="logGroup",\n        system_log_level="systemLogLevel"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['application_log_level', 'log_format', 'log_group', 'system_log_level']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.LoggingConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.RuntimeManagementConfigProperty
class CfnFunction_RuntimeManagementConfigPropertyDef(BaseStruct):
    update_runtime_on: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specify the runtime update mode. - *Auto (default)* - Automatically update to the most recent and secure runtime version using a `Two-phase runtime version rollout <https://docs.aws.amazon.com/lambda/latest/dg/runtimes-update.html#runtime-management-two-phase>`_ . This is the best choice for most customers to ensure they always benefit from runtime updates. - *FunctionUpdate* - Lambda updates the runtime of you function to the most recent and secure runtime version when you update your function. This approach synchronizes runtime updates with function deployments, giving you control over when runtime updates are applied and allowing you to detect and mitigate rare runtime update incompatibilities early. When using this setting, you need to regularly update your functions to keep their runtime up-to-date. - *Manual* - You specify a runtime version in your function configuration. The function will use this runtime version indefinitely. In the rare case where a new runtime version is incompatible with an existing function, this allows you to roll back your function to an earlier runtime version. For more information, see `Roll back a runtime version <https://docs.aws.amazon.com/lambda/latest/dg/runtimes-update.html#runtime-management-rollback>`_ . *Valid Values* : ``Auto`` | ``FunctionUpdate`` | ``Manual``\n')
    runtime_version_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of the runtime version you want the function to use. .. epigraph:: This is only required if you\'re using the *Manual* runtime update mode.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-runtimemanagementconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    runtime_management_config_property = lambda.CfnFunction.RuntimeManagementConfigProperty(\n        update_runtime_on="updateRuntimeOn",\n\n        # the properties below are optional\n        runtime_version_arn="runtimeVersionArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['update_runtime_on', 'runtime_version_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.RuntimeManagementConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.SnapStartProperty
class CfnFunction_SnapStartPropertyDef(BaseStruct):
    apply_on: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Set ``ApplyOn`` to ``PublishedVersions`` to create a snapshot of the initialized execution environment when you publish a function version.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-snapstart.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    snap_start_property = lambda.CfnFunction.SnapStartProperty(\n        apply_on="applyOn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['apply_on']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.SnapStartProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.SnapStartResponseProperty
class CfnFunction_SnapStartResponsePropertyDef(BaseStruct):
    apply_on: typing.Optional[str] = pydantic.Field(None, description='When set to ``PublishedVersions`` , Lambda creates a snapshot of the execution environment when you publish a function version.\n')
    optimization_status: typing.Optional[str] = pydantic.Field(None, description='When you provide a `qualified Amazon Resource Name (ARN) <https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html#versioning-versions-using>`_ , this response element indicates whether SnapStart is activated for the specified function version.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-snapstartresponse.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    snap_start_response_property = lambda.CfnFunction.SnapStartResponseProperty(\n        apply_on="applyOn",\n        optimization_status="optimizationStatus"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['apply_on', 'optimization_status']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.SnapStartResponseProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.TracingConfigProperty
class CfnFunction_TracingConfigPropertyDef(BaseStruct):
    mode: typing.Optional[str] = pydantic.Field(None, description='The tracing mode.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-tracingconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    tracing_config_property = lambda.CfnFunction.TracingConfigProperty(\n        mode="mode"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.TracingConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunction.VpcConfigProperty
class CfnFunction_VpcConfigPropertyDef(BaseStruct):
    ipv6_allowed_for_dual_stack: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets.\n')
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of VPC security group IDs.\n')
    subnet_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of VPC subnet IDs.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-vpcconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    vpc_config_property = lambda.CfnFunction.VpcConfigProperty(\n        ipv6_allowed_for_dual_stack=False,\n        security_group_ids=["securityGroupIds"],\n        subnet_ids=["subnetIds"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['ipv6_allowed_for_dual_stack', 'security_group_ids', 'subnet_ids']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction.VpcConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnLayerVersion.ContentProperty
class CfnLayerVersion_ContentPropertyDef(BaseStruct):
    s3_bucket: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon S3 bucket of the layer archive.\n')
    s3_key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon S3 key of the layer archive.\n')
    s3_object_version: typing.Optional[str] = pydantic.Field(None, description='For versioned objects, the version of the layer archive object to use.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-layerversion-content.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    content_property = lambda.CfnLayerVersion.ContentProperty(\n        s3_bucket="s3Bucket",\n        s3_key="s3Key",\n\n        # the properties below are optional\n        s3_object_version="s3ObjectVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_bucket', 's3_key', 's3_object_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnLayerVersion.ContentProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnUrl.CorsProperty
class CfnUrl_CorsPropertyDef(BaseStruct):
    allow_credentials: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Whether you want to allow cookies or other credentials in requests to your function URL. The default is ``false`` .\n')
    allow_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The HTTP headers that origins can include in requests to your function URL. For example: ``Date`` , ``Keep-Alive`` , ``X-Custom-Header`` .\n')
    allow_methods: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The HTTP methods that are allowed when calling your function URL. For example: ``GET`` , ``POST`` , ``DELETE`` , or the wildcard character ( ``*`` ).\n')
    allow_origins: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The origins that can access your function URL. You can list any number of specific origins, separated by a comma. For example: ``https://www.example.com`` , ``http://localhost:60905`` . Alternatively, you can grant access to all origins with the wildcard character ( ``*`` ).\n')
    expose_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The HTTP headers in your function response that you want to expose to origins that call your function URL. For example: ``Date`` , ``Keep-Alive`` , ``X-Custom-Header`` .\n')
    max_age: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum amount of time, in seconds, that browsers can cache results of a preflight request. By default, this is set to ``0`` , which means the browser will not cache results.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-url-cors.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cors_property = lambda.CfnUrl.CorsProperty(\n        allow_credentials=False,\n        allow_headers=["allowHeaders"],\n        allow_methods=["allowMethods"],\n        allow_origins=["allowOrigins"],\n        expose_headers=["exposeHeaders"],\n        max_age=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_credentials', 'allow_headers', 'allow_methods', 'allow_origins', 'expose_headers', 'max_age']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnUrl.CorsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnVersion.ProvisionedConcurrencyConfigurationProperty
class CfnVersion_ProvisionedConcurrencyConfigurationPropertyDef(BaseStruct):
    provisioned_concurrent_executions: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='The amount of provisioned concurrency to allocate for the version.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-version-provisionedconcurrencyconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    provisioned_concurrency_configuration_property = lambda.CfnVersion.ProvisionedConcurrencyConfigurationProperty(\n        provisioned_concurrent_executions=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['provisioned_concurrent_executions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnVersion.ProvisionedConcurrencyConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnVersion.RuntimePolicyProperty
class CfnVersion_RuntimePolicyPropertyDef(BaseStruct):
    update_runtime_on: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime update mode.\n')
    runtime_version_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of the runtime the function is configured to use. If the runtime update mode is manual, the ARN is returned, otherwise null is returned.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-version-runtimepolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    runtime_policy_property = lambda.CfnVersion.RuntimePolicyProperty(\n        update_runtime_on="updateRuntimeOn",\n\n        # the properties below are optional\n        runtime_version_arn="runtimeVersionArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['update_runtime_on', 'runtime_version_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnVersion.RuntimePolicyProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CodeConfig
class CodeConfigDef(BaseStruct):
    image: typing.Union[models.aws_lambda.CodeImageConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Docker image configuration (mutually exclusive with ``s3Location`` and ``inlineCode``). Default: - code is not an ECR container image\n')
    inline_code: typing.Optional[str] = pydantic.Field(None, description='Inline code (mutually exclusive with ``s3Location`` and ``image``). Default: - code is not inline code\n')
    s3_location: typing.Union[models.aws_s3.LocationDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The location of the code in S3 (mutually exclusive with ``inlineCode`` and ``image``). Default: - code is not an s3 location\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    code_config = lambda.CodeConfig(\n        image=lambda.CodeImageConfig(\n            image_uri="imageUri",\n\n            # the properties below are optional\n            cmd=["cmd"],\n            entrypoint=["entrypoint"],\n            working_directory="workingDirectory"\n        ),\n        inline_code="inlineCode",\n        s3_location=Location(\n            bucket_name="bucketName",\n            object_key="objectKey",\n\n            # the properties below are optional\n            object_version="objectVersion"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image', 'inline_code', 's3_location']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CodeConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CodeImageConfig
class CodeImageConfigDef(BaseStruct):
    image_uri: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='URI to the Docker image.\n')
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    code_image_config = lambda.CodeImageConfig(\n        image_uri="imageUri",\n\n        # the properties below are optional\n        cmd=["cmd"],\n        entrypoint=["entrypoint"],\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['image_uri', 'cmd', 'entrypoint', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CodeImageConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CodeSigningConfigProps
class CodeSigningConfigPropsDef(BaseStruct):
    signing_profiles: typing.Union[typing.Sequence[typing.Union[models.aws_signer.SigningProfileDef]], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='List of signing profiles that defines a trusted user who can sign a code package.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Code signing configuration description. Default: - No description.\n')
    untrusted_artifact_on_deployment: typing.Optional[aws_cdk.aws_lambda.UntrustedArtifactOnDeployment] = pydantic.Field(None, description='Code signing configuration policy for deployment validation failure. If you set the policy to Enforce, Lambda blocks the deployment request if signature validation checks fail. If you set the policy to Warn, Lambda allows the deployment and creates a CloudWatch log. Default: UntrustedArtifactOnDeployment.WARN\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_signer as signer\n\n\n    signing_profile = signer.SigningProfile(self, "SigningProfile",\n        platform=signer.Platform.AWS_LAMBDA_SHA384_ECDSA\n    )\n\n    code_signing_config = lambda_.CodeSigningConfig(self, "CodeSigningConfig",\n        signing_profiles=[signing_profile]\n    )\n\n    lambda_.Function(self, "Function",\n        code_signing_config=code_signing_config,\n        runtime=lambda_.Runtime.NODEJS_18_X,\n        handler="index.handler",\n        code=lambda_.Code.from_asset(path.join(__dirname, "lambda-handler"))\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['signing_profiles', 'description', 'untrusted_artifact_on_deployment']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CodeSigningConfigProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CustomCommandOptions
class CustomCommandOptionsDef(BaseStruct):
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. If ``assetHashType`` is set it must be set to ``AssetHashType.CUSTOM``. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - based on ``assetHashType``\n')
    asset_hash_type: typing.Optional[aws_cdk.AssetHashType] = pydantic.Field(None, description='Specifies the type of hash to calculate for this asset. If ``assetHash`` is configured, this option must be ``undefined`` or ``AssetHashType.CUSTOM``. Default: - the default is ``AssetHashType.SOURCE``, but if ``assetHash`` is explicitly specified this value defaults to ``AssetHashType.CUSTOM``.\n')
    bundling: typing.Union[models.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundle the asset by executing a command in a Docker container or a custom bundling provider. The asset path will be mounted at ``/asset-input``. The Docker container is responsible for putting content at ``/asset-output``. The content at ``/asset-output`` will be zipped and used as the final asset. Default: - uploaded as-is to S3 if the asset is a regular file or a .zip file, archived into a .zip file and uploaded to S3 otherwise\n')
    exclude: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='File paths matching the patterns will be excluded. See ``ignoreMode`` to set the matching behavior. Has no effect on Assets bundled using the ``bundling`` property. Default: - nothing is excluded\n')
    follow_symlinks: typing.Optional[aws_cdk.SymlinkFollowMode] = pydantic.Field(None, description='A strategy for how to handle symlinks. Default: SymlinkFollowMode.NEVER\n')
    ignore_mode: typing.Optional[aws_cdk.IgnoreMode] = pydantic.Field(None, description='The ignore behavior to use for ``exclude`` patterns. Default: IgnoreMode.GLOB\n')
    deploy_time: typing.Optional[bool] = pydantic.Field(None, description='Whether or not the asset needs to exist beyond deployment time; i.e. are copied over to a different location and not needed afterwards. Setting this property to true has an impact on the lifecycle of the asset, because we will assume that it is safe to delete after the CloudFormation deployment succeeds. For example, Lambda Function assets are copied over to Lambda during deployment. Therefore, it is not necessary to store the asset in S3, so we consider those deployTime assets. Default: false\n')
    readers: typing.Optional[typing.Sequence[models.AnyResource]] = pydantic.Field(None, description='A list of principals that should be able to read this asset from S3. You can use ``asset.grantRead(principal)`` to grant read permissions later. Default: - No principals that can read file asset.\n')
    command_options: typing.Optional[typing.Mapping[str, typing.Any]] = pydantic.Field(None, description='options that are passed to the spawned process, which determine the characteristics of the spawned process. Default: : see ``child_process.SpawnSyncOptions`` (https://nodejs.org/api/child_process.html#child_processspawnsynccommand-args-options).\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_lambda as lambda_\n\n    # command_options: Any\n    # docker_image: cdk.DockerImage\n    # grantable: iam.IGrantable\n    # local_bundling: cdk.ILocalBundling\n\n    custom_command_options = lambda.CustomCommandOptions(\n        asset_hash="assetHash",\n        asset_hash_type=cdk.AssetHashType.SOURCE,\n        bundling=cdk.BundlingOptions(\n            image=docker_image,\n\n            # the properties below are optional\n            bundling_file_access=cdk.BundlingFileAccess.VOLUME_COPY,\n            command=["command"],\n            entrypoint=["entrypoint"],\n            environment={\n                "environment_key": "environment"\n            },\n            local=local_bundling,\n            network="network",\n            output_type=cdk.BundlingOutput.ARCHIVED,\n            platform="platform",\n            security_opt="securityOpt",\n            user="user",\n            volumes=[cdk.DockerVolume(\n                container_path="containerPath",\n                host_path="hostPath",\n\n                # the properties below are optional\n                consistency=cdk.DockerVolumeConsistency.CONSISTENT\n            )],\n            volumes_from=["volumesFrom"],\n            working_directory="workingDirectory"\n        ),\n        command_options={\n            "command_options_key": command_options\n        },\n        deploy_time=False,\n        exclude=["exclude"],\n        follow_symlinks=cdk.SymlinkFollowMode.NEVER,\n        ignore_mode=cdk.IgnoreMode.GLOB,\n        readers=[grantable]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['asset_hash', 'asset_hash_type', 'bundling', 'exclude', 'follow_symlinks', 'ignore_mode', 'deploy_time', 'readers', 'command_options']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CustomCommandOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.DestinationConfig
class DestinationConfigDef(BaseStruct):
    destination: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the destination resource.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    destination_config = lambda.DestinationConfig(\n        destination="destination"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.DestinationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.DestinationOptions
class DestinationOptionsDef(BaseStruct):
    type: typing.Union[aws_cdk.aws_lambda.DestinationType, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The destination type.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    destination_options = lambda.DestinationOptions(\n        type=lambda_.DestinationType.FAILURE\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.DestinationOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.DlqDestinationConfig
class DlqDestinationConfigDef(BaseStruct):
    destination: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the destination resource.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    dlq_destination_config = lambda.DlqDestinationConfig(\n        destination="destination"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.DlqDestinationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.DockerBuildAssetOptions
class DockerBuildAssetOptionsDef(BaseStruct):
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build args. Default: - no build args\n')
    cache_disabled: typing.Optional[bool] = pydantic.Field(None, description='Disable the cache and pass ``--no-cache`` to the ``docker build`` command. Default: - cache is used\n')
    cache_from: typing.Optional[typing.Sequence[typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Cache from options to pass to the ``docker build`` command. Default: - no cache from args are passed\n')
    cache_to: typing.Union[models.DockerCacheOptionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Cache to options to pass to the ``docker build`` command. Default: - no cache to args are passed\n')
    file: typing.Optional[str] = pydantic.Field(None, description='Name of the Dockerfile, must relative to the docker build path. Default: ``Dockerfile``\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    target_stage: typing.Optional[str] = pydantic.Field(None, description='Set build target for multi-stage container builds. Any stage defined afterwards will be ignored. Example value: ``build-env`` Default: - Build all stages defined in the Dockerfile\n')
    image_path: typing.Optional[str] = pydantic.Field(None, description='The path in the Docker image where the asset is located after the build operation. Default: /asset\n')
    output_path: typing.Optional[str] = pydantic.Field(None, description='The path on the local filesystem where the asset will be copied using ``docker cp``. Default: - a unique temporary directory in the system temp directory\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    docker_build_asset_options = lambda.DockerBuildAssetOptions(\n        build_args={\n            "build_args_key": "buildArgs"\n        },\n        cache_disabled=False,\n        cache_from=[DockerCacheOption(\n            type="type",\n\n            # the properties below are optional\n            params={\n                "params_key": "params"\n            }\n        )],\n        cache_to=DockerCacheOption(\n            type="type",\n\n            # the properties below are optional\n            params={\n                "params_key": "params"\n            }\n        ),\n        file="file",\n        image_path="imagePath",\n        output_path="outputPath",\n        platform="platform",\n        target_stage="targetStage"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['build_args', 'cache_disabled', 'cache_from', 'cache_to', 'file', 'platform', 'target_stage', 'image_path', 'output_path']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.DockerBuildAssetOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.DockerImageFunctionProps
class DockerImageFunctionPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functions /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    code: typing.Union[models.aws_lambda.DockerImageCodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n\n:exampleMetadata: infused\n\nExample::\n\n    lambda_.DockerImageFunction(self, "AssetFunction",\n        code=lambda_.DockerImageCode.from_image_asset(path.join(__dirname, "docker-handler"))\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'code']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.DockerImageFunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.EcrImageCodeProps
class EcrImageCodePropsDef(BaseStruct):
    cmd: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the CMD on the specified Docker image or Dockerfile. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the CMD specified in the docker image or Dockerfile.\n")
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="Specify or override the ENTRYPOINT on the specified Docker image or Dockerfile. An ENTRYPOINT allows you to configure a container that will run as an executable. This needs to be in the 'exec form', viz., ``[ 'executable', 'param1', 'param2' ]``. Default: - use the ENTRYPOINT in the docker image or Dockerfile.\n")
    tag: typing.Optional[str] = pydantic.Field(None, description="(deprecated) The image tag to use when pulling the image from ECR. Default: 'latest'\n")
    tag_or_digest: typing.Optional[str] = pydantic.Field(None, description="The image tag or digest to use when pulling the image from ECR (digests must start with ``sha256:``). Default: 'latest'\n")
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Specify or override the WORKDIR on the specified Docker image or Dockerfile. A WORKDIR allows you to configure the working directory the container will use. Default: - use the WORKDIR in the docker image or Dockerfile.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    ecr_image_code_props = lambda.EcrImageCodeProps(\n        cmd=["cmd"],\n        entrypoint=["entrypoint"],\n        tag="tag",\n        tag_or_digest="tagOrDigest",\n        working_directory="workingDirectory"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cmd', 'entrypoint', 'tag', 'tag_or_digest', 'working_directory']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EcrImageCodeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.EnvironmentOptions
class EnvironmentOptionsDef(BaseStruct):
    remove_in_edge: typing.Optional[bool] = pydantic.Field(None, description='When used in Lambda@Edge via edgeArn() API, these environment variables will be removed. If not set, an error will be thrown. Default: false - using the function in Lambda@Edge will throw\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    environment_options = lambda.EnvironmentOptions(\n        remove_in_edge=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['remove_in_edge']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EnvironmentOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.EventInvokeConfigOptions
class EventInvokeConfigOptionsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    # destination: lambda.IDestination\n\n    event_invoke_config_options = lambda.EventInvokeConfigOptions(\n        max_event_age=cdk.Duration.minutes(30),\n        on_failure=destination,\n        on_success=destination,\n        retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EventInvokeConfigOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.EventInvokeConfigProps
class EventInvokeConfigPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    function: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Lambda function.\n')
    qualifier: typing.Optional[str] = pydantic.Field(None, description='The qualifier. Default: - latest version\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    # destination: lambda.IDestination\n    # function_: lambda.Function\n\n    event_invoke_config_props = lambda.EventInvokeConfigProps(\n        function=function_,\n\n        # the properties below are optional\n        max_event_age=cdk.Duration.minutes(30),\n        on_failure=destination,\n        on_success=destination,\n        qualifier="qualifier",\n        retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'function', 'qualifier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EventInvokeConfigProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.EventSourceMappingOptions
class EventSourceMappingOptionsDef(BaseStruct):
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    # event_source_dlq: lambda.IEventSourceDlq\n    # filters: Any\n    # source_access_configuration_type: lambda.SourceAccessConfigurationType\n\n    event_source_mapping_options = lambda.EventSourceMappingOptions(\n        batch_size=123,\n        bisect_batch_on_error=False,\n        enabled=False,\n        event_source_arn="eventSourceArn",\n        filters=[{\n            "filters_key": filters\n        }],\n        kafka_bootstrap_servers=["kafkaBootstrapServers"],\n        kafka_consumer_group_id="kafkaConsumerGroupId",\n        kafka_topic="kafkaTopic",\n        max_batching_window=cdk.Duration.minutes(30),\n        max_concurrency=123,\n        max_record_age=cdk.Duration.minutes(30),\n        on_failure=event_source_dlq,\n        parallelization_factor=123,\n        report_batch_item_failures=False,\n        retry_attempts=123,\n        source_access_configurations=[lambda.SourceAccessConfiguration(\n            type=source_access_configuration_type,\n            uri="uri"\n        )],\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON,\n        starting_position_timestamp=123,\n        support_s3_on_failure_destination=False,\n        tumbling_window=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['batch_size', 'bisect_batch_on_error', 'enabled', 'event_source_arn', 'filters', 'kafka_bootstrap_servers', 'kafka_consumer_group_id', 'kafka_topic', 'max_batching_window', 'max_concurrency', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'source_access_configurations', 'starting_position', 'starting_position_timestamp', 'support_s3_on_failure_destination', 'tumbling_window']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EventSourceMappingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.EventSourceMappingProps
class EventSourceMappingPropsDef(BaseStruct):
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None\n')
    target: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The target AWS Lambda function.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    # event_source_dlq: lambda.IEventSourceDlq\n    # filters: Any\n    # function_: lambda.Function\n    # source_access_configuration_type: lambda.SourceAccessConfigurationType\n\n    event_source_mapping_props = lambda.EventSourceMappingProps(\n        target=function_,\n\n        # the properties below are optional\n        batch_size=123,\n        bisect_batch_on_error=False,\n        enabled=False,\n        event_source_arn="eventSourceArn",\n        filters=[{\n            "filters_key": filters\n        }],\n        kafka_bootstrap_servers=["kafkaBootstrapServers"],\n        kafka_consumer_group_id="kafkaConsumerGroupId",\n        kafka_topic="kafkaTopic",\n        max_batching_window=cdk.Duration.minutes(30),\n        max_concurrency=123,\n        max_record_age=cdk.Duration.minutes(30),\n        on_failure=event_source_dlq,\n        parallelization_factor=123,\n        report_batch_item_failures=False,\n        retry_attempts=123,\n        source_access_configurations=[lambda.SourceAccessConfiguration(\n            type=source_access_configuration_type,\n            uri="uri"\n        )],\n        starting_position=lambda_.StartingPosition.TRIM_HORIZON,\n        starting_position_timestamp=123,\n        support_s3_on_failure_destination=False,\n        tumbling_window=cdk.Duration.minutes(30)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['batch_size', 'bisect_batch_on_error', 'enabled', 'event_source_arn', 'filters', 'kafka_bootstrap_servers', 'kafka_consumer_group_id', 'kafka_topic', 'max_batching_window', 'max_concurrency', 'max_record_age', 'on_failure', 'parallelization_factor', 'report_batch_item_failures', 'retry_attempts', 'source_access_configurations', 'starting_position', 'starting_position_timestamp', 'support_s3_on_failure_destination', 'tumbling_window', 'target']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.EventSourceMappingProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.FileSystemConfig
class FileSystemConfigDef(BaseStruct):
    arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='ARN of the access point.\n')
    local_mount_path: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='mount path in the lambda runtime environment.\n')
    connections: typing.Optional[models.aws_ec2.ConnectionsDef] = pydantic.Field(None, description='connections object used to allow ingress traffic from lambda function. Default: - no connections required to add extra ingress rules for Lambda function\n')
    dependency: typing.Optional[typing.Sequence[models.UnsupportedResource]] = pydantic.Field(None, description='array of IDependable that lambda function depends on. Default: - no dependency\n')
    policies: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='additional IAM policies required for the lambda function. Default: - no additional policies required\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_lambda as lambda_\n    import constructs as constructs\n\n    # connections: ec2.Connections\n    # dependable: constructs.IDependable\n    # policy_statement: iam.PolicyStatement\n\n    file_system_config = lambda.FileSystemConfig(\n        arn="arn",\n        local_mount_path="localMountPath",\n\n        # the properties below are optional\n        connections=connections,\n        dependency=[dependable],\n        policies=[policy_statement]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['arn', 'local_mount_path', 'connections', 'dependency', 'policies']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FileSystemConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.FunctionAttributes
class FunctionAttributesDef(BaseStruct):
    function_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of the Lambda function. Format: arn::lambda:::function:\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The architecture of this Lambda Function (this is an optional attribute and defaults to X86_64). Default: - Architecture.X86_64\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM execution role associated with this function. If the role is not specified, any role-related operations will no-op.\n')
    same_environment: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function is in the same environment as the stack. This affects certain behaviours such as, whether this function's permission can be modified. When not configured, the CDK attempts to auto-determine this. For environment agnostic stacks, i.e., stacks where the account is not specified with the ``env`` property, this is determined to be false. Set this to property *ONLY IF* the imported function is in the same account as the stack it's imported in. Default: - depends: true, if the Stack is configured with an explicit ``env`` (account and region) and the account is the same as this function. For environment-agnostic stacks this will default to ``false``.\n")
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group of this Lambda, if in a VPC. This needs to be given in order to support allowing connections to this Lambda.\n')
    skip_permissions: typing.Optional[bool] = pydantic.Field(None, description='Setting this property informs the CDK that the imported function ALREADY HAS the necessary permissions for what you are trying to do. When not configured, the CDK attempts to auto-determine whether or not additional permissions are necessary on the function when grant APIs are used. If the CDK tried to add permissions on an imported lambda, it will fail. Set this property *ONLY IF* you are committing to manage the imported function\'s permissions outside of CDK. You are acknowledging that your CDK code alone will have insufficient permissions to access the imported function. Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    fn = lambda_.Function.from_function_attributes(self, "Function",\n        function_arn="arn:aws:lambda:us-east-1:123456789012:function:MyFn",\n        # The following are optional properties for specific use cases and should be used with caution:\n\n        # Use Case: imported function is in the same account as the stack. This tells the CDK that it\n        # can modify the function\'s permissions.\n        same_environment=True,\n\n        # Use Case: imported function is in a different account and user commits to ensuring that the\n        # imported function has the correct permissions outside the CDK.\n        skip_permissions=True\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['function_arn', 'architecture', 'role', 'same_environment', 'security_group', 'skip_permissions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.FunctionOptions
class FunctionOptionsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functions /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_codeguruprofiler as codeguruprofiler\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_kms as kms\n    from aws_cdk import aws_lambda as lambda_\n    from aws_cdk import aws_logs as logs\n    from aws_cdk import aws_sns as sns\n    from aws_cdk import aws_sqs as sqs\n\n    # adot_layer_version: lambda.AdotLayerVersion\n    # architecture: lambda.Architecture\n    # code_signing_config: lambda.CodeSigningConfig\n    # destination: lambda.IDestination\n    # event_source: lambda.IEventSource\n    # file_system: lambda.FileSystem\n    # key: kms.Key\n    # lambda_insights_version: lambda.LambdaInsightsVersion\n    # layer_version: lambda.LayerVersion\n    # log_group: logs.LogGroup\n    # params_and_secrets_layer_version: lambda.ParamsAndSecretsLayerVersion\n    # policy_statement: iam.PolicyStatement\n    # profiling_group: codeguruprofiler.ProfilingGroup\n    # queue: sqs.Queue\n    # role: iam.Role\n    # runtime_management_mode: lambda.RuntimeManagementMode\n    # security_group: ec2.SecurityGroup\n    # size: cdk.Size\n    # snap_start_conf: lambda.SnapStartConf\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # topic: sns.Topic\n    # vpc: ec2.Vpc\n\n    function_options = lambda.FunctionOptions(\n        adot_instrumentation=lambda.AdotInstrumentationConfig(\n            exec_wrapper=lambda_.AdotLambdaExecWrapper.REGULAR_HANDLER,\n            layer_version=adot_layer_version\n        ),\n        allow_all_outbound=False,\n        allow_public_subnet=False,\n        application_log_level="applicationLogLevel",\n        application_log_level_v2=lambda_.ApplicationLogLevel.INFO,\n        architecture=architecture,\n        code_signing_config=code_signing_config,\n        current_version_options=lambda.VersionOptions(\n            code_sha256="codeSha256",\n            description="description",\n            max_event_age=cdk.Duration.minutes(30),\n            on_failure=destination,\n            on_success=destination,\n            provisioned_concurrent_executions=123,\n            removal_policy=cdk.RemovalPolicy.DESTROY,\n            retry_attempts=123\n        ),\n        dead_letter_queue=queue,\n        dead_letter_queue_enabled=False,\n        dead_letter_topic=topic,\n        description="description",\n        environment={\n            "environment_key": "environment"\n        },\n        environment_encryption=key,\n        ephemeral_storage_size=size,\n        events=[event_source],\n        filesystem=file_system,\n        function_name="functionName",\n        initial_policy=[policy_statement],\n        insights_version=lambda_insights_version,\n        ipv6_allowed_for_dual_stack=False,\n        layers=[layer_version],\n        log_format="logFormat",\n        logging_format=lambda_.LoggingFormat.TEXT,\n        log_group=log_group,\n        log_retention=logs.RetentionDays.ONE_DAY,\n        log_retention_retry_options=lambda.LogRetentionRetryOptions(\n            base=cdk.Duration.minutes(30),\n            max_retries=123\n        ),\n        log_retention_role=role,\n        max_event_age=cdk.Duration.minutes(30),\n        memory_size=123,\n        on_failure=destination,\n        on_success=destination,\n        params_and_secrets=params_and_secrets_layer_version,\n        profiling=False,\n        profiling_group=profiling_group,\n        reserved_concurrent_executions=123,\n        retry_attempts=123,\n        role=role,\n        runtime_management_mode=runtime_management_mode,\n        security_groups=[security_group],\n        snap_start=snap_start_conf,\n        system_log_level="systemLogLevel",\n        system_log_level_v2=lambda_.SystemLogLevel.INFO,\n        timeout=cdk.Duration.minutes(30),\n        tracing=lambda_.Tracing.ACTIVE,\n        vpc=vpc,\n        vpc_subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.FunctionProps
class FunctionPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functions /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_lambda as lambda_\n\n\n    fn = lambda_.Function(self, "MyFunc",\n        runtime=lambda_.Runtime.NODEJS_LATEST,\n        handler="index.handler",\n        code=lambda_.Code.from_inline("exports.handler = handler.toString()")\n    )\n\n    rule = events.Rule(self, "rule",\n        event_pattern=events.EventPattern(\n            source=["aws.ec2"]\n        )\n    )\n\n    queue = sqs.Queue(self, "Queue")\n\n    rule.add_target(targets.LambdaFunction(fn,\n        dead_letter_queue=queue,  # Optional: add a dead letter queue\n        max_event_age=Duration.hours(2),  # Optional: set the maxEventAge retry policy\n        retry_attempts=2\n    ))\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'code', 'handler', 'runtime']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.FunctionPropsDefConfig] = pydantic.Field(None)


class FunctionPropsDefConfig(pydantic.BaseModel):
    code_config: typing.Optional[models.aws_lambda.CodeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.FunctionUrlCorsOptions
class FunctionUrlCorsOptionsDef(BaseStruct):
    allow_credentials: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow cookies or other credentials in requests to your function URL. Default: false\n')
    allowed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Headers that are specified in the Access-Control-Request-Headers header. Default: - No headers allowed.\n')
    allowed_methods: typing.Optional[typing.Sequence[aws_cdk.aws_lambda.HttpMethod]] = pydantic.Field(None, description='An HTTP method that you allow the origin to execute. Default: - [HttpMethod.ALL]\n')
    allowed_origins: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more origins you want customers to be able to access the bucket from. Default: - No origins allowed.\n')
    exposed_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more headers in the response that you want customers to be able to access from their applications. Default: - No headers exposed.\n')
    max_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time in seconds that your browser is to cache the preflight response for the specified resource. Default: - Browser default of 5 seconds.\n\n:exampleMetadata: infused\n\nExample::\n\n    # fn: lambda.Function\n\n\n    fn.add_function_url(\n        auth_type=lambda_.FunctionUrlAuthType.NONE,\n        cors=lambda.FunctionUrlCorsOptions(\n            # Allow this to be called from websites on https://example.com.\n            # Can also be [\'*\'] to allow all domain.\n            allowed_origins=["https://example.com"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_credentials', 'allowed_headers', 'allowed_methods', 'allowed_origins', 'exposed_headers', 'max_age']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionUrlCorsOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.FunctionUrlOptions
class FunctionUrlOptionsDef(BaseStruct):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED\n\n:exampleMetadata: infused\n\nExample::\n\n    # Can be a Function or an Alias\n    # fn: lambda.Function\n\n\n    fn_url = fn.add_function_url(\n        auth_type=lambda_.FunctionUrlAuthType.NONE\n    )\n\n    CfnOutput(self, "TheUrl",\n        value=fn_url.url\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth_type', 'cors', 'invoke_mode']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionUrlOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.FunctionUrlProps
class FunctionUrlPropsDef(BaseStruct):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED\n')
    function: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The function to which this url refers. It can also be an ``Alias`` but not a ``Version``.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    # function_: lambda.Function\n\n    function_url_props = lambda.FunctionUrlProps(\n        function=function_,\n\n        # the properties below are optional\n        auth_type=lambda_.FunctionUrlAuthType.AWS_IAM,\n        cors=lambda.FunctionUrlCorsOptions(\n            allow_credentials=False,\n            allowed_headers=["allowedHeaders"],\n            allowed_methods=[lambda_.HttpMethod.GET],\n            allowed_origins=["allowedOrigins"],\n            exposed_headers=["exposedHeaders"],\n            max_age=cdk.Duration.minutes(30)\n        ),\n        invoke_mode=lambda_.InvokeMode.BUFFERED\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth_type', 'cors', 'invoke_mode', 'function']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.FunctionUrlProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.LambdaRuntimeProps
class LambdaRuntimePropsDef(BaseStruct):
    bundling_docker_image: typing.Optional[str] = pydantic.Field(None, description='The Docker image name to be used for bundling in this runtime. Default: - the latest docker image "amazon/public.ecr.aws/sam/build-" from https://gallery.ecr.aws')
    is_variable: typing.Optional[bool] = pydantic.Field(None, description='Whether the runtime enum is meant to change over time, IE NODEJS_LATEST. Default: false\n')
    supports_code_guru_profiling: typing.Optional[bool] = pydantic.Field(None, description='Whether this runtime is integrated with and supported for profiling using Amazon CodeGuru Profiler. Default: false\n')
    supports_inline_code: typing.Optional[bool] = pydantic.Field(None, description='Whether the ``ZipFile`` (aka inline code) property can be used with this runtime. Default: false\n')
    supports_snap_start: typing.Optional[bool] = pydantic.Field(None, description='Whether this runtime supports SnapStart. Default: false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    lambda_runtime_props = lambda.LambdaRuntimeProps(\n        bundling_docker_image="bundlingDockerImage",\n        is_variable=False,\n        supports_code_guru_profiling=False,\n        supports_inline_code=False,\n        supports_snap_start=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bundling_docker_image', 'is_variable', 'supports_code_guru_profiling', 'supports_inline_code', 'supports_snap_start']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LambdaRuntimeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.LayerVersionAttributes
class LayerVersionAttributesDef(BaseStruct):
    layer_version_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of the LayerVersion.\n')
    compatible_runtimes: typing.Optional[typing.Sequence[models.aws_lambda.RuntimeDef]] = pydantic.Field(None, description='The list of compatible runtimes with this Layer.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    # runtime: lambda.Runtime\n\n    layer_version_attributes = lambda.LayerVersionAttributes(\n        layer_version_arn="layerVersionArn",\n\n        # the properties below are optional\n        compatible_runtimes=[runtime]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['layer_version_arn', 'compatible_runtimes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LayerVersionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.LayerVersionOptions
class LayerVersionOptionsDef(BaseStruct):
    description: typing.Optional[str] = pydantic.Field(None, description='The description the this Lambda Layer. Default: - No description.\n')
    layer_version_name: typing.Optional[str] = pydantic.Field(None, description='The name of the layer. Default: - A name will be generated.\n')
    license: typing.Optional[str] = pydantic.Field(None, description='The SPDX licence identifier or URL to the license file for this layer. Default: - No license information will be recorded.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Whether to retain this version of the layer when a new version is added or when the stack is deleted. Default: RemovalPolicy.DESTROY\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    layer_version_options = lambda.LayerVersionOptions(\n        description="description",\n        layer_version_name="layerVersionName",\n        license="license",\n        removal_policy=cdk.RemovalPolicy.DESTROY\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['description', 'layer_version_name', 'license', 'removal_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LayerVersionOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.LayerVersionPermission
class LayerVersionPermissionDef(BaseStruct):
    account_id: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The AWS Account id of the account that is authorized to use a Lambda Layer Version. The wild-card ``\'*\'`` can be used to grant access to "any" account (or any account in an organization when ``organizationId`` is specified).\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The ID of the AWS Organization to which the grant is restricted. Can only be specified if ``accountId`` is ``\'*\'``\n\n:exampleMetadata: lit=aws-lambda/test/integ.layer-version.lit.ts infused\n\nExample::\n\n    layer = lambda_.LayerVersion(stack, "MyLayer",\n        code=lambda_.Code.from_asset(path.join(__dirname, "layer-code")),\n        compatible_runtimes=[lambda_.Runtime.NODEJS_LATEST],\n        license="Apache-2.0",\n        description="A layer to test the L2 construct"\n    )\n\n    # To grant usage by other AWS accounts\n    layer.add_permission("remote-account-grant", account_id=aws_account_id)\n\n    # To grant usage to all accounts in some AWS Ogranization\n    # layer.grantUsage({ accountId: \'*\', organizationId });\n\n    lambda_.Function(stack, "MyLayeredLambda",\n        code=lambda_.InlineCode("foo"),\n        handler="index.handler",\n        runtime=lambda_.Runtime.NODEJS_LATEST,\n        layers=[layer]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['account_id', 'organization_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LayerVersionPermission'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.LayerVersionProps
class LayerVersionPropsDef(BaseStruct):
    description: typing.Optional[str] = pydantic.Field(None, description='The description the this Lambda Layer. Default: - No description.')
    layer_version_name: typing.Optional[str] = pydantic.Field(None, description='The name of the layer. Default: - A name will be generated.\n')
    license: typing.Optional[str] = pydantic.Field(None, description='The SPDX licence identifier or URL to the license file for this layer. Default: - No license information will be recorded.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Whether to retain this version of the layer when a new version is added or when the stack is deleted. Default: RemovalPolicy.DESTROY\n')
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The content of this Layer. Using ``Code.fromInline`` is not supported.\n')
    compatible_architectures: typing.Optional[typing.Sequence[models.aws_lambda.ArchitectureDef]] = pydantic.Field(None, description='The system architectures compatible with this layer. Default: [Architecture.X86_64]\n')
    compatible_runtimes: typing.Optional[typing.Sequence[models.aws_lambda.RuntimeDef]] = pydantic.Field(None, description='The runtimes compatible with this Layer. Default: - All runtimes are supported.\n\n:exampleMetadata: infused\n\nExample::\n\n    lambda_.LayerVersion(self, "MyLayer",\n        removal_policy=RemovalPolicy.RETAIN,\n        code=lambda_.Code.from_asset(path.join(__dirname, "lambda-handler")),\n        compatible_architectures=[lambda_.Architecture.X86_64, lambda_.Architecture.ARM_64]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['description', 'layer_version_name', 'license', 'removal_policy', 'code', 'compatible_architectures', 'compatible_runtimes']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LayerVersionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.LayerVersionPropsDefConfig] = pydantic.Field(None)


class LayerVersionPropsDefConfig(pydantic.BaseModel):
    code_config: typing.Optional[models.aws_lambda.CodeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.LogRetentionRetryOptions
class LogRetentionRetryOptionsDef(BaseStruct):
    base: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) The base duration to use in the exponential backoff for operation retries. Default: - none, not used anymore\n')
    max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum amount of retries. Default: 5\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    log_retention_retry_options = lambda.LogRetentionRetryOptions(\n        base=cdk.Duration.minutes(30),\n        max_retries=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['base', 'max_retries']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.LogRetentionRetryOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.ParamsAndSecretsOptions
class ParamsAndSecretsOptionsDef(BaseStruct):
    cache_enabled: typing.Optional[bool] = pydantic.Field(None, description='Whether the Parameters and Secrets Extension will cache parameters and secrets. Default: true\n')
    cache_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of secrets and parameters to cache. Must be a value from 0 to 1000. A value of 0 means there is no caching. Note: This variable is ignored if parameterStoreTtl and secretsManagerTtl are 0. Default: 1000\n')
    http_port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the local HTTP server. Valid port numbers are 1 - 65535. Default: 2773\n')
    log_level: typing.Optional[aws_cdk.aws_lambda.ParamsAndSecretsLogLevel] = pydantic.Field(None, description='The level of logging provided by the Parameters and Secrets Extension. Note: Set to debug to see the cache configuration. Default: - Logging level will be ``info``\n')
    max_connections: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of connection for HTTP clients that the Parameters and Secrets Extension uses to make requests to Parameter Store or Secrets Manager. There is no maximum limit. Minimum is 1. Note: Every running copy of this Lambda function may open the number of connections specified by this property. Thus, the total number of connections may exceed this number. Default: 3\n')
    parameter_store_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout for requests to Parameter Store. A value of 0 means that there is no timeout. Default: 0\n')
    parameter_store_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time-to-live of a parameter in the cache. A value of 0 means there is no caching. The maximum time-to-live is 300 seconds. Note: This variable is ignored if cacheSize is 0. Default: 300 seconds\n')
    secrets_manager_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout for requests to Secrets Manager. A value of 0 means that there is no timeout. Default: 0\n')
    secrets_manager_ttl: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time-to-live of a secret in the cache. A value of 0 means there is no caching. The maximum time-to-live is 300 seconds. Note: This variable is ignored if cacheSize is 0. Default: 300 seconds\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_secretsmanager as sm\n    import aws_cdk.aws_ssm as ssm\n\n\n    secret = sm.Secret(self, "Secret")\n    parameter = ssm.StringParameter(self, "Parameter",\n        parameter_name="mySsmParameterName",\n        string_value="mySsmParameterValue"\n    )\n\n    params_and_secrets = lambda_.ParamsAndSecretsLayerVersion.from_version(lambda_.ParamsAndSecretsVersions.V1_0_103,\n        cache_size=500,\n        log_level=lambda_.ParamsAndSecretsLogLevel.DEBUG\n    )\n\n    lambda_function = lambda_.Function(self, "MyFunction",\n        runtime=lambda_.Runtime.NODEJS_18_X,\n        handler="index.handler",\n        architecture=lambda_.Architecture.ARM_64,\n        code=lambda_.Code.from_asset(path.join(__dirname, "lambda-handler")),\n        params_and_secrets=params_and_secrets\n    )\n\n    secret.grant_read(lambda_function)\n    parameter.grant_read(lambda_function)\n')
    _init_params: typing.ClassVar[list[str]] = ['cache_enabled', 'cache_size', 'http_port', 'log_level', 'max_connections', 'parameter_store_timeout', 'parameter_store_ttl', 'secrets_manager_timeout', 'secrets_manager_ttl']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.ParamsAndSecretsOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.Permission
class PermissionDef(BaseStruct):
    principal: typing.Union[_REQUIRED_INIT_PARAM, models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Grant permissions to a service\n    # fn: lambda.Function\n\n    principal = iam.ServicePrincipal("my-service")\n\n    fn.grant_invoke(principal)\n\n    # Equivalent to:\n    fn.add_permission("my-service Invocation",\n        principal=principal\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['principal', 'action', 'event_source_token', 'function_url_auth_type', 'organization_id', 'source_account', 'source_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.Permission'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.PermissionDefConfig] = pydantic.Field(None)


class PermissionDefConfig(pydantic.BaseModel):
    principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.ResourceBindOptions
class ResourceBindOptionsDef(BaseStruct):
    resource_property: typing.Optional[str] = pydantic.Field(None, description='The name of the CloudFormation property to annotate with asset metadata. Default: Code')
    _init_params: typing.ClassVar[list[str]] = ['resource_property']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.ResourceBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.SingletonFunctionProps
class SingletonFunctionPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functions /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n')
    uuid: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A unique identifier to identify this lambda. The identifier should be unique across all custom resource providers. We recommend generating a UUID per provider.\n')
    lambda_purpose: typing.Optional[str] = pydantic.Field(None, description='A descriptive name for the purpose of this Lambda. If the Lambda does not have a physical name, this string will be reflected its generated name. The combination of lambdaPurpose and uuid must be unique. Default: SingletonLambda\n\n:exampleMetadata: infused\n\nExample::\n\n    fn = lambda_.SingletonFunction(self, "MyProvider", function_props)\n\n    CustomResource(self, "MyResource",\n        service_token=fn.function_arn\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'code', 'handler', 'runtime', 'uuid', 'lambda_purpose']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.SingletonFunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.SingletonFunctionPropsDefConfig] = pydantic.Field(None)


class SingletonFunctionPropsDefConfig(pydantic.BaseModel):
    code_config: typing.Optional[models.aws_lambda.CodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.SourceAccessConfiguration
class SourceAccessConfigurationDef(BaseStruct):
    type: typing.Union[models.aws_lambda.SourceAccessConfigurationTypeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of authentication protocol or the VPC components for your event source. For example: "SASL_SCRAM_512_AUTH".\n')
    uri: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The value for your chosen configuration in type. For example: "URI": "arn:aws:secretsmanager:us-east-1:01234567890:secret:MyBrokerSecretName". The exact string depends on the type.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    # source_access_configuration_type: lambda.SourceAccessConfigurationType\n\n    source_access_configuration = lambda.SourceAccessConfiguration(\n        type=source_access_configuration_type,\n        uri="uri"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'uri']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.SourceAccessConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.UtilizationScalingOptions
class UtilizationScalingOptionsDef(BaseStruct):
    disable_scale_in: typing.Optional[bool] = pydantic.Field(None, description="Indicates whether scale in by the target tracking policy is disabled. If the value is true, scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. Otherwise, scale in is enabled and the target tracking policy can remove capacity from the scalable resource. Default: false\n")
    policy_name: typing.Optional[str] = pydantic.Field(None, description='A name for the scaling policy. Default: - Automatically generated name.\n')
    scale_in_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale in activity completes before another scale in activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    scale_out_cooldown: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Period after a scale out activity completes before another scale out activity can start. Default: Duration.seconds(300) for the following scalable targets: ECS services, Spot Fleet requests, EMR clusters, AppStream 2.0 fleets, Aurora DB clusters, Amazon SageMaker endpoint variants, Custom resources. For all other scalable targets, the default value is Duration.seconds(0): DynamoDB tables, DynamoDB global secondary indexes, Amazon Comprehend document classification endpoints, Lambda provisioned concurrency\n')
    utilization_target: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Utilization target for the attribute. For example, .5 indicates that 50 percent of allocated provisioned concurrency is in use.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_autoscaling as autoscaling\n\n    # fn: lambda.Function\n\n    alias = fn.add_alias("prod")\n\n    # Create AutoScaling target\n    as = alias.add_auto_scaling(max_capacity=50)\n\n    # Configure Target Tracking\n    as.scale_on_utilization(\n        utilization_target=0.5\n    )\n\n    # Configure Scheduled Scaling\n    as.scale_on_schedule("ScaleUpInTheMorning",\n        schedule=autoscaling.Schedule.cron(hour="8", minute="0"),\n        min_capacity=20\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['disable_scale_in', 'policy_name', 'scale_in_cooldown', 'scale_out_cooldown', 'utilization_target']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.UtilizationScalingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.VersionAttributes
class VersionAttributesDef(BaseStruct):
    lambda_: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The lambda function.')
    version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The version.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    # function_: lambda.Function\n\n    version_attributes = lambda.VersionAttributes(\n        lambda_=function_,\n        version="version"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['lambda_', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.VersionAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.VersionAttributesDefConfig] = pydantic.Field(None)


class VersionAttributesDefConfig(pydantic.BaseModel):
    lambda__config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.VersionOptions
class VersionOptionsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    code_sha256: typing.Optional[str] = pydantic.Field(None, description="SHA256 of the version of the Lambda source code. Specify to validate that you're deploying the right version. Default: No validation is performed\n")
    description: typing.Optional[str] = pydantic.Field(None, description='Description of the version. Default: Description of the Lambda\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's version. Default: No provisioned concurrency\n")
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Whether to retain old versions of this function when a new version is created. Default: RemovalPolicy.DESTROY\n\n:exampleMetadata: infused\n\nExample::\n\n    fn = lambda_.Function(self, "MyFunction",\n        current_version_options=lambda.VersionOptions(\n            removal_policy=RemovalPolicy.RETAIN,  # retain old versions\n            retry_attempts=1\n        ),\n        runtime=lambda_.Runtime.NODEJS_18_X,\n        handler="index.handler",\n        code=lambda_.Code.from_asset(path.join(__dirname, "lambda-handler"))\n    )\n\n    fn.add_alias("live")\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'code_sha256', 'description', 'provisioned_concurrent_executions', 'removal_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.VersionOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.VersionProps
class VersionPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    code_sha256: typing.Optional[str] = pydantic.Field(None, description="SHA256 of the version of the Lambda source code. Specify to validate that you're deploying the right version. Default: No validation is performed\n")
    description: typing.Optional[str] = pydantic.Field(None, description='Description of the version. Default: Description of the Lambda\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's version. Default: No provisioned concurrency\n")
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Whether to retain old versions of this function when a new version is created. Default: RemovalPolicy.DESTROY\n')
    lambda_: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Function to get the value of.\n\n:exampleMetadata: infused\n\nExample::\n\n    # fn: lambda.Function\n\n    version = lambda_.Version(self, "MyVersion",\n        lambda_=fn\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'code_sha256', 'description', 'provisioned_concurrent_executions', 'removal_policy', 'lambda_']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.VersionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.VersionPropsDefConfig] = pydantic.Field(None)


class VersionPropsDefConfig(pydantic.BaseModel):
    lambda__config: typing.Optional[models._interface_methods.AwsLambdaIFunctionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.VersionWeight
class VersionWeightDef(BaseStruct):
    version: typing.Union[_REQUIRED_INIT_PARAM, models.aws_lambda.VersionDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The version to route traffic to.\n')
    weight: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='How much weight to assign to this version (0..1).\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    # version: lambda.Version\n\n    version_weight = lambda.VersionWeight(\n        version=version,\n        weight=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version', 'weight']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.VersionWeight'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.VersionWeightDefConfig] = pydantic.Field(None)


class VersionWeightDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_lambda.AdotLambdaExecWrapper
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.ApplicationLogLevel
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.DestinationType
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.FunctionUrlAuthType
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.HttpMethod
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.InvokeMode
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.LogFormat
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.LoggingFormat
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.ParamsAndSecretsLogLevel
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.ParamsAndSecretsVersions
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.RuntimeFamily
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.StartingPosition
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.SystemLogLevel
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.Tracing
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.UntrustedArtifactOnDeployment
# skipping emum

#  autogenerated from aws_cdk.aws_lambda.IAlias
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.ICodeSigningConfig
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IDestination
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IEventSource
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IEventSourceDlq
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IEventSourceMapping
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IFunction
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IFunctionUrl
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.ILayerVersion
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IScalableFunctionAttribute
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.IVersion
#  skipping Interface

#  autogenerated from aws_cdk.aws_lambda.CfnAlias
class CfnAliasDef(BaseCfnResource):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or ARN of the Lambda function. **Name formats** - *Function name* - ``MyFunction`` . - *Function ARN* - ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction`` . - *Partial ARN* - ``123456789012:function:MyFunction`` . The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    function_version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The function version that the alias invokes.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the alias.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the alias.\n')
    provisioned_concurrency_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnAlias_ProvisionedConcurrencyConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Specifies a `provisioned concurrency <https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html>`_ configuration for a function's alias.\n")
    routing_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnAlias_AliasRoutingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The `routing configuration <https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html>`_ of the alias.')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'function_version', 'name', 'description', 'provisioned_concurrency_config', 'routing_config']
    _method_names: typing.ClassVar[list[str]] = ['AliasRoutingConfigurationProperty', 'ProvisionedConcurrencyConfigurationProperty', 'VersionWeightProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnAlias'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnAliasDefConfig] = pydantic.Field(None)


class CfnAliasDefConfig(pydantic.BaseModel):
    AliasRoutingConfigurationProperty: typing.Optional[list[models.aws_lambda.CfnAliasDefAliasroutingconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ProvisionedConcurrencyConfigurationProperty: typing.Optional[list[models.aws_lambda.CfnAliasDefProvisionedconcurrencyconfigurationpropertyParams]] = pydantic.Field(None, description='')
    VersionWeightProperty: typing.Optional[list[models.aws_lambda.CfnAliasDefVersionweightpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnAliasDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnAliasDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnAliasDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnAliasDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnAliasDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnAliasDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnAliasDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnAliasDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnAliasDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnAliasDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnAliasDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnAliasDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnAliasDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnAliasDefAliasroutingconfigurationpropertyParams(pydantic.BaseModel):
    additional_version_weights: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnAlias_VersionWeightPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    ...

class CfnAliasDefProvisionedconcurrencyconfigurationpropertyParams(pydantic.BaseModel):
    provisioned_concurrent_executions: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnAliasDefVersionweightpropertyParams(pydantic.BaseModel):
    function_version: str = pydantic.Field(..., description='')
    function_weight: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnAliasDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnAliasDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnAliasDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnAliasDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnAliasDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnAliasDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnAliasDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnAliasDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnAliasDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnAliasDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnAliasDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnAliasDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnAliasDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnAliasDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnCodeSigningConfig
class CfnCodeSigningConfigDef(BaseCfnResource):
    allowed_publishers: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_lambda.CfnCodeSigningConfig_AllowedPublishersPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='List of allowed publishers.\n')
    code_signing_policies: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnCodeSigningConfig_CodeSigningPoliciesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The code signing policy controls the validation failure action for signature mismatch or expiry.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Code signing configuration description.')
    _init_params: typing.ClassVar[list[str]] = ['allowed_publishers', 'code_signing_policies', 'description']
    _method_names: typing.ClassVar[list[str]] = ['AllowedPublishersProperty', 'CodeSigningPoliciesProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnCodeSigningConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnCodeSigningConfigDefConfig] = pydantic.Field(None)


class CfnCodeSigningConfigDefConfig(pydantic.BaseModel):
    AllowedPublishersProperty: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAllowedpublisherspropertyParams]] = pydantic.Field(None, description='')
    CodeSigningPoliciesProperty: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefCodesigningpoliciespropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnCodeSigningConfigDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnCodeSigningConfigDefAllowedpublisherspropertyParams(pydantic.BaseModel):
    signing_profile_version_arns: typing.Sequence[str] = pydantic.Field(..., description='')
    ...

class CfnCodeSigningConfigDefCodesigningpoliciespropertyParams(pydantic.BaseModel):
    untrusted_artifact_on_deployment: str = pydantic.Field(..., description='')
    ...

class CfnCodeSigningConfigDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnCodeSigningConfigDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCodeSigningConfigDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnCodeSigningConfigDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCodeSigningConfigDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnCodeSigningConfigDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnCodeSigningConfigDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnCodeSigningConfigDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnCodeSigningConfigDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnCodeSigningConfigDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCodeSigningConfigDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnCodeSigningConfigDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnCodeSigningConfigDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCodeSigningConfigDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnEventInvokeConfig
class CfnEventInvokeConfigDef(BaseCfnResource):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Lambda function. *Minimum* : ``1`` *Maximum* : ``64`` *Pattern* : ``([a-zA-Z0-9-_]+)``\n')
    qualifier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The identifier of a version or alias. - *Version* - A version number. - *Alias* - An alias name. - *Latest* - To specify the unpublished version, use ``$LATEST`` .\n')
    destination_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventInvokeConfig_DestinationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A destination for events after they have been sent to a function for processing. **Destinations** - *Function* - The Amazon Resource Name (ARN) of a Lambda function. - *Queue* - The ARN of a standard SQS queue. - *Topic* - The ARN of a standard SNS topic. - *Event Bus* - The ARN of an Amazon EventBridge event bus.\n')
    maximum_event_age_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing.\n')
    maximum_retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error.')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'qualifier', 'destination_config', 'maximum_event_age_in_seconds', 'maximum_retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['DestinationConfigProperty', 'OnFailureProperty', 'OnSuccessProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventInvokeConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnEventInvokeConfigDefConfig] = pydantic.Field(None)


class CfnEventInvokeConfigDefConfig(pydantic.BaseModel):
    DestinationConfigProperty: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefDestinationconfigpropertyParams]] = pydantic.Field(None, description='')
    OnFailureProperty: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefOnfailurepropertyParams]] = pydantic.Field(None, description='')
    OnSuccessProperty: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefOnsuccesspropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnEventInvokeConfigDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnEventInvokeConfigDefDestinationconfigpropertyParams(pydantic.BaseModel):
    on_failure: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventInvokeConfig_OnFailurePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    on_success: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventInvokeConfig_OnSuccessPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnEventInvokeConfigDefOnfailurepropertyParams(pydantic.BaseModel):
    destination: str = pydantic.Field(..., description='')
    ...

class CfnEventInvokeConfigDefOnsuccesspropertyParams(pydantic.BaseModel):
    destination: str = pydantic.Field(..., description='')
    ...

class CfnEventInvokeConfigDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnEventInvokeConfigDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventInvokeConfigDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnEventInvokeConfigDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventInvokeConfigDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnEventInvokeConfigDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnEventInvokeConfigDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnEventInvokeConfigDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnEventInvokeConfigDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnEventInvokeConfigDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventInvokeConfigDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnEventInvokeConfigDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnEventInvokeConfigDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventInvokeConfigDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMapping
class CfnEventSourceMappingDef(BaseCfnResource):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The name or ARN of the Lambda function. **Name formats** - *Function name*  ``MyFunction`` . - *Function ARN*  ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction`` . - *Version or Alias ARN*  ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD`` . - *Partial ARN*  ``123456789012:function:MyFunction`` . The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64 characters in length.\n")
    amazon_managed_kafka_event_source_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_AmazonManagedKafkaEventSourceConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB). - *Amazon Kinesis*  Default 100. Max 10,000. - *Amazon DynamoDB Streams*  Default 100. Max 10,000. - *Amazon Simple Queue Service*  Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10. - *Amazon Managed Streaming for Apache Kafka*  Default 100. Max 10,000. - *Self-managed Apache Kafka*  Default 100. Max 10,000. - *Amazon MQ (ActiveMQ and RabbitMQ)*  Default 100. Max 10,000. - *DocumentDB*  Default 100. Max 10,000.\n')
    bisect_batch_on_function_error: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.\n')
    destination_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_DestinationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.\n')
    document_db_event_source_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_DocumentDBEventSourceConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specific configuration settings for a DocumentDB event source.\n')
    enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When true, the event source mapping is active. When false, Lambda pauses polling and invocation. Default: True\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. - *Amazon Kinesis*  The ARN of the data stream or a stream consumer. - *Amazon DynamoDB Streams*  The ARN of the stream. - *Amazon Simple Queue Service*  The ARN of the queue. - *Amazon Managed Streaming for Apache Kafka*  The ARN of the cluster or the ARN of the VPC connection (for `cross-account event source mappings <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#msk-multi-vpc>`_ ). - *Amazon MQ*  The ARN of the broker. - *Amazon DocumentDB*  The ARN of the DocumentDB change stream.\n')
    filter_criteria: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_FilterCriteriaPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object that defines the filter criteria that determine whether Lambda should process an event. For more information, see `Lambda event filtering <https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventfiltering.html>`_ .\n')
    function_response_types: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='(Streams and SQS) A list of current response type enums applied to the event source mapping. Valid Values: ``ReportBatchItemFailures``\n')
    maximum_batching_window_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function. *Default ( Kinesis , DynamoDB , Amazon SQS event sources)* : 0 *Default ( Amazon MSK , Kafka, Amazon MQ , Amazon DocumentDB event sources)* : 500 ms *Related setting:* For Amazon SQS event sources, when you set ``BatchSize`` to a value greater than 10, you must set ``MaximumBatchingWindowInSeconds`` to at least 1.\n')
    maximum_record_age_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description="(Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records. .. epigraph:: The minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed\n")
    maximum_retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.\n')
    queues: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='(Amazon MQ) The name of the Amazon MQ broker destination queue to consume.\n')
    scaling_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_ScalingConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(Amazon SQS only) The scaling configuration for the event source. For more information, see `Configuring maximum concurrency for Amazon SQS event sources <https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#events-sqs-max-concurrency>`_ .\n')
    self_managed_event_source: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_SelfManagedEventSourcePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The self-managed Apache Kafka cluster for your event source.\n')
    self_managed_kafka_event_source_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_SelfManagedKafkaEventSourceConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specific configuration settings for a self-managed Apache Kafka event source.\n')
    source_access_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_SourceAccessConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.\n')
    starting_position: typing.Optional[str] = pydantic.Field(None, description='The position in a stream from which to start reading. Required for Amazon Kinesis and Amazon DynamoDB. - *LATEST* - Read only new records. - *TRIM_HORIZON* - Process all available records. - *AT_TIMESTAMP* - Specify a time from which to start reading records.\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='With ``StartingPosition`` set to ``AT_TIMESTAMP`` , the time from which to start reading, in Unix time seconds. ``StartingPositionTimestamp`` cannot be in the future.\n')
    topics: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The name of the Kafka topic.\n')
    tumbling_window_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'amazon_managed_kafka_event_source_config', 'batch_size', 'bisect_batch_on_function_error', 'destination_config', 'document_db_event_source_config', 'enabled', 'event_source_arn', 'filter_criteria', 'function_response_types', 'maximum_batching_window_in_seconds', 'maximum_record_age_in_seconds', 'maximum_retry_attempts', 'parallelization_factor', 'queues', 'scaling_config', 'self_managed_event_source', 'self_managed_kafka_event_source_config', 'source_access_configurations', 'starting_position', 'starting_position_timestamp', 'topics', 'tumbling_window_in_seconds']
    _method_names: typing.ClassVar[list[str]] = ['AmazonManagedKafkaEventSourceConfigProperty', 'DestinationConfigProperty', 'DocumentDBEventSourceConfigProperty', 'EndpointsProperty', 'FilterCriteriaProperty', 'FilterProperty', 'OnFailureProperty', 'ScalingConfigProperty', 'SelfManagedEventSourceProperty', 'SelfManagedKafkaEventSourceConfigProperty', 'SourceAccessConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMapping'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnEventSourceMappingDefConfig] = pydantic.Field(None)


class CfnEventSourceMappingDefConfig(pydantic.BaseModel):
    AmazonManagedKafkaEventSourceConfigProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAmazonmanagedkafkaeventsourceconfigpropertyParams]] = pydantic.Field(None, description='')
    DestinationConfigProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefDestinationconfigpropertyParams]] = pydantic.Field(None, description='')
    DocumentDBEventSourceConfigProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefDocumentdbeventsourceconfigpropertyParams]] = pydantic.Field(None, description='')
    EndpointsProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefEndpointspropertyParams]] = pydantic.Field(None, description='')
    FilterCriteriaProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefFiltercriteriapropertyParams]] = pydantic.Field(None, description='')
    FilterProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefFilterpropertyParams]] = pydantic.Field(None, description='')
    OnFailureProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefOnfailurepropertyParams]] = pydantic.Field(None, description='')
    ScalingConfigProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefScalingconfigpropertyParams]] = pydantic.Field(None, description='')
    SelfManagedEventSourceProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefSelfmanagedeventsourcepropertyParams]] = pydantic.Field(None, description='')
    SelfManagedKafkaEventSourceConfigProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefSelfmanagedkafkaeventsourceconfigpropertyParams]] = pydantic.Field(None, description='')
    SourceAccessConfigurationProperty: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefSourceaccessconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnEventSourceMappingDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnEventSourceMappingDefAmazonmanagedkafkaeventsourceconfigpropertyParams(pydantic.BaseModel):
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefDestinationconfigpropertyParams(pydantic.BaseModel):
    on_failure: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_OnFailurePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefDocumentdbeventsourceconfigpropertyParams(pydantic.BaseModel):
    collection_name: typing.Optional[str] = pydantic.Field(None, description='')
    database_name: typing.Optional[str] = pydantic.Field(None, description='')
    full_document: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefEndpointspropertyParams(pydantic.BaseModel):
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefFiltercriteriapropertyParams(pydantic.BaseModel):
    filters: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_FilterPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefFilterpropertyParams(pydantic.BaseModel):
    pattern: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefOnfailurepropertyParams(pydantic.BaseModel):
    destination: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefScalingconfigpropertyParams(pydantic.BaseModel):
    maximum_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefSelfmanagedeventsourcepropertyParams(pydantic.BaseModel):
    endpoints: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_EndpointsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefSelfmanagedkafkaeventsourceconfigpropertyParams(pydantic.BaseModel):
    consumer_group_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefSourceaccessconfigurationpropertyParams(pydantic.BaseModel):
    type: typing.Optional[str] = pydantic.Field(None, description='')
    uri: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEventSourceMappingDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnEventSourceMappingDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventSourceMappingDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnEventSourceMappingDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventSourceMappingDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnEventSourceMappingDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnEventSourceMappingDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnEventSourceMappingDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnEventSourceMappingDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnEventSourceMappingDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventSourceMappingDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnEventSourceMappingDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnEventSourceMappingDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventSourceMappingDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnFunction
class CfnFunctionDef(BaseCfnResource):
    code: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_lambda.CfnFunction_CodePropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The code for the function.\n')
    role: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The Amazon Resource Name (ARN) of the function's execution role.\n")
    architectures: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The instruction set architecture that the function supports. Enter a string array with one of the valid values (arm64 or x86_64). The default value is ``x86_64`` .\n')
    code_signing_config_arn: typing.Optional[str] = pydantic.Field(None, description='To enable code signing for this function, specify the ARN of a code-signing configuration. A code-signing configuration includes a set of signing profiles, which define the trusted publishers for this function.\n')
    dead_letter_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_DeadLetterConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A dead-letter queue configuration that specifies the queue or topic where Lambda sends asynchronous events when they fail processing. For more information, see `Dead-letter queues <https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-dlq>`_ .\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function.\n')
    environment: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_EnvironmentPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Environment variables that are accessible from function code during execution.\n')
    ephemeral_storage: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The size of the function's ``/tmp`` directory in MB. The default value is 512, but it can be any whole number between 512 and 10,240 MB.\n")
    file_system_configs: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_FileSystemConfigPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Connection settings for an Amazon EFS file system. To connect a function to a file system, a mount target must be available in every Availability Zone that your function connects to. If your template contains an `AWS::EFS::MountTarget <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-efs-mounttarget.html>`_ resource, you must also specify a ``DependsOn`` attribute to ensure that the mount target is created or updated before the function. For more information about using the ``DependsOn`` attribute, see `DependsOn Attribute <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-dependson.html>`_ .\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="The name of the Lambda function, up to 64 characters in length. If you don't specify a name, AWS CloudFormation generates one. If you specify a name, you cannot perform updates that require replacement of this resource. You can perform updates that require no or some interruption. If you must replace the resource, specify a new name.\n")
    handler: typing.Optional[str] = pydantic.Field(None, description='The name of the method within your code that Lambda calls to run your function. Handler is required if the deployment package is a .zip file archive. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see `Lambda programming model <https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html>`_ .\n')
    image_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_ImageConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration values that override the container image Dockerfile settings. For more information, see `Container image settings <https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-parms>`_ .\n')
    kms_key_arn: typing.Optional[str] = pydantic.Field(None, description="The ARN of the AWS Key Management Service ( AWS KMS ) customer managed key that's used to encrypt your function's `environment variables <https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html#configuration-envvars-encryption>`_ . When `Lambda SnapStart <https://docs.aws.amazon.com/lambda/latest/dg/snapstart-security.html>`_ is activated, Lambda also uses this key is to encrypt your function's snapshot. If you deploy your function using a container image, Lambda also uses this key to encrypt your function when it's deployed. Note that this is not the same key that's used to protect your container image in the Amazon Elastic Container Registry (Amazon ECR). If you don't provide a customer managed key, Lambda uses a default service key.\n")
    layers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of `function layers <https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html>`_ to add to the function's execution environment. Specify each layer by its ARN, including the version.\n")
    logging_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_LoggingConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The function's Amazon CloudWatch Logs configuration settings.\n")
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of `memory available to the function <https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html#configuration-memory-console>`_ at runtime. Increasing the function memory also increases its CPU allocation. The default value is 128 MB. The value can be any multiple of 1 MB. Note that new AWS accounts have reduced concurrency and memory quotas. AWS raises these quotas automatically based on your usage. You can also request a quota increase.\n')
    package_type: typing.Optional[str] = pydantic.Field(None, description='The type of deployment package. Set to ``Image`` for container image and set ``Zip`` for .zip file archive.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The number of simultaneous executions to reserve for the function.\n')
    runtime: typing.Optional[str] = pydantic.Field(None, description="The identifier of the function's `runtime <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html>`_ . Runtime is required if the deployment package is a .zip file archive. Specifying a runtime results in an error if you're deploying a function using a container image. The following list includes deprecated runtimes. Lambda blocks creating new functions and updating existing functions shortly after each runtime is deprecated. For more information, see `Runtime use after deprecation <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html#runtime-deprecation-levels>`_ . For a list of all currently supported runtimes, see `Supported runtimes <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html#runtimes-supported>`_ .\n")
    runtime_management_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_RuntimeManagementConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. For more information, see `Runtime updates <https://docs.aws.amazon.com/lambda/latest/dg/runtimes-update.html>`_ .\n")
    snap_start: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_SnapStartPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The function's `AWS Lambda SnapStart <https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html>`_ setting.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of `tags <https://docs.aws.amazon.com/lambda/latest/dg/tagging.html>`_ to apply to the function.\n')
    timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time (in seconds) that Lambda allows a function to run before stopping it. The default is 3 seconds. The maximum allowed value is 900 seconds. For more information, see `Lambda execution environment <https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html>`_ .\n')
    tracing_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_TracingConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Set ``Mode`` to ``Active`` to sample and trace a subset of incoming requests with `X-Ray <https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html>`_ .\n')
    vpc_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_VpcConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='For network connectivity to AWS resources in a VPC, specify a list of security groups and subnets in the VPC. When you connect a function to a VPC, it can access resources and the internet only through that VPC. For more information, see `Configuring a Lambda function to access resources in a VPC <https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html>`_ .')
    _init_params: typing.ClassVar[list[str]] = ['code', 'role', 'architectures', 'code_signing_config_arn', 'dead_letter_config', 'description', 'environment', 'ephemeral_storage', 'file_system_configs', 'function_name', 'handler', 'image_config', 'kms_key_arn', 'layers', 'logging_config', 'memory_size', 'package_type', 'reserved_concurrent_executions', 'runtime', 'runtime_management_config', 'snap_start', 'tags', 'timeout', 'tracing_config', 'vpc_config']
    _method_names: typing.ClassVar[list[str]] = ['CodeProperty', 'DeadLetterConfigProperty', 'EnvironmentProperty', 'EphemeralStorageProperty', 'FileSystemConfigProperty', 'ImageConfigProperty', 'LoggingConfigProperty', 'RuntimeManagementConfigProperty', 'SnapStartProperty', 'SnapStartResponseProperty', 'TracingConfigProperty', 'VpcConfigProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnFunctionDefConfig] = pydantic.Field(None)


class CfnFunctionDefConfig(pydantic.BaseModel):
    CodeProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefCodepropertyParams]] = pydantic.Field(None, description='')
    DeadLetterConfigProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefDeadletterconfigpropertyParams]] = pydantic.Field(None, description='')
    EnvironmentProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefEnvironmentpropertyParams]] = pydantic.Field(None, description='')
    EphemeralStorageProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefEphemeralstoragepropertyParams]] = pydantic.Field(None, description='')
    FileSystemConfigProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefFilesystemconfigpropertyParams]] = pydantic.Field(None, description='')
    ImageConfigProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefImageconfigpropertyParams]] = pydantic.Field(None, description='')
    LoggingConfigProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefLoggingconfigpropertyParams]] = pydantic.Field(None, description='')
    RuntimeManagementConfigProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefRuntimemanagementconfigpropertyParams]] = pydantic.Field(None, description='')
    SnapStartProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefSnapstartpropertyParams]] = pydantic.Field(None, description='')
    SnapStartResponseProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefSnapstartresponsepropertyParams]] = pydantic.Field(None, description='')
    TracingConfigProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefTracingconfigpropertyParams]] = pydantic.Field(None, description='')
    VpcConfigProperty: typing.Optional[list[models.aws_lambda.CfnFunctionDefVpcconfigpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnFunctionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnFunctionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnFunctionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnFunctionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnFunctionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnFunctionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnFunctionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnFunctionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnFunctionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnFunctionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnFunctionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnFunctionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnFunctionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    attr_snap_start_response_config: typing.Optional[models._interface_methods.CoreIResolvableDefConfig] = pydantic.Field(None)
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnFunctionDefCodepropertyParams(pydantic.BaseModel):
    image_uri: typing.Optional[str] = pydantic.Field(None, description='')
    s3_bucket: typing.Optional[str] = pydantic.Field(None, description='')
    s3_key: typing.Optional[str] = pydantic.Field(None, description='')
    s3_object_version: typing.Optional[str] = pydantic.Field(None, description='')
    zip_file: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefDeadletterconfigpropertyParams(pydantic.BaseModel):
    target_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefEnvironmentpropertyParams(pydantic.BaseModel):
    variables: typing.Union[models.UnsupportedResource, typing.Mapping[str, str], None] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefEphemeralstoragepropertyParams(pydantic.BaseModel):
    size: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnFunctionDefFilesystemconfigpropertyParams(pydantic.BaseModel):
    arn: str = pydantic.Field(..., description='')
    local_mount_path: str = pydantic.Field(..., description='')
    ...

class CfnFunctionDefImageconfigpropertyParams(pydantic.BaseModel):
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    entry_point: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefLoggingconfigpropertyParams(pydantic.BaseModel):
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='')
    log_format: typing.Optional[str] = pydantic.Field(None, description='')
    log_group: typing.Optional[str] = pydantic.Field(None, description='')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefRuntimemanagementconfigpropertyParams(pydantic.BaseModel):
    update_runtime_on: str = pydantic.Field(..., description='')
    runtime_version_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefSnapstartpropertyParams(pydantic.BaseModel):
    apply_on: str = pydantic.Field(..., description='')
    ...

class CfnFunctionDefSnapstartresponsepropertyParams(pydantic.BaseModel):
    apply_on: typing.Optional[str] = pydantic.Field(None, description='')
    optimization_status: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefTracingconfigpropertyParams(pydantic.BaseModel):
    mode: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefVpcconfigpropertyParams(pydantic.BaseModel):
    ipv6_allowed_for_dual_stack: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    subnet_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnFunctionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnFunctionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnFunctionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnFunctionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnFunctionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnFunctionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnFunctionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnFunctionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnFunctionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnFunctionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnFunctionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnFunctionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnFunctionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnLayerVersion
class CfnLayerVersionDef(BaseCfnResource):
    content: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_lambda.CfnLayerVersion_ContentPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The function layer archive.\n')
    compatible_architectures: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of compatible `instruction set architectures <https://docs.aws.amazon.com/lambda/latest/dg/foundation-arch.html>`_ .\n')
    compatible_runtimes: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of compatible `function runtimes <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html>`_ . Used for filtering with `ListLayers <https://docs.aws.amazon.com/lambda/latest/dg/API_ListLayers.html>`_ and `ListLayerVersions <https://docs.aws.amazon.com/lambda/latest/dg/API_ListLayerVersions.html>`_ .\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the version.\n')
    layer_name: typing.Optional[str] = pydantic.Field(None, description='The name or Amazon Resource Name (ARN) of the layer.\n')
    license_info: typing.Optional[str] = pydantic.Field(None, description="The layer's software license. It can be any of the following:. - An `SPDX license identifier <https://docs.aws.amazon.com/https://spdx.org/licenses/>`_ . For example, ``MIT`` . - The URL of a license hosted on the internet. For example, ``https://opensource.org/licenses/MIT`` . - The full text of the license.")
    _init_params: typing.ClassVar[list[str]] = ['content', 'compatible_architectures', 'compatible_runtimes', 'description', 'layer_name', 'license_info']
    _method_names: typing.ClassVar[list[str]] = ['ContentProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnLayerVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnLayerVersionDefConfig] = pydantic.Field(None)


class CfnLayerVersionDefConfig(pydantic.BaseModel):
    ContentProperty: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefContentpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnLayerVersionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnLayerVersionDefContentpropertyParams(pydantic.BaseModel):
    s3_bucket: str = pydantic.Field(..., description='')
    s3_key: str = pydantic.Field(..., description='')
    s3_object_version: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnLayerVersionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnLayerVersionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLayerVersionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnLayerVersionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLayerVersionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnLayerVersionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnLayerVersionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnLayerVersionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnLayerVersionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnLayerVersionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLayerVersionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnLayerVersionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnLayerVersionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLayerVersionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnLayerVersionPermission
class CfnLayerVersionPermissionDef(BaseCfnResource):
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The API action that grants access to the layer. For example, ``lambda:GetLayerVersion`` .\n')
    layer_version_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or Amazon Resource Name (ARN) of the layer.\n')
    principal: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An account ID, or ``*`` to grant layer usage permission to all accounts in an organization, or all AWS accounts (if ``organizationId`` is not specified). For the last case, make sure that you really do want all AWS accounts to have usage permission to this layer.\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='With the principal set to ``*`` , grant permission to all accounts in the specified organization.')
    _init_params: typing.ClassVar[list[str]] = ['action', 'layer_version_arn', 'principal', 'organization_id']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnLayerVersionPermission'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnLayerVersionPermissionDefConfig] = pydantic.Field(None)


class CfnLayerVersionPermissionDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnLayerVersionPermissionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnLayerVersionPermissionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnLayerVersionPermissionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLayerVersionPermissionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnLayerVersionPermissionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLayerVersionPermissionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnLayerVersionPermissionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnLayerVersionPermissionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnLayerVersionPermissionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnLayerVersionPermissionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnLayerVersionPermissionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLayerVersionPermissionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnLayerVersionPermissionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnLayerVersionPermissionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLayerVersionPermissionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnPermission
class CfnPermissionDef(BaseCfnResource):
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The action that the principal can use on the function. For example, ``lambda:InvokeFunction`` or ``lambda:GetFunction`` .\n')
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or ARN of the Lambda function, version, or alias. **Name formats** - *Function name*  ``my-function`` (name-only), ``my-function:v1`` (with alias). - *Function ARN*  ``arn:aws:lambda:us-west-2:123456789012:function:my-function`` . - *Partial ARN*  ``123456789012:function:my-function`` . You can append a version number or alias to any of the formats. The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    principal: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The AWS service or AWS account that invokes the function. If you specify a service, use ``SourceArn`` or ``SourceAccount`` to limit who can invoke the function through that service.\n')
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='For Alexa Smart Home functions, a token that the invoker must supply.\n')
    function_url_auth_type: typing.Optional[str] = pydantic.Field(None, description='The type of authentication that your function URL uses. Set to ``AWS_IAM`` if you want to restrict access to authenticated users only. Set to ``NONE`` if you want to bypass IAM authentication to create a public endpoint. For more information, see `Security and auth model for Lambda function URLs <https://docs.aws.amazon.com/lambda/latest/dg/urls-auth.html>`_ .\n')
    principal_org_id: typing.Optional[str] = pydantic.Field(None, description='The identifier for your organization in AWS Organizations . Use this to grant permissions to all the AWS accounts under this organization.\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description='For AWS service , the ID of the AWS account that owns the resource. Use this together with ``SourceArn`` to ensure that the specified account owns the resource. It is possible for an Amazon S3 bucket to be deleted by its owner and recreated by another account.\n')
    source_arn: typing.Optional[str] = pydantic.Field(None, description='For AWS services , the ARN of the AWS resource that invokes the function. For example, an Amazon S3 bucket or Amazon SNS topic. Note that Lambda configures the comparison using the ``StringLike`` operator.')
    _init_params: typing.ClassVar[list[str]] = ['action', 'function_name', 'principal', 'event_source_token', 'function_url_auth_type', 'principal_org_id', 'source_account', 'source_arn']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnPermission'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnPermissionDefConfig] = pydantic.Field(None)


class CfnPermissionDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnPermissionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnPermissionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnPermissionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnPermissionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnPermissionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnPermissionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnPermissionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnPermissionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnPermissionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnPermissionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnPermissionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnPermissionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnPermissionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnPermissionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnPermissionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnPermissionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnPermissionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnPermissionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnPermissionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnPermissionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnPermissionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnPermissionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnPermissionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnPermissionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnPermissionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnPermissionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnPermissionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnUrl
class CfnUrlDef(BaseCfnResource):
    auth_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of authentication that your function URL uses. Set to ``AWS_IAM`` if you want to restrict access to authenticated users only. Set to ``NONE`` if you want to bypass IAM authentication to create a public endpoint. For more information, see `Security and auth model for Lambda function URLs <https://docs.aws.amazon.com/lambda/latest/dg/urls-auth.html>`_ .\n')
    target_function_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Lambda function. **Name formats** - *Function name* - ``my-function`` . - *Function ARN* - ``arn:aws:lambda:us-west-2:123456789012:function:my-function`` . - *Partial ARN* - ``123456789012:function:my-function`` . The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    cors: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnUrl_CorsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The `Cross-Origin Resource Sharing (CORS) <https://docs.aws.amazon.com/https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS>`_ settings for your function URL.\n')
    invoke_mode: typing.Optional[str] = pydantic.Field(None, description='Use one of the following options:. - ``BUFFERED``  This is the default option. Lambda invokes your function using the ``Invoke`` API operation. Invocation results are available when the payload is complete. The maximum payload size is 6 MB. - ``RESPONSE_STREAM``  Your function streams payload results as they become available. Lambda invokes your function using the ``InvokeWithResponseStream`` API operation. The maximum response payload size is 20 MB, however, you can `request a quota increase <https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html>`_ .\n')
    qualifier: typing.Optional[str] = pydantic.Field(None, description='The alias name.')
    _init_params: typing.ClassVar[list[str]] = ['auth_type', 'target_function_arn', 'cors', 'invoke_mode', 'qualifier']
    _method_names: typing.ClassVar[list[str]] = ['CorsProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnUrl'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnUrlDefConfig] = pydantic.Field(None)


class CfnUrlDefConfig(pydantic.BaseModel):
    CorsProperty: typing.Optional[list[models.aws_lambda.CfnUrlDefCorspropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnUrlDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnUrlDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnUrlDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnUrlDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnUrlDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnUrlDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnUrlDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnUrlDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnUrlDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnUrlDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnUrlDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnUrlDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnUrlDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnUrlDefCorspropertyParams(pydantic.BaseModel):
    allow_credentials: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    allow_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    allow_methods: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    allow_origins: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    expose_headers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    max_age: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnUrlDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnUrlDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnUrlDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnUrlDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnUrlDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnUrlDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnUrlDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnUrlDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnUrlDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnUrlDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnUrlDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnUrlDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnUrlDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnUrlDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnVersion
class CfnVersionDef(BaseCfnResource):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or ARN of the Lambda function. **Name formats** - *Function name* - ``MyFunction`` . - *Function ARN* - ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction`` . - *Partial ARN* - ``123456789012:function:MyFunction`` . The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    code_sha256: typing.Optional[str] = pydantic.Field(None, description="Only publish a version if the hash value matches the value that's specified. Use this option to avoid publishing a version if the function code has changed since you last updated it. Updates are not supported for this property.\n")
    description: typing.Optional[str] = pydantic.Field(None, description='A description for the version to override the description in the function configuration. Updates are not supported for this property.\n')
    policy: typing.Any = pydantic.Field(None, description='The resource policy of your function.\n')
    provisioned_concurrency_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnVersion_ProvisionedConcurrencyConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's version. Updates are not supported for this property.\n")
    runtime_policy: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnVersion_RuntimePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Runtime Management Config of a function.')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'code_sha256', 'description', 'policy', 'provisioned_concurrency_config', 'runtime_policy']
    _method_names: typing.ClassVar[list[str]] = ['ProvisionedConcurrencyConfigurationProperty', 'RuntimePolicyProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_lambda.CfnVersionDefConfig] = pydantic.Field(None)


class CfnVersionDefConfig(pydantic.BaseModel):
    ProvisionedConcurrencyConfigurationProperty: typing.Optional[list[models.aws_lambda.CfnVersionDefProvisionedconcurrencyconfigurationpropertyParams]] = pydantic.Field(None, description='')
    RuntimePolicyProperty: typing.Optional[list[models.aws_lambda.CfnVersionDefRuntimepolicypropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_lambda.CfnVersionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_lambda.CfnVersionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_lambda.CfnVersionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_lambda.CfnVersionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_lambda.CfnVersionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_lambda.CfnVersionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_lambda.CfnVersionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_lambda.CfnVersionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_lambda.CfnVersionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_lambda.CfnVersionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_lambda.CfnVersionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_lambda.CfnVersionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_lambda.CfnVersionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnVersionDefProvisionedconcurrencyconfigurationpropertyParams(pydantic.BaseModel):
    provisioned_concurrent_executions: typing.Union[int, float] = pydantic.Field(..., description='')
    ...

class CfnVersionDefRuntimepolicypropertyParams(pydantic.BaseModel):
    update_runtime_on: str = pydantic.Field(..., description='')
    runtime_version_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnVersionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnVersionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnVersionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnVersionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnVersionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnVersionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnVersionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnVersionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnVersionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnVersionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnVersionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnVersionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnVersionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnVersionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_lambda.CfnAliasProps
class CfnAliasPropsDef(BaseCfnProperty):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or ARN of the Lambda function. **Name formats** - *Function name* - ``MyFunction`` . - *Function ARN* - ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction`` . - *Partial ARN* - ``123456789012:function:MyFunction`` . The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    function_version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The function version that the alias invokes.\n')
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the alias.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the alias.\n')
    provisioned_concurrency_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnAlias_ProvisionedConcurrencyConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Specifies a `provisioned concurrency <https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html>`_ configuration for a function's alias.\n")
    routing_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnAlias_AliasRoutingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The `routing configuration <https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html>`_ of the alias.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-alias.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_alias_props = lambda.CfnAliasProps(\n        function_name="functionName",\n        function_version="functionVersion",\n        name="name",\n\n        # the properties below are optional\n        description="description",\n        provisioned_concurrency_config=lambda.CfnAlias.ProvisionedConcurrencyConfigurationProperty(\n            provisioned_concurrent_executions=123\n        ),\n        routing_config=lambda.CfnAlias.AliasRoutingConfigurationProperty(\n            additional_version_weights=[lambda.CfnAlias.VersionWeightProperty(\n                function_version="functionVersion",\n                function_weight=123\n            )]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'function_version', 'name', 'description', 'provisioned_concurrency_config', 'routing_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnAliasProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnCodeSigningConfigProps
class CfnCodeSigningConfigPropsDef(BaseCfnProperty):
    allowed_publishers: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_lambda.CfnCodeSigningConfig_AllowedPublishersPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='List of allowed publishers.\n')
    code_signing_policies: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnCodeSigningConfig_CodeSigningPoliciesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The code signing policy controls the validation failure action for signature mismatch or expiry.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Code signing configuration description.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-codesigningconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_code_signing_config_props = lambda.CfnCodeSigningConfigProps(\n        allowed_publishers=lambda.CfnCodeSigningConfig.AllowedPublishersProperty(\n            signing_profile_version_arns=["signingProfileVersionArns"]\n        ),\n\n        # the properties below are optional\n        code_signing_policies=lambda.CfnCodeSigningConfig.CodeSigningPoliciesProperty(\n            untrusted_artifact_on_deployment="untrustedArtifactOnDeployment"\n        ),\n        description="description"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allowed_publishers', 'code_signing_policies', 'description']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnCodeSigningConfigProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventInvokeConfigProps
class CfnEventInvokeConfigPropsDef(BaseCfnProperty):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Lambda function. *Minimum* : ``1`` *Maximum* : ``64`` *Pattern* : ``([a-zA-Z0-9-_]+)``\n')
    qualifier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The identifier of a version or alias. - *Version* - A version number. - *Alias* - An alias name. - *Latest* - To specify the unpublished version, use ``$LATEST`` .\n')
    destination_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventInvokeConfig_DestinationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A destination for events after they have been sent to a function for processing. **Destinations** - *Function* - The Amazon Resource Name (ARN) of a Lambda function. - *Queue* - The ARN of a standard SQS queue. - *Topic* - The ARN of a standard SNS topic. - *Event Bus* - The ARN of an Amazon EventBridge event bus.\n')
    maximum_event_age_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing.\n')
    maximum_retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-eventinvokeconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_event_invoke_config_props = lambda.CfnEventInvokeConfigProps(\n        function_name="functionName",\n        qualifier="qualifier",\n\n        # the properties below are optional\n        destination_config=lambda.CfnEventInvokeConfig.DestinationConfigProperty(\n            on_failure=lambda.CfnEventInvokeConfig.OnFailureProperty(\n                destination="destination"\n            ),\n            on_success=lambda.CfnEventInvokeConfig.OnSuccessProperty(\n                destination="destination"\n            )\n        ),\n        maximum_event_age_in_seconds=123,\n        maximum_retry_attempts=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'qualifier', 'destination_config', 'maximum_event_age_in_seconds', 'maximum_retry_attempts']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventInvokeConfigProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnEventSourceMappingProps
class CfnEventSourceMappingPropsDef(BaseCfnProperty):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The name or ARN of the Lambda function. **Name formats** - *Function name*  ``MyFunction`` . - *Function ARN*  ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction`` . - *Version or Alias ARN*  ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD`` . - *Partial ARN*  ``123456789012:function:MyFunction`` . The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64 characters in length.\n")
    amazon_managed_kafka_event_source_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_AmazonManagedKafkaEventSourceConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB). - *Amazon Kinesis*  Default 100. Max 10,000. - *Amazon DynamoDB Streams*  Default 100. Max 10,000. - *Amazon Simple Queue Service*  Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10. - *Amazon Managed Streaming for Apache Kafka*  Default 100. Max 10,000. - *Self-managed Apache Kafka*  Default 100. Max 10,000. - *Amazon MQ (ActiveMQ and RabbitMQ)*  Default 100. Max 10,000. - *DocumentDB*  Default 100. Max 10,000.\n')
    bisect_batch_on_function_error: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.\n')
    destination_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_DestinationConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.\n')
    document_db_event_source_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_DocumentDBEventSourceConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specific configuration settings for a DocumentDB event source.\n')
    enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When true, the event source mapping is active. When false, Lambda pauses polling and invocation. Default: True\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. - *Amazon Kinesis*  The ARN of the data stream or a stream consumer. - *Amazon DynamoDB Streams*  The ARN of the stream. - *Amazon Simple Queue Service*  The ARN of the queue. - *Amazon Managed Streaming for Apache Kafka*  The ARN of the cluster or the ARN of the VPC connection (for `cross-account event source mappings <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#msk-multi-vpc>`_ ). - *Amazon MQ*  The ARN of the broker. - *Amazon DocumentDB*  The ARN of the DocumentDB change stream.\n')
    filter_criteria: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_FilterCriteriaPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='An object that defines the filter criteria that determine whether Lambda should process an event. For more information, see `Lambda event filtering <https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventfiltering.html>`_ .\n')
    function_response_types: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='(Streams and SQS) A list of current response type enums applied to the event source mapping. Valid Values: ``ReportBatchItemFailures``\n')
    maximum_batching_window_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function. *Default ( Kinesis , DynamoDB , Amazon SQS event sources)* : 0 *Default ( Amazon MSK , Kafka, Amazon MQ , Amazon DocumentDB event sources)* : 500 ms *Related setting:* For Amazon SQS event sources, when you set ``BatchSize`` to a value greater than 10, you must set ``MaximumBatchingWindowInSeconds`` to at least 1.\n')
    maximum_record_age_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description="(Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records. .. epigraph:: The minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed\n")
    maximum_retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.\n')
    queues: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='(Amazon MQ) The name of the Amazon MQ broker destination queue to consume.\n')
    scaling_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_ScalingConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(Amazon SQS only) The scaling configuration for the event source. For more information, see `Configuring maximum concurrency for Amazon SQS event sources <https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#events-sqs-max-concurrency>`_ .\n')
    self_managed_event_source: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_SelfManagedEventSourcePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The self-managed Apache Kafka cluster for your event source.\n')
    self_managed_kafka_event_source_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_SelfManagedKafkaEventSourceConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specific configuration settings for a self-managed Apache Kafka event source.\n')
    source_access_configurations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnEventSourceMapping_SourceAccessConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.\n')
    starting_position: typing.Optional[str] = pydantic.Field(None, description='The position in a stream from which to start reading. Required for Amazon Kinesis and Amazon DynamoDB. - *LATEST* - Read only new records. - *TRIM_HORIZON* - Process all available records. - *AT_TIMESTAMP* - Specify a time from which to start reading records.\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='With ``StartingPosition`` set to ``AT_TIMESTAMP`` , the time from which to start reading, in Unix time seconds. ``StartingPositionTimestamp`` cannot be in the future.\n')
    topics: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The name of the Kafka topic.\n')
    tumbling_window_in_seconds: typing.Union[int, float, None] = pydantic.Field(None, description='(Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-eventsourcemapping.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_event_source_mapping_props = lambda.CfnEventSourceMappingProps(\n        function_name="functionName",\n\n        # the properties below are optional\n        amazon_managed_kafka_event_source_config=lambda.CfnEventSourceMapping.AmazonManagedKafkaEventSourceConfigProperty(\n            consumer_group_id="consumerGroupId"\n        ),\n        batch_size=123,\n        bisect_batch_on_function_error=False,\n        destination_config=lambda.CfnEventSourceMapping.DestinationConfigProperty(\n            on_failure=lambda.CfnEventSourceMapping.OnFailureProperty(\n                destination="destination"\n            )\n        ),\n        document_db_event_source_config=lambda.CfnEventSourceMapping.DocumentDBEventSourceConfigProperty(\n            collection_name="collectionName",\n            database_name="databaseName",\n            full_document="fullDocument"\n        ),\n        enabled=False,\n        event_source_arn="eventSourceArn",\n        filter_criteria=lambda.CfnEventSourceMapping.FilterCriteriaProperty(\n            filters=[lambda.CfnEventSourceMapping.FilterProperty(\n                pattern="pattern"\n            )]\n        ),\n        function_response_types=["functionResponseTypes"],\n        maximum_batching_window_in_seconds=123,\n        maximum_record_age_in_seconds=123,\n        maximum_retry_attempts=123,\n        parallelization_factor=123,\n        queues=["queues"],\n        scaling_config=lambda.CfnEventSourceMapping.ScalingConfigProperty(\n            maximum_concurrency=123\n        ),\n        self_managed_event_source=lambda.CfnEventSourceMapping.SelfManagedEventSourceProperty(\n            endpoints=lambda.CfnEventSourceMapping.EndpointsProperty(\n                kafka_bootstrap_servers=["kafkaBootstrapServers"]\n            )\n        ),\n        self_managed_kafka_event_source_config=lambda.CfnEventSourceMapping.SelfManagedKafkaEventSourceConfigProperty(\n            consumer_group_id="consumerGroupId"\n        ),\n        source_access_configurations=[lambda.CfnEventSourceMapping.SourceAccessConfigurationProperty(\n            type="type",\n            uri="uri"\n        )],\n        starting_position="startingPosition",\n        starting_position_timestamp=123,\n        topics=["topics"],\n        tumbling_window_in_seconds=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'amazon_managed_kafka_event_source_config', 'batch_size', 'bisect_batch_on_function_error', 'destination_config', 'document_db_event_source_config', 'enabled', 'event_source_arn', 'filter_criteria', 'function_response_types', 'maximum_batching_window_in_seconds', 'maximum_record_age_in_seconds', 'maximum_retry_attempts', 'parallelization_factor', 'queues', 'scaling_config', 'self_managed_event_source', 'self_managed_kafka_event_source_config', 'source_access_configurations', 'starting_position', 'starting_position_timestamp', 'topics', 'tumbling_window_in_seconds']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnEventSourceMappingProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnFunctionProps
class CfnFunctionPropsDef(BaseCfnProperty):
    code: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_lambda.CfnFunction_CodePropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The code for the function.\n')
    role: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The Amazon Resource Name (ARN) of the function's execution role.\n")
    architectures: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The instruction set architecture that the function supports. Enter a string array with one of the valid values (arm64 or x86_64). The default value is ``x86_64`` .\n')
    code_signing_config_arn: typing.Optional[str] = pydantic.Field(None, description='To enable code signing for this function, specify the ARN of a code-signing configuration. A code-signing configuration includes a set of signing profiles, which define the trusted publishers for this function.\n')
    dead_letter_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_DeadLetterConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='A dead-letter queue configuration that specifies the queue or topic where Lambda sends asynchronous events when they fail processing. For more information, see `Dead-letter queues <https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-dlq>`_ .\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function.\n')
    environment: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_EnvironmentPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Environment variables that are accessible from function code during execution.\n')
    ephemeral_storage: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_EphemeralStoragePropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The size of the function's ``/tmp`` directory in MB. The default value is 512, but it can be any whole number between 512 and 10,240 MB.\n")
    file_system_configs: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_FileSystemConfigPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Connection settings for an Amazon EFS file system. To connect a function to a file system, a mount target must be available in every Availability Zone that your function connects to. If your template contains an `AWS::EFS::MountTarget <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-efs-mounttarget.html>`_ resource, you must also specify a ``DependsOn`` attribute to ensure that the mount target is created or updated before the function. For more information about using the ``DependsOn`` attribute, see `DependsOn Attribute <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-dependson.html>`_ .\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="The name of the Lambda function, up to 64 characters in length. If you don't specify a name, AWS CloudFormation generates one. If you specify a name, you cannot perform updates that require replacement of this resource. You can perform updates that require no or some interruption. If you must replace the resource, specify a new name.\n")
    handler: typing.Optional[str] = pydantic.Field(None, description='The name of the method within your code that Lambda calls to run your function. Handler is required if the deployment package is a .zip file archive. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see `Lambda programming model <https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html>`_ .\n')
    image_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_ImageConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration values that override the container image Dockerfile settings. For more information, see `Container image settings <https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-parms>`_ .\n')
    kms_key_arn: typing.Optional[str] = pydantic.Field(None, description="The ARN of the AWS Key Management Service ( AWS KMS ) customer managed key that's used to encrypt your function's `environment variables <https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html#configuration-envvars-encryption>`_ . When `Lambda SnapStart <https://docs.aws.amazon.com/lambda/latest/dg/snapstart-security.html>`_ is activated, Lambda also uses this key is to encrypt your function's snapshot. If you deploy your function using a container image, Lambda also uses this key to encrypt your function when it's deployed. Note that this is not the same key that's used to protect your container image in the Amazon Elastic Container Registry (Amazon ECR). If you don't provide a customer managed key, Lambda uses a default service key.\n")
    layers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of `function layers <https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html>`_ to add to the function's execution environment. Specify each layer by its ARN, including the version.\n")
    logging_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_LoggingConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The function's Amazon CloudWatch Logs configuration settings.\n")
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of `memory available to the function <https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html#configuration-memory-console>`_ at runtime. Increasing the function memory also increases its CPU allocation. The default value is 128 MB. The value can be any multiple of 1 MB. Note that new AWS accounts have reduced concurrency and memory quotas. AWS raises these quotas automatically based on your usage. You can also request a quota increase.\n')
    package_type: typing.Optional[str] = pydantic.Field(None, description='The type of deployment package. Set to ``Image`` for container image and set ``Zip`` for .zip file archive.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The number of simultaneous executions to reserve for the function.\n')
    runtime: typing.Optional[str] = pydantic.Field(None, description="The identifier of the function's `runtime <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html>`_ . Runtime is required if the deployment package is a .zip file archive. Specifying a runtime results in an error if you're deploying a function using a container image. The following list includes deprecated runtimes. Lambda blocks creating new functions and updating existing functions shortly after each runtime is deprecated. For more information, see `Runtime use after deprecation <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html#runtime-deprecation-levels>`_ . For a list of all currently supported runtimes, see `Supported runtimes <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html#runtimes-supported>`_ .\n")
    runtime_management_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_RuntimeManagementConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. For more information, see `Runtime updates <https://docs.aws.amazon.com/lambda/latest/dg/runtimes-update.html>`_ .\n")
    snap_start: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_SnapStartPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The function's `AWS Lambda SnapStart <https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html>`_ setting.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of `tags <https://docs.aws.amazon.com/lambda/latest/dg/tagging.html>`_ to apply to the function.\n')
    timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time (in seconds) that Lambda allows a function to run before stopping it. The default is 3 seconds. The maximum allowed value is 900 seconds. For more information, see `Lambda execution environment <https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html>`_ .\n')
    tracing_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_TracingConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Set ``Mode`` to ``Active`` to sample and trace a subset of incoming requests with `X-Ray <https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html>`_ .\n')
    vpc_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnFunction_VpcConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='For network connectivity to AWS resources in a VPC, specify a list of security groups and subnets in the VPC. When you connect a function to a VPC, it can access resources and the internet only through that VPC. For more information, see `Configuring a Lambda function to access resources in a VPC <https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_function_props = lambda.CfnFunctionProps(\n        code=lambda.CfnFunction.CodeProperty(\n            image_uri="imageUri",\n            s3_bucket="s3Bucket",\n            s3_key="s3Key",\n            s3_object_version="s3ObjectVersion",\n            zip_file="zipFile"\n        ),\n        role="role",\n\n        # the properties below are optional\n        architectures=["architectures"],\n        code_signing_config_arn="codeSigningConfigArn",\n        dead_letter_config=lambda.CfnFunction.DeadLetterConfigProperty(\n            target_arn="targetArn"\n        ),\n        description="description",\n        environment=lambda.CfnFunction.EnvironmentProperty(\n            variables={\n                "variables_key": "variables"\n            }\n        ),\n        ephemeral_storage=lambda.CfnFunction.EphemeralStorageProperty(\n            size=123\n        ),\n        file_system_configs=[lambda.CfnFunction.FileSystemConfigProperty(\n            arn="arn",\n            local_mount_path="localMountPath"\n        )],\n        function_name="functionName",\n        handler="handler",\n        image_config=lambda.CfnFunction.ImageConfigProperty(\n            command=["command"],\n            entry_point=["entryPoint"],\n            working_directory="workingDirectory"\n        ),\n        kms_key_arn="kmsKeyArn",\n        layers=["layers"],\n        logging_config=lambda.CfnFunction.LoggingConfigProperty(\n            application_log_level="applicationLogLevel",\n            log_format="logFormat",\n            log_group="logGroup",\n            system_log_level="systemLogLevel"\n        ),\n        memory_size=123,\n        package_type="packageType",\n        reserved_concurrent_executions=123,\n        runtime="runtime",\n        runtime_management_config=lambda.CfnFunction.RuntimeManagementConfigProperty(\n            update_runtime_on="updateRuntimeOn",\n\n            # the properties below are optional\n            runtime_version_arn="runtimeVersionArn"\n        ),\n        snap_start=lambda.CfnFunction.SnapStartProperty(\n            apply_on="applyOn"\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        timeout=123,\n        tracing_config=lambda.CfnFunction.TracingConfigProperty(\n            mode="mode"\n        ),\n        vpc_config=lambda.CfnFunction.VpcConfigProperty(\n            ipv6_allowed_for_dual_stack=False,\n            security_group_ids=["securityGroupIds"],\n            subnet_ids=["subnetIds"]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['code', 'role', 'architectures', 'code_signing_config_arn', 'dead_letter_config', 'description', 'environment', 'ephemeral_storage', 'file_system_configs', 'function_name', 'handler', 'image_config', 'kms_key_arn', 'layers', 'logging_config', 'memory_size', 'package_type', 'reserved_concurrent_executions', 'runtime', 'runtime_management_config', 'snap_start', 'tags', 'timeout', 'tracing_config', 'vpc_config']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnFunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnLayerVersionPermissionProps
class CfnLayerVersionPermissionPropsDef(BaseCfnProperty):
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The API action that grants access to the layer. For example, ``lambda:GetLayerVersion`` .\n')
    layer_version_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or Amazon Resource Name (ARN) of the layer.\n')
    principal: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An account ID, or ``*`` to grant layer usage permission to all accounts in an organization, or all AWS accounts (if ``organizationId`` is not specified). For the last case, make sure that you really do want all AWS accounts to have usage permission to this layer.\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='With the principal set to ``*`` , grant permission to all accounts in the specified organization.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-layerversionpermission.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_layer_version_permission_props = lambda.CfnLayerVersionPermissionProps(\n        action="action",\n        layer_version_arn="layerVersionArn",\n        principal="principal",\n\n        # the properties below are optional\n        organization_id="organizationId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'layer_version_arn', 'principal', 'organization_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnLayerVersionPermissionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnLayerVersionProps
class CfnLayerVersionPropsDef(BaseCfnProperty):
    content: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, models.aws_lambda.CfnLayerVersion_ContentPropertyDef, dict[str, typing.Any]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The function layer archive.\n')
    compatible_architectures: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of compatible `instruction set architectures <https://docs.aws.amazon.com/lambda/latest/dg/foundation-arch.html>`_ .\n')
    compatible_runtimes: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of compatible `function runtimes <https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html>`_ . Used for filtering with `ListLayers <https://docs.aws.amazon.com/lambda/latest/dg/API_ListLayers.html>`_ and `ListLayerVersions <https://docs.aws.amazon.com/lambda/latest/dg/API_ListLayerVersions.html>`_ .\n')
    description: typing.Optional[str] = pydantic.Field(None, description='The description of the version.\n')
    layer_name: typing.Optional[str] = pydantic.Field(None, description='The name or Amazon Resource Name (ARN) of the layer.\n')
    license_info: typing.Optional[str] = pydantic.Field(None, description='The layer\'s software license. It can be any of the following:. - An `SPDX license identifier <https://docs.aws.amazon.com/https://spdx.org/licenses/>`_ . For example, ``MIT`` . - The URL of a license hosted on the internet. For example, ``https://opensource.org/licenses/MIT`` . - The full text of the license.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-layerversion.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_layer_version_props = lambda.CfnLayerVersionProps(\n        content=lambda.CfnLayerVersion.ContentProperty(\n            s3_bucket="s3Bucket",\n            s3_key="s3Key",\n\n            # the properties below are optional\n            s3_object_version="s3ObjectVersion"\n        ),\n\n        # the properties below are optional\n        compatible_architectures=["compatibleArchitectures"],\n        compatible_runtimes=["compatibleRuntimes"],\n        description="description",\n        layer_name="layerName",\n        license_info="licenseInfo"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['content', 'compatible_architectures', 'compatible_runtimes', 'description', 'layer_name', 'license_info']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnLayerVersionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnParametersCodeProps
class CfnParametersCodePropsDef(BaseCfnProperty):
    bucket_name_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the name of the S3 Bucket where the Lambda code will be located in. Must be of type 'String'. Default: a new parameter will be created\n")
    object_key_param: typing.Optional[models.CfnParameterDef] = pydantic.Field(None, description="The CloudFormation parameter that represents the path inside the S3 Bucket where the Lambda code will be located at. Must be of type 'String'. Default: a new parameter will be created\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_lambda as lambda_\n\n    # cfn_parameter: cdk.CfnParameter\n\n    cfn_parameters_code_props = lambda.CfnParametersCodeProps(\n        bucket_name_param=cfn_parameter,\n        object_key_param=cfn_parameter\n    )\n")
    _init_params: typing.ClassVar[list[str]] = ['bucket_name_param', 'object_key_param']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnParametersCodeProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnPermissionProps
class CfnPermissionPropsDef(BaseCfnProperty):
    action: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The action that the principal can use on the function. For example, ``lambda:InvokeFunction`` or ``lambda:GetFunction`` .\n')
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or ARN of the Lambda function, version, or alias. **Name formats** - *Function name*  ``my-function`` (name-only), ``my-function:v1`` (with alias). - *Function ARN*  ``arn:aws:lambda:us-west-2:123456789012:function:my-function`` . - *Partial ARN*  ``123456789012:function:my-function`` . You can append a version number or alias to any of the formats. The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    principal: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The AWS service or AWS account that invokes the function. If you specify a service, use ``SourceArn`` or ``SourceAccount`` to limit who can invoke the function through that service.\n')
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='For Alexa Smart Home functions, a token that the invoker must supply.\n')
    function_url_auth_type: typing.Optional[str] = pydantic.Field(None, description='The type of authentication that your function URL uses. Set to ``AWS_IAM`` if you want to restrict access to authenticated users only. Set to ``NONE`` if you want to bypass IAM authentication to create a public endpoint. For more information, see `Security and auth model for Lambda function URLs <https://docs.aws.amazon.com/lambda/latest/dg/urls-auth.html>`_ .\n')
    principal_org_id: typing.Optional[str] = pydantic.Field(None, description='The identifier for your organization in AWS Organizations . Use this to grant permissions to all the AWS accounts under this organization.\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description='For AWS service , the ID of the AWS account that owns the resource. Use this together with ``SourceArn`` to ensure that the specified account owns the resource. It is possible for an Amazon S3 bucket to be deleted by its owner and recreated by another account.\n')
    source_arn: typing.Optional[str] = pydantic.Field(None, description='For AWS services , the ARN of the AWS resource that invokes the function. For example, an Amazon S3 bucket or Amazon SNS topic. Note that Lambda configures the comparison using the ``StringLike`` operator.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-permission.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_permission_props = lambda.CfnPermissionProps(\n        action="action",\n        function_name="functionName",\n        principal="principal",\n\n        # the properties below are optional\n        event_source_token="eventSourceToken",\n        function_url_auth_type="functionUrlAuthType",\n        principal_org_id="principalOrgId",\n        source_account="sourceAccount",\n        source_arn="sourceArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['action', 'function_name', 'principal', 'event_source_token', 'function_url_auth_type', 'principal_org_id', 'source_account', 'source_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnPermissionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnUrlProps
class CfnUrlPropsDef(BaseCfnProperty):
    auth_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of authentication that your function URL uses. Set to ``AWS_IAM`` if you want to restrict access to authenticated users only. Set to ``NONE`` if you want to bypass IAM authentication to create a public endpoint. For more information, see `Security and auth model for Lambda function URLs <https://docs.aws.amazon.com/lambda/latest/dg/urls-auth.html>`_ .\n')
    target_function_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the Lambda function. **Name formats** - *Function name* - ``my-function`` . - *Function ARN* - ``arn:aws:lambda:us-west-2:123456789012:function:my-function`` . - *Partial ARN* - ``123456789012:function:my-function`` . The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    cors: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnUrl_CorsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The `Cross-Origin Resource Sharing (CORS) <https://docs.aws.amazon.com/https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS>`_ settings for your function URL.\n')
    invoke_mode: typing.Optional[str] = pydantic.Field(None, description='Use one of the following options:. - ``BUFFERED``  This is the default option. Lambda invokes your function using the ``Invoke`` API operation. Invocation results are available when the payload is complete. The maximum payload size is 6 MB. - ``RESPONSE_STREAM``  Your function streams payload results as they become available. Lambda invokes your function using the ``InvokeWithResponseStream`` API operation. The maximum response payload size is 20 MB, however, you can `request a quota increase <https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html>`_ .\n')
    qualifier: typing.Optional[str] = pydantic.Field(None, description='The alias name.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-url.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    cfn_url_props = lambda.CfnUrlProps(\n        auth_type="authType",\n        target_function_arn="targetFunctionArn",\n\n        # the properties below are optional\n        cors=lambda.CfnUrl.CorsProperty(\n            allow_credentials=False,\n            allow_headers=["allowHeaders"],\n            allow_methods=["allowMethods"],\n            allow_origins=["allowOrigins"],\n            expose_headers=["exposeHeaders"],\n            max_age=123\n        ),\n        invoke_mode="invokeMode",\n        qualifier="qualifier"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth_type', 'target_function_arn', 'cors', 'invoke_mode', 'qualifier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnUrlProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda.CfnVersionProps
class CfnVersionPropsDef(BaseCfnProperty):
    function_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name or ARN of the Lambda function. **Name formats** - *Function name* - ``MyFunction`` . - *Function ARN* - ``arn:aws:lambda:us-west-2:123456789012:function:MyFunction`` . - *Partial ARN* - ``123456789012:function:MyFunction`` . The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n')
    code_sha256: typing.Optional[str] = pydantic.Field(None, description="Only publish a version if the hash value matches the value that's specified. Use this option to avoid publishing a version if the function code has changed since you last updated it. Updates are not supported for this property.\n")
    description: typing.Optional[str] = pydantic.Field(None, description='A description for the version to override the description in the function configuration. Updates are not supported for this property.\n')
    policy: typing.Any = pydantic.Field(None, description='The resource policy of your function.\n')
    provisioned_concurrency_config: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnVersion_ProvisionedConcurrencyConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's version. Updates are not supported for this property.\n")
    runtime_policy: typing.Union[models.UnsupportedResource, models.aws_lambda.CfnVersion_RuntimePolicyPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Runtime Management Config of a function.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-version.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_lambda as lambda_\n\n    # policy: Any\n\n    cfn_version_props = lambda.CfnVersionProps(\n        function_name="functionName",\n\n        # the properties below are optional\n        code_sha256="codeSha256",\n        description="description",\n        policy=policy,\n        provisioned_concurrency_config=lambda.CfnVersion.ProvisionedConcurrencyConfigurationProperty(\n            provisioned_concurrent_executions=123\n        ),\n        runtime_policy=lambda.CfnVersion.RuntimePolicyProperty(\n            update_runtime_on="updateRuntimeOn",\n\n            # the properties below are optional\n            runtime_version_arn="runtimeVersionArn"\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['function_name', 'code_sha256', 'description', 'policy', 'provisioned_concurrency_config', 'runtime_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda.CfnVersionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    AdotLambdaLayerGenericVersion: typing.Optional[dict[str, models.aws_lambda.AdotLambdaLayerGenericVersionDef]] = pydantic.Field(None)
    AdotLambdaLayerJavaAutoInstrumentationVersion: typing.Optional[dict[str, models.aws_lambda.AdotLambdaLayerJavaAutoInstrumentationVersionDef]] = pydantic.Field(None)
    AdotLambdaLayerJavaScriptSdkVersion: typing.Optional[dict[str, models.aws_lambda.AdotLambdaLayerJavaScriptSdkVersionDef]] = pydantic.Field(None)
    AdotLambdaLayerJavaSdkVersion: typing.Optional[dict[str, models.aws_lambda.AdotLambdaLayerJavaSdkVersionDef]] = pydantic.Field(None)
    AdotLambdaLayerPythonSdkVersion: typing.Optional[dict[str, models.aws_lambda.AdotLambdaLayerPythonSdkVersionDef]] = pydantic.Field(None)
    AdotLayerVersion: typing.Optional[dict[str, models.aws_lambda.AdotLayerVersionDef]] = pydantic.Field(None)
    Architecture: typing.Optional[dict[str, models.aws_lambda.ArchitectureDef]] = pydantic.Field(None)
    AssetCode: typing.Optional[dict[str, models.aws_lambda.AssetCodeDef]] = pydantic.Field(None)
    AssetImageCode: typing.Optional[dict[str, models.aws_lambda.AssetImageCodeDef]] = pydantic.Field(None)
    CfnParametersCode: typing.Optional[dict[str, models.aws_lambda.CfnParametersCodeDef]] = pydantic.Field(None)
    Code: typing.Optional[dict[str, models.aws_lambda.CodeDef]] = pydantic.Field(None)
    DockerImageCode: typing.Optional[dict[str, models.aws_lambda.DockerImageCodeDef]] = pydantic.Field(None)
    EcrImageCode: typing.Optional[dict[str, models.aws_lambda.EcrImageCodeDef]] = pydantic.Field(None)
    FileSystem: typing.Optional[dict[str, models.aws_lambda.FileSystemDef]] = pydantic.Field(None)
    FilterCriteria: typing.Optional[dict[str, models.aws_lambda.FilterCriteriaDef]] = pydantic.Field(None)
    FilterRule: typing.Optional[dict[str, models.aws_lambda.FilterRuleDef]] = pydantic.Field(None)
    FunctionBase: typing.Optional[dict[str, models.aws_lambda.FunctionBaseDef]] = pydantic.Field(None)
    FunctionVersionUpgrade: typing.Optional[dict[str, models.aws_lambda.FunctionVersionUpgradeDef]] = pydantic.Field(None)
    Handler: typing.Optional[dict[str, models.aws_lambda.HandlerDef]] = pydantic.Field(None)
    InlineCode: typing.Optional[dict[str, models.aws_lambda.InlineCodeDef]] = pydantic.Field(None)
    LambdaInsightsVersion: typing.Optional[dict[str, models.aws_lambda.LambdaInsightsVersionDef]] = pydantic.Field(None)
    ParamsAndSecretsLayerVersion: typing.Optional[dict[str, models.aws_lambda.ParamsAndSecretsLayerVersionDef]] = pydantic.Field(None)
    QualifiedFunctionBase: typing.Optional[dict[str, models.aws_lambda.QualifiedFunctionBaseDef]] = pydantic.Field(None)
    Runtime: typing.Optional[dict[str, models.aws_lambda.RuntimeDef]] = pydantic.Field(None)
    RuntimeManagementMode: typing.Optional[dict[str, models.aws_lambda.RuntimeManagementModeDef]] = pydantic.Field(None)
    S3Code: typing.Optional[dict[str, models.aws_lambda.S3CodeDef]] = pydantic.Field(None)
    SnapStartConf: typing.Optional[dict[str, models.aws_lambda.SnapStartConfDef]] = pydantic.Field(None)
    SourceAccessConfigurationType: typing.Optional[dict[str, models.aws_lambda.SourceAccessConfigurationTypeDef]] = pydantic.Field(None)
    Alias: typing.Optional[dict[str, models.aws_lambda.AliasDef]] = pydantic.Field(None)
    CodeSigningConfig: typing.Optional[dict[str, models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None)
    DockerImageFunction: typing.Optional[dict[str, models.aws_lambda.DockerImageFunctionDef]] = pydantic.Field(None)
    EventInvokeConfig: typing.Optional[dict[str, models.aws_lambda.EventInvokeConfigDef]] = pydantic.Field(None)
    EventSourceMapping: typing.Optional[dict[str, models.aws_lambda.EventSourceMappingDef]] = pydantic.Field(None)
    Function: typing.Optional[dict[str, models.aws_lambda.FunctionDef]] = pydantic.Field(None)
    FunctionUrl: typing.Optional[dict[str, models.aws_lambda.FunctionUrlDef]] = pydantic.Field(None)
    LayerVersion: typing.Optional[dict[str, models.aws_lambda.LayerVersionDef]] = pydantic.Field(None)
    SingletonFunction: typing.Optional[dict[str, models.aws_lambda.SingletonFunctionDef]] = pydantic.Field(None)
    Version: typing.Optional[dict[str, models.aws_lambda.VersionDef]] = pydantic.Field(None)
    AdotInstrumentationConfig: typing.Optional[dict[str, models.aws_lambda.AdotInstrumentationConfigDef]] = pydantic.Field(None)
    AliasAttributes: typing.Optional[dict[str, models.aws_lambda.AliasAttributesDef]] = pydantic.Field(None)
    AliasOptions: typing.Optional[dict[str, models.aws_lambda.AliasOptionsDef]] = pydantic.Field(None)
    AliasProps: typing.Optional[dict[str, models.aws_lambda.AliasPropsDef]] = pydantic.Field(None)
    AssetImageCodeProps: typing.Optional[dict[str, models.aws_lambda.AssetImageCodePropsDef]] = pydantic.Field(None)
    AutoScalingOptions: typing.Optional[dict[str, models.aws_lambda.AutoScalingOptionsDef]] = pydantic.Field(None)
    CfnAlias_AliasRoutingConfigurationProperty: typing.Optional[dict[str, models.aws_lambda.CfnAlias_AliasRoutingConfigurationPropertyDef]] = pydantic.Field(None)
    CfnAlias_ProvisionedConcurrencyConfigurationProperty: typing.Optional[dict[str, models.aws_lambda.CfnAlias_ProvisionedConcurrencyConfigurationPropertyDef]] = pydantic.Field(None)
    CfnAlias_VersionWeightProperty: typing.Optional[dict[str, models.aws_lambda.CfnAlias_VersionWeightPropertyDef]] = pydantic.Field(None)
    CfnCodeSigningConfig_AllowedPublishersProperty: typing.Optional[dict[str, models.aws_lambda.CfnCodeSigningConfig_AllowedPublishersPropertyDef]] = pydantic.Field(None)
    CfnCodeSigningConfig_CodeSigningPoliciesProperty: typing.Optional[dict[str, models.aws_lambda.CfnCodeSigningConfig_CodeSigningPoliciesPropertyDef]] = pydantic.Field(None)
    CfnEventInvokeConfig_DestinationConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventInvokeConfig_DestinationConfigPropertyDef]] = pydantic.Field(None)
    CfnEventInvokeConfig_OnFailureProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventInvokeConfig_OnFailurePropertyDef]] = pydantic.Field(None)
    CfnEventInvokeConfig_OnSuccessProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventInvokeConfig_OnSuccessPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_AmazonManagedKafkaEventSourceConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_AmazonManagedKafkaEventSourceConfigPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_DestinationConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_DestinationConfigPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_DocumentDBEventSourceConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_DocumentDBEventSourceConfigPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_EndpointsProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_EndpointsPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_FilterCriteriaProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_FilterCriteriaPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_FilterProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_FilterPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_OnFailureProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_OnFailurePropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_ScalingConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_ScalingConfigPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_SelfManagedEventSourceProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_SelfManagedEventSourcePropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_SelfManagedKafkaEventSourceConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_SelfManagedKafkaEventSourceConfigPropertyDef]] = pydantic.Field(None)
    CfnEventSourceMapping_SourceAccessConfigurationProperty: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMapping_SourceAccessConfigurationPropertyDef]] = pydantic.Field(None)
    CfnFunction_CodeProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_CodePropertyDef]] = pydantic.Field(None)
    CfnFunction_DeadLetterConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_DeadLetterConfigPropertyDef]] = pydantic.Field(None)
    CfnFunction_EnvironmentProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_EnvironmentPropertyDef]] = pydantic.Field(None)
    CfnFunction_EphemeralStorageProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_EphemeralStoragePropertyDef]] = pydantic.Field(None)
    CfnFunction_FileSystemConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_FileSystemConfigPropertyDef]] = pydantic.Field(None)
    CfnFunction_ImageConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_ImageConfigPropertyDef]] = pydantic.Field(None)
    CfnFunction_LoggingConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_LoggingConfigPropertyDef]] = pydantic.Field(None)
    CfnFunction_RuntimeManagementConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_RuntimeManagementConfigPropertyDef]] = pydantic.Field(None)
    CfnFunction_SnapStartProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_SnapStartPropertyDef]] = pydantic.Field(None)
    CfnFunction_SnapStartResponseProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_SnapStartResponsePropertyDef]] = pydantic.Field(None)
    CfnFunction_TracingConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_TracingConfigPropertyDef]] = pydantic.Field(None)
    CfnFunction_VpcConfigProperty: typing.Optional[dict[str, models.aws_lambda.CfnFunction_VpcConfigPropertyDef]] = pydantic.Field(None)
    CfnLayerVersion_ContentProperty: typing.Optional[dict[str, models.aws_lambda.CfnLayerVersion_ContentPropertyDef]] = pydantic.Field(None)
    CfnUrl_CorsProperty: typing.Optional[dict[str, models.aws_lambda.CfnUrl_CorsPropertyDef]] = pydantic.Field(None)
    CfnVersion_ProvisionedConcurrencyConfigurationProperty: typing.Optional[dict[str, models.aws_lambda.CfnVersion_ProvisionedConcurrencyConfigurationPropertyDef]] = pydantic.Field(None)
    CfnVersion_RuntimePolicyProperty: typing.Optional[dict[str, models.aws_lambda.CfnVersion_RuntimePolicyPropertyDef]] = pydantic.Field(None)
    CodeConfig: typing.Optional[dict[str, models.aws_lambda.CodeConfigDef]] = pydantic.Field(None)
    CodeImageConfig: typing.Optional[dict[str, models.aws_lambda.CodeImageConfigDef]] = pydantic.Field(None)
    CodeSigningConfigProps: typing.Optional[dict[str, models.aws_lambda.CodeSigningConfigPropsDef]] = pydantic.Field(None)
    CustomCommandOptions: typing.Optional[dict[str, models.aws_lambda.CustomCommandOptionsDef]] = pydantic.Field(None)
    DestinationConfig: typing.Optional[dict[str, models.aws_lambda.DestinationConfigDef]] = pydantic.Field(None)
    DestinationOptions: typing.Optional[dict[str, models.aws_lambda.DestinationOptionsDef]] = pydantic.Field(None)
    DlqDestinationConfig: typing.Optional[dict[str, models.aws_lambda.DlqDestinationConfigDef]] = pydantic.Field(None)
    DockerBuildAssetOptions: typing.Optional[dict[str, models.aws_lambda.DockerBuildAssetOptionsDef]] = pydantic.Field(None)
    DockerImageFunctionProps: typing.Optional[dict[str, models.aws_lambda.DockerImageFunctionPropsDef]] = pydantic.Field(None)
    EcrImageCodeProps: typing.Optional[dict[str, models.aws_lambda.EcrImageCodePropsDef]] = pydantic.Field(None)
    EnvironmentOptions: typing.Optional[dict[str, models.aws_lambda.EnvironmentOptionsDef]] = pydantic.Field(None)
    EventInvokeConfigOptions: typing.Optional[dict[str, models.aws_lambda.EventInvokeConfigOptionsDef]] = pydantic.Field(None)
    EventInvokeConfigProps: typing.Optional[dict[str, models.aws_lambda.EventInvokeConfigPropsDef]] = pydantic.Field(None)
    EventSourceMappingOptions: typing.Optional[dict[str, models.aws_lambda.EventSourceMappingOptionsDef]] = pydantic.Field(None)
    EventSourceMappingProps: typing.Optional[dict[str, models.aws_lambda.EventSourceMappingPropsDef]] = pydantic.Field(None)
    FileSystemConfig: typing.Optional[dict[str, models.aws_lambda.FileSystemConfigDef]] = pydantic.Field(None)
    FunctionAttributes: typing.Optional[dict[str, models.aws_lambda.FunctionAttributesDef]] = pydantic.Field(None)
    FunctionOptions: typing.Optional[dict[str, models.aws_lambda.FunctionOptionsDef]] = pydantic.Field(None)
    FunctionProps: typing.Optional[dict[str, models.aws_lambda.FunctionPropsDef]] = pydantic.Field(None)
    FunctionUrlCorsOptions: typing.Optional[dict[str, models.aws_lambda.FunctionUrlCorsOptionsDef]] = pydantic.Field(None)
    FunctionUrlOptions: typing.Optional[dict[str, models.aws_lambda.FunctionUrlOptionsDef]] = pydantic.Field(None)
    FunctionUrlProps: typing.Optional[dict[str, models.aws_lambda.FunctionUrlPropsDef]] = pydantic.Field(None)
    LambdaRuntimeProps: typing.Optional[dict[str, models.aws_lambda.LambdaRuntimePropsDef]] = pydantic.Field(None)
    LayerVersionAttributes: typing.Optional[dict[str, models.aws_lambda.LayerVersionAttributesDef]] = pydantic.Field(None)
    LayerVersionOptions: typing.Optional[dict[str, models.aws_lambda.LayerVersionOptionsDef]] = pydantic.Field(None)
    LayerVersionPermission: typing.Optional[dict[str, models.aws_lambda.LayerVersionPermissionDef]] = pydantic.Field(None)
    LayerVersionProps: typing.Optional[dict[str, models.aws_lambda.LayerVersionPropsDef]] = pydantic.Field(None)
    LogRetentionRetryOptions: typing.Optional[dict[str, models.aws_lambda.LogRetentionRetryOptionsDef]] = pydantic.Field(None)
    ParamsAndSecretsOptions: typing.Optional[dict[str, models.aws_lambda.ParamsAndSecretsOptionsDef]] = pydantic.Field(None)
    Permission: typing.Optional[dict[str, models.aws_lambda.PermissionDef]] = pydantic.Field(None)
    ResourceBindOptions: typing.Optional[dict[str, models.aws_lambda.ResourceBindOptionsDef]] = pydantic.Field(None)
    SingletonFunctionProps: typing.Optional[dict[str, models.aws_lambda.SingletonFunctionPropsDef]] = pydantic.Field(None)
    SourceAccessConfiguration: typing.Optional[dict[str, models.aws_lambda.SourceAccessConfigurationDef]] = pydantic.Field(None)
    UtilizationScalingOptions: typing.Optional[dict[str, models.aws_lambda.UtilizationScalingOptionsDef]] = pydantic.Field(None)
    VersionAttributes: typing.Optional[dict[str, models.aws_lambda.VersionAttributesDef]] = pydantic.Field(None)
    VersionOptions: typing.Optional[dict[str, models.aws_lambda.VersionOptionsDef]] = pydantic.Field(None)
    VersionProps: typing.Optional[dict[str, models.aws_lambda.VersionPropsDef]] = pydantic.Field(None)
    VersionWeight: typing.Optional[dict[str, models.aws_lambda.VersionWeightDef]] = pydantic.Field(None)
    CfnAlias: typing.Optional[dict[str, models.aws_lambda.CfnAliasDef]] = pydantic.Field(None)
    CfnCodeSigningConfig: typing.Optional[dict[str, models.aws_lambda.CfnCodeSigningConfigDef]] = pydantic.Field(None)
    CfnEventInvokeConfig: typing.Optional[dict[str, models.aws_lambda.CfnEventInvokeConfigDef]] = pydantic.Field(None)
    CfnEventSourceMapping: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMappingDef]] = pydantic.Field(None)
    CfnFunction: typing.Optional[dict[str, models.aws_lambda.CfnFunctionDef]] = pydantic.Field(None)
    CfnLayerVersion: typing.Optional[dict[str, models.aws_lambda.CfnLayerVersionDef]] = pydantic.Field(None)
    CfnLayerVersionPermission: typing.Optional[dict[str, models.aws_lambda.CfnLayerVersionPermissionDef]] = pydantic.Field(None)
    CfnPermission: typing.Optional[dict[str, models.aws_lambda.CfnPermissionDef]] = pydantic.Field(None)
    CfnUrl: typing.Optional[dict[str, models.aws_lambda.CfnUrlDef]] = pydantic.Field(None)
    CfnVersion: typing.Optional[dict[str, models.aws_lambda.CfnVersionDef]] = pydantic.Field(None)
    CfnAliasProps: typing.Optional[dict[str, models.aws_lambda.CfnAliasPropsDef]] = pydantic.Field(None)
    CfnCodeSigningConfigProps: typing.Optional[dict[str, models.aws_lambda.CfnCodeSigningConfigPropsDef]] = pydantic.Field(None)
    CfnEventInvokeConfigProps: typing.Optional[dict[str, models.aws_lambda.CfnEventInvokeConfigPropsDef]] = pydantic.Field(None)
    CfnEventSourceMappingProps: typing.Optional[dict[str, models.aws_lambda.CfnEventSourceMappingPropsDef]] = pydantic.Field(None)
    CfnFunctionProps: typing.Optional[dict[str, models.aws_lambda.CfnFunctionPropsDef]] = pydantic.Field(None)
    CfnLayerVersionPermissionProps: typing.Optional[dict[str, models.aws_lambda.CfnLayerVersionPermissionPropsDef]] = pydantic.Field(None)
    CfnLayerVersionProps: typing.Optional[dict[str, models.aws_lambda.CfnLayerVersionPropsDef]] = pydantic.Field(None)
    CfnParametersCodeProps: typing.Optional[dict[str, models.aws_lambda.CfnParametersCodePropsDef]] = pydantic.Field(None)
    CfnPermissionProps: typing.Optional[dict[str, models.aws_lambda.CfnPermissionPropsDef]] = pydantic.Field(None)
    CfnUrlProps: typing.Optional[dict[str, models.aws_lambda.CfnUrlPropsDef]] = pydantic.Field(None)
    CfnVersionProps: typing.Optional[dict[str, models.aws_lambda.CfnVersionPropsDef]] = pydantic.Field(None)
    ...

import models
