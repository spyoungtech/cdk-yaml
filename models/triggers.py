from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.triggers.Trigger
class TriggerDef(BaseConstruct):
    handler: typing.Union[models.aws_lambda.FunctionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The AWS Lambda function of the handler to execute.\n')
    invocation_type: typing.Optional[aws_cdk.triggers.InvocationType] = pydantic.Field(None, description='The invocation type to invoke the Lambda function with. Default: RequestResponse\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout of the invocation call of the Lambda function to be triggered. Default: Duration.minutes(2)\n')
    execute_after: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds trigger dependencies. Execute this trigger only after these construct scopes have been provisioned. You can also use ``trigger.executeAfter()`` to add additional dependencies. Default: []\n')
    execute_before: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs. This means that this trigger will get executed *before* the given construct(s). You can also use ``trigger.executeBefore()`` to add additional dependants. Default: []\n')
    execute_on_handler_change: typing.Optional[bool] = pydantic.Field(None, description='Re-executes the trigger every time the handler changes. This implies that the trigger is associated with the ``currentVersion`` of the handler, which gets recreated every time the handler or its configuration is updated. Default: true')
    _init_params: typing.ClassVar[list[str]] = ['handler', 'invocation_type', 'timeout', 'execute_after', 'execute_before', 'execute_on_handler_change']
    _method_names: typing.ClassVar[list[str]] = ['execute_after', 'execute_before']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.triggers.Trigger'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.triggers.TriggerDefConfig] = pydantic.Field(None)


class TriggerDefConfig(pydantic.BaseModel):
    execute_after: typing.Optional[list[models.triggers.TriggerDefExecuteAfterParams]] = pydantic.Field(None, description='Adds trigger dependencies.\nExecute this trigger only after these construct\nscopes have been provisioned.')
    execute_before: typing.Optional[list[models.triggers.TriggerDefExecuteBeforeParams]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs.\nThis means that this\ntrigger will get executed *before* the given construct(s).')

class TriggerDefExecuteAfterParams(pydantic.BaseModel):
    scopes: list[models.constructs.ConstructDef] = pydantic.Field(...)
    ...

class TriggerDefExecuteBeforeParams(pydantic.BaseModel):
    scopes: list[models.constructs.ConstructDef] = pydantic.Field(...)
    ...


#  autogenerated from aws_cdk.triggers.TriggerFunction
class TriggerFunctionDef(BaseConstruct):
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n')
    execute_after: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds trigger dependencies. Execute this trigger only after these construct scopes have been provisioned. You can also use ``trigger.executeAfter()`` to add additional dependencies. Default: []\n')
    execute_before: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs. This means that this trigger will get executed *before* the given construct(s). You can also use ``trigger.executeBefore()`` to add additional dependants. Default: []\n')
    execute_on_handler_change: typing.Optional[bool] = pydantic.Field(None, description='Re-executes the trigger every time the handler changes. This implies that the trigger is associated with the ``currentVersion`` of the handler, which gets recreated every time the handler or its configuration is updated. Default: true\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functionâ€™s /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['code', 'handler', 'runtime', 'execute_after', 'execute_before', 'execute_on_handler_change', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_alias', 'add_environment', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_layers', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'execute_after', 'execute_before', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_latest_version', 'grant_invoke_url', 'grant_invoke_version', 'invalidate_version_based_on', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = ['classify_version_property', 'from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.triggers.TriggerFunction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    ...


    from_function_arn: typing.Optional[models.triggers.TriggerFunctionDefFromFunctionArnParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its ARN.\nFor ``Function.addPermissions()`` to work on this imported lambda, make sure that is\nin the same account and region as the stack you are importing it into.')
    from_function_attributes: typing.Optional[models.triggers.TriggerFunctionDefFromFunctionAttributesParams] = pydantic.Field(None, description='Creates a Lambda function object which represents a function not defined within this stack.\nFor ``Function.addPermissions()`` to work on this imported lambda, set the sameEnvironment property to true\nif this imported lambda is in the same account and region as the stack you are importing it into.')
    from_function_name: typing.Optional[models.triggers.TriggerFunctionDefFromFunctionNameParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its name.')
    metric_all: typing.Optional[models.triggers.TriggerFunctionDefMetricAllParams] = pydantic.Field(None, description='Return the given named metric for this Lambda.')
    metric_all_concurrent_executions: typing.Optional[models.triggers.TriggerFunctionDefMetricAllConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of concurrent executions across all Lambdas.')
    metric_all_duration: typing.Optional[models.triggers.TriggerFunctionDefMetricAllDurationParams] = pydantic.Field(None, description='Metric for the Duration executing all Lambdas.')
    metric_all_errors: typing.Optional[models.triggers.TriggerFunctionDefMetricAllErrorsParams] = pydantic.Field(None, description='Metric for the number of Errors executing all Lambdas.')
    metric_all_invocations: typing.Optional[models.triggers.TriggerFunctionDefMetricAllInvocationsParams] = pydantic.Field(None, description='Metric for the number of invocations of all Lambdas.')
    metric_all_throttles: typing.Optional[models.triggers.TriggerFunctionDefMetricAllThrottlesParams] = pydantic.Field(None, description='Metric for the number of throttled invocations of all Lambdas.')
    metric_all_unreserved_concurrent_executions: typing.Optional[models.triggers.TriggerFunctionDefMetricAllUnreservedConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of unreserved concurrent executions across all Lambdas.')
    resource_config: typing.Optional[models.triggers.TriggerFunctionDefConfig] = pydantic.Field(None)


class TriggerFunctionDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[models.triggers.TriggerFunctionDefAddAliasParams]] = pydantic.Field(None, description='Defines an alias for this function.\nThe alias will automatically be updated to point to the latest version of\nthe function as it is being updated during a deployment::\n\n   # fn: lambda.Function\n\n\n   fn.add_alias("Live")\n\n   # Is equivalent to\n\n   lambda_.Alias(self, "AliasLive",\n       alias_name="Live",\n       version=fn.current_version\n   )')
    add_environment: typing.Optional[list[models.triggers.TriggerFunctionDefAddEnvironmentParams]] = pydantic.Field(None, description='Adds an environment variable to this Lambda function.\nIf this is a ref to a Lambda function, this operation results in a no-op.')
    add_event_source: typing.Optional[list[models.triggers.TriggerFunctionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.triggers.TriggerFunctionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.triggers.TriggerFunctionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_layers: typing.Optional[list[models.triggers.TriggerFunctionDefAddLayersParams]] = pydantic.Field(None, description='Adds one or more Lambda Layers to this Lambda function.')
    add_permission: typing.Optional[list[models.triggers.TriggerFunctionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.triggers.TriggerFunctionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    classify_version_property: typing.Optional[list[models.triggers.TriggerFunctionDefClassifyVersionPropertyParams]] = pydantic.Field(None, description="Record whether specific properties in the ``AWS::Lambda::Function`` resource should also be associated to the Version resource.\nSee 'currentVersion' section in the module README for more details.")
    configure_async_invoke: typing.Optional[list[models.triggers.TriggerFunctionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[list[models.triggers.TriggerFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams]] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    execute_after: typing.Optional[list[models.triggers.TriggerFunctionDefExecuteAfterParams]] = pydantic.Field(None, description='Adds trigger dependencies.\nExecute this trigger only after these construct\nscopes have been provisioned.')
    execute_before: typing.Optional[list[models.triggers.TriggerFunctionDefExecuteBeforeParams]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs.\nThis means that this\ntrigger will get executed *before* the given construct(s).')
    grant_invoke: typing.Optional[list[models.triggers.TriggerFunctionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.triggers.TriggerFunctionDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_latest_version: typing.Optional[list[models.triggers.TriggerFunctionDefGrantInvokeLatestVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the $LATEST version or unqualified version of this Lambda.')
    grant_invoke_url: typing.Optional[list[models.triggers.TriggerFunctionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    grant_invoke_version: typing.Optional[list[models.triggers.TriggerFunctionDefGrantInvokeVersionParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke the given version of this Lambda.')
    invalidate_version_based_on: typing.Optional[list[models.triggers.TriggerFunctionDefInvalidateVersionBasedOnParams]] = pydantic.Field(None, description='Mix additional information into the hash of the Version object.\nThe Lambda Function construct does its best to automatically create a new\nVersion when anything about the Function changes (its code, its layers,\nany of the other properties).\n\nHowever, you can sometimes source information from places that the CDK cannot\nlook into, like the deploy-time values of SSM parameters. In those cases,\nthe CDK would not force the creation of a new Version object when it actually\nshould.\n\nThis method can be used to invalidate the current Version object. Pass in\nany string into this method, and make sure the string changes when you know\na new Version needs to be created.\n\nThis method may be called more than once.')
    metric: typing.Optional[list[models.triggers.TriggerFunctionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.triggers.TriggerFunctionDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.triggers.TriggerFunctionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.triggers.TriggerFunctionDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.triggers.TriggerFunctionDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    current_version_config: typing.Optional[models.aws_lambda.VersionDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)
    trigger_config: typing.Optional[models.triggers.TriggerDefConfig] = pydantic.Field(None)

class TriggerFunctionDefAddAliasParams(pydantic.BaseModel):
    alias_name: str = pydantic.Field(..., description='The name of the alias.\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    return_config: typing.Optional[list[models.aws_lambda.AliasDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefAddEnvironmentParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='The environment variable key.\n')
    value: str = pydantic.Field(..., description="The environment variable's value.\n")
    remove_in_edge: typing.Optional[bool] = pydantic.Field(None, description='When used in Lambda@Edge via edgeArn() API, these environment variables will be removed. If not set, an error will be thrown. Default: false - using the function in Lambda@Edge will throw')
    return_config: typing.Optional[list[models.aws_lambda.FunctionDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.S3OnFailureDestinationDef, models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    support_s3_on_failure_destination: typing.Optional[bool] = pydantic.Field(None, description='Check if support S3 onfailure destination(ODF). Currently only MSK and self managed kafka event support S3 ODF Default: false\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefAddLayersParams(pydantic.BaseModel):
    layers: list[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]] = pydantic.Field(...)
    ...

class TriggerFunctionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class TriggerFunctionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefClassifyVersionPropertyParams(pydantic.BaseModel):
    property_name: str = pydantic.Field(..., description='The property to classify.\n')
    locked: bool = pydantic.Field(..., description='whether the property should be associated to the version or not.')
    ...

class TriggerFunctionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class TriggerFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    action: str = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefExecuteAfterParams(pydantic.BaseModel):
    scopes: list[models.constructs.ConstructDef] = pydantic.Field(...)
    ...

class TriggerFunctionDefExecuteBeforeParams(pydantic.BaseModel):
    scopes: list[models.constructs.ConstructDef] = pydantic.Field(...)
    ...

class TriggerFunctionDefFromFunctionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_arn: str = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefFromFunctionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The parent construct.\n')
    id: str = pydantic.Field(..., description='The name of the lambda construct.\n')
    function_arn: str = pydantic.Field(..., description='The ARN of the Lambda function. Format: arn::lambda:::function:\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The architecture of this Lambda Function (this is an optional attribute and defaults to X86_64). Default: - Architecture.X86_64\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM execution role associated with this function. If the role is not specified, any role-related operations will no-op.\n')
    same_environment: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function is in the same environment as the stack. This affects certain behaviours such as, whether this function's permission can be modified. When not configured, the CDK attempts to auto-determine this. For environment agnostic stacks, i.e., stacks where the account is not specified with the ``env`` property, this is determined to be false. Set this to property *ONLY IF* the imported function is in the same account as the stack it's imported in. Default: - depends: true, if the Stack is configured with an explicit ``env`` (account and region) and the account is the same as this function. For environment-agnostic stacks this will default to ``false``.\n")
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group of this Lambda, if in a VPC. This needs to be given in order to support allowing connections to this Lambda.\n')
    skip_permissions: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function ALREADY HAS the necessary permissions for what you are trying to do. When not configured, the CDK attempts to auto-determine whether or not additional permissions are necessary on the function when grant APIs are used. If the CDK tried to add permissions on an imported lambda, it will fail. Set this property *ONLY IF* you are committing to manage the imported function's permissions outside of CDK. You are acknowledging that your CDK code alone will have insufficient permissions to access the imported function. Default: false")
    ...

class TriggerFunctionDefFromFunctionNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_name: str = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefGrantInvokeLatestVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefGrantInvokeVersionParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    version: typing.Union[models.aws_lambda.VersionDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefInvalidateVersionBasedOnParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class TriggerFunctionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefMetricAllParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    ...

class TriggerFunctionDefMetricAllConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class TriggerFunctionDefMetricAllDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    ...

class TriggerFunctionDefMetricAllErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class TriggerFunctionDefMetricAllInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class TriggerFunctionDefMetricAllThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class TriggerFunctionDefMetricAllUnreservedConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class TriggerFunctionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class TriggerFunctionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.triggers.TriggerFunctionProps
class TriggerFunctionPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Do not specify this property if the ``securityGroups`` or ``securityGroup`` property is set. Instead, configure ``allowAllOutbound`` directly on the security group. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    application_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the application log level for the function. Default: "INFO"\n')
    application_log_level_v2: typing.Optional[aws_cdk.aws_lambda.ApplicationLogLevel] = pydantic.Field(None, description='Sets the application log level for the function. Default: ApplicationLogLevel.INFO\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functionâ€™s /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.S3EventSourceV2Def, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    ipv6_allowed_for_dual_stack: typing.Optional[bool] = pydantic.Field(None, description="Allows outbound IPv6 traffic on VPC functions that are connected to dual-stack subnets. Only used if 'vpc' is supplied. Default: false\n")
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_format: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the logFormat for the function. Default: "Text"\n')
    logging_format: typing.Optional[aws_cdk.aws_lambda.LoggingFormat] = pydantic.Field(None, description='Sets the loggingFormat for the function. Default: LoggingFormat.TEXT\n')
    log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='The log group the function sends logs to. By default, Lambda functions send logs to an automatically created default log group named /aws/lambda/<function name>. However you cannot change the properties of this auto-created log group using the AWS CDK, e.g. you cannot set a different log retention. Use the ``logGroup`` property to create a fully customizable LogGroup ahead of time, and instruct the Lambda function to send logs to it. Providing a user-controlled log group was rolled out to commercial regions on 2023-11-16. If you are deploying to another type of region, please check regional availability first. Default: ``/aws/lambda/${this.functionName}`` - default log group created by Lambda\n')
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. This is a legacy API and we strongly recommend you move away from it if you can. Instead create a fully customizable log group with ``logs.LogGroup`` and use the ``logGroup`` property to instruct the Lambda function to send logs to it. Migrating from ``logRetention`` to ``logGroup`` will cause the name of the log group to change. Users and code and referencing the name verbatim will have to adjust. In AWS CDK code, you can access the log group name directly from the LogGroup construct:: import * as logs from 'aws-cdk-lib/aws-logs'; declare const myLogGroup: logs.LogGroup; myLogGroup.logGroupName; Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. This is a legacy API and we strongly recommend you migrate to ``logGroup`` if you can. ``logGroup`` allows you to create a fully customizable log group and instruct the Lambda function to send logs to it. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    system_log_level: typing.Optional[str] = pydantic.Field(None, description='(deprecated) Sets the system log level for the function. Default: "INFO"\n')
    system_log_level_v2: typing.Optional[aws_cdk.aws_lambda.SystemLogLevel] = pydantic.Field(None, description='Sets the system log level for the function. Default: SystemLogLevel.INFO\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    code: typing.Union[models.aws_lambda.CodeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The source code of your Lambda function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text.\n')
    handler: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the method within your code that Lambda calls to execute your function. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see https://docs.aws.amazon.com/lambda/latest/dg/foundation-progmodel.html. Use ``Handler.FROM_IMAGE`` when defining a function from a Docker image. NOTE: If you specify your source code as inline text by specifying the ZipFile property within the Code property, specify index.function_name as the handler.\n')
    runtime: typing.Union[models.aws_lambda.RuntimeDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The runtime environment for the Lambda function that you are uploading. For valid values, see the Runtime property in the AWS Lambda Developer Guide. Use ``Runtime.FROM_IMAGE`` when defining a function from a Docker image.\n')
    execute_after: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds trigger dependencies. Execute this trigger only after these construct scopes have been provisioned. You can also use ``trigger.executeAfter()`` to add additional dependencies. Default: []\n')
    execute_before: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs. This means that this trigger will get executed *before* the given construct(s). You can also use ``trigger.executeBefore()`` to add additional dependants. Default: []\n')
    execute_on_handler_change: typing.Optional[bool] = pydantic.Field(None, description='Re-executes the trigger every time the handler changes. This implies that the trigger is associated with the ``currentVersion`` of the handler, which gets recreated every time the handler or its configuration is updated. Default: true\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.triggers as triggers\n\n\n    triggers.TriggerFunction(self, "MyTrigger",\n        runtime=lambda_.Runtime.NODEJS_18_X,\n        handler="index.handler",\n        code=lambda_.Code.from_asset(__dirname + "/my-trigger")\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'application_log_level', 'application_log_level_v2', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'ipv6_allowed_for_dual_stack', 'layers', 'log_format', 'logging_format', 'log_group', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'system_log_level', 'system_log_level_v2', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'code', 'handler', 'runtime', 'execute_after', 'execute_before', 'execute_on_handler_change']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.triggers.TriggerFunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.triggers.TriggerFunctionPropsDefConfig] = pydantic.Field(None)


class TriggerFunctionPropsDefConfig(pydantic.BaseModel):
    code_config: typing.Optional[models.aws_lambda.CodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.triggers.TriggerOptions
class TriggerOptionsDef(BaseStruct):
    execute_after: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds trigger dependencies. Execute this trigger only after these construct scopes have been provisioned. You can also use ``trigger.executeAfter()`` to add additional dependencies. Default: []\n')
    execute_before: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs. This means that this trigger will get executed *before* the given construct(s). You can also use ``trigger.executeBefore()`` to add additional dependants. Default: []\n')
    execute_on_handler_change: typing.Optional[bool] = pydantic.Field(None, description='Re-executes the trigger every time the handler changes. This implies that the trigger is associated with the ``currentVersion`` of the handler, which gets recreated every time the handler or its configuration is updated. Default: true\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import triggers\n    import constructs as constructs\n\n    # construct: constructs.Construct\n\n    trigger_options = triggers.TriggerOptions(\n        execute_after=[construct],\n        execute_before=[construct],\n        execute_on_handler_change=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execute_after', 'execute_before', 'execute_on_handler_change']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.triggers.TriggerOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.triggers.TriggerProps
class TriggerPropsDef(BaseStruct):
    execute_after: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds trigger dependencies. Execute this trigger only after these construct scopes have been provisioned. You can also use ``trigger.executeAfter()`` to add additional dependencies. Default: []\n')
    execute_before: typing.Optional[typing.Sequence[models.constructs.ConstructDef]] = pydantic.Field(None, description='Adds this trigger as a dependency on other constructs. This means that this trigger will get executed *before* the given construct(s). You can also use ``trigger.executeBefore()`` to add additional dependants. Default: []\n')
    execute_on_handler_change: typing.Optional[bool] = pydantic.Field(None, description='Re-executes the trigger every time the handler changes. This implies that the trigger is associated with the ``currentVersion`` of the handler, which gets recreated every time the handler or its configuration is updated. Default: true\n')
    handler: typing.Union[models.aws_lambda.FunctionDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The AWS Lambda function of the handler to execute.\n')
    invocation_type: typing.Optional[aws_cdk.triggers.InvocationType] = pydantic.Field(None, description='The invocation type to invoke the Lambda function with. Default: RequestResponse\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The timeout of the invocation call of the Lambda function to be triggered. Default: Duration.minutes(2)\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.triggers as triggers\n\n\n    func = lambda_.Function(self, "MyFunction",\n        handler="index.handler",\n        runtime=lambda_.Runtime.NODEJS_18_X,\n        code=lambda_.Code.from_inline("foo")\n    )\n\n    triggers.Trigger(self, "MyTrigger",\n        handler=func,\n        timeout=Duration.minutes(10),\n        invocation_type=triggers.InvocationType.EVENT\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['execute_after', 'execute_before', 'execute_on_handler_change', 'handler', 'invocation_type', 'timeout']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.triggers.TriggerProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.triggers.TriggerPropsDefConfig] = pydantic.Field(None)


class TriggerPropsDefConfig(pydantic.BaseModel):
    handler_config: typing.Optional[models.aws_lambda.FunctionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.triggers.InvocationType
# skipping emum

#  autogenerated from aws_cdk.triggers.TriggerInvalidation
# skipping emum

#  autogenerated from aws_cdk.triggers.ITrigger
#  skipping Interface

class ModuleModel(pydantic.BaseModel):
    Trigger: typing.Optional[dict[str, models.triggers.TriggerDef]] = pydantic.Field(None)
    TriggerFunction: typing.Optional[dict[str, models.triggers.TriggerFunctionDef]] = pydantic.Field(None)
    TriggerFunctionProps: typing.Optional[dict[str, models.triggers.TriggerFunctionPropsDef]] = pydantic.Field(None)
    TriggerOptions: typing.Optional[dict[str, models.triggers.TriggerOptionsDef]] = pydantic.Field(None)
    TriggerProps: typing.Optional[dict[str, models.triggers.TriggerPropsDef]] = pydantic.Field(None)
    ...

import models
